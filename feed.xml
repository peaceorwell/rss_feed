<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub Commits Feed</title>
    <link>https://github.com/username/repo/commits</link>
    <description>Recent commits from GitHub repo</description>
    <title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><item><title>Remove cuda dependencies when building AOTriton (#122982)</title><link>https://github.com/pytorch/pytorch/commit/76a87e33a0fdd0639842218ce26943e7b4f3838b</link><description><![CDATA[<p>Remove cuda dependencies when building AOTriton (#122982)</p>
<p>Downloading CUDA sometimes fails and breaks the build process, but AOTriton does not need these packages for its own Triton fork. This commit comments out the related downloading scripts.</p>
<p>The actual changes from Triton can be found at: https://github.com/ROCm/triton/commit/9b73a543a5545960bcaf2830900b0560eec443c5</p>
<p>Fixes the following building error<br />
<code>[2/6] cd /var/lib/jenkins/workspace/build/aotriton/src/third_party/triton/python &amp;&amp; /opt/conda/envs/py_3.8/bin/cmake -E env VIRTUAL_ENV=/var/lib/jenkins/workspace/build/aotriton/build/venv PATH="/var/lib/jenkins/workspace/build/aotriton/build/venv/bin:/opt/cache/bin:/opt/rocm/llvm/bin:/opt/rocm/opencl/bin:/opt/rocm/hip/bin:/opt/rocm/hcc/bin:/opt/rocm/bin:/opt/conda/envs/py_3.8/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" TRITON_BUILD_DIR=/var/lib/jenkins/workspace/build/aotriton/build/triton_build python setup.py develop
FAILED: CMakeFiles/aotriton_venv_triton /var/lib/jenkins/.local/lib/python3.8/site-packages/triton/_C/libtriton.so /var/lib/jenkins/workspace/build/aotriton/build/CMakeFiles/aotriton_venv_triton
cd /var/lib/jenkins/workspace/build/aotriton/src/third_party/triton/python &amp;&amp; /opt/conda/envs/py_3.8/bin/cmake -E env VIRTUAL_ENV=/var/lib/jenkins/workspace/build/aotriton/build/venv PATH="/var/lib/jenkins/workspace/build/aotriton/build/venv/bin:/opt/cache/bin:/opt/rocm/llvm/bin:/opt/rocm/opencl/bin:/opt/rocm/hip/bin:/opt/rocm/hcc/bin:/opt/rocm/bin:/opt/conda/envs/py_3.8/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" TRITON_BUILD_DIR=/var/lib/jenkins/workspace/build/aotriton/build/triton_build python setup.py develop
downloading and extracting https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-nvcc-12.1.105-0.tar.bz2 ...
downloading and extracting https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-cuobjdump-12.1.111-0.tar.bz2 ...
Traceback (most recent call last):
  File "/var/lib/jenkins/workspace/build/aotriton/src/third_party/triton/python/setup.py", line 325, in &lt;module&gt;
    download_and_copy(
  File "/var/lib/jenkins/workspace/build/aotriton/src/third_party/triton/python/setup.py", line 151, in download_and_copy
    ftpstream = urllib.request.urlopen(url)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 524:
ninja: build stopped: subcommand failed.</code></p>
<p>Example of failed build log: https://github.com/pytorch/pytorch/actions/runs/8483953034/job/23245996425<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122982<br />
Approved by: https://github.com/jansel</p>]]></description><pubDate>Mon, 01 Apr 2024 09:50:35 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/76a87e33a0fdd0639842218ce26943e7b4f3838b</guid></item><item><title>[inductor] Lower divide by constant as multiplication by reciprocal (#121924)</title><link>https://github.com/pytorch/pytorch/commit/03439d4c1c39d6957d09b6024946c92849b30ffc</link><description><![CDATA[<p>[inductor] Lower divide by constant as multiplication by reciprocal (#121924)</p>
<p>Fixes #101039</p>
<p>This lowers division by a constant value to be multipication by reciprocal.<br />
The same optimization is applied in eager mode on CUDA:</p>
<p>https://github.com/pytorch/pytorch/blob/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu#L36-L38</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/121924<br />
Approved by: https://github.com/lezcano</p>]]></description><pubDate>Mon, 01 Apr 2024 06:37:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/03439d4c1c39d6957d09b6024946c92849b30ffc</guid></item><item><title>[CI] Updated expected result files after https://github.com/pytorch/pytorch/pull/122846 (#123035)</title><link>https://github.com/pytorch/pytorch/commit/fa6178d246d0832fb90838f808b10c55aa631a70</link><description>&lt;p&gt;[CI] Updated expected result files after https://github.com/pytorch/pytorch/pull/122846 (#123035)&lt;/p&gt;
&lt;p&gt;Summary: Before https://github.com/pytorch/pytorch/pull/122846, pyhpc_isoneutral_mixing in AOTI inference run segfaults so its result was not logged in the expected result file. Now it does show as fail_to_run instead of None.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/123035&lt;br /&gt;
Approved by: https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Sun, 31 Mar 2024 05:56:00 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fa6178d246d0832fb90838f808b10c55aa631a70</guid></item><item><title>[inductor] make mask_rcnn inference work in max-autotune mode (#123008)</title><link>https://github.com/pytorch/pytorch/commit/ec58f1f74ebcec744d2ab90ad34abd09c1018e92</link><description>&lt;p&gt;[inductor] make mask_rcnn inference work in max-autotune mode (#123008)&lt;/p&gt;
&lt;p&gt;inference for vision_maskrcnn model fail when max-autotune is enabled.&lt;/p&gt;
&lt;p&gt;Repro:&lt;br /&gt;
&lt;code&gt;TORCHINDUCTOR_MAX_AUTOTUNE=1 time python benchmarks/dynamo/torchbench.py --accuracy --inference --bfloat16 --backend inductor --only vision_maskrcnn&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;It turns out that MA code receives empty input tensor for convolution and some places in MA related code does not handle this corner case properly. This PR enhance that and now the accuracy test above can pass.&lt;/p&gt;
&lt;p&gt;Regarding why the input tensor is empty, I think it's probably due to no objects are detected in the input images (random data?).&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/123008&lt;br /&gt;
Approved by: https://github.com/jansel&lt;/p&gt;</description><pubDate>Sat, 30 Mar 2024 08:39:57 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ec58f1f74ebcec744d2ab90ad34abd09c1018e92</guid></item><item><title>Revert "[aoti] clear precomputed symbol replacements before cpp wrapper compilation (#122882)"</title><link>https://github.com/pytorch/pytorch/commit/a236fa9f061097107520fcc4fdca5f731b16a04a</link><description>&lt;p&gt;Revert "[aoti] clear precomputed symbol replacements before cpp wrapper compilation (#122882)"&lt;/p&gt;
&lt;p&gt;This reverts commit 384de46395234e793a319325e5c9d20a60407a64.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122882 on behalf of https://github.com/jithunnair-amd due to broke ROCm CI (&lt;a href="https://github.com/pytorch/pytorch/pull/122882#issuecomment-2027544640"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 29 Mar 2024 09:52:39 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a236fa9f061097107520fcc4fdca5f731b16a04a</guid></item><item><title>Add inductor fx pass unit test for shape propagation (#122897)</title><link>https://github.com/pytorch/pytorch/commit/315bd951e4401dedf4c1bda24d7b8caf38fdbdb2</link><description>&lt;p&gt;Add inductor fx pass unit test for shape propagation (#122897)&lt;/p&gt;
&lt;p&gt;Summary: Pre-grad fx passes expect information from shape propagation to be present. D55221119 ensured that &lt;code&gt;pass_execution_and_save&lt;/code&gt; invokes shape propagation, and this diff adds a covering unit test to prevent regression.&lt;/p&gt;
&lt;p&gt;Test Plan: New UT passes locally.&lt;/p&gt;
&lt;p&gt;Differential Revision: D55440240&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122897&lt;br /&gt;
Approved by: https://github.com/khabinov, https://github.com/Skylion007&lt;/p&gt;</description><pubDate>Fri, 29 Mar 2024 08:44:22 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/315bd951e4401dedf4c1bda24d7b8caf38fdbdb2</guid></item><item><title>[AOTI][refactor] Improve logging (#122932)</title><link>https://github.com/pytorch/pytorch/commit/375a8041ed53a5028516389b1bbfd917b678959e</link><description>&lt;p&gt;[AOTI][refactor] Improve logging (#122932)&lt;/p&gt;
&lt;p&gt;Summary: Improve some logging msgs, and change a data type to remove a compile time warning.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122932&lt;br /&gt;
Approved by: https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Fri, 29 Mar 2024 06:02:23 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/375a8041ed53a5028516389b1bbfd917b678959e</guid></item><item><title>Add wrapper for fbgemm quantization operations (#122763)</title><link>https://github.com/pytorch/pytorch/commit/966ae943dfcd3dfcfac7bcf2267a730b4ca4606b</link><description>&lt;p&gt;Add wrapper for fbgemm quantization operations (#122763)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
We add wrappers for fbgemm's packing so we can pass it through PT2 to&lt;br /&gt;
lowering phase of AOTInductor.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
Included in commit.&lt;br /&gt;
test_quantized_ops::test_wrapped_fbgemm_linear_fp16&lt;/p&gt;
&lt;p&gt;Reviewers:&lt;/p&gt;
&lt;p&gt;Subscribers:&lt;/p&gt;
&lt;p&gt;Tasks:&lt;/p&gt;
&lt;p&gt;Tags:&lt;/p&gt;
&lt;p&gt;Differential Revision: &lt;a href="https://our.internmc.facebook.com/intern/diff/D55433204"&gt;D55433204&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122763&lt;br /&gt;
Approved by: https://github.com/jerryzh168&lt;br /&gt;
ghstack dependencies: #122762&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 10:41:18 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/966ae943dfcd3dfcfac7bcf2267a730b4ca4606b</guid></item><item><title>[AOTInductor] Support use_runtime_constant_folding for CPU. (#122563)</title><link>https://github.com/pytorch/pytorch/commit/091a24495b29d7aa077b16b508972f88d3a5aad5</link><description>&lt;p&gt;[AOTInductor] Support use_runtime_constant_folding for CPU. (#122563)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
We allow CPU to use the config use_runtime_constant_folding.&lt;br /&gt;
Changes include&lt;br /&gt;
1. Rearrange USE_CUDA flags. Add CPU sections that consumes memory directly.&lt;br /&gt;
2. Codegen changes to accomodate cpp fusions for CPU only. Specifically, we shouldn't generate 2 headers that would cause re-declaration.&lt;/p&gt;
&lt;p&gt;Test Plan: Activate tests that were deactivated for CPU before.&lt;/p&gt;
&lt;p&gt;Reviewed By: khabinov&lt;/p&gt;
&lt;p&gt;Differential Revision: D55234300&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122563&lt;br /&gt;
Approved by: https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 09:49:05 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/091a24495b29d7aa077b16b508972f88d3a5aad5</guid></item><item><title>[Inductor]Fix a couple of broken unit tests (#122714)</title><link>https://github.com/pytorch/pytorch/commit/4670dcc94c64f8f34673476ca2db3b840e41daf4</link><description>&lt;p&gt;[Inductor]Fix a couple of broken unit tests (#122714)&lt;/p&gt;
&lt;p&gt;Summary: Titled&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
&lt;code&gt;buck2 test mode/dev-nosan //caffe2/test/inductor:group_batch_fusion&lt;/code&gt;&lt;br /&gt;
Buck UI: https://www.internalfb.com/buck2/ad05a43c-cb4a-443e-8904-b4d53e4f4b1e&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/13510798909218388&lt;br /&gt;
Network: Up: 107KiB  Down: 28KiB  (reSessionID-d7146e4f-773a-46ea-9852-f10f59302479)&lt;br /&gt;
Jobs completed: 24. Time elapsed: 1:49.3s.&lt;br /&gt;
Cache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)&lt;br /&gt;
Tests finished: Pass 9. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;/p&gt;
&lt;p&gt;&lt;code&gt;buck2 test 'fbcode//mode/opt' fbcode//caffe2/test/inductor/fb:split_cat_fx_passes_fb&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Buck UI: https://www.internalfb.com/buck2/82dbf3b0-c747-4c07-98b8-53b69afa3157&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/1125900267699118&lt;br /&gt;
Network: Up: 1.4GiB  Down: 2.3GiB  (reSessionID-0bd22c6d-5dfe-4b4a-bc24-705eadac884b)&lt;br /&gt;
Jobs completed: 252570. Time elapsed: 7:25.2s.&lt;br /&gt;
Cache hits: 95%. Commands: 123778 (cached: 117999, remote: 2779, local: 3000)&lt;br /&gt;
Tests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;/p&gt;
&lt;p&gt;Differential Revision: D55378009&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122714&lt;br /&gt;
Approved by: https://github.com/SherlockNoMad&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 09:44:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4670dcc94c64f8f34673476ca2db3b840e41daf4</guid></item><item><title>[PT2][Inductor][Observability] Improve the optimus scuba log (#122361)</title><link>https://github.com/pytorch/pytorch/commit/9693797491a3b20dc1ea7825b6d26f8aace90322</link><description>&lt;p&gt;[PT2][Inductor][Observability] Improve the optimus scuba log (#122361)&lt;/p&gt;
&lt;p&gt;Summary: Titled&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
&lt;code&gt;buck2 test mode/dev-nosan //caffe2/test/inductor:group_batch_fusion&lt;/code&gt;&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/18014398535709463&lt;br /&gt;
Network: Up: 113KiB           Down: 480KiB           (reSessionID-1d2e3558-15b5-4a4e-8c5d-10c983afb389)&lt;br /&gt;
Discovered 9. Pass 0. Fail 0. Fatal 0. Skip 0. Timeout 0&lt;br /&gt;
Command: test.                                                                                 Remaining: 9/24. Cache hits: 0%. Time elapsed: 44.3s&lt;br /&gt;
Command: test.                                                                                 Remaining: 9/24. Cache hits: 0%. Time elapsed: 44.4s&lt;br /&gt;
Command: test.                                                                                 Remaining: 9/24. Cache hits: 0%. Time elapsed: 44.5s&lt;br /&gt;
Network: Up: 117KiB  Down: 507KiB  (reSessionID-1d2e3558-15b5-4a4e-8c5d-10c983afb389)&lt;br /&gt;
Jobs completed: 24. Time elapsed: 1:48.3s.&lt;br /&gt;
Cache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)&lt;br /&gt;
Tests finished: Pass 9. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;br /&gt;
&lt;code&gt;buck2 test mode/dev-nosan //caffe2/test/inductor:split_cat_fx_passes&lt;/code&gt;&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/16044073698893554&lt;br /&gt;
Network: Up: 120KiB  Down: 60KiB  (reSessionID-57f2c21b-3f4e-462b-9e5b-fe3dd15f6b7d)&lt;br /&gt;
Jobs completed: 28. Time elapsed: 1:47.5s.&lt;br /&gt;
Cache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)&lt;br /&gt;
Tests finished: Pass 11. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;/p&gt;
&lt;p&gt;optimus_scuba_log:&lt;br /&gt;
&lt;code&gt;{'before_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GIbj2haUwKx69H8BAKXdGqXZSpoybr0LAAAz', 'group_batch_fusion_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GFqhiRYcJ_C4JFoDABKPTsfpzjJ_br0LAAAz', 'normalization_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GIvswhaiAVyipcoGAJZ5sUi8Bb5qbr0LAAAz', 'remove_split_with_size_one_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GFneTxcVBPaqVuwCADCiI4q1mEwlbr0LAAAz', 'merge_getitem_cat_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GJc0Phn87ljuMO0CADBPGqqehKp2br0LAAAz', 'merge_splits_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GLWB_BbvLyT7D_0DABmygDYPDjJ_br0LAAAz', 'after_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GO6eQBeIj6oV3o4JAFLzQ3ECMTIrbr0LAAAz', 'inductor_pre_grad': Counter({'pattern_matcher_nodes': 2006, 'pattern_matcher_count': 1806, 'normalization_pass': 861, 'remove_split_with_size_one_pass': 748, 'merge_splits_pass': 82, 'merge_getitem_cat_pass': 11, 'scmerge_split_sections_removed': 4, 'batch_layernorm': 1, 'batch_sigmoid': 1, 'scmerge_split_added': 1, 'scmerge_cat_added': 1, 'scmerge_split_removed': 1, 'scmerge_cat_removed': 1}), 'before_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GMoKmxYg6AUeQ40KAMDaJ4EVDwYmbr0LAAAz', 'group_batch_fusion_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GHIvQxkrV1PMBggEACv7786a2bE8br0LAAAz', 'after_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GIpBNxXupQTHWx8BALSiVrKgDbtfbr0LAAAz', 'inductor_post_grad': Counter({'pattern_matcher_nodes': 2093, 'pattern_matcher_count': 1893, 'normalization_pass': 861, 'remove_split_with_size_one_pass': 748, 'merge_splits_pass': 82, 'merge_getitem_cat_pass': 11, 'scmerge_split_sections_removed': 4, 'batch_layernorm': 1, 'batch_sigmoid': 1, 'scmerge_split_added': 1, 'scmerge_cat_added': 1, 'scmerge_split_removed': 1, 'scmerge_cat_removed': 1, 'batch_aten_mul': 1})}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Differential Revision: D55107000&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122361&lt;br /&gt;
Approved by: https://github.com/jackiexu1992&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 09:13:32 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9693797491a3b20dc1ea7825b6d26f8aace90322</guid></item><item><title>[inductor][Autotune] Add matrix_instr_nonkdim to triton_meta (#122852)</title><link>https://github.com/pytorch/pytorch/commit/049d68d8bb1c0a10b41ae70ddb60d84b052c2cb5</link><description>&lt;p&gt;[inductor][Autotune] Add matrix_instr_nonkdim to triton_meta (#122852)&lt;/p&gt;
&lt;p&gt;Summary: Previous work &lt;code&gt;https://github.com/pytorch/pytorch/pull/120742&lt;/code&gt; to enable &lt;code&gt;matrix_instr_nonkdim&lt;/code&gt; only dealt with the autotuner benchmarking, but failed to enable the parameter in Triton meta for real runs. &lt;code&gt;matrix_instr_nonkdim&lt;/code&gt; needs to be visible to the compiler driver to set up the optimization pipeline, so it's unlike other kernel parameters such as &lt;code&gt;BLOCK_N&lt;/code&gt; that can be just set inside the kernel itself.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
P1201466917&lt;/p&gt;
&lt;p&gt;triton_heuristics.template(&lt;br /&gt;
    num_stages=1,&lt;br /&gt;
    num_warps=4,&lt;br /&gt;
    triton_meta={'signature': {0: '&lt;em&gt;fp32', 1: '&lt;/em&gt;fp32', 2: '*fp32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())], 'matrix_instr_nonkdim': 16},&lt;br /&gt;
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_0', 'backend_hash': None},&lt;br /&gt;
  )&lt;/p&gt;
&lt;p&gt;Perf :&lt;br /&gt;
Before: 1.693ms    0.134GB    79.28GB/s&lt;br /&gt;
After:    1.577ms    0.134GB    85.12GB/s&lt;/p&gt;
&lt;p&gt;Differential Revision: D55456401&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122852&lt;br /&gt;
Approved by: https://github.com/xw285cornell&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 08:58:38 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/049d68d8bb1c0a10b41ae70ddb60d84b052c2cb5</guid></item><item><title>[dynamo] Fix traceback generation on runtime errors (#122746)</title><link>https://github.com/pytorch/pytorch/commit/f178d996a893737f44c97c18a7ed6358780374db</link><description>&lt;p&gt;[dynamo] Fix traceback generation on runtime errors (#122746)&lt;/p&gt;
&lt;p&gt;Fixes &lt;code&gt;During handling of the above exception, another exception occurred: [...] torch._dynamo.exc.Unsupported: generator&lt;/code&gt;. traceback.format_exc uses generators which isn't supported by dynamo yet.&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;current error message&lt;/summary&gt;

```
======================================================================
ERROR: test_custom_fn_saved_tensors (__main__.TestCompiledAutograd)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 307, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1537, in _call_impl
    return forward_call(*args, **kwargs)
  File "&lt;eval_with_key&gt;.0", line 4, in forward
    def forward(self, inputs, sizes, hooks):
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xmfan/core/pytorch/torch/testing/_internal/common_utils.py", line 2741, in wrapper
    method(*args, **kwargs)
  File "/home/xmfan/core/pytorch/test/inductor/test_compiled_autograd.py", line 499, in test_custom_fn_saved_tensors
    self.check_output_and_recompiles(fn, 1)
  File "/home/xmfan/core/pytorch/test/inductor/test_compiled_autograd.py", line 61, in check_output_and_recompiles
    actual = list(opt_fn())
  File "/home/xmfan/core/pytorch/test/inductor/test_compiled_autograd.py", line 495, in fn
    loss.backward()
  File "/home/xmfan/core/pytorch/torch/_tensor.py", line 534, in backward
    torch.autograd.backward(
  File "/home/xmfan/core/pytorch/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/xmfan/core/pytorch/torch/autograd/graph.py", line 766, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1537, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/_dynamo/eval_frame.py", line 397, in _fn
    res = fn(*args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 741, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 315, in __call__
    _WrappedCall._generate_error_message(topmost_framesummary),
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 289, in _generate_error_message
    tb_repr = get_traceback()
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 288, in get_traceback
    return traceback.format_exc()
  File "/home/xmfan/.conda/envs/benchmarks/lib/python3.10/traceback.py", line 183, in format_exc
    return "".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))
  File "/home/xmfan/.conda/envs/benchmarks/lib/python3.10/traceback.py", line 136, in format_exception
    return list(te.format(chain=chain))
  File "/home/xmfan/core/pytorch/torch/_dynamo/convert_frame.py", line 941, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/home/xmfan/core/pytorch/torch/_dynamo/convert_frame.py", line 348, in _convert_frame_assert
    unimplemented("generator")
  File "/home/xmfan/core/pytorch/torch/_dynamo/exc.py", line 199, in unimplemented
    raise Unsupported(msg)
torch._dynamo.exc.Unsupported: generator
```

&lt;/details&gt;

&lt;p&gt;With this change, we get back the descriptive error message:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;post-fix error message&lt;/summary&gt;

```
Traceback (most recent call last):
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 307, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1537, in _call_impl
    return forward_call(*args, **kwargs)
  File "&lt;eval_with_key&gt;.0", line 4, in forward
    def forward(self, inputs, sizes, hooks):
IndexError: list index out of range

Call using an FX-traced Module, line 4 of the traced Module's generated forward function:

def forward(self, inputs, sizes, hooks):

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE
    getitem = inputs[0]

    getitem_1 = inputs[1];  inputs = None
```

&lt;/details&gt;

&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122746&lt;br /&gt;
Approved by: https://github.com/jansel, https://github.com/anijain2305&lt;br /&gt;
ghstack dependencies: #122691&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 06:40:54 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f178d996a893737f44c97c18a7ed6358780374db</guid></item><item><title>Revert "[Inductor] Run pattern matcher over the original graph (#122519)"</title><link>https://github.com/pytorch/pytorch/commit/b63f6f78dc8a4e6f059b9abc4eb1eed1d490bcfc</link><description>&lt;p&gt;Revert "[Inductor] Run pattern matcher over the original graph (#122519)"&lt;/p&gt;
&lt;p&gt;This reverts commit 1f5fcb4e203eb343e8c53f6444015c98e8f68d60.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122519 on behalf of https://github.com/atalman due to Breaks internal tests (&lt;a href="https://github.com/pytorch/pytorch/pull/122519#issuecomment-2023022311"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Wed, 27 Mar 2024 07:13:26 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b63f6f78dc8a4e6f059b9abc4eb1eed1d490bcfc</guid></item><item><title>[inductor][cpp] simplify CppVecKernelChecker (remove bool/int8 load as mask and load as float flags) (#119734)</title><link>https://github.com/pytorch/pytorch/commit/105381ea1159eb025a580f008c78f2682caee930</link><description>&lt;p&gt;[inductor][cpp] simplify CppVecKernelChecker (remove bool/int8 load as mask and load as float flags) (#119734)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/119734&lt;br /&gt;
Approved by: https://github.com/leslie-fang-intel, https://github.com/jansel&lt;br /&gt;
ghstack dependencies: #119654, #119655&lt;/p&gt;</description><pubDate>Wed, 27 Mar 2024 03:20:35 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/105381ea1159eb025a580f008c78f2682caee930</guid></item><item><title>[Inductor] Support custom op in JIT with cpp wrapper (#122554)</title><link>https://github.com/pytorch/pytorch/commit/537cd66e73e8a9b33c843d55d546471f3074a390</link><description>&lt;p&gt;[Inductor] Support custom op in JIT with cpp wrapper (#122554)&lt;/p&gt;
&lt;p&gt;Summary:  To call custom ops in an ABI-compatible way requires doing boxed call with varargs across C shim. In the JIT mode, we can get around it by calling into Python.  https://gist.github.com/desertfire/be2a65b0a9b47780bb716b53ac2cd2b3 is an example of generated code.&lt;/p&gt;
&lt;p&gt;Differential Revision: &lt;a href="https://our.internmc.facebook.com/intern/diff/D55326556"&gt;D55326556&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122554&lt;br /&gt;
Approved by: https://github.com/jansel, https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Tue, 26 Mar 2024 10:48:45 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/537cd66e73e8a9b33c843d55d546471f3074a390</guid></item><item><title>Log autotune time in scuba (#122637)</title><link>https://github.com/pytorch/pytorch/commit/e61aaab72562fe95d76e77b01678d71467e1739e</link><description>&lt;p&gt;Log autotune time in scuba (#122637)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
This diff&lt;br /&gt;
* Refactors triton and autotune caches to be child classes of the original memcache based cache infra&lt;br /&gt;
* Swaps scuba table for autotune&lt;br /&gt;
* Adds autotune time spent/saved to scuba table&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
Local testing using:&lt;br /&gt;
&lt;code&gt;buck run mode/opt fbcode//caffe2/test/inductor/:max_autotune -- -r test_max_autotune_remote_caching_dynamic_False&lt;/code&gt;&lt;br /&gt;
and&lt;br /&gt;
&lt;code&gt;TORCH_INDUCTOR_AUTOTUNE_REMOTE_CACHE=1 buck2 run mode/opt //scripts/oulgen:runner&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Differential Revision: D55332620&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122637&lt;br /&gt;
Approved by: https://github.com/jamesjwu&lt;/p&gt;</description><pubDate>Tue, 26 Mar 2024 09:51:33 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e61aaab72562fe95d76e77b01678d71467e1739e</guid></item><item><title>[Inductor] Run pattern matcher over the original graph (#122519)</title><link>https://github.com/pytorch/pytorch/commit/1f5fcb4e203eb343e8c53f6444015c98e8f68d60</link><description>&lt;p&gt;[Inductor] Run pattern matcher over the original graph (#122519)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122519&lt;br /&gt;
Approved by: https://github.com/jansel&lt;/p&gt;</description><pubDate>Tue, 26 Mar 2024 09:30:32 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1f5fcb4e203eb343e8c53f6444015c98e8f68d60</guid></item><item><title>[ez] Add more files to trigger inductor (#122669)</title><link>https://github.com/pytorch/pytorch/commit/8bad7b63c8b1ba4e7d1a9d86023d5b29068c2cb4</link><description>&lt;p&gt;[ez] Add more files to trigger inductor (#122669)&lt;/p&gt;
&lt;p&gt;To catch https://github.com/pytorch/pytorch/pull/122562/files&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122669&lt;br /&gt;
Approved by: https://github.com/desertfire&lt;/p&gt;</description><pubDate>Tue, 26 Mar 2024 07:19:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8bad7b63c8b1ba4e7d1a9d86023d5b29068c2cb4</guid></item><item><title>[BE][CPUInductor] Use C++17 helper templates (#122607)</title><link>https://github.com/pytorch/pytorch/commit/9d1de31634a5b9c8c990a6356ab8864bb4d9bede</link><description>&lt;p&gt;[BE][CPUInductor] Use C++17 helper templates (#122607)&lt;/p&gt;
&lt;p&gt;Such as &lt;code&gt;std::is_same_v&lt;/code&gt; ,&lt;code&gt;std::is_integral_v&lt;/code&gt; and C++14 one &lt;code&gt;std::enable_if_t&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122607&lt;br /&gt;
Approved by: https://github.com/jansel, https://github.com/Skylion007&lt;/p&gt;</description><pubDate>Mon, 25 Mar 2024 11:01:44 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9d1de31634a5b9c8c990a6356ab8864bb4d9bede</guid></item><item><title>[inductor] Improve error message for shape errors in slice_scatter (#122543)</title><link>https://github.com/pytorch/pytorch/commit/2db7d874a92f26ada3926b35de5716c1f457220e</link><description>&lt;p&gt;[inductor] Improve error message for shape errors in slice_scatter (#122543)&lt;/p&gt;
&lt;p&gt;Fixes #122291&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122543&lt;br /&gt;
Approved by: https://github.com/shunting314&lt;/p&gt;</description><pubDate>Mon, 25 Mar 2024 10:57:16 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2db7d874a92f26ada3926b35de5716c1f457220e</guid></item><item><title>GPT2 SDPA inference pattern-matching for Inductor-CPU (#121866)</title><link>https://github.com/pytorch/pytorch/commit/5152945441b2a1387f6e9fc8fe1531bfeb6cabe4</link><description>&lt;p&gt;GPT2 SDPA inference pattern-matching for Inductor-CPU (#121866)&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;With this PR, SDPA pattern of GPT2 is being mapped to &lt;code&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;.&lt;br /&gt;
While GPT2 supports both a causal mask &amp;amp; an attention mask, this PR considers the case of attention mask being absent.&lt;br /&gt;
TorchBench inference workload for GPT2 also doesn't use an attention-mask.&lt;/p&gt;
&lt;p&gt;This pattern's replacement is being disabled for CUDA because &lt;a href="https://github.com/pytorch/pytorch/actions/runs/8319111885/job/22762567770"&gt;CUDA AOT Inductor&lt;/a&gt; CI job's &lt;code&gt;GPT2ForSequenceClassification&lt;/code&gt; accuracy test failed, although all other trunk CUDA Inductor CI checks had passed.&lt;br /&gt;
Created #122429 to track that particular issue.&lt;/p&gt;
&lt;h3&gt;CPU performance data with TorchBench&lt;/h3&gt;
&lt;p&gt;|MODEL |BATCH SIZE | DTYPE | BEFORE: Speedup over eager-mode with the default Inductor implementation | AFTER: Speedup over eager mode with SDPA op mapped| Perf boost = (AFTER - BEFORE)/BEFORE * 100|&lt;br /&gt;
|--------------------------|-------------|---------|-----------------------------|--------------------------|------------|&lt;br /&gt;
|hf_GPT2| 1 | FP32 | 1.522x | 1.791x| 17.67%|&lt;br /&gt;
|hf_GPT2| 1 | BF16 (AMP) | 1.795x | 2.387x| 32.98%|&lt;br /&gt;
|hf_GPT2| 2 | FP32 |  1.313x |1.629x | 19.3%|&lt;br /&gt;
|hf_GPT2|2| BF16 (AMP) | 1.556x | 1.924x | 23.65%|&lt;br /&gt;
|hf_GPT2_large| 1 | FP32 | 1.380x |1.585x | 12.93%|&lt;br /&gt;
|hf_GPT2_large| 1 | BF16 (AMP) | 1.208x | 1.567x | 22.91%|&lt;br /&gt;
|hf_GPT2_large| 2 | FP32 | 1.188x | 1.490x | 25.42%|&lt;br /&gt;
|hf_GPT2_large|2| BF16 (AMP) | 0.991x | 1.575x | 58.93%|&lt;/p&gt;
&lt;p&gt;Machine - Intel(R) Xeon(R) Platinum 8468H (Xeon 4th gen Sapphire Rapids)&lt;br /&gt;
48 physical cores were used. Intel OpenMP &amp;amp; libtcmalloc were preloaded.&lt;/p&gt;
&lt;p&gt;Example command -&lt;br /&gt;
&lt;code&gt;OMP_NUM_THREADS=48 MKL_NUM_THREADS=48 numactl --membind=0 --cpunodebind=0 -C 0-47 python benchmarks/dynamo/torchbench.py --performance --inference --inductor --float32 -dcpu --only hf_GPT2_large --freezing --batch-size 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121866&lt;br /&gt;
Approved by: https://github.com/Valentine233, https://github.com/jgong5, https://github.com/desertfire&lt;/p&gt;</description><pubDate>Mon, 25 Mar 2024 07:04:03 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5152945441b2a1387f6e9fc8fe1531bfeb6cabe4</guid></item><item><title>[EZ][BE] Add missing `acosh` op to vec256_float_neon.h (#122513)</title><link>https://github.com/pytorch/pytorch/commit/bf40e3f880cc6820656d5d5fc32a21ca8334c520</link><description>&lt;p&gt;[EZ][BE] Add missing &lt;code&gt;acosh&lt;/code&gt; op to vec256_float_neon.h (#122513)&lt;/p&gt;
&lt;p&gt;As base class has it&lt;br /&gt;
https://github.com/pytorch/pytorch/blob/ed15370aabf951eec2ba0140de5ff71634868791/aten/src/ATen/cpu/vec/vec_base.h#L367-L369&lt;/p&gt;
&lt;p&gt;Discovered while attempting to enabling Inductor vectorization on ARM platform&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122513&lt;br /&gt;
Approved by: https://github.com/Skylion007&lt;/p&gt;</description><pubDate>Sat, 23 Mar 2024 06:18:02 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bf40e3f880cc6820656d5d5fc32a21ca8334c520</guid></item><item><title>[aoti] Add handling of ir.Constants in promote_constants (#122419)</title><link>https://github.com/pytorch/pytorch/commit/ed15370aabf951eec2ba0140de5ff71634868791</link><description>&lt;p&gt;[aoti] Add handling of ir.Constants in promote_constants (#122419)&lt;/p&gt;
&lt;p&gt;This issue popped up when enabling predispatch IR on the benchmarks (https://github.com/pytorch/pytorch/pull/122225)&lt;/p&gt;
&lt;p&gt;On the following model:&lt;br /&gt;
```&lt;br /&gt;
class M(torch.nn.Module):&lt;br /&gt;
    def &lt;strong&gt;init&lt;/strong&gt;(self, device):&lt;br /&gt;
        super().&lt;strong&gt;init&lt;/strong&gt;()&lt;br /&gt;
        self.device = device&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def forward(self, x):
    t = torch.tensor(x.size(-1), device=self.device, dtype=torch.float)
    t = torch.sqrt(t * 3)
    return x * t
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;We get the following error:&lt;br /&gt;
```&lt;br /&gt;
======================================================================&lt;br /&gt;
ERROR: test_constant_abi_compatible_cuda (&lt;strong&gt;main&lt;/strong&gt;.AOTInductorTestABICompatibleCuda)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Traceback (most recent call last):&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/testing/_internal/common_utils.py", line 2741, in wrapper&lt;br /&gt;
    method(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_torchinductor.py", line 9232, in new_test&lt;br /&gt;
    return value(self)&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;/em&gt;args, &lt;strong&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor.py", line 922, in test_constant&lt;br /&gt;
    self.check_model(M(self.device), (torch.randn(5, 5, device=self.device),))&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor.py", line 91, in check_model&lt;br /&gt;
    actual = AOTIRunnerUtil.run(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor_utils.py", line 102, in run&lt;br /&gt;
    so_path = AOTIRunnerUtil.compile(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor_utils.py", line 40, in compile&lt;br /&gt;
    so_path = torch._inductor.aot_compile_ep(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/&lt;strong&gt;init&lt;/strong&gt;.py", line 150, in aot_compile_ep&lt;br /&gt;
    return compile_fx_aot(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1005, in compile_fx_aot&lt;br /&gt;
    compiled_lib_path = compile_fx(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(*args, &lt;/strong&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1111, in compile_fx&lt;br /&gt;
    return compile_fx(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1145, in compile_fx&lt;br /&gt;
    return compile_fx(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;/em&gt;args, &lt;strong&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/&lt;em&gt;inductor/compile_fx.py", line 1336, in compile_fx&lt;br /&gt;
    return inference_compiler(unlifted_gm, example_inputs&lt;/em&gt;)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/utils.py", line 265, in time_wrapper&lt;br /&gt;
    r = func(*args, &lt;/strong&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1266, in fw_compiler_base&lt;br /&gt;
    return inner_compile(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper&lt;br /&gt;
    inner_compiled_fn = compiler_fn(gm, example_inputs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/debug.py", line 304, in inner&lt;br /&gt;
    return fn(&lt;/em&gt;args, &lt;strong&gt;kwargs)&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(*args, &lt;/strong&gt;kwds)&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/utils.py", line 265, in time_wrapper&lt;br /&gt;
    r = func(&lt;/em&gt;args, &lt;strong&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 447, in compile_fx_inner&lt;br /&gt;
    compiled_graph = fx_codegen_and_compile(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 707, in fx_codegen_and_compile&lt;br /&gt;
    graph.run(&lt;em&gt;example_inputs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/utils.py", line 265, in time_wrapper&lt;br /&gt;
    r = func(&lt;/em&gt;args, &lt;/strong&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 612, in run&lt;br /&gt;
    return super().run(&lt;em&gt;args)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/fx/interpreter.py", line 145, in run&lt;br /&gt;
    self.env[node] = self.run_node(node)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 957, in run_node&lt;br /&gt;
    result = super().run_node(n)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/fx/interpreter.py", line 202, in run_node&lt;br /&gt;
    return getattr(self, n.op)(n.target, args, kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 819, in call_function&lt;br /&gt;
    raise LoweringException(e, target, args, kwargs).with_traceback(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 816, in call_function&lt;br /&gt;
    out = lowerings&lt;a href="*args, **kwargs"&gt;target&lt;/a&gt;&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 298, in wrapped&lt;br /&gt;
    out = decomp_fn(&lt;/em&gt;args, **kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 5340, in mul&lt;br /&gt;
    return make_pointwise(fn)(a, b)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 409, in inner&lt;br /&gt;
    inputs = promote_constants(inputs, override_return_dtype)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 373, in promote_constants&lt;br /&gt;
    ex = next(x for x in inputs if isinstance(x, (TensorBox, ExpandView)))&lt;br /&gt;
torch._inductor.exc.LoweringException: StopIteration:&lt;br /&gt;
  target: aten.mul.Tensor&lt;br /&gt;
  args[0]: Constant(value=5.0, dtype=torch.float32, device=device(type='cuda', index=0))&lt;br /&gt;
  args[1]: 3&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;So I added an additional casing in &lt;code&gt;promote_constants&lt;/code&gt; to handle the ir.Constants and now it works! Although please let me know if this is the wrong approach. Here's a paste of the full run with the inductor logs: P1198927007&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122419&lt;br /&gt;
Approved by: https://github.com/eellison, https://github.com/desertfire, https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 10:39:36 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ed15370aabf951eec2ba0140de5ff71634868791</guid></item><item><title>[inductor] device guard for max autotune benchmark (#122479)</title><link>https://github.com/pytorch/pytorch/commit/a3d4eaf2531a9aae974ed2eab157a3b3594e7e7b</link><description>&lt;p&gt;[inductor] device guard for max autotune benchmark (#122479)&lt;/p&gt;
&lt;p&gt;Internal users reported that they get failure for max-autotune if tensors are not on device 0. It turns out that we may use tensors on device say 6 and run kernel on them at device 0.&lt;/p&gt;
&lt;p&gt;This PR enforces that we do benchmarking for max-autotune on the correct device.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122479&lt;br /&gt;
Approved by: https://github.com/xintwfb, https://github.com/Chillee&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 09:27:53 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a3d4eaf2531a9aae974ed2eab157a3b3594e7e7b</guid></item><item><title>[aot_inductor] added runtime checks for input/output tensors in debug compile mode (#122047)</title><link>https://github.com/pytorch/pytorch/commit/86082f1fdc336ca89f9b6f8888fba9e87f3a4c06</link><description>&lt;p&gt;[aot_inductor] added runtime checks for input/output tensors in debug compile mode (#122047)&lt;/p&gt;
&lt;p&gt;This PR added runtime checks to guard the dtypes and shapes of input/output tensors.&lt;br /&gt;
Currently, we enable these only for debug compilation&lt;br /&gt;
(i.e. aot_inductor.debug_compile is True) in abi_compatible mode.&lt;/p&gt;
&lt;p&gt;Differential Revision: &lt;a href="https://our.internmc.facebook.com/intern/diff/D54993148"&gt;D54993148&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122047&lt;br /&gt;
Approved by: https://github.com/desertfire&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 08:40:33 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/86082f1fdc336ca89f9b6f8888fba9e87f3a4c06</guid></item><item><title>Revert "[Quant] [PT2] Add SiLU  into X86InductorQuantizer Conv2d Unary Annotation (#122267)"</title><link>https://github.com/pytorch/pytorch/commit/60bc29aa0b22dcc06c5778589a9d2b5c4b9fc965</link><description>&lt;p&gt;Revert "[Quant] [PT2] Add SiLU  into X86InductorQuantizer Conv2d Unary Annotation (#122267)"&lt;/p&gt;
&lt;p&gt;This reverts commit 2c6eeb26d3f61fba352ad51fd8653120937a20f3.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122267 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/60bc29aa0b22dcc06c5778589a9d2b5c4b9fc965</guid></item><item><title>Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op SiLU (#122268)"</title><link>https://github.com/pytorch/pytorch/commit/b30b396d057f3462fbf2a1b46b8f480947dcb6d0</link><description>&lt;p&gt;Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op SiLU (#122268)"&lt;/p&gt;
&lt;p&gt;This reverts commit 99f0fec7d0873d627e8c7f2dec65818d725424b0.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122268 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b30b396d057f3462fbf2a1b46b8f480947dcb6d0</guid></item><item><title>Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardSwish with int8-mix-bf16 (#122373)"</title><link>https://github.com/pytorch/pytorch/commit/777ac511cc98a7edf916e61d0770afac4e37fccc</link><description>&lt;p&gt;Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardSwish with int8-mix-bf16 (#122373)"&lt;/p&gt;
&lt;p&gt;This reverts commit 783fd89ff1cf401e484c20d14b16823abf20d87d.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122373 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/777ac511cc98a7edf916e61d0770afac4e37fccc</guid></item><item><title>Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)"</title><link>https://github.com/pytorch/pytorch/commit/dbedc6bb7cf4cfaab5df85aff2e4dbaf049e6746</link><description>&lt;p&gt;Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)"&lt;/p&gt;
&lt;p&gt;This reverts commit 23a6d74f9352e0afb37750fee300d077c4ba9393.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122374 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dbedc6bb7cf4cfaab5df85aff2e4dbaf049e6746</guid></item><item><title>[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)</title><link>https://github.com/pytorch/pytorch/commit/23a6d74f9352e0afb37750fee300d077c4ba9393</link><description>&lt;p&gt;[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;br /&gt;
Enable the fusion pattern of &lt;code&gt;QConv2d -&amp;gt; hardtanh&lt;/code&gt; lowering for int8-mixed-bf16 case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Test Plan&lt;/strong&gt;&lt;br /&gt;
&lt;code&gt;python -m pytest test_mkldnn_pattern_matcher.py -k test_qconv2d_hardtanh_int8_mixed_bf16_cpu&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122374&lt;br /&gt;
Approved by: https://github.com/leslie-fang-intel, https://github.com/jgong5&lt;br /&gt;
ghstack dependencies: #122266, #122267, #122268, #122373&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 05:13:14 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/23a6d74f9352e0afb37750fee300d077c4ba9393</guid></item><item><title>Precompile triton templates (#121998)</title><link>https://github.com/pytorch/pytorch/commit/b8df2f0ca530ebe01fa079c891c170a1f4b22823</link><description>&lt;p&gt;Precompile triton templates (#121998)&lt;/p&gt;
&lt;p&gt;Before this PR we were not precompiling triton templates in parallel. Compilation would occur during benchmarking.&lt;/p&gt;
&lt;p&gt;Triton benchmarking templates were emitted as :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;@triton.jit
def triton_mm(arg_A, arg_B, out_ptr0):&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In order to precompile we need to give the full kernel specification, as we do when we emit the template in the final output code generation.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'backend_hash': 'cdeecfeccd31ad7810f96b5752194b1c2406d0a81e39a6ca09c8ee150baae183'},
)
@triton.jit
def triton_mm(arg_A, arg_B, out_ptr0):&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121998&lt;br /&gt;
Approved by: https://github.com/jansel&lt;br /&gt;
ghstack dependencies: #121996, #120275, #121997&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 09:04:53 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b8df2f0ca530ebe01fa079c891c170a1f4b22823</guid></item><item><title>Introduce XPU implementation for PyTorch ATen operators (#120891)</title><link>https://github.com/pytorch/pytorch/commit/148a8de6397be6e4b4ca1508b03b82d117bfb03c</link><description>&lt;p&gt;Introduce XPU implementation for PyTorch ATen operators (#120891)&lt;/p&gt;
&lt;p&gt;As a follow-up to #114835 and #119682, we add limited ATen operators implementation for XPU. With this PR, the blocking issue for oneDNN operations and Inductor XPU backend will be resolved as the two components depend on these operations to support its basic features, respectively.&lt;/p&gt;
&lt;p&gt;The added ATen operators include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;copy_&lt;/code&gt;, &lt;code&gt;_to_copy&lt;/code&gt;, &lt;code&gt;_copy_from_and_resize&lt;/code&gt;, , &lt;code&gt;clone&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;view&lt;/code&gt;, &lt;code&gt;view_as_real&lt;/code&gt;, &lt;code&gt;view_as_complex&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;as_strided&lt;/code&gt;, &lt;code&gt;_reshape_alias&lt;/code&gt;, &lt;code&gt;resize_&lt;/code&gt;, &lt;code&gt;resize_as_&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;add&lt;/code&gt;/&lt;code&gt;add_&lt;/code&gt;, &lt;code&gt;sub&lt;/code&gt;/&lt;code&gt;sub_&lt;/code&gt;, &lt;code&gt;mul&lt;/code&gt;/&lt;code&gt;mul_&lt;/code&gt;, &lt;code&gt;div&lt;/code&gt;/&lt;code&gt;div_&lt;/code&gt;, &lt;code&gt;abs&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;empty&lt;/code&gt;, &lt;code&gt;empty_strided&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fill_&lt;/code&gt;, &lt;code&gt;zeros_&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Co-authored-by: Wang, Eikan &lt;a href="&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#116;&amp;#111;&amp;#58;&amp;#101;&amp;#105;&amp;#107;&amp;#97;&amp;#110;&amp;#46;&amp;#119;&amp;#97;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;"&gt;&amp;#101;&amp;#105;&amp;#107;&amp;#97;&amp;#110;&amp;#46;&amp;#119;&amp;#97;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/120891&lt;br /&gt;
Approved by: https://github.com/EikanWang, https://github.com/jgong5, https://github.com/gujinghui, https://github.com/atalman&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 07:42:20 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/148a8de6397be6e4b4ca1508b03b82d117bfb03c</guid></item><item><title>[BE] Enable torch inductor tests running on MacOS (#122360)</title><link>https://github.com/pytorch/pytorch/commit/f9996ed7643820d9b2191c5bfb8cf28962d41664</link><description>&lt;p&gt;[BE] Enable torch inductor tests running on MacOS (#122360)&lt;/p&gt;
&lt;p&gt;Original idea was limit the testing to just x86 Macs, but right now it will be skipped on all Apple Silicon ones, as all of them have Metal capable GPU&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122360&lt;br /&gt;
Approved by: https://github.com/jansel&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 06:47:05 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f9996ed7643820d9b2191c5bfb8cf28962d41664</guid></item><item><title>[inductor] Support non-Tensor predicate in torch.cond (#122378)</title><link>https://github.com/pytorch/pytorch/commit/456b112dca17a7d221f1d9c1a83e7b94cf915cbb</link><description>&lt;p&gt;[inductor] Support non-Tensor predicate in torch.cond (#122378)&lt;/p&gt;
&lt;p&gt;Summary: Previously, we only supported torch.Tensor boolean scalar predicate in &lt;code&gt;torch.cond&lt;/code&gt; in Inductor. This PR adds support for SymBool and Python bool predicate, to match the &lt;code&gt;torch.cond&lt;/code&gt; &lt;a href="https://pytorch.org/docs/stable/generated/torch.cond.html"&gt;sematics&lt;/a&gt; in Dynamo / Export.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;p&gt;```&lt;br /&gt;
$ python test/inductor/test_control_flow.py&lt;br /&gt;
...&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 34 tests in 56.980s&lt;/p&gt;
&lt;p&gt;OK&lt;/p&gt;
&lt;p&gt;$ python test/inductor/test_aot_inductor.py -k test_cond&lt;br /&gt;
...&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 54 tests in 460.093s&lt;/p&gt;
&lt;p&gt;OK (skipped=4)&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122378&lt;br /&gt;
Approved by: https://github.com/jansel, https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 06:35:01 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/456b112dca17a7d221f1d9c1a83e7b94cf915cbb</guid></item><item><title>Skip nonzero unbacked SymInt memo in inference mode (#122147)</title><link>https://github.com/pytorch/pytorch/commit/2e02e1efad957b86dbcc5b64748e03acfb8d330c</link><description>&lt;p&gt;Skip nonzero unbacked SymInt memo in inference mode (#122147)&lt;/p&gt;
&lt;p&gt;Summary: In &lt;code&gt;torch.inference_mode()&lt;/code&gt;, fake tensors don't have &lt;code&gt;_version&lt;/code&gt;s. This breaks unbacked SymInt memoization in &lt;code&gt;torch.nonzero&lt;/code&gt; tracing. Here we disable the latter in inference mode.&lt;/p&gt;
&lt;p&gt;Fixes https://github.com/pytorch/pytorch/issues/122127&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;p&gt;```&lt;br /&gt;
$ python test/inductor/test_unbacked_symints.py -k test_nonzero_in_inference_mode&lt;br /&gt;
...&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 2 tests in 14.060s&lt;/p&gt;
&lt;p&gt;OK&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;Reviewers:&lt;/p&gt;
&lt;p&gt;Subscribers:&lt;/p&gt;
&lt;p&gt;Tasks:&lt;/p&gt;
&lt;p&gt;Tags:&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122147&lt;br /&gt;
Approved by: https://github.com/ezyang&lt;/p&gt;</description><pubDate>Wed, 20 Mar 2024 06:44:55 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2e02e1efad957b86dbcc5b64748e03acfb8d330c</guid></item><item><title>Revert "Skip nonzero unbacked SymInt memo in inference mode (#122147)"</title><link>https://github.com/pytorch/pytorch/commit/7673cb534ade295ffd0b8ebfa9f0528b82945f7d</link><description>&lt;p&gt;Revert "Skip nonzero unbacked SymInt memo in inference mode (#122147)"&lt;/p&gt;
&lt;p&gt;This reverts commit 5e2687391229cee6e4dc0214f9208b4ecbe058c1.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122147 on behalf of https://github.com/jeanschmidt due to Reverting to see if trunk error in inductor are related (&lt;a href="https://github.com/pytorch/pytorch/pull/122147#issuecomment-2007513000"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Tue, 19 Mar 2024 07:37:24 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7673cb534ade295ffd0b8ebfa9f0528b82945f7d</guid></item><item><title>[inductor] disable linear weight prepacking pass on double (#121478)</title><link>https://github.com/pytorch/pytorch/commit/6d9588a12b5834f29bd8970936749a9725e7f609</link><description>&lt;p&gt;[inductor] disable linear weight prepacking pass on double (#121478)&lt;/p&gt;
&lt;p&gt;Fix #121175&lt;/p&gt;
&lt;p&gt;Co-authored-by: Jiong Gong &lt;a href="&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#116;&amp;#111;&amp;#58;&amp;#106;&amp;#105;&amp;#111;&amp;#110;&amp;#103;&amp;#46;&amp;#103;&amp;#111;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;"&gt;&amp;#106;&amp;#105;&amp;#111;&amp;#110;&amp;#103;&amp;#46;&amp;#103;&amp;#111;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/121478&lt;br /&gt;
Approved by: https://github.com/jgong5, https://github.com/desertfire&lt;/p&gt;</description><pubDate>Sat, 16 Mar 2024 05:24:21 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6d9588a12b5834f29bd8970936749a9725e7f609</guid></item><item><title>Enable FX graph caching in another batch of inductor tests (#121697)</title><link>https://github.com/pytorch/pytorch/commit/535bc71d037d44167270d99577b71b8d97db1d8f</link><description>&lt;p&gt;Enable FX graph caching in another batch of inductor tests (#121697)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121697&lt;br /&gt;
Approved by: https://github.com/eellison&lt;/p&gt;</description><pubDate>Fri, 15 Mar 2024 11:38:51 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/535bc71d037d44167270d99577b71b8d97db1d8f</guid></item><item><title>Fall back to eager mode when viewing with differing bitwidths (#120998) (#121786)</title><link>https://github.com/pytorch/pytorch/commit/3ee319c49ca5478a27aad00bf5d30946a5b784d2</link><description>&lt;p&gt;Fall back to eager mode when viewing with differing bitwidths (#120998) (#121786)&lt;/p&gt;
&lt;p&gt;The inductor lowering code for viewing a tensor as a type with a different bitwidth currently doesn't generate valid triton code. This change looks for a source and destination dtype and, if different sizes, falls back to the eager mode aten implementation.  Prior to this change, this condition would throw an exception.&lt;/p&gt;
&lt;p&gt;Fixes #120998.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121786&lt;br /&gt;
Approved by: https://github.com/peterbell10, https://github.com/bertmaher&lt;/p&gt;</description><pubDate>Fri, 15 Mar 2024 11:33:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3ee319c49ca5478a27aad00bf5d30946a5b784d2</guid></item><item><title>Disable inductor (default) and inductor (dynamic) by default in the perf run launcher (#121914)</title><link>https://github.com/pytorch/pytorch/commit/3c3d7455a3ef81fb90fb18199d7e0db316dbf159</link><description>&lt;p&gt;Disable inductor (default) and inductor (dynamic) by default in the perf run launcher (#121914)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121914&lt;br /&gt;
Approved by: https://github.com/desertfire&lt;/p&gt;</description><pubDate>Fri, 15 Mar 2024 08:46:24 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3c3d7455a3ef81fb90fb18199d7e0db316dbf159</guid></item><item><title>[AOTInductor] Include build cmds at the end of wrapper file (#121872)</title><link>https://github.com/pytorch/pytorch/commit/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da</link><description>&lt;p&gt;[AOTInductor] Include build cmds at the end of wrapper file (#121872)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
For easier debugging, include build commands at the end of codegen wrapper.&lt;/p&gt;
&lt;p&gt;{F1468438991}&lt;/p&gt;
&lt;p&gt;Test Plan: CI&lt;/p&gt;
&lt;p&gt;Differential Revision: D54882164&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121872&lt;br /&gt;
Approved by: https://github.com/chenyang78, https://github.com/desertfire&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 10:41:17 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da</guid></item><item><title>[sigmoid] Use deserializer from oss. (#121839)</title><link>https://github.com/pytorch/pytorch/commit/c40929219749f3c11be2220497f2a371cbe82ef7</link><description>&lt;p&gt;[sigmoid] Use deserializer from oss. (#121839)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
Old path:&lt;br /&gt;
thrift -&amp;gt; thrift deserializer -&amp;gt; graph module.&lt;br /&gt;
new path:&lt;br /&gt;
thrift -&amp;gt; python dataclass -&amp;gt; oss deserializer -&amp;gt; graph_module&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
CI&lt;br /&gt;
buck2 test mode/dev-nosan caffe2/test/inductor/fb:test_aot_inductor_pt2_inference&lt;/p&gt;
&lt;p&gt;Reviewed By: SherlockNoMad&lt;/p&gt;
&lt;p&gt;Differential Revision: D54855251&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121839&lt;br /&gt;
Approved by: https://github.com/angelayi&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 10:38:58 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c40929219749f3c11be2220497f2a371cbe82ef7</guid></item><item><title>[Inductor] Fix a dynamic shape problem when lowering diagonal (#121881)</title><link>https://github.com/pytorch/pytorch/commit/499136a4dd518775374bcdc8e2f2dc391cf43498</link><description>&lt;p&gt;[Inductor] Fix a dynamic shape problem when lowering diagonal (#121881)&lt;/p&gt;
&lt;p&gt;Summary: when computing the diagonal size, we need to use correct symbolic min/max function.&lt;/p&gt;
&lt;p&gt;Differential Revision: &lt;a href="https://our.internmc.facebook.com/intern/diff/D54884899"&gt;D54884899&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/121881&lt;br /&gt;
Approved by: https://github.com/aakhundov&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 10:36:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/499136a4dd518775374bcdc8e2f2dc391cf43498</guid></item><item><title>Add CUTLASS kernel as choice for _int_mm() Inductor autotuning (#119685)</title><link>https://github.com/pytorch/pytorch/commit/1251f0fa31aca84f6bbd4c32f1b9bfdf33774642</link><description>&lt;p&gt;Add CUTLASS kernel as choice for _int_mm() Inductor autotuning (#119685)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/119685&lt;br /&gt;
Approved by: https://github.com/cpuhrsch, https://github.com/kadeng&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 05:25:23 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1251f0fa31aca84f6bbd4c32f1b9bfdf33774642</guid></item><item><title>Handle transitive replacements in Triton kernel mutation analysis (#121867)</title><link>https://github.com/pytorch/pytorch/commit/3eb322ff293169c19a3e7381039562e2b08f32be</link><description>&lt;p&gt;Handle transitive replacements in Triton kernel mutation analysis (#121867)&lt;/p&gt;
&lt;p&gt;Summary: Previously, we didn't handle transitive replacements in MLIR walk-based function info mining in the Triton kernel mutation analysis pass. As a result, for the TTIR below:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tt.func private @cumsum__fp32S1_16S__1cconstexpr_1__2cconstexpr_False_(%arg0: tensor&amp;lt;1x16xf32&amp;gt; loc("...":296:0)) -&amp;gt; tensor&amp;lt;1x16xf32&amp;gt; attributes {noinline = false} {
    %0 = "tt.scan"(%arg0) &amp;lt;{axis = 1 : i32, reverse = false}&amp;gt; ({
    ^bb0(%arg1: f32 loc(unknown), %arg2: f32 loc(unknown)):
      %1 = tt.call @_sum_combine__fp32_fp32__(%arg1, %arg2) : (f32, f32) -&amp;gt; f32 loc(#loc16)
      tt.scan.return %1 : f32 loc(#loc16)
    }) : (tensor&amp;lt;1x16xf32&amp;gt;) -&amp;gt; tensor&amp;lt;1x16xf32&amp;gt; loc(#loc16)
    tt.return %0 : tensor&amp;lt;1x16xf32&amp;gt; loc(#loc18)
  } loc(#loc15)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the mined function dict looked like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{Intermediate(idx=25): [Op(name='tt.call',
                           fn_call_name='_sum_combine__fp32_fp32__',
                           args=[Intermediate(idx=26),
                                 Intermediate(idx=26)])],
 Intermediate(idx=27): [Op(name='tt.scan.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=25)])],
 Intermediate(idx=-4): [Op(name='tt.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=27)])]}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;whereas it should look like this (not the &lt;code&gt;Param(idx=0)&lt;/code&gt; arguments of the &lt;code&gt;tt.call&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{Intermediate(idx=25): [Op(name='tt.call',
                           fn_call_name='_sum_combine__fp32_fp32__',
                           args=[Param(idx=0),
                                 Param(idx=0)])],
 Intermediate(idx=27): [Op(name='tt.scan.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=25)])],
 Intermediate(idx=-4): [Op(name='tt.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=27)])]}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This is fixed in the PR.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;p&gt;```&lt;br /&gt;
$ python test/inductor/test_triton_kernels.py -k test_cumsum&lt;br /&gt;
.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 1 test in 1.771s&lt;/p&gt;
&lt;p&gt;OK&lt;br /&gt;
```&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/121867&lt;br /&gt;
Approved by: https://github.com/oulgen&lt;/p&gt;</description><pubDate>Wed, 13 Mar 2024 20:06:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3eb322ff293169c19a3e7381039562e2b08f32be</guid></item><item><title>Enable FX graph cache for a batch of inductor tests (#121696)</title><link>https://github.com/pytorch/pytorch/commit/4cd503c1f3efdb80204e288237fce13025a18350</link><description>&lt;p&gt;Enable FX graph cache for a batch of inductor tests (#121696)&lt;/p&gt;
&lt;p&gt;Summary: Get more FX graph cache coverage by enabling it for these unit tests&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121696&lt;br /&gt;
Approved by: https://github.com/eellison&lt;/p&gt;</description><pubDate>Wed, 13 Mar 2024 19:39:59 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4cd503c1f3efdb80204e288237fce13025a18350</guid></item><item>
      <title>Update torchbench commit pin, add sam_fast benchmark (#121420)</title>
      <link>https://github.com/pytorch/pytorch/commit/ae22bdaefe2a0d1cc8d997d87ba5d88b2d8eafd1</link>
      <description>&lt;p&gt;Update torchbench commit pin, add sam_fast benchmark (#121420)&lt;/p&gt;
&lt;p&gt;After this, the sam_fast benchmark can now be run in the pytorch repo:&lt;br /&gt;
&lt;code&gt;SEGMENT_ANYTHING_FAST_USE_FLASH_4=0 benchmarks/dynamo/torchbench.py --inference --amp --performance --backend=inductor --explain --only sam_fast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;sam_fast is designed for inference only, with cuda and amp on. The code adds these restrictions to the benchmark.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121420&lt;br /&gt;
Approved by: https://github.com/oulgen, https://github.com/msaroufim&lt;/p&gt;</description>
      <pubDate>Mon, 11 Mar 2024 11:48:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ae22bdaefe2a0d1cc8d997d87ba5d88b2d8eafd1</guid>
    </item>
    <item>
      <title>Upgrade submodule onednn to v3.3.5 (#120767)</title>
      <link>https://github.com/pytorch/pytorch/commit/d1510e01faceb7de9095caff75401955ab323310</link>
      <description>&lt;p&gt;Upgrade submodule onednn to v3.3.5 (#120767)&lt;/p&gt;
&lt;p&gt;This upgrade contains the fixes to the known issues brought by oneDNN v3.3.2, including issues https://github.com/pytorch/pytorch/issues/115346, https://github.com/pytorch/pytorch/issues/120211 and https://github.com/pytorch/pytorch/issues/120406 and those listed in PR #112700.&lt;/p&gt;
&lt;p&gt;Issue https://github.com/pytorch/pytorch/issues/115346 (perf regression) was fixed by oneDNN v3.3.4. No new regression was found with v3.3.5. The detailed results of v3.3.4 are given below and compared with v3.1.1 (the oneDNN version in PyTorch before it was updated to v3.3.2).&lt;br /&gt;
1. A performance regression with 5.8% perf drop from &lt;code&gt;pytorch_stargan-train&lt;/code&gt; (see https://github.com/pytorch/benchmark/issues/2076#issuecomment-1847545843)&lt;br /&gt;
Validation results with this patch: Latency increased by 0.60%&lt;br /&gt;
&lt;code&gt;Tested on an Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz instance (IceLake)
oneDNN v3.1.1
metrics-1484287.json
{
    "name": "cpu",
    "environ": {
        "pytorch_git_version": "6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0"
    },
    "metrics": {
        "latency": 418.851717
    }
}
oneDNN v3.3.4
{
    "name": "cpu",
    "environ": {
        "pytorch_git_version": "6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0"
    },
    "metrics": {
        "latency": 421.381313
    }
}&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Performance regression of FP32 rexnet_100 with Inductor, dynamic shape, multi-threads (see https://github.com/pytorch/pytorch/issues/115346#issue-2030859592)&lt;br /&gt;
Validation results with this patch: Latency reduced by 3.23%&lt;br /&gt;
```&lt;br /&gt;
Tested on an Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz instance (IceLake)&lt;br /&gt;
oneDNN v3.1.1&lt;br /&gt;
(inductor speedup over eager mode) 2.876x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,rexnet_100,128,2.875904,113.314765,18.455283,0.990437,1302.636134,1315.212902,351,1,0,0&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;oneDNN v3.3.4&lt;br /&gt;
(inductor speedup over eager mode) 3.003x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,rexnet_100,128,3.003012,109.653012,91.547260,0.990048,1302.532506,1315.625370,351,1,0,0&lt;br /&gt;
```&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Performance regression of AMP hf_T5_generate and tinynet_a with Inductor, static shape, multi-threads (see https://github.com/pytorch/pytorch/issues/115346#issuecomment-1856029962)&lt;br /&gt;
Validation results with this patch: Latency reduced by 0.85%&lt;br /&gt;
```&lt;br /&gt;
Tested on an AWS spr metal instance&lt;br /&gt;
oneDNN v3.1.1&lt;br /&gt;
(inductor speedup over eager mode) 1.120x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,hf_T5_generate,1,1.120018,1197.807729,205.905466,0.442803,125.179904,282.698957,10550,48,8,4&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;oneDNN v3.3.4&lt;br /&gt;
(inductor speedup over eager mode) 1.134x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,hf_T5_generate,1,1.133594,1187.701514,205.855527,0.422012,128.405094,304.268493,10550,48,8,4&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;The following issues about functionality are fixed by this upgrade. Test cases are also added for these issues.&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120211&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120406&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120547&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Below are detailed data of torchbench CPU userbenchmark test and Inductor FP32/AMP inference tests. No regression of perf or functionality was found.&lt;br /&gt;
I.  &lt;em&gt;torchbench CPU userbenchmark test&lt;/em&gt;&lt;br /&gt;
Suite | Speedup&lt;br /&gt;
-- | --&lt;br /&gt;
eager_throughtput_bf16_infer | 1.001848&lt;br /&gt;
eager_throughtput_fp32_infer | 1.000257&lt;br /&gt;
eager_throughtput_fx_int8 | 1.003069&lt;br /&gt;
jit_llga_throughtput_amp_bf16 | 1.000682&lt;br /&gt;
jit_llga_throughtput_fp32 | 1.000313&lt;br /&gt;
eager_throughtput_bf16_train | 0.998222&lt;br /&gt;
eager_throughtput_fp32_train | 1.003384&lt;/p&gt;
&lt;p&gt;II. &lt;em&gt;Inductor FP32/AMP inference tests&lt;/em&gt;&lt;br /&gt;
i.  FP32 static default&lt;br /&gt;
suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.09&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.14&lt;/p&gt;
&lt;p&gt;ii.  FP32 dynamic default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | alexnet | multiple | 128 | 1.08&lt;br /&gt;
torchbench | basic_gnn_edgecnn | multiple | 1 | 0.98&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.08&lt;/p&gt;
&lt;p&gt;iii. AMP static default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | hf_distil_whisper | multiple | 1 | 1.18&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.32&lt;br /&gt;
huggingface | BartForConditionalGeneration | multiple | 2 | 1.19&lt;br /&gt;
timm_models | eca_halonext26ts | multiple | 128 | 1.13&lt;br /&gt;
timm_models | nfnet_l0 | multiple | 128 | 1.13&lt;br /&gt;
timm_models | rexnet_100 | multiple | 128 | 1.45&lt;br /&gt;
timm_models | spnasnet_100 | multiple | 128 | 1.15&lt;br /&gt;
timm_models | tf_efficientnet_b0 | multiple | 128 | 1.22&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.49&lt;br /&gt;
torchbench | hf_Bert_large | single | 1 | 1.16&lt;br /&gt;
huggingface | XLNetLMHeadModel | single | 1 | 1.07&lt;/p&gt;
&lt;p&gt;iv.  AMP dynamic default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.32&lt;br /&gt;
huggingface | PLBartForConditionalGeneration | multiple | 4 | 1.14&lt;br /&gt;
timm_models | nfnet_l0 | multiple | 128 | 1.15&lt;br /&gt;
timm_models | rexnet_100 | multiple | 128 | 1.45&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.34&lt;br /&gt;
huggingface | XLNetLMHeadModel | single | 1 | 1.09&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Co-authored-by: Nikita Shulga &lt;a href="&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#116;&amp;#111;&amp;#58;&amp;#50;&amp;#52;&amp;#53;&amp;#51;&amp;#53;&amp;#50;&amp;#52;&amp;#43;&amp;#109;&amp;#97;&amp;#108;&amp;#102;&amp;#101;&amp;#116;&amp;#64;&amp;#117;&amp;#115;&amp;#101;&amp;#114;&amp;#115;&amp;#46;&amp;#110;&amp;#111;&amp;#114;&amp;#101;&amp;#112;&amp;#108;&amp;#121;&amp;#46;&amp;#103;&amp;#105;&amp;#116;&amp;#104;&amp;#117;&amp;#98;&amp;#46;&amp;#99;&amp;#111;&amp;#109;"&gt;&amp;#50;&amp;#52;&amp;#53;&amp;#51;&amp;#53;&amp;#50;&amp;#52;&amp;#43;&amp;#109;&amp;#97;&amp;#108;&amp;#102;&amp;#101;&amp;#116;&amp;#64;&amp;#117;&amp;#115;&amp;#101;&amp;#114;&amp;#115;&amp;#46;&amp;#110;&amp;#111;&amp;#114;&amp;#101;&amp;#112;&amp;#108;&amp;#121;&amp;#46;&amp;#103;&amp;#105;&amp;#116;&amp;#104;&amp;#117;&amp;#98;&amp;#46;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/120767&lt;br /&gt;
Approved by: https://github.com/chuanqi129, https://github.com/jgong5, https://github.com/atalman&lt;/p&gt;</description>
      <pubDate>Mon, 11 Mar 2024 04:56:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d1510e01faceb7de9095caff75401955ab323310</guid>
    </item>
  </channel>
</rss>
