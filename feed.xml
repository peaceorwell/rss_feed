<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><item><title>[inductor] skip launching kernels with zero grid in AOTInductor when using backed symints (#118654)</title><link>https://github.com/pytorch/pytorch/commit/babd6c776dfdc5db0f0f1bc69e41a5a5d76ca719</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] skip launching kernels with zero grid in AOTInductor when using backed symints (#118654)&lt;br&gt;&lt;br&gt;Like #110312 but we also run this check when backed symints are in the grid (e.g. s1 / 512)&lt;br&gt;&lt;br&gt;### Why?&lt;br&gt;&lt;br&gt;Let's say we lower a model and generate GPU kernel grid with symbolic shapes, for e.g. `s1 / 512`. If at some point later, we ran the lowered model with inputs s.t. `s1 = 0`, then we'll launch the kernel with a `0` sized grid. This surfaces as `CUDA driver error: invalid argument`.&lt;br&gt;&lt;br&gt;To avoid this, we check for a `0` sized grid whenever there's symbolic shapes which includes backed and unbacked symints.&lt;br&gt;&lt;br&gt;This adds non-zero overhead to the CPU. However, in return, we get better reliability when encountering this scenario. This scenario happened when serving an internal model.&lt;br&gt;&lt;br&gt;### Test&lt;br&gt;&lt;br&gt;```&lt;br&gt;$ python test/inductor/test_aot_inductor.py -k test_zero_grid_with_unbacked_symbols&lt;br&gt;OK (skipped=3)&lt;br&gt;&lt;br&gt;$ python test/inductor/test_aot_inductor.py -k test_zero_grid_with_backed_symbols&lt;br&gt;&lt;br&gt;# Before&lt;br&gt;Error: CUDA driver error: invalid argument&lt;br&gt;FAILED (errors=2, skipped=3)&lt;br&gt;&lt;br&gt;# Now&lt;br&gt;OK (skipped=3)&lt;br&gt;```&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118654&lt;br&gt;Approved by: https://github.com/chenyang78, https://github.com/desertfire&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/babd6c776dfdc5db0f0f1bc69e41a5a5d76ca719'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 01 Feb 2024 19:19:52 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/babd6c776dfdc5db0f0f1bc69e41a5a5d76ca719</guid></item><item><title>[inductor] Fix an internal test issue (#118903)</title><link>https://github.com/pytorch/pytorch/commit/946ea47a4fcf7883560743e3b290547dc506056a</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] Fix an internal test issue (#118903)&lt;br&gt;&lt;br&gt;Summary: test_add_complex4 that introduced in https://github.com/pytorch/pytorch/pull/117929  fails internally, because of a cpp compilation issue for cpu. Specify the right device in the test instead.&lt;br&gt;&lt;br&gt;Differential Revision: [D53333919](https://our.internmc.facebook.com/intern/diff/D53333919)&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118903&lt;br&gt;Approved by: https://github.com/clee2000&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/946ea47a4fcf7883560743e3b290547dc506056a'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 01 Feb 2024 19:18:12 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/946ea47a4fcf7883560743e3b290547dc506056a</guid></item><item><title>Revert "[inductor] make multi-kernel work with cpp-wrapper (#117813)"</title><link>https://github.com/pytorch/pytorch/commit/796278b57e95c9a6c2bdcdea413d61b31fb6344a</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Revert "[inductor] make multi-kernel work with cpp-wrapper (#117813)"&lt;br&gt;&lt;br&gt;This reverts commit 20484a193626ef72e0b3f35914f17deb2a89b8fc.&lt;br&gt;&lt;br&gt;Reverted https://github.com/pytorch/pytorch/pull/117813 on behalf of https://github.com/atalman due to broke linux-focal-rocm5.7-py3.8 tests ([comment](https://github.com/pytorch/pytorch/pull/117813#issuecomment-1922613135))&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/796278b57e95c9a6c2bdcdea413d61b31fb6344a'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 01 Feb 2024 17:19:19 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/796278b57e95c9a6c2bdcdea413d61b31fb6344a</guid></item><item><title>Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)</title><link>https://github.com/pytorch/pytorch/commit/08d90a1ea97d17b9fb233a17c605415e13497922</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)&lt;br&gt;&lt;br&gt;Info about super in dynamic classes:&lt;br&gt;https://stackoverflow.com/questions/71879642/how-to-pass-function-with-super-when-creating-class-dynamically&lt;br&gt;https://stackoverflow.com/questions/43782944/super-does-not-work-together-with-type-supertype-obj-obj-must-be-an-i&lt;br&gt;&lt;br&gt;Calling super(TestCase) actually calls TestCase's parent's functions, bypassing TestCase itself's functions&lt;br&gt;&lt;br&gt;Mainly doing this because it's making disable bot spam&lt;br&gt;&lt;br&gt;Test: checked locally and check that https://github.com/pytorch/pytorch/issues/117954 actually got skipped&lt;br&gt;&lt;br&gt;Logs for `inductor/test_torchinductor_dynamic_shapes.py::TestInductorDynamicCUDA::test_unbacked_index_select_cuda`&lt;br&gt;https://ossci-raw-job-status.s3.amazonaws.com/log/21083466405&lt;br&gt;Afaik this PR doesn't actually cause the test to fail, it just surfaces the error since the mem leak check wasn't running previously&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118586&lt;br&gt;Approved by: https://github.com/huydhn&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/08d90a1ea97d17b9fb233a17c605415e13497922'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 01 Feb 2024 16:40:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/08d90a1ea97d17b9fb233a17c605415e13497922</guid></item><item><title>[inductor] more accurate throughput calculations for kernel benchmarks (#118858)</title><link>https://github.com/pytorch/pytorch/commit/61b572ed5638dea4b47c16672eb7e7361e806bfc</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] more accurate throughput calculations for kernel benchmarks (#118858)&lt;br&gt;&lt;br&gt;Our current throughput calculations for kernel benchmarks have some issues,&lt;br&gt;particularly when we slice inputs in the kernel. In such cases, we count&lt;br&gt;the original inputs as part of the memory traffic passed across the kernel.&lt;br&gt;This is incorrect because it may result in a much larger throughput&lt;br&gt;calculation, which can even exceed the theoretical bandwidth.&lt;br&gt;&lt;br&gt;Instead, we should only count the size of the "slices" that contribute to&lt;br&gt;the actual memory traffic.&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118858&lt;br&gt;Approved by: https://github.com/jansel&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/61b572ed5638dea4b47c16672eb7e7361e806bfc'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 01 Feb 2024 13:42:14 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/61b572ed5638dea4b47c16672eb7e7361e806bfc</guid></item><item><title>[inductor] make multi-kernel work with cpp-wrapper (#117813)</title><link>https://github.com/pytorch/pytorch/commit/20484a193626ef72e0b3f35914f17deb2a89b8fc</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] make multi-kernel work with cpp-wrapper (#117813)&lt;br&gt;&lt;br&gt;Make multi-kernel work with cpp-wrapper. multi-kernel generates two equivalent variants for a reduction. At runtime the faster one is picked. But cpp-wrapper need save cubin file during codegen. They don't work with each other at the beginning.&lt;br&gt;&lt;br&gt;Thanks Jason for suggesting a neat way to integrate these two. cpp-wrapper does 2 passes codegen right now. For the first pass, we still generate multi-kernel code and run it; for the second pass, we load the cubin file for the faster kernel directly. And multi-kernel python code is not generated for the second pass since they should not be needed.&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/117813&lt;br&gt;Approved by: https://github.com/jansel&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/20484a193626ef72e0b3f35914f17deb2a89b8fc'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 01 Feb 2024 13:29:02 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/20484a193626ef72e0b3f35914f17deb2a89b8fc</guid></item><item><title>[inductor] Handle special values correctly in ir.Scan codegen (#118788)</title><link>https://github.com/pytorch/pytorch/commit/9c2b43cc5057cb840879224e3bdced58d6334c0a</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] Handle special values correctly in ir.Scan codegen (#118788)&lt;br&gt;&lt;br&gt;Special values (`NaN`/`+/-Inf`) are not correctly during codegen for `ir.Scan` nodes. This&lt;br&gt;is a fairly minor bugfix that has not come up since the only two scan&lt;br&gt;ops with lowerings use "normal" values.&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118788&lt;br&gt;Approved by: https://github.com/peterbell10&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/9c2b43cc5057cb840879224e3bdced58d6334c0a'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 01 Feb 2024 06:54:20 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9c2b43cc5057cb840879224e3bdced58d6334c0a</guid></item><item><title>[AOTInductor] Add Runtime Constant-folding for AOTInductor (#118765)</title><link>https://github.com/pytorch/pytorch/commit/2b48891e62e5c4b57c8cac92cee5eb71228a203a</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[AOTInductor] Add Runtime Constant-folding for AOTInductor (#118765)&lt;br&gt;&lt;br&gt;Summary:&lt;br&gt;Add Runtime Constant-folding for AOTInductor.&lt;br&gt;This also include the invocation of constant folding at load time.&lt;br&gt;&lt;br&gt;The constant folding lowering is a 2-step process.&lt;br&gt;First, we split the graph into 2 modules, one of it is the constant module, which doesn't depend on any input and the whole module could be inferred (constant-folded) one-time and be reused. The constant module, is lowered, and being codegen-ed as usual and cached (let's call this constant code). The constant code reuses the whole lowering/profiling/etc. process, only difference is that we do not generate any headers or initialization for the constant code.&lt;br&gt;Second, after handling the constant module, we take care of the main module (which is the part that would depend on the user input.) For the main module, we take in one additional component, the constant code, compare with a normal lowering. Addition step we do here is that, we inject the constant code into the codegen-ed main module, and create the caller for the main module to consume the result of the constant module.&lt;br&gt;&lt;br&gt;Test Plan: Unit tests included in commit.&lt;br&gt;&lt;br&gt;Differential Revision: D53274382&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118765&lt;br&gt;Approved by: https://github.com/chenyang78&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/2b48891e62e5c4b57c8cac92cee5eb71228a203a'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Wed, 31 Jan 2024 20:54:25 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2b48891e62e5c4b57c8cac92cee5eb71228a203a</guid></item><item><title>[Inductor] Skip triton templates for mixedmm on SM70- (#118591)</title><link>https://github.com/pytorch/pytorch/commit/6c67f3333a539e8f29515375a87612897214f8f2</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[Inductor] Skip triton templates for mixedmm on SM70- (#118591)&lt;br&gt;&lt;br&gt;As it results in numerical errors, see https://github.com/pytorch/pytorch/issues/117144&lt;br&gt;&lt;br&gt;Fixes https://github.com/pytorch/pytorch/issues/117144&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118591&lt;br&gt;Approved by: https://github.com/jansel&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/6c67f3333a539e8f29515375a87612897214f8f2'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Wed, 31 Jan 2024 15:30:45 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6c67f3333a539e8f29515375a87612897214f8f2</guid></item><item><title>Revert "Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)"</title><link>https://github.com/pytorch/pytorch/commit/483001e8468209911292aa7f23c2a25fbfb6e31b</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Revert "Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)"&lt;br&gt;&lt;br&gt;This reverts commit f2682e75e6fd735c4a84afe59eafd541f7643f4a.&lt;br&gt;&lt;br&gt;Reverted https://github.com/pytorch/pytorch/pull/118586 on behalf of https://github.com/atalman due to Broke slow tests ([comment](https://github.com/pytorch/pytorch/pull/118586#issuecomment-1919810802))&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/483001e8468209911292aa7f23c2a25fbfb6e31b'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Wed, 31 Jan 2024 11:44:29 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/483001e8468209911292aa7f23c2a25fbfb6e31b</guid></item><item><title>[AOTI] Support _embedding_bag in C shim (#118706)</title><link>https://github.com/pytorch/pytorch/commit/1128cf96f078a76f27b038d7adbc2b72fe8927e2</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[AOTI] Support _embedding_bag in C shim (#118706)&lt;br&gt;&lt;br&gt;Summary: At some point I will stop manually adding ops to C shim, but use torchgen to generate those code. For the near term, I need to add a few more in order to switch the AOTInductor dashboard run.&lt;br&gt;&lt;br&gt;Differential Revision: [D53249074](https://our.internmc.facebook.com/intern/diff/D53249074)&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118706&lt;br&gt;Approved by: https://github.com/frank-wei, https://github.com/aakhundov&lt;br&gt;ghstack dependencies: #118704, #118705&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/1128cf96f078a76f27b038d7adbc2b72fe8927e2'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Wed, 31 Jan 2024 07:02:40 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1128cf96f078a76f27b038d7adbc2b72fe8927e2</guid></item><item><title>[inductor] Refactor ir.ComplexView (#118704)</title><link>https://github.com/pytorch/pytorch/commit/dd52939438834545632ccc9e986029d63f68e064</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] Refactor ir.ComplexView (#118704)&lt;br&gt;&lt;br&gt;Summary: Make ir.ComplexView a subclass of ir.FallbackKernel, to unify the codegen logic&lt;br&gt;&lt;br&gt;Differential Revision: [D53248972](https://our.internmc.facebook.com/intern/diff/D53248972)&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118704&lt;br&gt;Approved by: https://github.com/frank-wei&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/dd52939438834545632ccc9e986029d63f68e064'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Wed, 31 Jan 2024 06:42:29 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dd52939438834545632ccc9e986029d63f68e064</guid></item><item><title>[Cutlass 3.3.0 submodule upgrade] (#118629)</title><link>https://github.com/pytorch/pytorch/commit/35f3ccffd4dd836d71b1db6d560800597979ec5b</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[Cutlass 3.3.0 submodule upgrade] (#118629)&lt;br&gt;&lt;br&gt;Cutlass 3.3 offers the following improvements:&lt;br&gt;&lt;br&gt;Adds support for mixed precision GEMMs On Hopper and Ampere Adds support for &lt; 16B aligned GEMMs on Hopper&lt;br&gt;Enhancements to EVT&lt;br&gt;Enhancements to Python interface&lt;br&gt;Enhancements to Sub-byte type handling in CuTe&lt;br&gt;Several other bug-fixes and performance improvements. minor doc update&lt;br&gt;Test Plan:&lt;br&gt;&lt;br&gt;CI ( ciflow/trunk, ciflow/inductor )&lt;br&gt;pytest test/inductor/test_max_autotune.py&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118629&lt;br&gt;Approved by: https://github.com/drisspg, https://github.com/Skylion007, https://github.com/khabinov&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/35f3ccffd4dd836d71b1db6d560800597979ec5b'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Wed, 31 Jan 2024 05:53:58 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/35f3ccffd4dd836d71b1db6d560800597979ec5b</guid></item><item><title>Reland: [aotinductor] Replicate split_cat from torch IR to predispatch IR" (#118590)</title><link>https://github.com/pytorch/pytorch/commit/6fa162e68148c10626bc824bc3eb31d56db4c2d8</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Reland: [aotinductor] Replicate split_cat from torch IR to predispatch IR" (#118590)&lt;br&gt;&lt;br&gt;Summary:&lt;br&gt;This is part the pass migration efforts. The final target is removing the acc tracer in AOTI.&lt;br&gt;In this diff, I did a few things:&lt;br&gt;1. copy and modify the `fx_passes/split_cat.py` passes based on predispatch IR.&lt;br&gt;2. verify the correctness by copying the `test_split_cat_fx_passes.py` and create a new file `test_split_cat_fx_passes_aten_fb.py` which is executed in AOTI and checked the counters&lt;br&gt;3. create a util function to execute the pass and compare the before/after graph to give user more information like pass effect and time spent. It will create logs like&lt;br&gt;```&lt;br&gt;[2024-01-25 20:26:48,997] torch._inductor.utils: [INFO] [Pre grad(predispatch IR)]Apply split_cat, index: 0, save before/after graph to /tmp/tmpvlpwrklp, graph before/after are the same = False, time elapsed = 0:00:00.001585&lt;br&gt;[2024-01-25 20:26:49,000] torch._inductor.utils: [INFO] [Pre grad(predispatch IR)]Apply split_cat, index: 1, save before/after graph to /tmp/tmpz_onjfeu, graph before/after are the same = False, time elapsed = 0:00:00.001873&lt;br&gt;[2024-01-25 20:26:49,002] torch._inductor.utils: [INFO] [Pre grad(predispatch IR)]Apply split_cat, index: 2, save before/after graph to /tmp/tmpgkck8yko, graph before/after are the same = True, time elapsed = 0:00:00.000269&lt;br&gt;[2024-01-25 20:26:49,007] torch._inductor.utils: [INFO] [Pre grad(predispatch IR)]Apply split_cat, index: 3, save before/after graph to /tmp/tmpquenq06y, graph before/after are the same = False, time elapsed = 0:00:00.003621&lt;br&gt;[2024-01-25 20:26:49,009] torch._inductor.utils: [INFO] [Pre grad(predispatch IR)]Apply split_cat, index: 4, save before/after graph to /tmp/tmpi8fia0dv, graph before/after are the same = True, time elapsed = 0:00:00.000190&lt;br&gt;```&lt;br&gt;&lt;br&gt;Differential Revision: D53171027&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118590&lt;br&gt;Approved by: https://github.com/kflu, https://github.com/khabinov, https://github.com/chenyang78&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/6fa162e68148c10626bc824bc3eb31d56db4c2d8'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Tue, 30 Jan 2024 16:09:46 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6fa162e68148c10626bc824bc3eb31d56db4c2d8</guid></item><item><title>Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)</title><link>https://github.com/pytorch/pytorch/commit/f2682e75e6fd735c4a84afe59eafd541f7643f4a</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)&lt;br&gt;&lt;br&gt;Info about super in dynamic classes:&lt;br&gt;https://stackoverflow.com/questions/71879642/how-to-pass-function-with-super-when-creating-class-dynamically&lt;br&gt;https://stackoverflow.com/questions/43782944/super-does-not-work-together-with-type-supertype-obj-obj-must-be-an-i&lt;br&gt;&lt;br&gt;Calling super(TestCase) actually calls TestCase's parent's functions, bypassing TestCase itself's functions&lt;br&gt;&lt;br&gt;Mainly doing this because it's making disable bot spam&lt;br&gt;&lt;br&gt;Test: checked locally and check that https://github.com/pytorch/pytorch/issues/117954 actually got skipped&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118586&lt;br&gt;Approved by: https://github.com/huydhn&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/f2682e75e6fd735c4a84afe59eafd541f7643f4a'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Tue, 30 Jan 2024 13:34:05 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f2682e75e6fd735c4a84afe59eafd541f7643f4a</guid></item><item><title>[inductor] Use at::detail::empty_strided_* in cpp_wraper mode (#118490)</title><link>https://github.com/pytorch/pytorch/commit/e332653eb38b6f97e8088b4d3adc5b112fe8f761</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] Use at::detail::empty_strided_* in cpp_wraper mode (#118490)&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118490&lt;br&gt;Approved by: https://github.com/desertfire&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/e332653eb38b6f97e8088b4d3adc5b112fe8f761'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Tue, 30 Jan 2024 13:03:19 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e332653eb38b6f97e8088b4d3adc5b112fe8f761</guid></item><item><title>[inductor][cpp] support scalar value in vec reduction (#118511)</title><link>https://github.com/pytorch/pytorch/commit/e5bb527d3e91b244404e142dfac8e9e55e02aa81</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor][cpp] support scalar value in vec reduction (#118511)&lt;br&gt;&lt;br&gt;Fix https://github.com/pytorch/pytorch/issues/118379&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118511&lt;br&gt;Approved by: https://github.com/leslie-fang-intel, https://github.com/lezcano, https://github.com/jansel&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/e5bb527d3e91b244404e142dfac8e9e55e02aa81'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Tue, 30 Jan 2024 05:07:43 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e5bb527d3e91b244404e142dfac8e9e55e02aa81</guid></item><item><title>[inductor] Fix codegen bug with Native Triton kernels with  ReinterpretView args (#118569)</title><link>https://github.com/pytorch/pytorch/commit/8be6dee14b76f63dea1e9329d6cd0fdc9552a2d6</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] Fix codegen bug with Native Triton kernels with  ReinterpretView args (#118569)&lt;br&gt;&lt;br&gt;Summary:&lt;br&gt;### Context&lt;br&gt;&lt;br&gt;It's possible for the args of a user-defined Triton Kernel to be codegen-ed twiced. But this only happens if the arg is a `ReinterpretView`.&lt;br&gt;* First via `arg.codegen_reference()` in `define_user_defined_triton_kernel()`&lt;br&gt;* Second in `self.codegen_kwargs()`.&lt;br&gt;&lt;br&gt;When using `abi_compatible=True`, the duplicate codegen will look like the code below. The issue in the code is that one of the Tensors, internal to the graph, isn't properly freed. This scenario was eventually exposed as a memory leak when we re-ran an AOTInductor model many times and observed `memory.used` increase after each iteration.&lt;br&gt;```&lt;br&gt;auto tmp_tensor_handle_0 = reinterpret_tensor_wrapper(buf1, 2, int_array_0, int_array_1, 0L);&lt;br&gt;auto tmp_tensor_handle_1 = reinterpret_tensor_wrapper(buf1, 2, int_array_0, int_array_1, 0L);&lt;br&gt;...&lt;br&gt;// There's no wrap_with_raii_handle_if_needed() for tmp_tensor_handle_0.&lt;br&gt;// And there's no reference to tmp_tensor_handle_0.&lt;br&gt;// Thus, tmp_tensor_handle_0 is left as an AtenTensorHandle which isn't&lt;br&gt;// automatically cleaned-up like RAIIAtenTensorHandle&lt;br&gt;CUdeviceptr var_6;&lt;br&gt;aoti_torch_get_data_ptr(wrap_with_raii_handle_if_needed(tmp_tensor_handle_1), reinterpret_cast&lt;void**&gt;(&amp;var_6));&lt;br&gt;void* kernel_args_var_2[] = {..., &amp;var_6, ...};&lt;br&gt;launchKernel(kernels.add_kernel_0, ..., kernel_args_var_2);&lt;br&gt;```&lt;br&gt;&lt;br&gt;### Solution&lt;br&gt;We just need the arg's buffer name when creating the `TensorArg` in `define_user_defined_triton_kernel()`. Thus, just return the buffer's name and avoid any potential side-effects with `arg.codegen_reference()`.&lt;br&gt;&lt;br&gt;Test Plan:&lt;br&gt;### Inspect device memory allocated&lt;br&gt;```&lt;br&gt;# Before diff&lt;br&gt;0 device memory 2048&lt;br&gt;1 device memory 2560&lt;br&gt;2 device memory 3072&lt;br&gt;3 device memory 3584&lt;br&gt;4 device memory 4096&lt;br&gt;5 device memory 4608&lt;br&gt;&lt;br&gt;# With diff (memory usage doesn't grow)&lt;br&gt;0 device memory 1536&lt;br&gt;1 device memory 1536&lt;br&gt;2 device memory 1536&lt;br&gt;3 device memory 1536&lt;br&gt;4 device memory 1536&lt;br&gt;5 device memory 1536&lt;br&gt;```&lt;br&gt;&lt;br&gt;Reviewed By: jingsh, tissue3&lt;br&gt;&lt;br&gt;Differential Revision: D53190934&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118569&lt;br&gt;Approved by: https://github.com/oulgen&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/8be6dee14b76f63dea1e9329d6cd0fdc9552a2d6'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Mon, 29 Jan 2024 21:19:32 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8be6dee14b76f63dea1e9329d6cd0fdc9552a2d6</guid></item><item><title>Reland PR117393 [inductor/fb] log config dict when compilation finishes (#118552)</title><link>https://github.com/pytorch/pytorch/commit/5dfcf07449c9ffc7a2277c9782c88444ca11a77c</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Reland PR117393 [inductor/fb] log config dict when compilation finishes (#118552)&lt;br&gt;&lt;br&gt;Summary: Reverted due to merge conflict&lt;br&gt;&lt;br&gt;Differential Revision: D53188124&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118552&lt;br&gt;Approved by: https://github.com/mengluy0125&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/5dfcf07449c9ffc7a2277c9782c88444ca11a77c'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Mon, 29 Jan 2024 20:34:22 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5dfcf07449c9ffc7a2277c9782c88444ca11a77c</guid></item><item><title>[ez][inductor] fix a typo in should_pad_bench (#118598)</title><link>https://github.com/pytorch/pytorch/commit/0d47f6a44f4a72324ee096c5bbdb107b379a55c2</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[ez][inductor] fix a typo in should_pad_bench (#118598)&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118598&lt;br&gt;Approved by: https://github.com/eellison&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/0d47f6a44f4a72324ee096c5bbdb107b379a55c2'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Mon, 29 Jan 2024 19:49:44 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0d47f6a44f4a72324ee096c5bbdb107b379a55c2</guid></item><item><title>[inductor] Remove ROCm xfail on test_cum{sum,prod}_zero_dim (#118558)</title><link>https://github.com/pytorch/pytorch/commit/f99adbb4ec7a738684925d111f81f6b523cc83d0</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] Remove ROCm xfail on test_cum{sum,prod}_zero_dim (#118558)&lt;br&gt;&lt;br&gt;Fixes #118540, fixes #118541&lt;br&gt;&lt;br&gt;Since the zero-dim case reduces to a pointwise operation, we don't fallback on&lt;br&gt;ROCm.&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118558&lt;br&gt;Approved by: https://github.com/malfet&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/f99adbb4ec7a738684925d111f81f6b523cc83d0'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Mon, 29 Jan 2024 12:23:40 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f99adbb4ec7a738684925d111f81f6b523cc83d0</guid></item><item><title>[inductor][cpp] enable vectorization with constant bool (#118380)</title><link>https://github.com/pytorch/pytorch/commit/04c1df651aa58bea50977f4efcf19b09ce27cefd</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor][cpp] enable vectorization with constant bool (#118380)&lt;br&gt;&lt;br&gt;Related model DebertaForQuestionAnswering etc. For DebertaForQuestionAnswering, single thread, measured on ICX:&lt;br&gt;Before: 0.990x, After: 1.043x&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118380&lt;br&gt;Approved by: https://github.com/leslie-fang-intel, https://github.com/jansel&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/04c1df651aa58bea50977f4efcf19b09ce27cefd'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Mon, 29 Jan 2024 05:31:22 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/04c1df651aa58bea50977f4efcf19b09ce27cefd</guid></item><item><title>[Inductor] Fix Argmax codegen with Nan input (#118358)</title><link>https://github.com/pytorch/pytorch/commit/ee3dfbbe470822a50516310384ecce071ec79f7c</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[Inductor] Fix Argmax codegen with Nan input (#118358)&lt;br&gt;&lt;br&gt;**Summary**&lt;br&gt;Fix issue https://github.com/pytorch/pytorch/issues/118266, current `torch.argmax` and `torch.argmin` has different return values with eager and Inductor cpp backend when inputs has `Nan` value. Align cpp backend results to eager by reusing the compare function.&lt;br&gt;&lt;br&gt;**Test Plan**&lt;br&gt;```&lt;br&gt;python -u -m pytest -s -v test_cpu_repro.py -k test_argmin_cpu_only&lt;br&gt;python -u -m pytest -s -v test_cpu_repro.py -k test_argmax_argmin_with_nan_value&lt;br&gt;```&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118358&lt;br&gt;Approved by: https://github.com/lezcano, https://github.com/jgong5, https://github.com/jansel&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/ee3dfbbe470822a50516310384ecce071ec79f7c'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Mon, 29 Jan 2024 01:09:46 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ee3dfbbe470822a50516310384ecce071ec79f7c</guid></item><item><title>Add some type annotations to torch._inductor.codegen.wrapper (#118491)</title><link>https://github.com/pytorch/pytorch/commit/2951bbf0f72dcbf8841dafbd6e764c07eb72edc6</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Add some type annotations to torch._inductor.codegen.wrapper (#118491)&lt;br&gt;&lt;br&gt;Signed-off-by: Edward Z. Yang &lt;ezyang@meta.com&gt;&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118491&lt;br&gt;Approved by: https://github.com/Skylion007&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/2951bbf0f72dcbf8841dafbd6e764c07eb72edc6'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Sun, 28 Jan 2024 22:17:27 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2951bbf0f72dcbf8841dafbd6e764c07eb72edc6</guid></item><item><title>Unify MYPYINDUCTOR and MYPY (#118432)</title><link>https://github.com/pytorch/pytorch/commit/d03173e88cf43544fbeb458e1b8d7122038bef5b</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Unify MYPYINDUCTOR and MYPY (#118432)&lt;br&gt;&lt;br&gt;The original motivation for MYPYINDUCTOR was a faster type checking configuration that only checked a subset of files. With the removal of `follow_imports = ignore`, we are now able to use dmypy to do fast incremental typechecking, eliminating the need for this.&lt;br&gt;&lt;br&gt;Perhaps erroneously, when I tee'ed up this PR I elected to delete the `follow_imports = skip` designations in the mypy-inductor.ini. This lead to a number of extra type error suppressions that I manually edited. You will need to review.&lt;br&gt;&lt;br&gt;Signed-off-by: Edward Z. Yang &lt;ezyang@meta.com&gt;&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118432&lt;br&gt;Approved by: https://github.com/Skylion007&lt;br&gt;ghstack dependencies: #118414, #118418&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/d03173e88cf43544fbeb458e1b8d7122038bef5b'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Sat, 27 Jan 2024 09:23:20 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d03173e88cf43544fbeb458e1b8d7122038bef5b</guid></item><item><title>Replace follow_imports = silent with normal (#118414)</title><link>https://github.com/pytorch/pytorch/commit/9bce208dfbdb71e38f9e9ee38a07d43645ffb82a</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Replace follow_imports = silent with normal (#118414)&lt;br&gt;&lt;br&gt;This is a lot of files changed! Don't panic! Here's how it works:&lt;br&gt;&lt;br&gt;* Previously, we set `follow_imports = silent` for our mypy.ini configuration. Per https://mypy.readthedocs.io/en/stable/running_mypy.html#follow-imports, what this does is whenever we have an import to a module which is not listed as a file to be typechecked in mypy, we typecheck it as normal but suppress all errors that occurred in that file.&lt;br&gt;* When mypy is run inside lintrunner, the list of files is precisely the files covered by the glob in lintrunner.toml, but with files in excludes excluded.&lt;br&gt;* The top-level directive `# mypy: ignore-errors` instructs mypy to typecheck the file as normal, but ignore all errors.&lt;br&gt;* Therefore, it should be equivalent to set `follow_imports = normal`, if we put `# mypy: ignore-errors` on all files that were previously excluded from the file list.&lt;br&gt;* Having done this, we can remove the exclude list from .lintrunner.toml, since excluding a file from typechecking is baked into the files themselves.&lt;br&gt;* torch/_dynamo and torch/_inductor were previously in the exclude list, because they were covered by MYPYINDUCTOR. It is not OK to mark these as `# mypy: ignore-errors` as this will impede typechecking on the alternate configuration. So they are temporarily being checked twice, but I am suppressing the errors in these files as the configurations are not quite the same. I plan to unify the configurations so this is only a temporary state.&lt;br&gt;* There were some straggler type errors after these changes somehow, so I fixed them as needed. There weren't that many.&lt;br&gt;&lt;br&gt;In the future, to start type checking a file, just remove the ignore-errors directive from the top of the file.&lt;br&gt;&lt;br&gt;The codemod was done with this script authored by GPT-4:&lt;br&gt;&lt;br&gt;```&lt;br&gt;import glob&lt;br&gt;&lt;br&gt;exclude_patterns = [&lt;br&gt;    ...&lt;br&gt;]&lt;br&gt;&lt;br&gt;for pattern in exclude_patterns:&lt;br&gt;    for filepath in glob.glob(pattern, recursive=True):&lt;br&gt;        if filepath.endswith('.py'):&lt;br&gt;            with open(filepath, 'r+') as f:&lt;br&gt;                content = f.read()&lt;br&gt;                f.seek(0, 0)&lt;br&gt;                f.write('# mypy: ignore-errors\n\n' + content)&lt;br&gt;```&lt;br&gt;&lt;br&gt;Signed-off-by: Edward Z. Yang &lt;ezyang@meta.com&gt;&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118414&lt;br&gt;Approved by: https://github.com/thiagocrepaldi, https://github.com/albanD&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/9bce208dfbdb71e38f9e9ee38a07d43645ffb82a'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Fri, 26 Jan 2024 18:44:11 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9bce208dfbdb71e38f9e9ee38a07d43645ffb82a</guid></item><item><title>[inductor] Handle cum{sum,prod} on zero-dim tensors (#117990)</title><link>https://github.com/pytorch/pytorch/commit/f129e3fe034c0052d1d45856ac10fc4b3a1e9700</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] Handle cum{sum,prod} on zero-dim tensors (#117990)&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/117990&lt;br&gt;Approved by: https://github.com/lezcano&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/f129e3fe034c0052d1d45856ac10fc4b3a1e9700'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Fri, 26 Jan 2024 14:21:42 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f129e3fe034c0052d1d45856ac10fc4b3a1e9700</guid></item><item><title>Fix several bugs related to unbacked SymInt codegen in inductor (#117862)</title><link>https://github.com/pytorch/pytorch/commit/96d94f574e32c63421dc7badb0a5c42bb29324c7</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Fix several bugs related to unbacked SymInt codegen in inductor (#117862)&lt;br&gt;&lt;br&gt;Let me tell you, this was a *journey.*&lt;br&gt;&lt;br&gt;* When we repropagate through FX interpreter in AOTAutograd, this will reallocate unbacked SymInts. We can eliminate all of these fresh allocations by appropriately asserting equalities on them setting up replacements. See also https://github.com/pytorch/pytorch/issues/111950&lt;br&gt;* The `inner_fn` of Loops can contain references to unbacked SymInts. We must collect them to prevent DCE.&lt;br&gt;* Export naughtily accessed `_expr` when it should have accessed `expr` on SymNode. Fixed two sites of this.&lt;br&gt;&lt;br&gt;Signed-off-by: Edward Z. Yang &lt;ezyang@meta.com&gt;&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/117862&lt;br&gt;Approved by: https://github.com/bdhirsh&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/96d94f574e32c63421dc7badb0a5c42bb29324c7'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Fri, 26 Jan 2024 10:08:03 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/96d94f574e32c63421dc7badb0a5c42bb29324c7</guid></item><item><title>[Inductor] Add Thread Number Checker in scatter_reduce_ fallback for CPP backend (#118278)</title><link>https://github.com/pytorch/pytorch/commit/b66c4eda61f4e6be536a096c3fcdf464a3608aef</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[Inductor] Add Thread Number Checker in scatter_reduce_ fallback for CPP backend (#118278)&lt;br&gt;&lt;br&gt;**Summary**&lt;br&gt;Follow up of https://github.com/pytorch/pytorch/pull/108220 which improves performance of `basic_gnn_gin`, `basic_gnn_sage` and `basic_gnn_gcn` in multi thread test cases. However, it causes performance regression of these 3 models in single thread test case as reported in https://github.com/pytorch/pytorch/issues/117740. Fix the single thread issues in this PR by adding the thread number check to decide whether fallback `scatter_reduce_` or not.&lt;br&gt;&lt;br&gt;**Test Plan**&lt;br&gt;```&lt;br&gt;python -u -m pytest -s -v test_cpu_repro.py -k test_scatter_using_atomic_add&lt;br&gt;```&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118278&lt;br&gt;Approved by: https://github.com/jansel, https://github.com/jgong5&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/b66c4eda61f4e6be536a096c3fcdf464a3608aef'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Fri, 26 Jan 2024 04:43:25 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b66c4eda61f4e6be536a096c3fcdf464a3608aef</guid></item><item><title>[c10d_functional] fix an issue where mutation on views fails in inductor (#118333)</title><link>https://github.com/pytorch/pytorch/commit/0857a3a753b5d04e01a57c313dec7e0b53f1bf2d</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[c10d_functional] fix an issue where mutation on views fails in inductor (#118333)&lt;br&gt;&lt;br&gt;`_CollectiveKernel.create_inplace` expresses mutation with the newly introduced `MutationOutput` which requires the `layout` of the input. Currently, there's a bug where if the input is a view, `inp.layout` fails. This PR fixes the issue by unwrapping the input if it's a view.&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118333&lt;br&gt;Approved by: https://github.com/wanchaol&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/0857a3a753b5d04e01a57c313dec7e0b53f1bf2d'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Fri, 26 Jan 2024 03:13:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0857a3a753b5d04e01a57c313dec7e0b53f1bf2d</guid></item><item><title>fix key error in pre_grad fx_passes_numeric_check (#118325)</title><link>https://github.com/pytorch/pytorch/commit/4d0b471389dd38865a2ff214ba7df185d02b18b1</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;fix key error in pre_grad fx_passes_numeric_check (#118325)&lt;br&gt;&lt;br&gt;Summary:&lt;br&gt;```&lt;br&gt;I0125 121749.865 pyper_config_utils.py:8225] torchdynamo pyper config = TorchDynamoConfig(backend='inductor', optimize_ddp=False, log_compile_graph=False, inductor_config=TorchInductorConfig(enable_cudagraph=False, max_autotune=False, max_autotune_pointwise=True, max_autotune_gemm=False, search_autotune_cache=False, autotune_in_subproc=False, aggressive_fusion=False, shape_padding=True, permute_fusion=False, epilogue_fusion_first=False, debug=True, triton=None, trace_enabled=False, log_kernel_source=False, split_cat_fx_passes=False, group_fusion=False, batch_fusion=False, coordinate_descent_tuning=False, coordinate_descent_check_all_directions=False, coordinate_descent_search_radius=1, layout_optimization=True, pre_grad_fusion_options={}, post_grad_fusion_options={}, max_pointwise_cat_inputs=4, fx_passes_numeric_check={}), automatic_dynamic_shapes=True)&lt;br&gt;```&lt;br&gt;In trainer&lt;br&gt;```&lt;br&gt;I0125 12:58:51.832000 4011.139732263132160 torchdynamo_wrapper.py:291  trainer:0:1 ] [pt2] creating torchdynamo backend wrapper with settings TorchDynamoConfig(backend='inductor', optimize_ddp=False, log_compile_graph=False, inductor_config=TorchInductorConfig(enable_cudagraph=False, max_autotune=False, max_autotune_pointwise=True, max_autotune_gemm=False, search_autotune_cache=False, autotune_in_subproc=False, aggressive_fusion=False, shape_padding=True, permute_fusion=False, epilogue_fusion_first=False, debug=True, triton=None, trace_enabled=False, log_kernel_source=False, split_cat_fx_passes=False, group_fusion=False, batch_fusion=False, coordinate_descent_tuning=False, coordinate_descent_check_all_directions=False, coordinate_descent_search_radius=1, layout_optimization=True, pre_grad_fusion_options={}, post_grad_fusion_options={}, max_pointwise_cat_inputs=4, fx_passes_numeric_check={}), automatic_dynamic_shapes=True) #ai_training_job_id="febe34d9-b2fb-493e-a5cc-6a0b1dc85ad4" #ai_training_local_rank="1" #ai_training_role_rank="1" #mast_job_attempt="2" #mast_job_name="f525072920-TrainingApplication"&lt;br&gt;...&lt;br&gt;if config.fx_passes_numeric_check["pre_grad"]:&lt;br&gt;```&lt;br&gt;&lt;br&gt;https://www.internalfb.com/diff/D52826442?dst_version_fbid=1115735309429172&amp;transaction_fbid=682438900759710&lt;br&gt;&lt;br&gt;https://www.internalfb.com/diff/D51838043?dst_version_fbid=336373395892373&amp;transaction_fbid=349901787874069&lt;br&gt;&lt;br&gt;This diff first fixes the key error to restore broken tests.  Its pyper changes can be addressed later.&lt;br&gt;&lt;br&gt;https://www.internalfb.com/code/fbsource/[72c19313ed73]/fbcode/caffe2/torch/_inductor/config.py?lines=142-147&lt;br&gt;&lt;br&gt;Test Plan: buck2 run //caffe2/torch/fb/training_toolkit/integration_tests/training_lifecycle/cogwheel_tests/pyper_release_v2:cogwheel_smallworld_mimo_cmf_deterministic_ne_pt2_training_platform__canary_offline_training-launcher -- --build-fbpkg --run-disabled --tests test&lt;br&gt;&lt;br&gt;Reviewed By: yusuo&lt;br&gt;&lt;br&gt;Differential Revision: D53102344&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118325&lt;br&gt;Approved by: https://github.com/mengluy0125&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/4d0b471389dd38865a2ff214ba7df185d02b18b1'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Fri, 26 Jan 2024 03:02:12 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4d0b471389dd38865a2ff214ba7df185d02b18b1</guid></item><item><title>[Inductor] Use sleef implementation for CPP backend acosh codegen (#118350)</title><link>https://github.com/pytorch/pytorch/commit/8dd1be49b7c17223ed6de18fac4471abc6735aae</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[Inductor] Use sleef implementation for CPP backend acosh codegen (#118350)&lt;br&gt;&lt;br&gt;**Summary**&lt;br&gt;Fix https://github.com/pytorch/pytorch/issues/118267. Current cpp backend using `f"({x} + ({x}*{x} - {vec_one}).sqrt()).log()"` to calculate `acosh`, the issue happens when input is a large negative value like `-910685.8125`. In this case, `(x*x - 1).sqrt() + x` equals to 0, and `0.log()` returns `-inf`. However, based on the document: https://pytorch.org/docs/stable/generated/torch.acosh.html, negative inputs should returns `Nan`. Using acosh sleef implementation to fix this issue.&lt;br&gt;&lt;br&gt;**Test Plan**&lt;br&gt;```&lt;br&gt;python -u -m pytest -s -v test_cpu_repro.py -k test_acosh_with_negative_large_input&lt;br&gt;```&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118350&lt;br&gt;Approved by: https://github.com/jgong5, https://github.com/lezcano&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/8dd1be49b7c17223ed6de18fac4471abc6735aae'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Fri, 26 Jan 2024 02:19:40 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8dd1be49b7c17223ed6de18fac4471abc6735aae</guid></item><item><title>Fix mergeability check for ghstack PRs (#118258)</title><link>https://github.com/pytorch/pytorch/commit/b599f5608c309331df7a599c3b7f21a74375b2fa</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Fix mergeability check for ghstack PRs (#118258)&lt;br&gt;&lt;br&gt;# Changes&lt;br&gt;* introduce `--check-mergeability` trymerge flag that attempts to merge PR locally, using the same merge logic as the mergebot, but requires just a read-only `GITHUB_TOKEN` and git repo.&lt;br&gt;* change mergeability workflow to utilize the new --check-mergeability logic&lt;br&gt;&lt;br&gt;# Alternatives considered&lt;br&gt;&lt;br&gt;1.&lt;br&gt;&gt; Rewrite `https://github.com/pytorch/test-infra/actions/workflows/pr-dependencies-check.yml` to correctly support partially merged ghstacks.&lt;br&gt;&lt;br&gt;That would be a slightly better approach, but ROI is lower, as it requires reimplementing trymerge logic and additional effort to consolidate the codebase (trymerge lives in pytorch repo).&lt;br&gt;&lt;br&gt;`pr-dependencies-check.yml` still produces human-readable results for partially merged ghstack prs (even if it falsely reports them as non-mergeable).&lt;br&gt;&lt;br&gt;2.&lt;br&gt;&lt;br&gt;&gt; Instead of introducing new trymerge flag, use existing flags, including `--dry-run`.&lt;br&gt;&lt;br&gt;That didn't work, as no combination of existing flags skips the rule checks and ROCKSET lookups.&lt;br&gt;&lt;br&gt;# Testing&lt;br&gt;&lt;br&gt;1. Manual testing  `trymerge.py --check-mergeability`  on the regular and ghstack PRs:&lt;br&gt;&lt;br&gt;```&lt;br&gt;export GITHUB_TOKEN=&lt;br&gt;export GIT_REPO_DIR=`pwd`&lt;br&gt;export GITHUB_REPOSITORY=pytorch/pytorch&lt;br&gt;export GIT_REMOTE_URL=https://github.com/pytorch/pytorch&lt;br&gt;&lt;br&gt;# Test 1 (2 prs, 1 is closed)&lt;br&gt;python3 ../pytorch/.github/scripts/trymerge.py --check-mergeability  117862&lt;br&gt;Skipping 1 of 2 PR (#117859) as its already been merged&lt;br&gt;&lt;br&gt;echo $?&lt;br&gt;0&lt;br&gt;&lt;br&gt;# Test 2 (3 prs, 1 is closed)&lt;br&gt;python3 ../pytorch/.github/scripts/trymerge.py --check-mergeability  118125&lt;br&gt;Skipping 1 of 3 PR (#117859) as its already been merged&lt;br&gt;&lt;br&gt;echo $?&lt;br&gt;0&lt;br&gt;&lt;br&gt;# Test 3 (3 prs, intentional conflicts introduced into `main`):&lt;br&gt;&lt;br&gt;python3 ../pytorch/.github/scripts/trymerge.py --check-mergeability  118125&lt;br&gt;Skipping 1 of 3 PR (#117859) as its already been merged&lt;br&gt;stdout:&lt;br&gt;Auto-merging torch/_inductor/ir.py&lt;br&gt;Auto-merging torch/_inductor/lowering.py&lt;br&gt;CONFLICT (content): Merge conflict in torch/_inductor/lowering.py&lt;br&gt;error: could not apply 66ba5b8792f... Realize inputs to DynamicScalar before unwrapping&lt;br&gt;...&lt;br&gt;RuntimeError: Command `git -C /Users/ivanzaitsev/pytorch2 cherry-pick -x 66ba5b8792fa076c4e512d920651e5b6b7e466f4` returned non-zero exit code 1&lt;br&gt;```&lt;br&gt;&lt;br&gt;2.  Workflow run:&lt;br&gt;https://github.com/pytorch/pytorch/actions/runs/7660736172/job/20878651852?pr=118258&lt;br&gt;&lt;br&gt;&lt;img width="516" alt="image" src="https://github.com/pytorch/pytorch/assets/108101595/28fbf0d2-ac2a-4518-b41d-b32b41373747"&gt;&lt;br&gt;&lt;img width="621" alt="image" src="https://github.com/pytorch/pytorch/assets/108101595/ddbf8566-a417-43ec-9d0e-f623f4a71313"&gt;&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118258&lt;br&gt;Approved by: https://github.com/PaliC, https://github.com/huydhn&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/b599f5608c309331df7a599c3b7f21a74375b2fa'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 25 Jan 2024 19:15:56 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b599f5608c309331df7a599c3b7f21a74375b2fa</guid></item><item><title>[inductor][easy] dump triton kernel names in the log (#118313)</title><link>https://github.com/pytorch/pytorch/commit/3cdd4e236e2c58deeb39b620b5aa1c2bf70ec602</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor][easy] dump triton kernel names in the log (#118313)&lt;br&gt;&lt;br&gt;This may help debugging.&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118313&lt;br&gt;Approved by: https://github.com/desertfire&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/3cdd4e236e2c58deeb39b620b5aa1c2bf70ec602'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 25 Jan 2024 18:00:04 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3cdd4e236e2c58deeb39b620b5aa1c2bf70ec602</guid></item><item><title>[inductor] Slightly faster memory allocation on CUDA (#118255)</title><link>https://github.com/pytorch/pytorch/commit/2de24c11f66e0a11ccd392983f865e1b216281e6</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] Slightly faster memory allocation on CUDA (#118255)&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118255&lt;br&gt;Approved by: https://github.com/peterbell10&lt;br&gt;ghstack dependencies: #118065, #118070, #118171&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/2de24c11f66e0a11ccd392983f865e1b216281e6'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 25 Jan 2024 12:49:14 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2de24c11f66e0a11ccd392983f865e1b216281e6</guid></item><item><title>[inductor] correctly generate grid info for benchmark_kernel (#118202)</title><link>https://github.com/pytorch/pytorch/commit/1565d58ad909dfa1b058cbe97cf5c51088721a9f</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] correctly generate grid info for benchmark_kernel (#118202)&lt;br&gt;&lt;br&gt;Previously, we generated the grid argument with tree.numel for&lt;br&gt;a benchmark TritonKernel. This was not correct, because it&lt;br&gt;didn't match the launch config used for profiling and running.&lt;br&gt;&lt;br&gt;This PR fixed the issue by emitting the grid value computed&lt;br&gt;by the kernel's grid_fn, which is used by the profiler and&lt;br&gt;the kernel's runner.&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118202&lt;br&gt;Approved by: https://github.com/shunting314, https://github.com/jansel&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/1565d58ad909dfa1b058cbe97cf5c51088721a9f'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 25 Jan 2024 12:37:44 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1565d58ad909dfa1b058cbe97cf5c51088721a9f</guid></item><item><title>[mypy] Enable follow_imports = normal for mypy-torch.backends.* (#116311)</title><link>https://github.com/pytorch/pytorch/commit/1f6aa4b336a7aac756092759fc54eacc0b84fbba</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[mypy] Enable follow_imports = normal for mypy-torch.backends.* (#116311)&lt;br&gt;&lt;br&gt;Summary:&lt;br&gt;&lt;br&gt;Test Plan:&lt;br&gt;&lt;br&gt;```&lt;br&gt;lintrunner --take MYPYINDUCTOR --all-files&lt;br&gt;ok No lint issues.&lt;br&gt;&lt;br&gt;lintrunner -a&lt;br&gt;ok No lint issues.&lt;br&gt;Successfully applied all patches.&lt;br&gt;```&lt;br&gt;&lt;br&gt;Reviewers:&lt;br&gt;&lt;br&gt;Subscribers:&lt;br&gt;&lt;br&gt;Tasks:&lt;br&gt;&lt;br&gt;Tags:&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/116311&lt;br&gt;Approved by: https://github.com/int3&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/1f6aa4b336a7aac756092759fc54eacc0b84fbba'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 25 Jan 2024 12:17:22 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1f6aa4b336a7aac756092759fc54eacc0b84fbba</guid></item><item><title>[inductor] Slightly faster memory allocation on CPU (#118171)</title><link>https://github.com/pytorch/pytorch/commit/817debeb8976adede8f781c312d85595a4dbb83c</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor] Slightly faster memory allocation on CPU (#118171)&lt;br&gt;&lt;br&gt;Based on `python benchmarks/dynamo/microbenchmarks/overheads.py`:&lt;br&gt;- Before `12.2us`&lt;br&gt;- After `10.5us`&lt;br&gt;&lt;br&gt;This is inspired by https://github.com/pytorch/pytorch/commit/a2c17a2b00f7c41866bbde28d33b8c50e5632e01 -- but in Python rather than C++&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118171&lt;br&gt;Approved by: https://github.com/jgong5, https://github.com/peterbell10&lt;br&gt;ghstack dependencies: #118065, #118070&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/817debeb8976adede8f781c312d85595a4dbb83c'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 25 Jan 2024 08:54:57 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/817debeb8976adede8f781c312d85595a4dbb83c</guid></item><item><title>Fix failure of test_dynamo_distributed &amp; test_inductor_collectives (#117741)</title><link>https://github.com/pytorch/pytorch/commit/b5b36cf0c4e1958f1ff25120f5d4beeef3288187</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Fix failure of test_dynamo_distributed &amp; test_inductor_collectives (#117741)&lt;br&gt;&lt;br&gt;When CUDA is not available `c10d.init_process_group("nccl"...)` will fail with&lt;br&gt;&gt; RuntimeError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!&lt;br&gt;&lt;br&gt;Hence add a corresponding skip marker to the classes deriving from DynamoDistributedSingleProcTestCase next to the `requires_nccl` marker.&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/117741&lt;br&gt;Approved by: https://github.com/ezyang, https://github.com/malfet&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/b5b36cf0c4e1958f1ff25120f5d4beeef3288187'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 25 Jan 2024 05:25:36 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b5b36cf0c4e1958f1ff25120f5d4beeef3288187</guid></item><item><title>Revert "Update triton ROCm version to 6.0" (#118179)</title><link>https://github.com/pytorch/pytorch/commit/e6288820e3a2ae385a0ad51b412c43f384827787</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Revert "Update triton ROCm version to 6.0" (#118179)&lt;br&gt;&lt;br&gt;Reverting [this commit](https://github.com/pytorch/pytorch/pull/117433) due to failures observed in wheel environment e.g:&lt;br&gt;```&lt;br&gt;ImportError: /tmp/torchinductor_root/triton/0/ebfa57c0b7b95873c96cad6f9bca148d/hip_utils.so: undefined symbol: hipGetDevicePropertiesR0600`&lt;br&gt;```&lt;br&gt;&lt;br&gt;Will revert for now and investigate and aim to re-land this as part of https://github.com/pytorch/pytorch/pull/116270&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118179&lt;br&gt;Approved by: https://github.com/jeffdaily, https://github.com/malfet&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/e6288820e3a2ae385a0ad51b412c43f384827787'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Wed, 24 Jan 2024 14:01:27 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e6288820e3a2ae385a0ad51b412c43f384827787</guid></item><item><title>[AOTI] Enable for MacOS (#118076)</title><link>https://github.com/pytorch/pytorch/commit/bd991152767c1241e9a658cfd6a98e6efa028872</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[AOTI] Enable for MacOS (#118076)&lt;br&gt;&lt;br&gt;- Add `darwin` to the list of supported platform&lt;br&gt;- Add `#include &lt;sstream&gt;` to `aoti_runtime/model.h`&lt;br&gt;- Refactor Linux specific constant compilation logic to `_compile_consts_linux`&lt;br&gt;- Add `_compile_consts_darwin` that converts consts to .S file that is linked into a shared library&lt;br&gt;   - Patch file using magic to avoid converting bytes to large hexadecimal string&lt;br&gt;- Generate integer constants with `LL` suffix on MacOS (corresponds to int64_t definition)&lt;br&gt;- Enable test_aot_inductor.py tests on MacOS&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118076&lt;br&gt;Approved by: https://github.com/desertfire&lt;br&gt;ghstack dependencies: #118077&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/bd991152767c1241e9a658cfd6a98e6efa028872'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Wed, 24 Jan 2024 06:24:05 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bd991152767c1241e9a658cfd6a98e6efa028872</guid></item><item><title>[Inductor] Fix `argument unused during compilation` warning (#118077)</title><link>https://github.com/pytorch/pytorch/commit/7d396918c6acc84ceb134d99844c09484de8694f</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[Inductor] Fix `argument unused during compilation` warning (#118077)&lt;br&gt;&lt;br&gt;By not passing linker flag if `compile_only` is set to `True`&lt;br&gt;Before that change every invocation of AOTI compiler resulted in emitting at least 4 warnings:&lt;br&gt;```&lt;br&gt;clang: warning: -lomp: 'linker' input unused [-Wunused-command-line-argument]&lt;br&gt;clang: warning: argument unused during compilation: '-shared' [-Wunused-command-line-argument]&lt;br&gt;clang: warning: argument unused during compilation: '-undefined dynamic_lookup' [-Wunused-command-line-argument]&lt;br&gt;clang: warning: argument unused during compilation: '-L/Users/nshulga/miniforge3/lib' [-Wunused-command-line-argument]&lt;br&gt;```&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118077&lt;br&gt;Approved by: https://github.com/desertfire&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/7d396918c6acc84ceb134d99844c09484de8694f'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Wed, 24 Jan 2024 01:52:16 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7d396918c6acc84ceb134d99844c09484de8694f</guid></item><item><title>[inductor][fx] Fix broadcast_tensors with unbacked symints when translation validation is off (#118066)</title><link>https://github.com/pytorch/pytorch/commit/21e8546b1180d16425c82dbececc87e31d544328</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Commit Message:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;[inductor][fx] Fix broadcast_tensors with unbacked symints when translation validation is off (#118066)&lt;br&gt;&lt;br&gt;## Context&lt;br&gt;This is an example that runs into an AssertionError while lowering in Inductor.&lt;br&gt;```&lt;br&gt;# While lowering, b will be expanded because b.size(1) == 1.&lt;br&gt;a = torch.zeros([u0, 512])&lt;br&gt;b = torch.ones([u0, 1])&lt;br&gt;return a * b&lt;br&gt;```&lt;br&gt;&lt;br&gt;Below's the tail-end of the stack trace. Here's the important bits:&lt;br&gt;1. In _inductor/sizevars.py, we'll call `self.shape_env.defer_runtime_assert(expr, msg, fx_node=V.graph.current_node)`.&lt;br&gt;2. This leads to the creation of a `ShapeEnvEvent` with an FX node via `kwargs={"fx_node": V.graph.current_node}` ([see](https://github.com/pytorch/pytorch/blob/0c9b5134701ab04552bc1724c5f148d22f98f20d/torch/fx/experimental/recording.py#L245-L247)).&lt;br&gt;3. Eventually, we try to call `maybe_convert_node()` but it expects translation validation to be on ([see](https://github.com/pytorch/pytorch/blob/0c9b5134701ab04552bc1724c5f148d22f98f20d/torch/fx/experimental/recording.py#L118-L121)).&lt;br&gt;```&lt;br&gt;  File "pytorch/torch/_inductor/lowering.py", line 221, in transform_args&lt;br&gt;    for i, x in zip(indices, broadcast_tensors(*[args[i] for i in indices])):&lt;br&gt;  File "pytorch/torch/_inductor/lowering.py", line 294, in wrapped&lt;br&gt;    out = decomp_fn(*args, **kwargs)&lt;br&gt;  File "pytorch/torch/_inductor/lowering.py", line 676, in broadcast_tensors&lt;br&gt;    x = expand(x, target)&lt;br&gt;  File "pytorch/torch/_inductor/lowering.py", line 294, in wrapped&lt;br&gt;    out = decomp_fn(*args, **kwargs)&lt;br&gt;  File "pytorch/torch/_inductor/lowering.py", line 793, in expand&lt;br&gt;    return TensorBox(ExpandView.create(x.data, tuple(sizes)))&lt;br&gt;  File "pytorch/torch/_inductor/ir.py", line 1871, in create&lt;br&gt;    new_size = cls._normalize_size(x, new_size)&lt;br&gt;  File "pytorch/torch/_inductor/ir.py", line 1862, in _normalize_size&lt;br&gt;    new_size[i] = V.graph.sizevars.expect_equals(&lt;br&gt;  File "pytorch/torch/_inductor/sizevars.py", line 338, in expect_equals&lt;br&gt;    self.expect_true(sympy.Eq(left, right), msg=msg)&lt;br&gt;  File "pytorch/torch/_inductor/sizevars.py", line 333, in expect_true&lt;br&gt;    self.shape_env.defer_runtime_assert(expr, msg, fx_node=V.graph.current_node)  # (1) is here&lt;br&gt;  File "pytorch/torch/fx/experimental/recording.py", line 257, in wrapper&lt;br&gt;    return event.run(self)   # (2) happens right before this&lt;br&gt;  File "pytorch/torch/fx/experimental/recording.py", line 155, in run&lt;br&gt;    replacearg(index=3, key="fx_node", fn=maybe_convert_node)&lt;br&gt;  File "pytorch/torch/fx/experimental/recording.py", line 138, in replacearg&lt;br&gt;    kwargs[key] = fn(kwargs[key])&lt;br&gt;  File "pytorch/torch/fx/experimental/recording.py", line 128, in maybe_convert_node&lt;br&gt;    assert hasattr(shape_env, "name_to_node")  # (3) is here&lt;br&gt;```&lt;br&gt;&lt;br&gt;## Approach&lt;br&gt;Since [translation validation](https://github.com/pytorch/pytorch/blob/c6be5d55a56cc12b7a004acdb6a7da92ee2142f7/torch/fx/experimental/validator.py#L574) may not be on during Inductor lowering, we can check if that's True and return the FX node's name in this case.&lt;br&gt;&lt;br&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/118066&lt;br&gt;Approved by: https://github.com/ezyang, https://github.com/peterbell10&lt;/p&gt;&lt;p&gt;&lt;a href='https://github.com/pytorch/pytorch/commit/21e8546b1180d16425c82dbececc87e31d544328'&gt;View Commit on GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Tue, 23 Jan 2024 19:07:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/21e8546b1180d16425c82dbececc87e31d544328</guid></item></channel></rss>