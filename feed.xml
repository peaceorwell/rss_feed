<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub Commits Feed</title>
    <link>https://github.com/username/repo/commits</link>
    <description>Recent commits from GitHub repo</description>
    <title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><item><title>[aoti][reland] clear precomputed symbol replacements before cpp wrapper compilation (#123136)</title><link>https://github.com/pytorch/pytorch/commit/e4e5449dfc46e0d90fd1980f3ee748da8edc983c</link><description><![CDATA[<p>[aoti][reland] clear precomputed symbol replacements before cpp wrapper compilation (#123136)</p>
<p>After we codegen a triton kernel in the triton codegen backend,<br />
we cache the generated triton source code in the wrapper to avoid<br />
producing multiple triton kernels with the same content.</p>
<p>In AOTI compilation flow, this caching mechanism imposes a strong requirement<br />
on the codegen that we must generate the same triton source code<br />
for the same schedule node in both python and cpp codegen phases.<br />
Otherwise, we would end up with a mismatch between the kernel name<br />
formed in the cpp codegen and the cuda kernel key produced from<br />
the python codegen. Consequently, we would hit an missing-cuda-kernel<br />
error.</p>
<p>The precomputed symbol replacements saved in V.graph.sizevars<br />
can cause such source-code inconsistency related to the code for indexing<br />
tensors. For example, let's say in the python codegen phase,<br />
we produce "ks2*48" as part of indexing an input for schedule<br />
node A while yielding a replacement pair "ks0 -&gt; ks2*48" in<br />
the precomputed replacements. In the second cpp codegen phase,<br />
we would produce "ks0" for the same indexing code of schedule<br />
node A due to the "ks0 -&gt; ks2*48" replacement pair.</p>
<p>This PR fixed the issue by clearing precomputed_replacements<br />
and inv_precomputed_replacements before cpp wrapper codegen.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/123136<br />
Approved by: https://github.com/desertfire</p>]]></description><pubDate>Mon, 08 Apr 2024 08:51:43 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e4e5449dfc46e0d90fd1980f3ee748da8edc983c</guid></item><item><title>[aoti][reland] clear precomputed symbol replacements before cpp wrapper compilation (#123136)</title><link>https://github.com/pytorch/pytorch/commit/e4e5449dfc46e0d90fd1980f3ee748da8edc983c</link><description><![CDATA[<p>[aoti][reland] clear precomputed symbol replacements before cpp wrapper compilation (#123136)</p>
<p>After we codegen a triton kernel in the triton codegen backend,<br />
we cache the generated triton source code in the wrapper to avoid<br />
producing multiple triton kernels with the same content.</p>
<p>In AOTI compilation flow, this caching mechanism imposes a strong requirement<br />
on the codegen that we must generate the same triton source code<br />
for the same schedule node in both python and cpp codegen phases.<br />
Otherwise, we would end up with a mismatch between the kernel name<br />
formed in the cpp codegen and the cuda kernel key produced from<br />
the python codegen. Consequently, we would hit an missing-cuda-kernel<br />
error.</p>
<p>The precomputed symbol replacements saved in V.graph.sizevars<br />
can cause such source-code inconsistency related to the code for indexing<br />
tensors. For example, let's say in the python codegen phase,<br />
we produce "ks2*48" as part of indexing an input for schedule<br />
node A while yielding a replacement pair "ks0 -&gt; ks2*48" in<br />
the precomputed replacements. In the second cpp codegen phase,<br />
we would produce "ks0" for the same indexing code of schedule<br />
node A due to the "ks0 -&gt; ks2*48" replacement pair.</p>
<p>This PR fixed the issue by clearing precomputed_replacements<br />
and inv_precomputed_replacements before cpp wrapper codegen.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/123136<br />
Approved by: https://github.com/desertfire</p>]]></description><pubDate>Mon, 08 Apr 2024 08:51:43 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e4e5449dfc46e0d90fd1980f3ee748da8edc983c</guid></item><item><title>Use graph.find_nodes in inductor (#122256)</title><link>https://github.com/pytorch/pytorch/commit/f8465df9f0dd6e536ea5737383bc2fc413602760</link><description>&lt;p&gt;Use graph.find_nodes in inductor (#122256)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122256&lt;br /&gt;
Approved by: https://github.com/jansel&lt;br /&gt;
ghstack dependencies: #121565, #122255&lt;/p&gt;</description><pubDate>Sun, 07 Apr 2024 10:51:14 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f8465df9f0dd6e536ea5737383bc2fc413602760</guid></item><item><title>Use graph.find_nodes in inductor/fx_passes (#122255)</title><link>https://github.com/pytorch/pytorch/commit/33783e43e9446db10946e2bb354618ded3d3f446</link><description>&lt;p&gt;Use graph.find_nodes in inductor/fx_passes (#122255)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122255&lt;br /&gt;
Approved by: https://github.com/jansel&lt;br /&gt;
ghstack dependencies: #121565&lt;/p&gt;</description><pubDate>Sun, 07 Apr 2024 10:51:09 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/33783e43e9446db10946e2bb354618ded3d3f446</guid></item><item><title>[Bug Fix] Fix Cuda 12.4 compilation - Refactor SFINAE boxing logic  (#123377)</title><link>https://github.com/pytorch/pytorch/commit/0355f6e9549d4ba383112732f349c15b40169075</link><description>&lt;p&gt;[Bug Fix] Fix Cuda 12.4 compilation - Refactor SFINAE boxing logic  (#123377)&lt;/p&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;p&gt;PyTorch fails to compile from source using CUDA 12.4. The relevant log is extracted below. This was a recurring issue, which would cause the compilation to fail again on further objects if the first offending object was skipped.&lt;/p&gt;
&lt;p&gt;While searching for whether others had experienced this issue before attempting a fix myself, I found this suggested fix by @christian-heusel in https://github.com/pytorch/pytorch/issues/122169#issuecomment-2008455468 written by @lahwaacz. The code written by @lahwaacz at https://gitlab.archlinux.org/archlinux/packaging/packages/python-pytorch/-/commit/bb1f1a4c546c9692fb56db57172f14d25b95e645 fixes the issue.&lt;/p&gt;
&lt;p&gt;The original issue (#122169) seems to have gone quiet, so I am submitting this PR. I made no substantive adjustments to @lahwaacz' code. My only adjustment was, for the sake of consistency, to remove the double underscores in the struct name, as double underscores are reserved to the implementation in C++ Standard. My change has no functional effect on the original code.&lt;/p&gt;
&lt;p&gt;The ArchLinux package from which the original code was committed is licensed under the BSD license. https://archlinux.org/packages/extra/x86_64/python-pytorch/&lt;/p&gt;
&lt;p&gt;```&lt;br /&gt;
[7900/8804] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/&lt;strong&gt;/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu.o&lt;br /&gt;
FAILED: caffe2/CMakeFiles/torch_cuda.dir/&lt;/strong&gt;/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu.o&lt;br /&gt;
/usr/bin/ccache /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DAT_PER_OPERATOR_HEADERS -DFLASHATTENTION_DISABLE_ALIBI -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DIDEEP_USE_MKL -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTORCH_CUDA_BUILD_MAIN_LIB -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_CUDA -DUSE_CUSPARSELT -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_FLASH_ATTENTION -DUSE_MEM_EFF_ATTENTION -DUSE_NCCL -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cuda_EXPORTS -I/home/elliot/compile_test-pytorch/build/aten/src -I/home/elliot/compile_test-pytorch/aten/src -I/home/elliot/compile_test-pytorch/build -I/home/elliot/compile_test-pytorch -I/home/elliot/compile_test-pytorch/cmake/../third_party/benchmark/include -I/home/elliot/compile_test-pytorch/third_party/onnx -I/home/elliot/compile_test-pytorch/build/third_party/onnx -I/home/elliot/compile_test-pytorch/third_party/foxi -I/home/elliot/compile_test-pytorch/build/third_party/foxi -I/home/elliot/compile_test-pytorch/aten/src/THC -I/home/elliot/compile_test-pytorch/aten/src/ATen/cuda -I/home/elliot/compile_test-pytorch/aten/src/ATen/../../../third_party/cutlass/include -I/home/elliot/compile_test-pytorch/build/caffe2/aten/src -I/home/elliot/compile_test-pytorch/aten/src/ATen/.. -I/home/elliot/compile_test-pytorch/build/nccl/include -I/home/elliot/compile_test-pytorch/c10/cuda/../.. -I/home/elliot/compile_test-pytorch/c10/.. -I/home/elliot/compile_test-pytorch/third_party/tensorpipe -I/home/elliot/compile_test-pytorch/build/third_party/tensorpipe -I/home/elliot/compile_test-pytorch/third_party/tensorpipe/third_party/libnop/include -I/home/elliot/compile_test-pytorch/torch/csrc/api -I/home/elliot/compile_test-pytorch/torch/csrc/api/include -isystem /home/elliot/compile_test-pytorch/build/third_party/gloo -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/gloo -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/tensorpipe/third_party/libuv/include -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/googletest/googlemock/include -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/googletest/googletest/include -isystem /home/elliot/compile_test-pytorch/third_party/protobuf/src -isystem /home/elliot/miniforge3/envs/torchtest/include -isystem /home/elliot/compile_test-pytorch/third_party/gemmlowp -isystem /home/elliot/compile_test-pytorch/third_party/neon2sse -isystem /home/elliot/compile_test-pytorch/third_party/XNNPACK/include -isystem /home/elliot/compile_test-pytorch/third_party/ittapi/include -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/eigen -isystem /usr/local/cuda-12.4/include -isystem /home/elliot/compile_test-pytorch/third_party/ideep/mkl-dnn/include/oneapi/dnnl -isystem /home/elliot/compile_test-pytorch/third_party/ideep/include -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/cudnn_frontend/include -DLIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_OPERATIONS -D_GLIBCXX_USE_CXX11_ABI=1 -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_86,code=sm_86 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -O3 -DNDEBUG -std=c++17 -Xcompiler=-fPIC -DMKL_HAS_SBGEMM -DMKL_HAS_SHGEMM -DTORCH_USE_LIBUV -DCAFFE2_USE_GLOO -Xcompiler=-Wall,-Wextra,-Wdeprecated,-Wno-unused-parameter,-Wno-unused-function,-Wno-missing-field-initializers,-Wno-unknown-pragmas,-Wno-type-limits,-Wno-array-bounds,-Wno-unknown-pragmas,-Wno-strict-overflow,-Wno-strict-aliasing,-Wno-maybe-uninitialized -Wno-deprecated-copy -MD -MT caffe2/CMakeFiles/torch_cuda.dir/&lt;strong&gt;/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu.o -MF caffe2/CMakeFiles/torch_cuda.dir/&lt;/strong&gt;/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu.o.d -x cu -c /home/elliot/compile_test-pytorch/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu -o caffe2/CMakeFiles/torch_cuda.dir/__/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu.o&lt;br /&gt;
/home/elliot/compile_test-pytorch/aten/src/ATen/core/IListRef_inl.h: In static member function ‘static c10::detail::IListRefConstRef&lt;at::OptionalTensorRef&gt; c10::detail::IListRefTagImpl&lt;c10::IListRefTag::Boxed, at::OptionalTensorRef&gt;::iterator_get(const c10::List&lt;std::optional\&lt;at::Tensor&gt; &gt;::const_iterator&amp;amp;)’:&lt;br /&gt;
/home/elliot/compile_test-pytorch/aten/src/ATen/core/IListRef_inl.h:171:13: warning: possibly dangling reference to a temporary [-Wdangling-reference]&lt;br /&gt;
  171 |     const auto&amp;amp; ivalue = (&lt;em&gt;it).get();&lt;br /&gt;
      |             ^~~~~~&lt;br /&gt;
/home/elliot/compile_test-pytorch/aten/src/ATen/core/IListRef_inl.h:171:33: note: the temporary was destroyed at the end of the full expression ‘(&amp;amp; it)-&amp;gt;c10::impl::ListIterator&lt;std::optional\&lt;at::Tensor&gt;, __gnu_cxx::__normal_iterator\&lt;c10::IValue*, std::vector\&lt;c10::IValue&gt; &gt; &gt;::operator&lt;/em&gt;().c10::impl::ListElementReference&lt;std::optional\&lt;at::Tensor&gt;, __gnu_cxx::__normal_iterator\&lt;c10::IValue*, std::vector\&lt;c10::IValue&gt; &gt; &gt;::get()’&lt;br /&gt;
  171 |     const auto&amp;amp; ivalue = (&lt;em&gt;it).get();&lt;br /&gt;
      |                      ~~~~~~~~~~~^~&lt;br /&gt;
/home/elliot/compile_test-pytorch/aten/src/ATen/core/boxing/impl/boxing.h: At global scope:&lt;br /&gt;
/home/elliot/compile_test-pytorch/aten/src/ATen/core/boxing/impl/boxing.h:42:103: error: expected primary-expression before ‘&amp;gt;’ token&lt;br /&gt;
   42 | struct has_ivalue_to&lt;T, std::void_t\&lt;decltype(std::declval\&lt;IValue&gt;().to\&lt;T&gt;())&gt;&gt;&lt;br /&gt;
      |                                                                                                       ^&lt;br /&gt;
/home/elliot/compile_test-pytorch/aten/src/ATen/core/boxing/impl/boxing.h:42:106: error: expected primary-expression before ‘)’ token&lt;br /&gt;
   42 | struct has_ivalue_to&lt;T, std::void_t\&lt;decltype(std::declval\&lt;IValue&gt;().to\&lt;T&gt;())&gt;&gt;&lt;br /&gt;
      |                                                                                                          ^&lt;br /&gt;
/home/elliot/compile_test-pytorch/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h: In lambda function:&lt;br /&gt;
/home/elliot/compile_test-pytorch/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h:154:24: warning: possibly dangling reference to a temporary [-Wdangling-reference]&lt;br /&gt;
  154 |         for (const at::Tensor&amp;amp; tensor : ivalue.toTensorList()) {&lt;br /&gt;
      |                        ^~~~~~&lt;br /&gt;
/home/elliot/compile_test-pytorch/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h:154:53: note: the temporary was destroyed at the end of the full expression ‘__for_begin .c10::impl::ListIterator&lt;at::Tensor, __gnu_cxx::__normal_iterator\&lt;c10::IValue*, std::vector\&lt;c10::IValue&gt; &gt; &gt;::operator&lt;/em&gt;().c10::impl::ListElementReference&lt;at::Tensor, __gnu_cxx::__normal_iterator\&lt;c10::IValue*, std::vector\&lt;c10::IValue&gt; &gt; &gt;::operator std::conditional_t&lt;true, const at::Tensor&amp;, at::Tensor&gt;()’&lt;br /&gt;
  154 |         for (const at::Tensor&amp;amp; tensor : ivalue.toTensorList()) {&lt;br /&gt;
      |                                                     ^&lt;br /&gt;
...&lt;/p&gt;
&lt;p&gt;ninja: build stopped: subcommand failed.&lt;br /&gt;
&lt;code&gt;&lt;/code&gt;&lt;br /&gt;
PyTorch version: 2.4.0a0+git595613d&lt;br /&gt;
Is debug build: False&lt;br /&gt;
CUDA used to build PyTorch: 12.4&lt;br /&gt;
ROCM used to build PyTorch: N/A&lt;/p&gt;
&lt;p&gt;OS: Ubuntu 23.10 (x86_64)&lt;br /&gt;
GCC version: (Ubuntu 13.2.0-4ubuntu3) 13.2.0&lt;br /&gt;
Clang version: 16.0.6 (15)&lt;br /&gt;
CMake version: version 3.29.0&lt;br /&gt;
Libc version: glibc-2.38&lt;/p&gt;
&lt;p&gt;Python version: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:53:32) [GCC 12.3.0] (64-bit runtime)&lt;br /&gt;
Python platform: Linux-6.5.0-26-generic-x86_64-with-glibc2.38&lt;br /&gt;
Is CUDA available: True&lt;br /&gt;
CUDA runtime version: 12.4.131&lt;br /&gt;
CUDA_MODULE_LOADING set to: LAZY&lt;br /&gt;
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Ti&lt;br /&gt;
Nvidia driver version: 550.67&lt;br /&gt;
cuDNN version: Probably one of the following:&lt;br /&gt;
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0&lt;br /&gt;
/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0&lt;br /&gt;
/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0&lt;br /&gt;
/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0&lt;br /&gt;
/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0&lt;br /&gt;
/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0&lt;br /&gt;
/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0&lt;br /&gt;
/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0&lt;br /&gt;
HIP runtime version: N/A&lt;br /&gt;
MIOpen runtime version: N/A&lt;br /&gt;
Is XNNPACK available: True&lt;/p&gt;
&lt;p&gt;CPU:&lt;br /&gt;
Architecture:                       x86_64&lt;br /&gt;
CPU op-mode(s):                     32-bit, 64-bit&lt;br /&gt;
Address sizes:                      46 bits physical, 48 bits virtual&lt;br /&gt;
Byte Order:                         Little Endian&lt;br /&gt;
CPU(s):                             24&lt;br /&gt;
On-line CPU(s) list:                0-23&lt;br /&gt;
Vendor ID:                          GenuineIntel&lt;br /&gt;
Model name:                         13th Gen Intel(R) Core(TM) i7-13700K&lt;br /&gt;
CPU family:                         6&lt;br /&gt;
Model:                              183&lt;br /&gt;
Thread(s) per core:                 2&lt;br /&gt;
Core(s) per socket:                 16&lt;br /&gt;
Socket(s):                          1&lt;br /&gt;
Stepping:                           1&lt;br /&gt;
CPU(s) scaling MHz:                 19%&lt;br /&gt;
CPU max MHz:                        5400.0000&lt;br /&gt;
CPU min MHz:                        800.0000&lt;br /&gt;
BogoMIPS:                           6835.20&lt;br /&gt;
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities&lt;br /&gt;
Virtualization:                     VT-x&lt;br /&gt;
L1d cache:                          640 KiB (16 instances)&lt;br /&gt;
L1i cache:                          768 KiB (16 instances)&lt;br /&gt;
L2 cache:                           24 MiB (10 instances)&lt;br /&gt;
L3 cache:                           30 MiB (1 instance)&lt;br /&gt;
NUMA node(s):                       1&lt;br /&gt;
NUMA node0 CPU(s):                  0-23&lt;br /&gt;
Vulnerability Gather data sampling: Not affected&lt;br /&gt;
Vulnerability Itlb multihit:        Not affected&lt;br /&gt;
Vulnerability L1tf:                 Not affected&lt;br /&gt;
Vulnerability Mds:                  Not affected&lt;br /&gt;
Vulnerability Meltdown:             Not affected&lt;br /&gt;
Vulnerability Mmio stale data:      Not affected&lt;br /&gt;
Vulnerability Retbleed:             Not affected&lt;br /&gt;
Vulnerability Spec rstack overflow: Not affected&lt;br /&gt;
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl&lt;br /&gt;
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization&lt;br /&gt;
Vulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence&lt;br /&gt;
Vulnerability Srbds:                Not affected&lt;br /&gt;
Vulnerability Tsx async abort:      Not affected&lt;/p&gt;
&lt;p&gt;Versions of relevant libraries:&lt;br /&gt;
[pip3] numpy==1.26.4&lt;br /&gt;
[pip3] optree==0.11.0&lt;br /&gt;
[pip3] pytorch-triton==3.0.0+989adb9a29&lt;br /&gt;
[pip3] torch==2.4.0a0+git595613d&lt;br /&gt;
[conda] magma-cuda124             2.6.1                         1    pytorch&lt;br /&gt;
[conda] mkl-include               2024.1.0              intel_691    intel&lt;br /&gt;
[conda] mkl-static                2024.1.0              intel_691    intel&lt;br /&gt;
[conda] numpy                     1.26.4          py311h64a7726_0    conda-forge&lt;br /&gt;
[conda] optree                    0.11.0          py311h9547e67_0    conda-forge&lt;br /&gt;
[conda] pytorch-triton            3.0.0+989adb9a29          pypi_0    pypi&lt;br /&gt;
[conda] torch                     2.4.0a0+git595613d          pypi_0    pypi&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;Tagging @colesbury per https://github.com/pytorch/pytorch/issues/122169#issuecomment-2008232619&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/123377&lt;br /&gt;
Approved by: https://github.com/cyyever, https://github.com/malfet&lt;/p&gt;</description><pubDate>Sun, 07 Apr 2024 10:37:47 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0355f6e9549d4ba383112732f349c15b40169075</guid></item><item><title>profile pt2 compile time with strobelight (#123311)</title><link>https://github.com/pytorch/pytorch/commit/caed7f67271e975fc429fb46bef977095942296f</link><description>&lt;p&gt;profile pt2 compile time with strobelight (#123311)&lt;/p&gt;
&lt;p&gt;For oss this diff adds a decorator @profile_sb_fbcode that is a nop for non meta workload.&lt;/p&gt;
&lt;p&gt;Facebook:&lt;br /&gt;
With this diff someone can generate a strobelight profile for pt2 compilation.&lt;br /&gt;
users need to set the env variable TORCH_COMPILE_SL_PROFILE =TRUE .&lt;/p&gt;
&lt;p&gt;For example:&lt;br /&gt;
&lt;code&gt;TORCH_COMPILE_SL_PROFILE =TRUE buck2 run  @//mode/inplace  @//mode/opt  //caffe2/fb/strobelight:compiletime_profile_example&lt;/code&gt;&lt;br /&gt;
see sample output bellow, at the end of summary.&lt;/p&gt;
&lt;p&gt;The way this works, is that a unique id is generated and associated with all samples that are collected for functions that are decorated with profile_sb_fbcode.&lt;br /&gt;
This id can then be used to combine different strobe light profile into one. (for example three compilation events happens in the code bellow).&lt;/p&gt;
&lt;p&gt;Right now the following two functions are annotated with  profile_sb_fbcode.  bw_compiler and _compile. if two profile_sl_fbcode is called recursively, recursive invocations are ignored and a log is printed.&lt;/p&gt;
&lt;p&gt;The output is:&lt;br /&gt;
&lt;code&gt;Strobelight is enabled for pt2 compilation
Unique user-id for this run is: 2024-04-03-13:59:49147091devvm4561.ash0.facebook.com
You can use the following link to access the strobelight profile at the end of the run:
https://www.internalfb.com/intern/scuba/query/?dataset=pyperf_experimental%2Fon_demand&amp;amp;drillstate=%7B%22purposes%22%3A[]%2C%22end%22%3A%22now%22%2C%22start%22%3A%22-30%20days%22%2C%22filterMode%22%3A%22DEFAULT%22%2C%22modifiers%22%3A[]%2C%22sampleCols%22%3A[]%2C%22cols%22%3A[%22namespace_id%22%2C%22namespace_process_id%22]%2C%22derivedCols%22%3A[]%2C%22mappedCols%22%3A[]%2C%22enumCols%22%3A[]%2C%22return_remainder%22%3Afalse%2C%22should_pivot%22%3Afalse%2C%22is_timeseries%22%3Afalse%2C%22hideEmptyColumns%22%3Afalse%2C%22timezone%22%3A%22America%2FLos_Angeles%22%2C%22compare%22%3A%22none%22%2C%22samplingRatio%22%3A%221%22%2C%22metric%22%3A%22count%22%2C%22aggregation_field%22%3A%22async_stack_complete%22%2C%22top%22%3A10000%2C%22aggregateList%22%3A[]%2C%22param_dimensions%22%3A[%7B%22dim%22%3A%22py_async_stack%22%2C%22op%22%3A%22edge%22%2C%22param%22%3A%220%22%2C%22anchor%22%3A%220%22%7D]%2C%22order%22%3A%22weight%22%2C%22order_desc%22%3Atrue%2C%22constraints%22%3A[[%7B%22column%22%3A%22run_user%22%2C%22op%22%3A%22eq%22%2C%22value%22%3A[%22[%5C%222024-04-03-13:59:49147091devvm4561.ash0.facebook.com%5C%22]%22]%7D]]%2C%22c_constraints%22%3A[[]]%2C%22b_constraints%22%3A[[]]%2C%22ignoreGroupByInComparison%22%3Afalse%7D&amp;amp;view=GraphProfilerView&amp;amp;&amp;amp;pool=uber&amp;amp;graphprofiler_filter=&amp;amp;graphprofiler_column_to_sort_by=exclusive
the link below takes you to the collected strobelight profile
https://www.internalfb.com/intern/scuba/query/?dataset=pyperf_experimental%2Fon_demand&amp;amp;drillstate=%7B%22dimensions%22%3A%5B%5D%2C%22param_dimensions%22%3A%5B%7B%22anchor%22%3A%220%22%2C%22param%22%3A%220%22%2C%22op%22%3A%22edge%22%2C%22dim%22%3A%22py_async_stack%22%7D%5D%2C%22constraints%22%3A%5B%5B%7B%22value%22%3A%5B%22%5B%5C%22-6800545191281321%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_id%22%7D%2C%7B%22value%22%3A%5B%22%5B%5C%222024-04-03-13%3A59%3A49147091devvm4561.ash0.facebook.com%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_user%22%7D%5D%5D%2C%22top%22%3A10000%2C%22end%22%3A%221712181610%22%2C%22start%22%3A%221712174410%22%7D&amp;amp;view=GraphProfilerView&amp;amp;
1 storbelight success runs out of 1 non-ignored runs.
strobelight run id is: 6181728288420687
the link below takes you to the collected strobelight profile
https://www.internalfb.com/intern/scuba/query/?dataset=pyperf_experimental%2Fon_demand&amp;amp;drillstate=%7B%22dimensions%22%3A%5B%5D%2C%22param_dimensions%22%3A%5B%7B%22anchor%22%3A%220%22%2C%22param%22%3A%220%22%2C%22op%22%3A%22edge%22%2C%22dim%22%3A%22py_async_stack%22%7D%5D%2C%22constraints%22%3A%5B%5B%7B%22value%22%3A%5B%22%5B%5C%226181728288420687%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_id%22%7D%2C%7B%22value%22%3A%5B%22%5B%5C%222024-04-03-13%3A59%3A49147091devvm4561.ash0.facebook.com%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_user%22%7D%5D%5D%2C%22top%22%3A10000%2C%22end%22%3A%221712181621%22%2C%22start%22%3A%221712174421%22%7D&amp;amp;view=GraphProfilerView&amp;amp;
2 storbelight success runs out of 2 non-ignored runs.
strobelight run id is: -1026103682715688
the link below takes you to the collected strobelight profile
https://www.internalfb.com/intern/scuba/query/?dataset=pyperf_experimental%2Fon_demand&amp;amp;drillstate=%7B%22dimensions%22%3A%5B%5D%2C%22param_dimensions%22%3A%5B%7B%22anchor%22%3A%220%22%2C%22param%22%3A%220%22%2C%22op%22%3A%22edge%22%2C%22dim%22%3A%22py_async_stack%22%7D%5D%2C%22constraints%22%3A%5B%5B%7B%22value%22%3A%5B%22%5B%5C%22-1026103682715688%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_id%22%7D%2C%7B%22value%22%3A%5B%22%5B%5C%222024-04-03-13%3A59%3A49147091devvm4561.ash0.facebook.com%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_user%22%7D%5D%5D%2C%22top%22%3A10000%2C%22end%22%3A%221712181647%22%2C%22start%22%3A%221712174447%22%7D&amp;amp;view=GraphProfilerView&amp;amp;
3 storbelight success runs out of 3 non-ignored runs.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
Was tested on buck2 run  @//mode/inplace  @//mode/opt  //caffe2/fb/strobelight:compiletime_profile_example&lt;/p&gt;
&lt;p&gt;This was also tested in one of the ads benchmarks&lt;br /&gt;
&lt;code&gt;TORCH_COMPILE_SL_PROFILE =TRUE buck2 run mode/opt mode/inplace //pytorch/benchmark:run -- ads_mc_igctr_mc3_v0 -d cuda -t train --torchdynamo inductor&lt;/code&gt;&lt;br /&gt;
The results matches the results reported in&lt;br /&gt;
https://fb.workplace.com/groups/257735836456307/permalink/657458576484029&lt;/p&gt;
&lt;p&gt;Differential Revision: D55672271&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/123311&lt;br /&gt;
Approved by: https://github.com/aorenste&lt;/p&gt;</description><pubDate>Sat, 06 Apr 2024 10:57:44 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/caed7f67271e975fc429fb46bef977095942296f</guid></item><item><title>Revert "[inductor] Fix fresh_inductor_cache() (#122661)"</title><link>https://github.com/pytorch/pytorch/commit/a808559fc65235c59a647369edec2e8d813989bd</link><description>&lt;p&gt;Revert "[inductor] Fix fresh_inductor_cache() (#122661)"&lt;/p&gt;
&lt;p&gt;This reverts commit ba7d396eb73e91c1846ed770f470245ef578a923.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122661 on behalf of https://github.com/clee2000 due to new test is failing internally (&lt;a href="https://github.com/pytorch/pytorch/pull/122661#issuecomment-2037977934"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Thu, 04 Apr 2024 10:55:55 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a808559fc65235c59a647369edec2e8d813989bd</guid></item><item><title>Back out "Precompile triton templates (#121998)" (#123305)</title><link>https://github.com/pytorch/pytorch/commit/e0c9764660095df7aedc6aa4e7826b5cac5bd251</link><description>&lt;p&gt;Back out "Precompile triton templates (#121998)" (#123305)&lt;/p&gt;
&lt;p&gt;Summary: We are reverting #121998 because the change plus search-autotune-cache led to significant compilation time increase, causing stuck job detector to trigger and then kill the training job.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
CI tests&lt;/p&gt;
&lt;p&gt;Reviewed By: nmacchioni&lt;/p&gt;
&lt;p&gt;Differential Revision: D55712203&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/123305&lt;br /&gt;
Approved by: https://github.com/eellison, https://github.com/nmacchioni, https://github.com/xw285cornell&lt;/p&gt;</description><pubDate>Thu, 04 Apr 2024 08:05:10 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e0c9764660095df7aedc6aa4e7826b5cac5bd251</guid></item><item><title>Make u/int8 cat inductor fallback cpu-only (#123278)</title><link>https://github.com/pytorch/pytorch/commit/d9cbd57dfe47d298d7795cc806015d716e461b30</link><description>&lt;p&gt;Make u/int8 cat inductor fallback cpu-only (#123278)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/123278&lt;br /&gt;
Approved by: https://github.com/Chillee&lt;/p&gt;</description><pubDate>Thu, 04 Apr 2024 05:54:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d9cbd57dfe47d298d7795cc806015d716e461b30</guid></item><item><title>Reduce CPU overhead of copying inputs in CUDAGraph trees via foreach_copy (#123162)</title><link>https://github.com/pytorch/pytorch/commit/9aed9c8c873fb74cfbc629ed16c5e12dfe302cc1</link><description>&lt;p&gt;Reduce CPU overhead of copying inputs in CUDAGraph trees via foreach_copy (#123162)&lt;/p&gt;
&lt;p&gt;I noticed that when enabling CUDA graphs in Inductor, most of the CPU time was spent issuing copies from the new inputs to the graph's input tensors. This meant that my workload was still somewhat CPU bound.&lt;br /&gt;
&lt;img width="1204" alt="Screenshot 2024-03-28 at 14 18 49" src="https://github.com/pytorch/pytorch/assets/120810/9ac2462d-ef46-4051-8b22-e677845ca83e"&gt;&lt;/p&gt;
&lt;p&gt;I tried to improve this situation by using the new &lt;code&gt;_foreach_copy_&lt;/code&gt; operator, in order to group all the copies into one operator. There was already a comment in the code indicating that this was a desired optimization. It did indeed improve the situation substantially:&lt;br /&gt;
&lt;img width="908" alt="Screenshot 2024-03-28 at 14 21 21" src="https://github.com/pytorch/pytorch/assets/120810/67548ac8-2b41-46ba-8588-cea6470301cc"&gt;&lt;/p&gt;
&lt;p&gt;On device, the situation also improved, with the memcpys being merged into fewer larger kernels:&lt;br /&gt;
Before:&lt;br /&gt;
&lt;img width="848" alt="Screenshot 2024-03-28 at 14 24 48" src="https://github.com/pytorch/pytorch/assets/120810/e12e27c4-6d86-40cf-9478-061bc10920d7"&gt;&lt;br /&gt;
After:&lt;br /&gt;
&lt;img width="824" alt="Screenshot 2024-03-28 at 14 24 06" src="https://github.com/pytorch/pytorch/assets/120810/a4771b5c-6848-4510-a841-ffa5bba3023f"&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/123162&lt;br /&gt;
Approved by: https://github.com/eellison&lt;/p&gt;</description><pubDate>Wed, 03 Apr 2024 11:34:41 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9aed9c8c873fb74cfbc629ed16c5e12dfe302cc1</guid></item><item><title>[quant][be] Simplify fake_quant_per_channel (#123186)</title><link>https://github.com/pytorch/pytorch/commit/fe29a8fbea6d3b90a029e7ca2b6ca8f9c47098d2</link><description>&lt;p&gt;[quant][be] Simplify fake_quant_per_channel (#123186)&lt;/p&gt;
&lt;p&gt;Summary: We probably don't need&lt;br /&gt;
&lt;code&gt;torch._C._AutoDispatchBelowAutograd()&lt;/code&gt;, which is to prevent&lt;br /&gt;
infinite recursion if the implementation calls itself. Let's&lt;br /&gt;
remove it and see if anything breaks. The other major change&lt;br /&gt;
is registering the op to the more general Autograd dispatch&lt;br /&gt;
key so it can be used on cuda as well.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
python test/inductor/test_cpu_repro.py -k test_decomposed_fake_quant_per_channel&lt;/p&gt;
&lt;p&gt;Reviewers: zou3519, bdhirsh&lt;/p&gt;
&lt;p&gt;Subscribers: zou3519, bdhirsh, jerryzh168, leslie-fang-intel&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/123186&lt;br /&gt;
Approved by: https://github.com/zou3519, https://github.com/leslie-fang-intel&lt;/p&gt;</description><pubDate>Wed, 03 Apr 2024 10:06:45 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fe29a8fbea6d3b90a029e7ca2b6ca8f9c47098d2</guid></item><item><title>[Inductor Cutlass backend] GEMM size threshold for Cutlass backend usage (#121491)</title><link>https://github.com/pytorch/pytorch/commit/74b3a7920ef9b96315ef20adef858b00cc067af6</link><description>&lt;p&gt;[Inductor Cutlass backend] GEMM size threshold for Cutlass backend usage (#121491)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Adds a configurable GEMM size threshold for the usage of Cutlass GEMM Kernels &lt;strong&gt;_inductor.config.cutlass_backend_min_gemm_size&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;During GEMM algorithm choice generation: &lt;strong&gt;if no viable choices can be generated using the configured backends, the ATen backend will be used as a fallback backend&lt;/strong&gt;, even if it is not enabled in &lt;strong&gt;_inductor.config.max_autotune_gemm_backends&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Test plan:&lt;br /&gt;
CI&lt;br /&gt;
Additional unit test in test_cutlass_backend.py&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121491&lt;br /&gt;
Approved by: https://github.com/jansel&lt;br /&gt;
ghstack dependencies: #121490&lt;/p&gt;</description><pubDate>Wed, 03 Apr 2024 05:34:16 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/74b3a7920ef9b96315ef20adef858b00cc067af6</guid></item><item><title>[Inductor] Make codecache CUDA compilation more robust &amp; flexible (#121490)</title><link>https://github.com/pytorch/pytorch/commit/f2e67179eeace264a5f39ae1f45ccf6e5e557aff</link><description>&lt;p&gt;[Inductor] Make codecache CUDA compilation more robust &amp;amp; flexible (#121490)&lt;/p&gt;
&lt;p&gt;Minor changes which make the CUDA compilation within _inductor/codecache.py&lt;br /&gt;
more robust and flexible.&lt;/p&gt;
&lt;p&gt;Test plan:&lt;br /&gt;
CI&lt;br /&gt;
Additional test in test_codecache.py&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121490&lt;br /&gt;
Approved by: https://github.com/jansel&lt;/p&gt;</description><pubDate>Wed, 03 Apr 2024 04:56:48 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f2e67179eeace264a5f39ae1f45ccf6e5e557aff</guid></item><item><title>[Inductor][1/n]Split cat customization (#123045)</title><link>https://github.com/pytorch/pytorch/commit/c40f386afd732efb49bd4ce10003999f77113ed4</link><description>&lt;p&gt;[Inductor][1/n]Split cat customization (#123045)&lt;/p&gt;
&lt;p&gt;Summary: Change the config and revise the group batch fusion in order not to reuse the exsiting pre_grad and post_grad fusion options&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;h1&gt;unit test&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;buck2 test @mode/dev-nosan //caffe2/test/inductor:split_cat_fx_passes&lt;/code&gt;&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/17732923560510096&lt;br /&gt;
Network: Up: 15MiB  Down: 155MiB  (reSessionID-6a577a14-1772-42d9-9ae8-bfdc62f406a3)&lt;br /&gt;
Jobs completed: 267487. Time elapsed: 2:39.7s.&lt;br /&gt;
Cache hits: 99%. Commands: 104465 (cached: 104457, remote: 8, local: 0)&lt;br /&gt;
Tests finished: Pass 11. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;/p&gt;
&lt;p&gt;&lt;code&gt;buck2 test @mode/dev-nosan //caffe2/test/inductor/fb:split_cat_fx_passes_fb&lt;/code&gt;&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/9007199283031382&lt;br /&gt;
Network: Up: 28MiB  Down: 177MiB  (reSessionID-a3081518-7cba-4c83-b442-c16655ecb2cd)&lt;br /&gt;
Jobs completed: 183164. Time elapsed: 1:41.4s.&lt;br /&gt;
Cache hits: 99%. Commands: 75875 (cached: 75862, remote: 12, local: 1)&lt;br /&gt;
Tests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;/p&gt;
&lt;p&gt;&lt;code&gt;buck2 test @mode/dev-nosan //caffe2/test/inductor:group_batch_fusion&lt;/code&gt;&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/10133099189612276&lt;br /&gt;
Network: Up: 1.3MiB           Down: 3.1MiB           (reSessionID-0d312a2d-e19e-4ba6-9f96-7eb5863734e7)&lt;br /&gt;
Discovered 9. Pass 0. Fail 0. Fatal 0. Skip 0. Timeout 0&lt;br /&gt;
Network: Up: 1.4MiB  Down: 3.2MiB  (reSessionID-0d312a2d-e19e-4ba6-9f96-7eb5863734e7)&lt;br /&gt;
Jobs completed: 68. Time elapsed: 2:19.9s.&lt;br /&gt;
Cache hits: 0%. Commands: 13 (cached: 0, remote: 1, local: 12)&lt;br /&gt;
Tests finished: Pass 9. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;br /&gt;
&lt;code&gt;buck2 test @mode/dev-nosan //caffe2/test/inductor:perf&lt;/code&gt;&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/5066549804623287&lt;br /&gt;
Network: Up: 1.5MiB  Down: 1.1MiB  (reSessionID-8d912a20-fceb-4698-89c3-d28e0708831f)&lt;br /&gt;
Jobs completed: 164. Time elapsed: 1:42.2s.&lt;br /&gt;
Cache hits: 0%. Commands: 13 (cached: 0, remote: 1, local: 12)&lt;br /&gt;
Tests finished: Pass 57. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;/p&gt;
&lt;h1&gt;local reproduce&lt;/h1&gt;
&lt;p&gt;case 1: with split cat&lt;br /&gt;
&lt;code&gt;buck2 run @mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode batch-split --model_type "cmf" --flow_id 524546542&lt;/code&gt;&lt;br /&gt;
optimus parameter sent to the scuba:&lt;br /&gt;
&lt;code&gt;{'before_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GLL6RBZb-ssXJYcBAMzw0oaKtp80br0LAAAz', 'BatchLayernormFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GH1LAxcxv0Ae_BkFAHVav3K3oosDbr0LAAAz', 'BatchSigmoidPreGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GNb0jwR-Ukkqns4CAGRmOqucfedDbr0LAAAz', 'normalization_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GHsIQxm-hn3SPrgCAKq1E-HBsoZHbr0LAAAz', 'remove_split_with_size_one_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GOrJORmbMTV_xlQDAOwolqclPsIAbr0LAAAz', 'merge_getitem_cat_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GCqkmRblvVKybGUDACVxkwVIrWxLbr0LAAAz', 'merge_splits_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GCB1QBfko_kVN0wFAKGjSZv4DJULbr0LAAAz', 'after_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GMwJPRmu4ry88swDAO1gdA5RCKIXbr0LAAAz', 'before_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GLXCORnNiKeQFmoDABR93CRKmP8Sbr0LAAAz', 'BatchMulPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GBMIPRnlwQyjSD4BANPuaMhV7MUjbr0LAAAz', 'after_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GJ9KPxkOv4LL8_0DAA65D4kh4JYDbr0LAAAz', 'inductor': Counter({'pattern_matcher_nodes': 2844, 'pattern_matcher_count': 2604, 'normalization_pass': 886, 'remove_split_with_size_one_pass': 748, 'merge_splits_pass': 82, 'merge_getitem_cat_pass': 11, 'scmerge_split_sections_removed': 4, 'batch_aten_mul': 4, 'batch_sigmoid': 2, 'batch_aten_sub': 2, 'batch_layernorm': 1, 'scmerge_split_added': 1, 'scmerge_cat_added': 1, 'scmerge_split_removed': 1, 'scmerge_cat_removed': 1, 'batch_aten_add': 1}), 'BatchAddPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GEcvPxmxBj-pd8gCABE1QgB-d6N6br0LAAAz', 'BatchSubPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GEvQxhYomJGj2FMBAEXXAI8Vgzhmbr0LAAAz'}&lt;/code&gt;&lt;br /&gt;
P1202819405&lt;/p&gt;
&lt;p&gt;case 2: without split cat&lt;br /&gt;
&lt;code&gt;buck2 run @mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode batch --model_type "cmf" --flow_id 524546542&lt;/code&gt;&lt;br /&gt;
optimus parameter sent to the scuba:&lt;br /&gt;
&lt;code&gt;{'before_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GAY7PxmGthuyjSwEAHF_A767YbMkbr0LAAAz', 'BatchLayernormFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GLDPtBacXyybEOICAKaGCPatq5oabr0LAAAz', 'BatchSigmoidPreGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GBu7ORkiDJu42QAEAGmlVTgO_Mpbbr0LAAAz', 'after_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GC893BZNl99ftY4BAHm5Z8sM4ptSbr0LAAAz', 'before_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GCAeuRYgzPO5RcsCAPO3Z7tdMNMKbr0LAAAz', 'BatchMulPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GHBIQxm1jlU-xhsFAONkzhh2mgknbr0LAAAz', 'after_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GDoUPhmZ0noiaGMDAJHYuuiwHEAUbr0LAAAz', 'inductor': Counter({'pattern_matcher_nodes': 1189, 'pattern_matcher_count': 757, 'batch_aten_mul': 9, 'batch_aten_sub': 3, 'batch_sigmoid': 2, 'batch_aten_add': 2, 'batch_layernorm': 1, 'batch_linear_post_grad': 1}), 'BatchAddPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GAluthYxi8uxpI4BAIQDzn3OyywUbr0LAAAz', 'BatchSubPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GDjsJhTK5VAcot4CADIcAixghrYibr0LAAAz', 'PostGradBatchLinearFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GEPfJxfJwktC7wsEAA0QbkqYNuVAbr0LAAAz'}&lt;/code&gt;&lt;br /&gt;
P1202823734&lt;/p&gt;
&lt;h1&gt;e2e&lt;/h1&gt;
&lt;p&gt;training_platform:fd4f02cd855f5cc0ccb49317a5a6c8bb&lt;/p&gt;
&lt;p&gt;with split cat&lt;br /&gt;
f546646358&lt;/p&gt;
&lt;p&gt;without split cat&lt;br /&gt;
f546647159&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/123045&lt;br /&gt;
Approved by: https://github.com/jackiexu1992&lt;/p&gt;</description><pubDate>Tue, 02 Apr 2024 06:36:22 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c40f386afd732efb49bd4ce10003999f77113ed4</guid></item><item><title>Revert "[aoti][reland] clear precomputed symbol replacements before cpp wrapper compilation (#123136)"</title><link>https://github.com/pytorch/pytorch/commit/1f503dffb3edb0ca4a326a8cba8c36832abc979f</link><description>&lt;p&gt;Revert "[aoti][reland] clear precomputed symbol replacements before cpp wrapper compilation (#123136)"&lt;/p&gt;
&lt;p&gt;This reverts commit 7eadb157bd96a9e641f64cdfa759aa1dfaaa7dd5.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/123136 on behalf of https://github.com/albanD due to broke ROCm CI (&lt;a href="https://github.com/pytorch/pytorch/pull/123136#issuecomment-2032163699"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Tue, 02 Apr 2024 06:17:03 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1f503dffb3edb0ca4a326a8cba8c36832abc979f</guid></item><item><title>Remove cuda dependencies when building AOTriton (#122982)</title><link>https://github.com/pytorch/pytorch/commit/76a87e33a0fdd0639842218ce26943e7b4f3838b</link><description>&lt;p&gt;Remove cuda dependencies when building AOTriton (#122982)&lt;/p&gt;
&lt;p&gt;Downloading CUDA sometimes fails and breaks the build process, but AOTriton does not need these packages for its own Triton fork. This commit comments out the related downloading scripts.&lt;/p&gt;
&lt;p&gt;The actual changes from Triton can be found at: https://github.com/ROCm/triton/commit/9b73a543a5545960bcaf2830900b0560eec443c5&lt;/p&gt;
&lt;p&gt;Fixes the following building error&lt;br /&gt;
&lt;code&gt;[2/6] cd /var/lib/jenkins/workspace/build/aotriton/src/third_party/triton/python &amp;amp;&amp;amp; /opt/conda/envs/py_3.8/bin/cmake -E env VIRTUAL_ENV=/var/lib/jenkins/workspace/build/aotriton/build/venv PATH="/var/lib/jenkins/workspace/build/aotriton/build/venv/bin:/opt/cache/bin:/opt/rocm/llvm/bin:/opt/rocm/opencl/bin:/opt/rocm/hip/bin:/opt/rocm/hcc/bin:/opt/rocm/bin:/opt/conda/envs/py_3.8/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" TRITON_BUILD_DIR=/var/lib/jenkins/workspace/build/aotriton/build/triton_build python setup.py develop
FAILED: CMakeFiles/aotriton_venv_triton /var/lib/jenkins/.local/lib/python3.8/site-packages/triton/_C/libtriton.so /var/lib/jenkins/workspace/build/aotriton/build/CMakeFiles/aotriton_venv_triton
cd /var/lib/jenkins/workspace/build/aotriton/src/third_party/triton/python &amp;amp;&amp;amp; /opt/conda/envs/py_3.8/bin/cmake -E env VIRTUAL_ENV=/var/lib/jenkins/workspace/build/aotriton/build/venv PATH="/var/lib/jenkins/workspace/build/aotriton/build/venv/bin:/opt/cache/bin:/opt/rocm/llvm/bin:/opt/rocm/opencl/bin:/opt/rocm/hip/bin:/opt/rocm/hcc/bin:/opt/rocm/bin:/opt/conda/envs/py_3.8/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" TRITON_BUILD_DIR=/var/lib/jenkins/workspace/build/aotriton/build/triton_build python setup.py develop
downloading and extracting https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-nvcc-12.1.105-0.tar.bz2 ...
downloading and extracting https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-cuobjdump-12.1.111-0.tar.bz2 ...
Traceback (most recent call last):
  File "/var/lib/jenkins/workspace/build/aotriton/src/third_party/triton/python/setup.py", line 325, in &amp;lt;module&amp;gt;
    download_and_copy(
  File "/var/lib/jenkins/workspace/build/aotriton/src/third_party/triton/python/setup.py", line 151, in download_and_copy
    ftpstream = urllib.request.urlopen(url)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 524:
ninja: build stopped: subcommand failed.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Example of failed build log: https://github.com/pytorch/pytorch/actions/runs/8483953034/job/23245996425&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122982&lt;br /&gt;
Approved by: https://github.com/jansel&lt;/p&gt;</description><pubDate>Mon, 01 Apr 2024 09:50:35 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/76a87e33a0fdd0639842218ce26943e7b4f3838b</guid></item><item><title>[inductor] Lower divide by constant as multiplication by reciprocal (#121924)</title><link>https://github.com/pytorch/pytorch/commit/03439d4c1c39d6957d09b6024946c92849b30ffc</link><description>&lt;p&gt;[inductor] Lower divide by constant as multiplication by reciprocal (#121924)&lt;/p&gt;
&lt;p&gt;Fixes #101039&lt;/p&gt;
&lt;p&gt;This lowers division by a constant value to be multipication by reciprocal.&lt;br /&gt;
The same optimization is applied in eager mode on CUDA:&lt;/p&gt;
&lt;p&gt;https://github.com/pytorch/pytorch/blob/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu#L36-L38&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121924&lt;br /&gt;
Approved by: https://github.com/lezcano&lt;/p&gt;</description><pubDate>Mon, 01 Apr 2024 06:37:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/03439d4c1c39d6957d09b6024946c92849b30ffc</guid></item><item><title>[CI] Updated expected result files after https://github.com/pytorch/pytorch/pull/122846 (#123035)</title><link>https://github.com/pytorch/pytorch/commit/fa6178d246d0832fb90838f808b10c55aa631a70</link><description>&lt;p&gt;[CI] Updated expected result files after https://github.com/pytorch/pytorch/pull/122846 (#123035)&lt;/p&gt;
&lt;p&gt;Summary: Before https://github.com/pytorch/pytorch/pull/122846, pyhpc_isoneutral_mixing in AOTI inference run segfaults so its result was not logged in the expected result file. Now it does show as fail_to_run instead of None.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/123035&lt;br /&gt;
Approved by: https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Sun, 31 Mar 2024 05:56:00 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fa6178d246d0832fb90838f808b10c55aa631a70</guid></item><item><title>[inductor] make mask_rcnn inference work in max-autotune mode (#123008)</title><link>https://github.com/pytorch/pytorch/commit/ec58f1f74ebcec744d2ab90ad34abd09c1018e92</link><description>&lt;p&gt;[inductor] make mask_rcnn inference work in max-autotune mode (#123008)&lt;/p&gt;
&lt;p&gt;inference for vision_maskrcnn model fail when max-autotune is enabled.&lt;/p&gt;
&lt;p&gt;Repro:&lt;br /&gt;
&lt;code&gt;TORCHINDUCTOR_MAX_AUTOTUNE=1 time python benchmarks/dynamo/torchbench.py --accuracy --inference --bfloat16 --backend inductor --only vision_maskrcnn&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;It turns out that MA code receives empty input tensor for convolution and some places in MA related code does not handle this corner case properly. This PR enhance that and now the accuracy test above can pass.&lt;/p&gt;
&lt;p&gt;Regarding why the input tensor is empty, I think it's probably due to no objects are detected in the input images (random data?).&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/123008&lt;br /&gt;
Approved by: https://github.com/jansel&lt;/p&gt;</description><pubDate>Sat, 30 Mar 2024 08:39:57 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ec58f1f74ebcec744d2ab90ad34abd09c1018e92</guid></item><item><title>Revert "[aoti] clear precomputed symbol replacements before cpp wrapper compilation (#122882)"</title><link>https://github.com/pytorch/pytorch/commit/a236fa9f061097107520fcc4fdca5f731b16a04a</link><description>&lt;p&gt;Revert "[aoti] clear precomputed symbol replacements before cpp wrapper compilation (#122882)"&lt;/p&gt;
&lt;p&gt;This reverts commit 384de46395234e793a319325e5c9d20a60407a64.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122882 on behalf of https://github.com/jithunnair-amd due to broke ROCm CI (&lt;a href="https://github.com/pytorch/pytorch/pull/122882#issuecomment-2027544640"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 29 Mar 2024 09:52:39 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a236fa9f061097107520fcc4fdca5f731b16a04a</guid></item><item><title>Add inductor fx pass unit test for shape propagation (#122897)</title><link>https://github.com/pytorch/pytorch/commit/315bd951e4401dedf4c1bda24d7b8caf38fdbdb2</link><description>&lt;p&gt;Add inductor fx pass unit test for shape propagation (#122897)&lt;/p&gt;
&lt;p&gt;Summary: Pre-grad fx passes expect information from shape propagation to be present. D55221119 ensured that &lt;code&gt;pass_execution_and_save&lt;/code&gt; invokes shape propagation, and this diff adds a covering unit test to prevent regression.&lt;/p&gt;
&lt;p&gt;Test Plan: New UT passes locally.&lt;/p&gt;
&lt;p&gt;Differential Revision: D55440240&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122897&lt;br /&gt;
Approved by: https://github.com/khabinov, https://github.com/Skylion007&lt;/p&gt;</description><pubDate>Fri, 29 Mar 2024 08:44:22 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/315bd951e4401dedf4c1bda24d7b8caf38fdbdb2</guid></item><item><title>[AOTI][refactor] Improve logging (#122932)</title><link>https://github.com/pytorch/pytorch/commit/375a8041ed53a5028516389b1bbfd917b678959e</link><description>&lt;p&gt;[AOTI][refactor] Improve logging (#122932)&lt;/p&gt;
&lt;p&gt;Summary: Improve some logging msgs, and change a data type to remove a compile time warning.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122932&lt;br /&gt;
Approved by: https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Fri, 29 Mar 2024 06:02:23 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/375a8041ed53a5028516389b1bbfd917b678959e</guid></item><item><title>Add wrapper for fbgemm quantization operations (#122763)</title><link>https://github.com/pytorch/pytorch/commit/966ae943dfcd3dfcfac7bcf2267a730b4ca4606b</link><description>&lt;p&gt;Add wrapper for fbgemm quantization operations (#122763)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
We add wrappers for fbgemm's packing so we can pass it through PT2 to&lt;br /&gt;
lowering phase of AOTInductor.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
Included in commit.&lt;br /&gt;
test_quantized_ops::test_wrapped_fbgemm_linear_fp16&lt;/p&gt;
&lt;p&gt;Reviewers:&lt;/p&gt;
&lt;p&gt;Subscribers:&lt;/p&gt;
&lt;p&gt;Tasks:&lt;/p&gt;
&lt;p&gt;Tags:&lt;/p&gt;
&lt;p&gt;Differential Revision: &lt;a href="https://our.internmc.facebook.com/intern/diff/D55433204"&gt;D55433204&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122763&lt;br /&gt;
Approved by: https://github.com/jerryzh168&lt;br /&gt;
ghstack dependencies: #122762&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 10:41:18 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/966ae943dfcd3dfcfac7bcf2267a730b4ca4606b</guid></item><item><title>[AOTInductor] Support use_runtime_constant_folding for CPU. (#122563)</title><link>https://github.com/pytorch/pytorch/commit/091a24495b29d7aa077b16b508972f88d3a5aad5</link><description>&lt;p&gt;[AOTInductor] Support use_runtime_constant_folding for CPU. (#122563)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
We allow CPU to use the config use_runtime_constant_folding.&lt;br /&gt;
Changes include&lt;br /&gt;
1. Rearrange USE_CUDA flags. Add CPU sections that consumes memory directly.&lt;br /&gt;
2. Codegen changes to accomodate cpp fusions for CPU only. Specifically, we shouldn't generate 2 headers that would cause re-declaration.&lt;/p&gt;
&lt;p&gt;Test Plan: Activate tests that were deactivated for CPU before.&lt;/p&gt;
&lt;p&gt;Reviewed By: khabinov&lt;/p&gt;
&lt;p&gt;Differential Revision: D55234300&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122563&lt;br /&gt;
Approved by: https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 09:49:05 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/091a24495b29d7aa077b16b508972f88d3a5aad5</guid></item><item><title>[Inductor]Fix a couple of broken unit tests (#122714)</title><link>https://github.com/pytorch/pytorch/commit/4670dcc94c64f8f34673476ca2db3b840e41daf4</link><description>&lt;p&gt;[Inductor]Fix a couple of broken unit tests (#122714)&lt;/p&gt;
&lt;p&gt;Summary: Titled&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
&lt;code&gt;buck2 test mode/dev-nosan //caffe2/test/inductor:group_batch_fusion&lt;/code&gt;&lt;br /&gt;
Buck UI: https://www.internalfb.com/buck2/ad05a43c-cb4a-443e-8904-b4d53e4f4b1e&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/13510798909218388&lt;br /&gt;
Network: Up: 107KiB  Down: 28KiB  (reSessionID-d7146e4f-773a-46ea-9852-f10f59302479)&lt;br /&gt;
Jobs completed: 24. Time elapsed: 1:49.3s.&lt;br /&gt;
Cache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)&lt;br /&gt;
Tests finished: Pass 9. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;/p&gt;
&lt;p&gt;&lt;code&gt;buck2 test 'fbcode//mode/opt' fbcode//caffe2/test/inductor/fb:split_cat_fx_passes_fb&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Buck UI: https://www.internalfb.com/buck2/82dbf3b0-c747-4c07-98b8-53b69afa3157&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/1125900267699118&lt;br /&gt;
Network: Up: 1.4GiB  Down: 2.3GiB  (reSessionID-0bd22c6d-5dfe-4b4a-bc24-705eadac884b)&lt;br /&gt;
Jobs completed: 252570. Time elapsed: 7:25.2s.&lt;br /&gt;
Cache hits: 95%. Commands: 123778 (cached: 117999, remote: 2779, local: 3000)&lt;br /&gt;
Tests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;/p&gt;
&lt;p&gt;Differential Revision: D55378009&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122714&lt;br /&gt;
Approved by: https://github.com/SherlockNoMad&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 09:44:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4670dcc94c64f8f34673476ca2db3b840e41daf4</guid></item><item><title>[PT2][Inductor][Observability] Improve the optimus scuba log (#122361)</title><link>https://github.com/pytorch/pytorch/commit/9693797491a3b20dc1ea7825b6d26f8aace90322</link><description>&lt;p&gt;[PT2][Inductor][Observability] Improve the optimus scuba log (#122361)&lt;/p&gt;
&lt;p&gt;Summary: Titled&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
&lt;code&gt;buck2 test mode/dev-nosan //caffe2/test/inductor:group_batch_fusion&lt;/code&gt;&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/18014398535709463&lt;br /&gt;
Network: Up: 113KiB           Down: 480KiB           (reSessionID-1d2e3558-15b5-4a4e-8c5d-10c983afb389)&lt;br /&gt;
Discovered 9. Pass 0. Fail 0. Fatal 0. Skip 0. Timeout 0&lt;br /&gt;
Command: test.                                                                                 Remaining: 9/24. Cache hits: 0%. Time elapsed: 44.3s&lt;br /&gt;
Command: test.                                                                                 Remaining: 9/24. Cache hits: 0%. Time elapsed: 44.4s&lt;br /&gt;
Command: test.                                                                                 Remaining: 9/24. Cache hits: 0%. Time elapsed: 44.5s&lt;br /&gt;
Network: Up: 117KiB  Down: 507KiB  (reSessionID-1d2e3558-15b5-4a4e-8c5d-10c983afb389)&lt;br /&gt;
Jobs completed: 24. Time elapsed: 1:48.3s.&lt;br /&gt;
Cache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)&lt;br /&gt;
Tests finished: Pass 9. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;br /&gt;
&lt;code&gt;buck2 test mode/dev-nosan //caffe2/test/inductor:split_cat_fx_passes&lt;/code&gt;&lt;br /&gt;
Test UI: https://www.internalfb.com/intern/testinfra/testrun/16044073698893554&lt;br /&gt;
Network: Up: 120KiB  Down: 60KiB  (reSessionID-57f2c21b-3f4e-462b-9e5b-fe3dd15f6b7d)&lt;br /&gt;
Jobs completed: 28. Time elapsed: 1:47.5s.&lt;br /&gt;
Cache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)&lt;br /&gt;
Tests finished: Pass 11. Fail 0. Fatal 0. Skip 0. Build failure 0&lt;/p&gt;
&lt;p&gt;optimus_scuba_log:&lt;br /&gt;
&lt;code&gt;{'before_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GIbj2haUwKx69H8BAKXdGqXZSpoybr0LAAAz', 'group_batch_fusion_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GFqhiRYcJ_C4JFoDABKPTsfpzjJ_br0LAAAz', 'normalization_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GIvswhaiAVyipcoGAJZ5sUi8Bb5qbr0LAAAz', 'remove_split_with_size_one_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GFneTxcVBPaqVuwCADCiI4q1mEwlbr0LAAAz', 'merge_getitem_cat_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GJc0Phn87ljuMO0CADBPGqqehKp2br0LAAAz', 'merge_splits_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GLWB_BbvLyT7D_0DABmygDYPDjJ_br0LAAAz', 'after_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GO6eQBeIj6oV3o4JAFLzQ3ECMTIrbr0LAAAz', 'inductor_pre_grad': Counter({'pattern_matcher_nodes': 2006, 'pattern_matcher_count': 1806, 'normalization_pass': 861, 'remove_split_with_size_one_pass': 748, 'merge_splits_pass': 82, 'merge_getitem_cat_pass': 11, 'scmerge_split_sections_removed': 4, 'batch_layernorm': 1, 'batch_sigmoid': 1, 'scmerge_split_added': 1, 'scmerge_cat_added': 1, 'scmerge_split_removed': 1, 'scmerge_cat_removed': 1}), 'before_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GMoKmxYg6AUeQ40KAMDaJ4EVDwYmbr0LAAAz', 'group_batch_fusion_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GHIvQxkrV1PMBggEACv7786a2bE8br0LAAAz', 'after_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;amp;handle=GIpBNxXupQTHWx8BALSiVrKgDbtfbr0LAAAz', 'inductor_post_grad': Counter({'pattern_matcher_nodes': 2093, 'pattern_matcher_count': 1893, 'normalization_pass': 861, 'remove_split_with_size_one_pass': 748, 'merge_splits_pass': 82, 'merge_getitem_cat_pass': 11, 'scmerge_split_sections_removed': 4, 'batch_layernorm': 1, 'batch_sigmoid': 1, 'scmerge_split_added': 1, 'scmerge_cat_added': 1, 'scmerge_split_removed': 1, 'scmerge_cat_removed': 1, 'batch_aten_mul': 1})}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Differential Revision: D55107000&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122361&lt;br /&gt;
Approved by: https://github.com/jackiexu1992&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 09:13:32 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9693797491a3b20dc1ea7825b6d26f8aace90322</guid></item><item><title>[inductor][Autotune] Add matrix_instr_nonkdim to triton_meta (#122852)</title><link>https://github.com/pytorch/pytorch/commit/049d68d8bb1c0a10b41ae70ddb60d84b052c2cb5</link><description>&lt;p&gt;[inductor][Autotune] Add matrix_instr_nonkdim to triton_meta (#122852)&lt;/p&gt;
&lt;p&gt;Summary: Previous work &lt;code&gt;https://github.com/pytorch/pytorch/pull/120742&lt;/code&gt; to enable &lt;code&gt;matrix_instr_nonkdim&lt;/code&gt; only dealt with the autotuner benchmarking, but failed to enable the parameter in Triton meta for real runs. &lt;code&gt;matrix_instr_nonkdim&lt;/code&gt; needs to be visible to the compiler driver to set up the optimization pipeline, so it's unlike other kernel parameters such as &lt;code&gt;BLOCK_N&lt;/code&gt; that can be just set inside the kernel itself.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
P1201466917&lt;/p&gt;
&lt;p&gt;triton_heuristics.template(&lt;br /&gt;
    num_stages=1,&lt;br /&gt;
    num_warps=4,&lt;br /&gt;
    triton_meta={'signature': {0: '&lt;em&gt;fp32', 1: '&lt;/em&gt;fp32', 2: '*fp32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())], 'matrix_instr_nonkdim': 16},&lt;br /&gt;
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_0', 'backend_hash': None},&lt;br /&gt;
  )&lt;/p&gt;
&lt;p&gt;Perf :&lt;br /&gt;
Before: 1.693ms    0.134GB    79.28GB/s&lt;br /&gt;
After:    1.577ms    0.134GB    85.12GB/s&lt;/p&gt;
&lt;p&gt;Differential Revision: D55456401&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122852&lt;br /&gt;
Approved by: https://github.com/xw285cornell&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 08:58:38 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/049d68d8bb1c0a10b41ae70ddb60d84b052c2cb5</guid></item><item><title>[dynamo] Fix traceback generation on runtime errors (#122746)</title><link>https://github.com/pytorch/pytorch/commit/f178d996a893737f44c97c18a7ed6358780374db</link><description>&lt;p&gt;[dynamo] Fix traceback generation on runtime errors (#122746)&lt;/p&gt;
&lt;p&gt;Fixes &lt;code&gt;During handling of the above exception, another exception occurred: [...] torch._dynamo.exc.Unsupported: generator&lt;/code&gt;. traceback.format_exc uses generators which isn't supported by dynamo yet.&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;current error message&lt;/summary&gt;

```
======================================================================
ERROR: test_custom_fn_saved_tensors (__main__.TestCompiledAutograd)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 307, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1537, in _call_impl
    return forward_call(*args, **kwargs)
  File "&lt;eval_with_key&gt;.0", line 4, in forward
    def forward(self, inputs, sizes, hooks):
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xmfan/core/pytorch/torch/testing/_internal/common_utils.py", line 2741, in wrapper
    method(*args, **kwargs)
  File "/home/xmfan/core/pytorch/test/inductor/test_compiled_autograd.py", line 499, in test_custom_fn_saved_tensors
    self.check_output_and_recompiles(fn, 1)
  File "/home/xmfan/core/pytorch/test/inductor/test_compiled_autograd.py", line 61, in check_output_and_recompiles
    actual = list(opt_fn())
  File "/home/xmfan/core/pytorch/test/inductor/test_compiled_autograd.py", line 495, in fn
    loss.backward()
  File "/home/xmfan/core/pytorch/torch/_tensor.py", line 534, in backward
    torch.autograd.backward(
  File "/home/xmfan/core/pytorch/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/xmfan/core/pytorch/torch/autograd/graph.py", line 766, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1537, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/_dynamo/eval_frame.py", line 397, in _fn
    res = fn(*args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 741, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 315, in __call__
    _WrappedCall._generate_error_message(topmost_framesummary),
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 289, in _generate_error_message
    tb_repr = get_traceback()
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 288, in get_traceback
    return traceback.format_exc()
  File "/home/xmfan/.conda/envs/benchmarks/lib/python3.10/traceback.py", line 183, in format_exc
    return "".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))
  File "/home/xmfan/.conda/envs/benchmarks/lib/python3.10/traceback.py", line 136, in format_exception
    return list(te.format(chain=chain))
  File "/home/xmfan/core/pytorch/torch/_dynamo/convert_frame.py", line 941, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/home/xmfan/core/pytorch/torch/_dynamo/convert_frame.py", line 348, in _convert_frame_assert
    unimplemented("generator")
  File "/home/xmfan/core/pytorch/torch/_dynamo/exc.py", line 199, in unimplemented
    raise Unsupported(msg)
torch._dynamo.exc.Unsupported: generator
```

&lt;/details&gt;

&lt;p&gt;With this change, we get back the descriptive error message:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;post-fix error message&lt;/summary&gt;

```
Traceback (most recent call last):
  File "/home/xmfan/core/pytorch/torch/fx/graph_module.py", line 307, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/xmfan/core/pytorch/torch/nn/modules/module.py", line 1537, in _call_impl
    return forward_call(*args, **kwargs)
  File "&lt;eval_with_key&gt;.0", line 4, in forward
    def forward(self, inputs, sizes, hooks):
IndexError: list index out of range

Call using an FX-traced Module, line 4 of the traced Module's generated forward function:

def forward(self, inputs, sizes, hooks):

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE
    getitem = inputs[0]

    getitem_1 = inputs[1];  inputs = None
```

&lt;/details&gt;

&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122746&lt;br /&gt;
Approved by: https://github.com/jansel, https://github.com/anijain2305&lt;br /&gt;
ghstack dependencies: #122691&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 06:40:54 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f178d996a893737f44c97c18a7ed6358780374db</guid></item><item><title>Revert "[Inductor] Run pattern matcher over the original graph (#122519)"</title><link>https://github.com/pytorch/pytorch/commit/b63f6f78dc8a4e6f059b9abc4eb1eed1d490bcfc</link><description>&lt;p&gt;Revert "[Inductor] Run pattern matcher over the original graph (#122519)"&lt;/p&gt;
&lt;p&gt;This reverts commit 1f5fcb4e203eb343e8c53f6444015c98e8f68d60.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122519 on behalf of https://github.com/atalman due to Breaks internal tests (&lt;a href="https://github.com/pytorch/pytorch/pull/122519#issuecomment-2023022311"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Wed, 27 Mar 2024 07:13:26 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b63f6f78dc8a4e6f059b9abc4eb1eed1d490bcfc</guid></item><item><title>[inductor][cpp] simplify CppVecKernelChecker (remove bool/int8 load as mask and load as float flags) (#119734)</title><link>https://github.com/pytorch/pytorch/commit/105381ea1159eb025a580f008c78f2682caee930</link><description>&lt;p&gt;[inductor][cpp] simplify CppVecKernelChecker (remove bool/int8 load as mask and load as float flags) (#119734)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/119734&lt;br /&gt;
Approved by: https://github.com/leslie-fang-intel, https://github.com/jansel&lt;br /&gt;
ghstack dependencies: #119654, #119655&lt;/p&gt;</description><pubDate>Wed, 27 Mar 2024 03:20:35 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/105381ea1159eb025a580f008c78f2682caee930</guid></item><item><title>[Inductor] Support custom op in JIT with cpp wrapper (#122554)</title><link>https://github.com/pytorch/pytorch/commit/537cd66e73e8a9b33c843d55d546471f3074a390</link><description>&lt;p&gt;[Inductor] Support custom op in JIT with cpp wrapper (#122554)&lt;/p&gt;
&lt;p&gt;Summary:  To call custom ops in an ABI-compatible way requires doing boxed call with varargs across C shim. In the JIT mode, we can get around it by calling into Python.  https://gist.github.com/desertfire/be2a65b0a9b47780bb716b53ac2cd2b3 is an example of generated code.&lt;/p&gt;
&lt;p&gt;Differential Revision: &lt;a href="https://our.internmc.facebook.com/intern/diff/D55326556"&gt;D55326556&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122554&lt;br /&gt;
Approved by: https://github.com/jansel, https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Tue, 26 Mar 2024 10:48:45 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/537cd66e73e8a9b33c843d55d546471f3074a390</guid></item><item><title>Log autotune time in scuba (#122637)</title><link>https://github.com/pytorch/pytorch/commit/e61aaab72562fe95d76e77b01678d71467e1739e</link><description>&lt;p&gt;Log autotune time in scuba (#122637)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
This diff&lt;br /&gt;
* Refactors triton and autotune caches to be child classes of the original memcache based cache infra&lt;br /&gt;
* Swaps scuba table for autotune&lt;br /&gt;
* Adds autotune time spent/saved to scuba table&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
Local testing using:&lt;br /&gt;
&lt;code&gt;buck run mode/opt fbcode//caffe2/test/inductor/:max_autotune -- -r test_max_autotune_remote_caching_dynamic_False&lt;/code&gt;&lt;br /&gt;
and&lt;br /&gt;
&lt;code&gt;TORCH_INDUCTOR_AUTOTUNE_REMOTE_CACHE=1 buck2 run mode/opt //scripts/oulgen:runner&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Differential Revision: D55332620&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122637&lt;br /&gt;
Approved by: https://github.com/jamesjwu&lt;/p&gt;</description><pubDate>Tue, 26 Mar 2024 09:51:33 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e61aaab72562fe95d76e77b01678d71467e1739e</guid></item><item><title>[Inductor] Run pattern matcher over the original graph (#122519)</title><link>https://github.com/pytorch/pytorch/commit/1f5fcb4e203eb343e8c53f6444015c98e8f68d60</link><description>&lt;p&gt;[Inductor] Run pattern matcher over the original graph (#122519)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122519&lt;br /&gt;
Approved by: https://github.com/jansel&lt;/p&gt;</description><pubDate>Tue, 26 Mar 2024 09:30:32 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1f5fcb4e203eb343e8c53f6444015c98e8f68d60</guid></item><item><title>[ez] Add more files to trigger inductor (#122669)</title><link>https://github.com/pytorch/pytorch/commit/8bad7b63c8b1ba4e7d1a9d86023d5b29068c2cb4</link><description>&lt;p&gt;[ez] Add more files to trigger inductor (#122669)&lt;/p&gt;
&lt;p&gt;To catch https://github.com/pytorch/pytorch/pull/122562/files&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122669&lt;br /&gt;
Approved by: https://github.com/desertfire&lt;/p&gt;</description><pubDate>Tue, 26 Mar 2024 07:19:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8bad7b63c8b1ba4e7d1a9d86023d5b29068c2cb4</guid></item><item><title>[BE][CPUInductor] Use C++17 helper templates (#122607)</title><link>https://github.com/pytorch/pytorch/commit/9d1de31634a5b9c8c990a6356ab8864bb4d9bede</link><description>&lt;p&gt;[BE][CPUInductor] Use C++17 helper templates (#122607)&lt;/p&gt;
&lt;p&gt;Such as &lt;code&gt;std::is_same_v&lt;/code&gt; ,&lt;code&gt;std::is_integral_v&lt;/code&gt; and C++14 one &lt;code&gt;std::enable_if_t&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122607&lt;br /&gt;
Approved by: https://github.com/jansel, https://github.com/Skylion007&lt;/p&gt;</description><pubDate>Mon, 25 Mar 2024 11:01:44 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9d1de31634a5b9c8c990a6356ab8864bb4d9bede</guid></item><item><title>[inductor] Improve error message for shape errors in slice_scatter (#122543)</title><link>https://github.com/pytorch/pytorch/commit/2db7d874a92f26ada3926b35de5716c1f457220e</link><description>&lt;p&gt;[inductor] Improve error message for shape errors in slice_scatter (#122543)&lt;/p&gt;
&lt;p&gt;Fixes #122291&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122543&lt;br /&gt;
Approved by: https://github.com/shunting314&lt;/p&gt;</description><pubDate>Mon, 25 Mar 2024 10:57:16 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2db7d874a92f26ada3926b35de5716c1f457220e</guid></item><item><title>GPT2 SDPA inference pattern-matching for Inductor-CPU (#121866)</title><link>https://github.com/pytorch/pytorch/commit/5152945441b2a1387f6e9fc8fe1531bfeb6cabe4</link><description>&lt;p&gt;GPT2 SDPA inference pattern-matching for Inductor-CPU (#121866)&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;With this PR, SDPA pattern of GPT2 is being mapped to &lt;code&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;.&lt;br /&gt;
While GPT2 supports both a causal mask &amp;amp; an attention mask, this PR considers the case of attention mask being absent.&lt;br /&gt;
TorchBench inference workload for GPT2 also doesn't use an attention-mask.&lt;/p&gt;
&lt;p&gt;This pattern's replacement is being disabled for CUDA because &lt;a href="https://github.com/pytorch/pytorch/actions/runs/8319111885/job/22762567770"&gt;CUDA AOT Inductor&lt;/a&gt; CI job's &lt;code&gt;GPT2ForSequenceClassification&lt;/code&gt; accuracy test failed, although all other trunk CUDA Inductor CI checks had passed.&lt;br /&gt;
Created #122429 to track that particular issue.&lt;/p&gt;
&lt;h3&gt;CPU performance data with TorchBench&lt;/h3&gt;
&lt;p&gt;|MODEL |BATCH SIZE | DTYPE | BEFORE: Speedup over eager-mode with the default Inductor implementation | AFTER: Speedup over eager mode with SDPA op mapped| Perf boost = (AFTER - BEFORE)/BEFORE * 100|&lt;br /&gt;
|--------------------------|-------------|---------|-----------------------------|--------------------------|------------|&lt;br /&gt;
|hf_GPT2| 1 | FP32 | 1.522x | 1.791x| 17.67%|&lt;br /&gt;
|hf_GPT2| 1 | BF16 (AMP) | 1.795x | 2.387x| 32.98%|&lt;br /&gt;
|hf_GPT2| 2 | FP32 |  1.313x |1.629x | 19.3%|&lt;br /&gt;
|hf_GPT2|2| BF16 (AMP) | 1.556x | 1.924x | 23.65%|&lt;br /&gt;
|hf_GPT2_large| 1 | FP32 | 1.380x |1.585x | 12.93%|&lt;br /&gt;
|hf_GPT2_large| 1 | BF16 (AMP) | 1.208x | 1.567x | 22.91%|&lt;br /&gt;
|hf_GPT2_large| 2 | FP32 | 1.188x | 1.490x | 25.42%|&lt;br /&gt;
|hf_GPT2_large|2| BF16 (AMP) | 0.991x | 1.575x | 58.93%|&lt;/p&gt;
&lt;p&gt;Machine - Intel(R) Xeon(R) Platinum 8468H (Xeon 4th gen Sapphire Rapids)&lt;br /&gt;
48 physical cores were used. Intel OpenMP &amp;amp; libtcmalloc were preloaded.&lt;/p&gt;
&lt;p&gt;Example command -&lt;br /&gt;
&lt;code&gt;OMP_NUM_THREADS=48 MKL_NUM_THREADS=48 numactl --membind=0 --cpunodebind=0 -C 0-47 python benchmarks/dynamo/torchbench.py --performance --inference --inductor --float32 -dcpu --only hf_GPT2_large --freezing --batch-size 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121866&lt;br /&gt;
Approved by: https://github.com/Valentine233, https://github.com/jgong5, https://github.com/desertfire&lt;/p&gt;</description><pubDate>Mon, 25 Mar 2024 07:04:03 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5152945441b2a1387f6e9fc8fe1531bfeb6cabe4</guid></item><item><title>[EZ][BE] Add missing `acosh` op to vec256_float_neon.h (#122513)</title><link>https://github.com/pytorch/pytorch/commit/bf40e3f880cc6820656d5d5fc32a21ca8334c520</link><description>&lt;p&gt;[EZ][BE] Add missing &lt;code&gt;acosh&lt;/code&gt; op to vec256_float_neon.h (#122513)&lt;/p&gt;
&lt;p&gt;As base class has it&lt;br /&gt;
https://github.com/pytorch/pytorch/blob/ed15370aabf951eec2ba0140de5ff71634868791/aten/src/ATen/cpu/vec/vec_base.h#L367-L369&lt;/p&gt;
&lt;p&gt;Discovered while attempting to enabling Inductor vectorization on ARM platform&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122513&lt;br /&gt;
Approved by: https://github.com/Skylion007&lt;/p&gt;</description><pubDate>Sat, 23 Mar 2024 06:18:02 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bf40e3f880cc6820656d5d5fc32a21ca8334c520</guid></item><item><title>[aoti] Add handling of ir.Constants in promote_constants (#122419)</title><link>https://github.com/pytorch/pytorch/commit/ed15370aabf951eec2ba0140de5ff71634868791</link><description>&lt;p&gt;[aoti] Add handling of ir.Constants in promote_constants (#122419)&lt;/p&gt;
&lt;p&gt;This issue popped up when enabling predispatch IR on the benchmarks (https://github.com/pytorch/pytorch/pull/122225)&lt;/p&gt;
&lt;p&gt;On the following model:&lt;br /&gt;
```&lt;br /&gt;
class M(torch.nn.Module):&lt;br /&gt;
    def &lt;strong&gt;init&lt;/strong&gt;(self, device):&lt;br /&gt;
        super().&lt;strong&gt;init&lt;/strong&gt;()&lt;br /&gt;
        self.device = device&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def forward(self, x):
    t = torch.tensor(x.size(-1), device=self.device, dtype=torch.float)
    t = torch.sqrt(t * 3)
    return x * t
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;We get the following error:&lt;br /&gt;
```&lt;br /&gt;
======================================================================&lt;br /&gt;
ERROR: test_constant_abi_compatible_cuda (&lt;strong&gt;main&lt;/strong&gt;.AOTInductorTestABICompatibleCuda)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Traceback (most recent call last):&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/testing/_internal/common_utils.py", line 2741, in wrapper&lt;br /&gt;
    method(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_torchinductor.py", line 9232, in new_test&lt;br /&gt;
    return value(self)&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;/em&gt;args, &lt;strong&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor.py", line 922, in test_constant&lt;br /&gt;
    self.check_model(M(self.device), (torch.randn(5, 5, device=self.device),))&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor.py", line 91, in check_model&lt;br /&gt;
    actual = AOTIRunnerUtil.run(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor_utils.py", line 102, in run&lt;br /&gt;
    so_path = AOTIRunnerUtil.compile(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor_utils.py", line 40, in compile&lt;br /&gt;
    so_path = torch._inductor.aot_compile_ep(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/&lt;strong&gt;init&lt;/strong&gt;.py", line 150, in aot_compile_ep&lt;br /&gt;
    return compile_fx_aot(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1005, in compile_fx_aot&lt;br /&gt;
    compiled_lib_path = compile_fx(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(*args, &lt;/strong&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1111, in compile_fx&lt;br /&gt;
    return compile_fx(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1145, in compile_fx&lt;br /&gt;
    return compile_fx(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;/em&gt;args, &lt;strong&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/&lt;em&gt;inductor/compile_fx.py", line 1336, in compile_fx&lt;br /&gt;
    return inference_compiler(unlifted_gm, example_inputs&lt;/em&gt;)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/utils.py", line 265, in time_wrapper&lt;br /&gt;
    r = func(*args, &lt;/strong&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1266, in fw_compiler_base&lt;br /&gt;
    return inner_compile(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper&lt;br /&gt;
    inner_compiled_fn = compiler_fn(gm, example_inputs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/debug.py", line 304, in inner&lt;br /&gt;
    return fn(&lt;/em&gt;args, &lt;strong&gt;kwargs)&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(*args, &lt;/strong&gt;kwds)&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/utils.py", line 265, in time_wrapper&lt;br /&gt;
    r = func(&lt;/em&gt;args, &lt;strong&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 447, in compile_fx_inner&lt;br /&gt;
    compiled_graph = fx_codegen_and_compile(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 707, in fx_codegen_and_compile&lt;br /&gt;
    graph.run(&lt;em&gt;example_inputs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/utils.py", line 265, in time_wrapper&lt;br /&gt;
    r = func(&lt;/em&gt;args, &lt;/strong&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 612, in run&lt;br /&gt;
    return super().run(&lt;em&gt;args)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/fx/interpreter.py", line 145, in run&lt;br /&gt;
    self.env[node] = self.run_node(node)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 957, in run_node&lt;br /&gt;
    result = super().run_node(n)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/fx/interpreter.py", line 202, in run_node&lt;br /&gt;
    return getattr(self, n.op)(n.target, args, kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 819, in call_function&lt;br /&gt;
    raise LoweringException(e, target, args, kwargs).with_traceback(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 816, in call_function&lt;br /&gt;
    out = lowerings&lt;a href="*args, **kwargs"&gt;target&lt;/a&gt;&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 298, in wrapped&lt;br /&gt;
    out = decomp_fn(&lt;/em&gt;args, **kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 5340, in mul&lt;br /&gt;
    return make_pointwise(fn)(a, b)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 409, in inner&lt;br /&gt;
    inputs = promote_constants(inputs, override_return_dtype)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 373, in promote_constants&lt;br /&gt;
    ex = next(x for x in inputs if isinstance(x, (TensorBox, ExpandView)))&lt;br /&gt;
torch._inductor.exc.LoweringException: StopIteration:&lt;br /&gt;
  target: aten.mul.Tensor&lt;br /&gt;
  args[0]: Constant(value=5.0, dtype=torch.float32, device=device(type='cuda', index=0))&lt;br /&gt;
  args[1]: 3&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;So I added an additional casing in &lt;code&gt;promote_constants&lt;/code&gt; to handle the ir.Constants and now it works! Although please let me know if this is the wrong approach. Here's a paste of the full run with the inductor logs: P1198927007&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122419&lt;br /&gt;
Approved by: https://github.com/eellison, https://github.com/desertfire, https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 10:39:36 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ed15370aabf951eec2ba0140de5ff71634868791</guid></item><item><title>[inductor] device guard for max autotune benchmark (#122479)</title><link>https://github.com/pytorch/pytorch/commit/a3d4eaf2531a9aae974ed2eab157a3b3594e7e7b</link><description>&lt;p&gt;[inductor] device guard for max autotune benchmark (#122479)&lt;/p&gt;
&lt;p&gt;Internal users reported that they get failure for max-autotune if tensors are not on device 0. It turns out that we may use tensors on device say 6 and run kernel on them at device 0.&lt;/p&gt;
&lt;p&gt;This PR enforces that we do benchmarking for max-autotune on the correct device.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122479&lt;br /&gt;
Approved by: https://github.com/xintwfb, https://github.com/Chillee&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 09:27:53 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a3d4eaf2531a9aae974ed2eab157a3b3594e7e7b</guid></item><item><title>[aot_inductor] added runtime checks for input/output tensors in debug compile mode (#122047)</title><link>https://github.com/pytorch/pytorch/commit/86082f1fdc336ca89f9b6f8888fba9e87f3a4c06</link><description>&lt;p&gt;[aot_inductor] added runtime checks for input/output tensors in debug compile mode (#122047)&lt;/p&gt;
&lt;p&gt;This PR added runtime checks to guard the dtypes and shapes of input/output tensors.&lt;br /&gt;
Currently, we enable these only for debug compilation&lt;br /&gt;
(i.e. aot_inductor.debug_compile is True) in abi_compatible mode.&lt;/p&gt;
&lt;p&gt;Differential Revision: &lt;a href="https://our.internmc.facebook.com/intern/diff/D54993148"&gt;D54993148&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122047&lt;br /&gt;
Approved by: https://github.com/desertfire&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 08:40:33 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/86082f1fdc336ca89f9b6f8888fba9e87f3a4c06</guid></item><item><title>Revert "[Quant] [PT2] Add SiLU  into X86InductorQuantizer Conv2d Unary Annotation (#122267)"</title><link>https://github.com/pytorch/pytorch/commit/60bc29aa0b22dcc06c5778589a9d2b5c4b9fc965</link><description>&lt;p&gt;Revert "[Quant] [PT2] Add SiLU  into X86InductorQuantizer Conv2d Unary Annotation (#122267)"&lt;/p&gt;
&lt;p&gt;This reverts commit 2c6eeb26d3f61fba352ad51fd8653120937a20f3.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122267 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/60bc29aa0b22dcc06c5778589a9d2b5c4b9fc965</guid></item><item><title>Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op SiLU (#122268)"</title><link>https://github.com/pytorch/pytorch/commit/b30b396d057f3462fbf2a1b46b8f480947dcb6d0</link><description>&lt;p&gt;Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op SiLU (#122268)"&lt;/p&gt;
&lt;p&gt;This reverts commit 99f0fec7d0873d627e8c7f2dec65818d725424b0.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122268 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b30b396d057f3462fbf2a1b46b8f480947dcb6d0</guid></item><item><title>Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardSwish with int8-mix-bf16 (#122373)"</title><link>https://github.com/pytorch/pytorch/commit/777ac511cc98a7edf916e61d0770afac4e37fccc</link><description>&lt;p&gt;Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardSwish with int8-mix-bf16 (#122373)"&lt;/p&gt;
&lt;p&gt;This reverts commit 783fd89ff1cf401e484c20d14b16823abf20d87d.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122373 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/777ac511cc98a7edf916e61d0770afac4e37fccc</guid></item><item><title>Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)"</title><link>https://github.com/pytorch/pytorch/commit/dbedc6bb7cf4cfaab5df85aff2e4dbaf049e6746</link><description>&lt;p&gt;Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)"&lt;/p&gt;
&lt;p&gt;This reverts commit 23a6d74f9352e0afb37750fee300d077c4ba9393.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122374 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dbedc6bb7cf4cfaab5df85aff2e4dbaf049e6746</guid></item><item><title>[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)</title><link>https://github.com/pytorch/pytorch/commit/23a6d74f9352e0afb37750fee300d077c4ba9393</link><description>&lt;p&gt;[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;br /&gt;
Enable the fusion pattern of &lt;code&gt;QConv2d -&amp;gt; hardtanh&lt;/code&gt; lowering for int8-mixed-bf16 case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Test Plan&lt;/strong&gt;&lt;br /&gt;
&lt;code&gt;python -m pytest test_mkldnn_pattern_matcher.py -k test_qconv2d_hardtanh_int8_mixed_bf16_cpu&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122374&lt;br /&gt;
Approved by: https://github.com/leslie-fang-intel, https://github.com/jgong5&lt;br /&gt;
ghstack dependencies: #122266, #122267, #122268, #122373&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 05:13:14 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/23a6d74f9352e0afb37750fee300d077c4ba9393</guid></item><item><title>Precompile triton templates (#121998)</title><link>https://github.com/pytorch/pytorch/commit/b8df2f0ca530ebe01fa079c891c170a1f4b22823</link><description>&lt;p&gt;Precompile triton templates (#121998)&lt;/p&gt;
&lt;p&gt;Before this PR we were not precompiling triton templates in parallel. Compilation would occur during benchmarking.&lt;/p&gt;
&lt;p&gt;Triton benchmarking templates were emitted as :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;@triton.jit
def triton_mm(arg_A, arg_B, out_ptr0):&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In order to precompile we need to give the full kernel specification, as we do when we emit the template in the final output code generation.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'backend_hash': 'cdeecfeccd31ad7810f96b5752194b1c2406d0a81e39a6ca09c8ee150baae183'},
)
@triton.jit
def triton_mm(arg_A, arg_B, out_ptr0):&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121998&lt;br /&gt;
Approved by: https://github.com/jansel&lt;br /&gt;
ghstack dependencies: #121996, #120275, #121997&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 09:04:53 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b8df2f0ca530ebe01fa079c891c170a1f4b22823</guid></item><item><title>Introduce XPU implementation for PyTorch ATen operators (#120891)</title><link>https://github.com/pytorch/pytorch/commit/148a8de6397be6e4b4ca1508b03b82d117bfb03c</link><description>&lt;p&gt;Introduce XPU implementation for PyTorch ATen operators (#120891)&lt;/p&gt;
&lt;p&gt;As a follow-up to #114835 and #119682, we add limited ATen operators implementation for XPU. With this PR, the blocking issue for oneDNN operations and Inductor XPU backend will be resolved as the two components depend on these operations to support its basic features, respectively.&lt;/p&gt;
&lt;p&gt;The added ATen operators include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;copy_&lt;/code&gt;, &lt;code&gt;_to_copy&lt;/code&gt;, &lt;code&gt;_copy_from_and_resize&lt;/code&gt;, , &lt;code&gt;clone&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;view&lt;/code&gt;, &lt;code&gt;view_as_real&lt;/code&gt;, &lt;code&gt;view_as_complex&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;as_strided&lt;/code&gt;, &lt;code&gt;_reshape_alias&lt;/code&gt;, &lt;code&gt;resize_&lt;/code&gt;, &lt;code&gt;resize_as_&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;add&lt;/code&gt;/&lt;code&gt;add_&lt;/code&gt;, &lt;code&gt;sub&lt;/code&gt;/&lt;code&gt;sub_&lt;/code&gt;, &lt;code&gt;mul&lt;/code&gt;/&lt;code&gt;mul_&lt;/code&gt;, &lt;code&gt;div&lt;/code&gt;/&lt;code&gt;div_&lt;/code&gt;, &lt;code&gt;abs&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;empty&lt;/code&gt;, &lt;code&gt;empty_strided&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fill_&lt;/code&gt;, &lt;code&gt;zeros_&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Co-authored-by: Wang, Eikan &lt;a href="&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#116;&amp;#111;&amp;#58;&amp;#101;&amp;#105;&amp;#107;&amp;#97;&amp;#110;&amp;#46;&amp;#119;&amp;#97;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;"&gt;&amp;#101;&amp;#105;&amp;#107;&amp;#97;&amp;#110;&amp;#46;&amp;#119;&amp;#97;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/120891&lt;br /&gt;
Approved by: https://github.com/EikanWang, https://github.com/jgong5, https://github.com/gujinghui, https://github.com/atalman&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 07:42:20 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/148a8de6397be6e4b4ca1508b03b82d117bfb03c</guid></item><item><title>[BE] Enable torch inductor tests running on MacOS (#122360)</title><link>https://github.com/pytorch/pytorch/commit/f9996ed7643820d9b2191c5bfb8cf28962d41664</link><description>&lt;p&gt;[BE] Enable torch inductor tests running on MacOS (#122360)&lt;/p&gt;
&lt;p&gt;Original idea was limit the testing to just x86 Macs, but right now it will be skipped on all Apple Silicon ones, as all of them have Metal capable GPU&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122360&lt;br /&gt;
Approved by: https://github.com/jansel&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 06:47:05 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f9996ed7643820d9b2191c5bfb8cf28962d41664</guid></item><item><title>[inductor] Support non-Tensor predicate in torch.cond (#122378)</title><link>https://github.com/pytorch/pytorch/commit/456b112dca17a7d221f1d9c1a83e7b94cf915cbb</link><description>&lt;p&gt;[inductor] Support non-Tensor predicate in torch.cond (#122378)&lt;/p&gt;
&lt;p&gt;Summary: Previously, we only supported torch.Tensor boolean scalar predicate in &lt;code&gt;torch.cond&lt;/code&gt; in Inductor. This PR adds support for SymBool and Python bool predicate, to match the &lt;code&gt;torch.cond&lt;/code&gt; &lt;a href="https://pytorch.org/docs/stable/generated/torch.cond.html"&gt;sematics&lt;/a&gt; in Dynamo / Export.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;p&gt;```&lt;br /&gt;
$ python test/inductor/test_control_flow.py&lt;br /&gt;
...&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 34 tests in 56.980s&lt;/p&gt;
&lt;p&gt;OK&lt;/p&gt;
&lt;p&gt;$ python test/inductor/test_aot_inductor.py -k test_cond&lt;br /&gt;
...&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 54 tests in 460.093s&lt;/p&gt;
&lt;p&gt;OK (skipped=4)&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122378&lt;br /&gt;
Approved by: https://github.com/jansel, https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 06:35:01 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/456b112dca17a7d221f1d9c1a83e7b94cf915cbb</guid></item><item><title>Skip nonzero unbacked SymInt memo in inference mode (#122147)</title><link>https://github.com/pytorch/pytorch/commit/2e02e1efad957b86dbcc5b64748e03acfb8d330c</link><description>&lt;p&gt;Skip nonzero unbacked SymInt memo in inference mode (#122147)&lt;/p&gt;
&lt;p&gt;Summary: In &lt;code&gt;torch.inference_mode()&lt;/code&gt;, fake tensors don't have &lt;code&gt;_version&lt;/code&gt;s. This breaks unbacked SymInt memoization in &lt;code&gt;torch.nonzero&lt;/code&gt; tracing. Here we disable the latter in inference mode.&lt;/p&gt;
&lt;p&gt;Fixes https://github.com/pytorch/pytorch/issues/122127&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;p&gt;```&lt;br /&gt;
$ python test/inductor/test_unbacked_symints.py -k test_nonzero_in_inference_mode&lt;br /&gt;
...&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 2 tests in 14.060s&lt;/p&gt;
&lt;p&gt;OK&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;Reviewers:&lt;/p&gt;
&lt;p&gt;Subscribers:&lt;/p&gt;
&lt;p&gt;Tasks:&lt;/p&gt;
&lt;p&gt;Tags:&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122147&lt;br /&gt;
Approved by: https://github.com/ezyang&lt;/p&gt;</description><pubDate>Wed, 20 Mar 2024 06:44:55 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2e02e1efad957b86dbcc5b64748e03acfb8d330c</guid></item><item><title>Revert "Skip nonzero unbacked SymInt memo in inference mode (#122147)"</title><link>https://github.com/pytorch/pytorch/commit/7673cb534ade295ffd0b8ebfa9f0528b82945f7d</link><description>&lt;p&gt;Revert "Skip nonzero unbacked SymInt memo in inference mode (#122147)"&lt;/p&gt;
&lt;p&gt;This reverts commit 5e2687391229cee6e4dc0214f9208b4ecbe058c1.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122147 on behalf of https://github.com/jeanschmidt due to Reverting to see if trunk error in inductor are related (&lt;a href="https://github.com/pytorch/pytorch/pull/122147#issuecomment-2007513000"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Tue, 19 Mar 2024 07:37:24 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7673cb534ade295ffd0b8ebfa9f0528b82945f7d</guid></item><item><title>[inductor] disable linear weight prepacking pass on double (#121478)</title><link>https://github.com/pytorch/pytorch/commit/6d9588a12b5834f29bd8970936749a9725e7f609</link><description>&lt;p&gt;[inductor] disable linear weight prepacking pass on double (#121478)&lt;/p&gt;
&lt;p&gt;Fix #121175&lt;/p&gt;
&lt;p&gt;Co-authored-by: Jiong Gong &lt;a href="&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#116;&amp;#111;&amp;#58;&amp;#106;&amp;#105;&amp;#111;&amp;#110;&amp;#103;&amp;#46;&amp;#103;&amp;#111;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;"&gt;&amp;#106;&amp;#105;&amp;#111;&amp;#110;&amp;#103;&amp;#46;&amp;#103;&amp;#111;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/121478&lt;br /&gt;
Approved by: https://github.com/jgong5, https://github.com/desertfire&lt;/p&gt;</description><pubDate>Sat, 16 Mar 2024 05:24:21 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6d9588a12b5834f29bd8970936749a9725e7f609</guid></item><item><title>Enable FX graph caching in another batch of inductor tests (#121697)</title><link>https://github.com/pytorch/pytorch/commit/535bc71d037d44167270d99577b71b8d97db1d8f</link><description>&lt;p&gt;Enable FX graph caching in another batch of inductor tests (#121697)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121697&lt;br /&gt;
Approved by: https://github.com/eellison&lt;/p&gt;</description><pubDate>Fri, 15 Mar 2024 11:38:51 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/535bc71d037d44167270d99577b71b8d97db1d8f</guid></item><item><title>Fall back to eager mode when viewing with differing bitwidths (#120998) (#121786)</title><link>https://github.com/pytorch/pytorch/commit/3ee319c49ca5478a27aad00bf5d30946a5b784d2</link><description>&lt;p&gt;Fall back to eager mode when viewing with differing bitwidths (#120998) (#121786)&lt;/p&gt;
&lt;p&gt;The inductor lowering code for viewing a tensor as a type with a different bitwidth currently doesn't generate valid triton code. This change looks for a source and destination dtype and, if different sizes, falls back to the eager mode aten implementation.  Prior to this change, this condition would throw an exception.&lt;/p&gt;
&lt;p&gt;Fixes #120998.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121786&lt;br /&gt;
Approved by: https://github.com/peterbell10, https://github.com/bertmaher&lt;/p&gt;</description><pubDate>Fri, 15 Mar 2024 11:33:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3ee319c49ca5478a27aad00bf5d30946a5b784d2</guid></item><item><title>Disable inductor (default) and inductor (dynamic) by default in the perf run launcher (#121914)</title><link>https://github.com/pytorch/pytorch/commit/3c3d7455a3ef81fb90fb18199d7e0db316dbf159</link><description>&lt;p&gt;Disable inductor (default) and inductor (dynamic) by default in the perf run launcher (#121914)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121914&lt;br /&gt;
Approved by: https://github.com/desertfire&lt;/p&gt;</description><pubDate>Fri, 15 Mar 2024 08:46:24 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3c3d7455a3ef81fb90fb18199d7e0db316dbf159</guid></item><item><title>[AOTInductor] Include build cmds at the end of wrapper file (#121872)</title><link>https://github.com/pytorch/pytorch/commit/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da</link><description>&lt;p&gt;[AOTInductor] Include build cmds at the end of wrapper file (#121872)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
For easier debugging, include build commands at the end of codegen wrapper.&lt;/p&gt;
&lt;p&gt;{F1468438991}&lt;/p&gt;
&lt;p&gt;Test Plan: CI&lt;/p&gt;
&lt;p&gt;Differential Revision: D54882164&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121872&lt;br /&gt;
Approved by: https://github.com/chenyang78, https://github.com/desertfire&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 10:41:17 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da</guid></item><item><title>[sigmoid] Use deserializer from oss. (#121839)</title><link>https://github.com/pytorch/pytorch/commit/c40929219749f3c11be2220497f2a371cbe82ef7</link><description>&lt;p&gt;[sigmoid] Use deserializer from oss. (#121839)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
Old path:&lt;br /&gt;
thrift -&amp;gt; thrift deserializer -&amp;gt; graph module.&lt;br /&gt;
new path:&lt;br /&gt;
thrift -&amp;gt; python dataclass -&amp;gt; oss deserializer -&amp;gt; graph_module&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
CI&lt;br /&gt;
buck2 test mode/dev-nosan caffe2/test/inductor/fb:test_aot_inductor_pt2_inference&lt;/p&gt;
&lt;p&gt;Reviewed By: SherlockNoMad&lt;/p&gt;
&lt;p&gt;Differential Revision: D54855251&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121839&lt;br /&gt;
Approved by: https://github.com/angelayi&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 10:38:58 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c40929219749f3c11be2220497f2a371cbe82ef7</guid></item><item><title>[Inductor] Fix a dynamic shape problem when lowering diagonal (#121881)</title><link>https://github.com/pytorch/pytorch/commit/499136a4dd518775374bcdc8e2f2dc391cf43498</link><description>&lt;p&gt;[Inductor] Fix a dynamic shape problem when lowering diagonal (#121881)&lt;/p&gt;
&lt;p&gt;Summary: when computing the diagonal size, we need to use correct symbolic min/max function.&lt;/p&gt;
&lt;p&gt;Differential Revision: &lt;a href="https://our.internmc.facebook.com/intern/diff/D54884899"&gt;D54884899&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/121881&lt;br /&gt;
Approved by: https://github.com/aakhundov&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 10:36:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/499136a4dd518775374bcdc8e2f2dc391cf43498</guid></item><item><title>Add CUTLASS kernel as choice for _int_mm() Inductor autotuning (#119685)</title><link>https://github.com/pytorch/pytorch/commit/1251f0fa31aca84f6bbd4c32f1b9bfdf33774642</link><description>&lt;p&gt;Add CUTLASS kernel as choice for _int_mm() Inductor autotuning (#119685)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/119685&lt;br /&gt;
Approved by: https://github.com/cpuhrsch, https://github.com/kadeng&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 05:25:23 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1251f0fa31aca84f6bbd4c32f1b9bfdf33774642</guid></item><item><title>Handle transitive replacements in Triton kernel mutation analysis (#121867)</title><link>https://github.com/pytorch/pytorch/commit/3eb322ff293169c19a3e7381039562e2b08f32be</link><description>&lt;p&gt;Handle transitive replacements in Triton kernel mutation analysis (#121867)&lt;/p&gt;
&lt;p&gt;Summary: Previously, we didn't handle transitive replacements in MLIR walk-based function info mining in the Triton kernel mutation analysis pass. As a result, for the TTIR below:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tt.func private @cumsum__fp32S1_16S__1cconstexpr_1__2cconstexpr_False_(%arg0: tensor&amp;lt;1x16xf32&amp;gt; loc("...":296:0)) -&amp;gt; tensor&amp;lt;1x16xf32&amp;gt; attributes {noinline = false} {
    %0 = "tt.scan"(%arg0) &amp;lt;{axis = 1 : i32, reverse = false}&amp;gt; ({
    ^bb0(%arg1: f32 loc(unknown), %arg2: f32 loc(unknown)):
      %1 = tt.call @_sum_combine__fp32_fp32__(%arg1, %arg2) : (f32, f32) -&amp;gt; f32 loc(#loc16)
      tt.scan.return %1 : f32 loc(#loc16)
    }) : (tensor&amp;lt;1x16xf32&amp;gt;) -&amp;gt; tensor&amp;lt;1x16xf32&amp;gt; loc(#loc16)
    tt.return %0 : tensor&amp;lt;1x16xf32&amp;gt; loc(#loc18)
  } loc(#loc15)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the mined function dict looked like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{Intermediate(idx=25): [Op(name='tt.call',
                           fn_call_name='_sum_combine__fp32_fp32__',
                           args=[Intermediate(idx=26),
                                 Intermediate(idx=26)])],
 Intermediate(idx=27): [Op(name='tt.scan.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=25)])],
 Intermediate(idx=-4): [Op(name='tt.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=27)])]}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;whereas it should look like this (not the &lt;code&gt;Param(idx=0)&lt;/code&gt; arguments of the &lt;code&gt;tt.call&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{Intermediate(idx=25): [Op(name='tt.call',
                           fn_call_name='_sum_combine__fp32_fp32__',
                           args=[Param(idx=0),
                                 Param(idx=0)])],
 Intermediate(idx=27): [Op(name='tt.scan.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=25)])],
 Intermediate(idx=-4): [Op(name='tt.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=27)])]}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This is fixed in the PR.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;p&gt;```&lt;br /&gt;
$ python test/inductor/test_triton_kernels.py -k test_cumsum&lt;br /&gt;
.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 1 test in 1.771s&lt;/p&gt;
&lt;p&gt;OK&lt;br /&gt;
```&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/121867&lt;br /&gt;
Approved by: https://github.com/oulgen&lt;/p&gt;</description><pubDate>Wed, 13 Mar 2024 20:06:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3eb322ff293169c19a3e7381039562e2b08f32be</guid></item><item><title>Enable FX graph cache for a batch of inductor tests (#121696)</title><link>https://github.com/pytorch/pytorch/commit/4cd503c1f3efdb80204e288237fce13025a18350</link><description>&lt;p&gt;Enable FX graph cache for a batch of inductor tests (#121696)&lt;/p&gt;
&lt;p&gt;Summary: Get more FX graph cache coverage by enabling it for these unit tests&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121696&lt;br /&gt;
Approved by: https://github.com/eellison&lt;/p&gt;</description><pubDate>Wed, 13 Mar 2024 19:39:59 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4cd503c1f3efdb80204e288237fce13025a18350</guid></item><item>
      <title>Update torchbench commit pin, add sam_fast benchmark (#121420)</title>
      <link>https://github.com/pytorch/pytorch/commit/ae22bdaefe2a0d1cc8d997d87ba5d88b2d8eafd1</link>
      <description>&lt;p&gt;Update torchbench commit pin, add sam_fast benchmark (#121420)&lt;/p&gt;
&lt;p&gt;After this, the sam_fast benchmark can now be run in the pytorch repo:&lt;br /&gt;
&lt;code&gt;SEGMENT_ANYTHING_FAST_USE_FLASH_4=0 benchmarks/dynamo/torchbench.py --inference --amp --performance --backend=inductor --explain --only sam_fast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;sam_fast is designed for inference only, with cuda and amp on. The code adds these restrictions to the benchmark.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121420&lt;br /&gt;
Approved by: https://github.com/oulgen, https://github.com/msaroufim&lt;/p&gt;</description>
      <pubDate>Mon, 11 Mar 2024 11:48:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ae22bdaefe2a0d1cc8d997d87ba5d88b2d8eafd1</guid>
    </item>
    <item>
      <title>Upgrade submodule onednn to v3.3.5 (#120767)</title>
      <link>https://github.com/pytorch/pytorch/commit/d1510e01faceb7de9095caff75401955ab323310</link>
      <description>&lt;p&gt;Upgrade submodule onednn to v3.3.5 (#120767)&lt;/p&gt;
&lt;p&gt;This upgrade contains the fixes to the known issues brought by oneDNN v3.3.2, including issues https://github.com/pytorch/pytorch/issues/115346, https://github.com/pytorch/pytorch/issues/120211 and https://github.com/pytorch/pytorch/issues/120406 and those listed in PR #112700.&lt;/p&gt;
&lt;p&gt;Issue https://github.com/pytorch/pytorch/issues/115346 (perf regression) was fixed by oneDNN v3.3.4. No new regression was found with v3.3.5. The detailed results of v3.3.4 are given below and compared with v3.1.1 (the oneDNN version in PyTorch before it was updated to v3.3.2).&lt;br /&gt;
1. A performance regression with 5.8% perf drop from &lt;code&gt;pytorch_stargan-train&lt;/code&gt; (see https://github.com/pytorch/benchmark/issues/2076#issuecomment-1847545843)&lt;br /&gt;
Validation results with this patch: Latency increased by 0.60%&lt;br /&gt;
&lt;code&gt;Tested on an Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz instance (IceLake)
oneDNN v3.1.1
metrics-1484287.json
{
    "name": "cpu",
    "environ": {
        "pytorch_git_version": "6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0"
    },
    "metrics": {
        "latency": 418.851717
    }
}
oneDNN v3.3.4
{
    "name": "cpu",
    "environ": {
        "pytorch_git_version": "6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0"
    },
    "metrics": {
        "latency": 421.381313
    }
}&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Performance regression of FP32 rexnet_100 with Inductor, dynamic shape, multi-threads (see https://github.com/pytorch/pytorch/issues/115346#issue-2030859592)&lt;br /&gt;
Validation results with this patch: Latency reduced by 3.23%&lt;br /&gt;
```&lt;br /&gt;
Tested on an Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz instance (IceLake)&lt;br /&gt;
oneDNN v3.1.1&lt;br /&gt;
(inductor speedup over eager mode) 2.876x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,rexnet_100,128,2.875904,113.314765,18.455283,0.990437,1302.636134,1315.212902,351,1,0,0&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;oneDNN v3.3.4&lt;br /&gt;
(inductor speedup over eager mode) 3.003x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,rexnet_100,128,3.003012,109.653012,91.547260,0.990048,1302.532506,1315.625370,351,1,0,0&lt;br /&gt;
```&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Performance regression of AMP hf_T5_generate and tinynet_a with Inductor, static shape, multi-threads (see https://github.com/pytorch/pytorch/issues/115346#issuecomment-1856029962)&lt;br /&gt;
Validation results with this patch: Latency reduced by 0.85%&lt;br /&gt;
```&lt;br /&gt;
Tested on an AWS spr metal instance&lt;br /&gt;
oneDNN v3.1.1&lt;br /&gt;
(inductor speedup over eager mode) 1.120x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,hf_T5_generate,1,1.120018,1197.807729,205.905466,0.442803,125.179904,282.698957,10550,48,8,4&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;oneDNN v3.3.4&lt;br /&gt;
(inductor speedup over eager mode) 1.134x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,hf_T5_generate,1,1.133594,1187.701514,205.855527,0.422012,128.405094,304.268493,10550,48,8,4&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;The following issues about functionality are fixed by this upgrade. Test cases are also added for these issues.&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120211&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120406&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120547&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Below are detailed data of torchbench CPU userbenchmark test and Inductor FP32/AMP inference tests. No regression of perf or functionality was found.&lt;br /&gt;
I.  &lt;em&gt;torchbench CPU userbenchmark test&lt;/em&gt;&lt;br /&gt;
Suite | Speedup&lt;br /&gt;
-- | --&lt;br /&gt;
eager_throughtput_bf16_infer | 1.001848&lt;br /&gt;
eager_throughtput_fp32_infer | 1.000257&lt;br /&gt;
eager_throughtput_fx_int8 | 1.003069&lt;br /&gt;
jit_llga_throughtput_amp_bf16 | 1.000682&lt;br /&gt;
jit_llga_throughtput_fp32 | 1.000313&lt;br /&gt;
eager_throughtput_bf16_train | 0.998222&lt;br /&gt;
eager_throughtput_fp32_train | 1.003384&lt;/p&gt;
&lt;p&gt;II. &lt;em&gt;Inductor FP32/AMP inference tests&lt;/em&gt;&lt;br /&gt;
i.  FP32 static default&lt;br /&gt;
suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.09&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.14&lt;/p&gt;
&lt;p&gt;ii.  FP32 dynamic default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | alexnet | multiple | 128 | 1.08&lt;br /&gt;
torchbench | basic_gnn_edgecnn | multiple | 1 | 0.98&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.08&lt;/p&gt;
&lt;p&gt;iii. AMP static default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | hf_distil_whisper | multiple | 1 | 1.18&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.32&lt;br /&gt;
huggingface | BartForConditionalGeneration | multiple | 2 | 1.19&lt;br /&gt;
timm_models | eca_halonext26ts | multiple | 128 | 1.13&lt;br /&gt;
timm_models | nfnet_l0 | multiple | 128 | 1.13&lt;br /&gt;
timm_models | rexnet_100 | multiple | 128 | 1.45&lt;br /&gt;
timm_models | spnasnet_100 | multiple | 128 | 1.15&lt;br /&gt;
timm_models | tf_efficientnet_b0 | multiple | 128 | 1.22&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.49&lt;br /&gt;
torchbench | hf_Bert_large | single | 1 | 1.16&lt;br /&gt;
huggingface | XLNetLMHeadModel | single | 1 | 1.07&lt;/p&gt;
&lt;p&gt;iv.  AMP dynamic default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.32&lt;br /&gt;
huggingface | PLBartForConditionalGeneration | multiple | 4 | 1.14&lt;br /&gt;
timm_models | nfnet_l0 | multiple | 128 | 1.15&lt;br /&gt;
timm_models | rexnet_100 | multiple | 128 | 1.45&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.34&lt;br /&gt;
huggingface | XLNetLMHeadModel | single | 1 | 1.09&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Co-authored-by: Nikita Shulga &lt;a href="&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#116;&amp;#111;&amp;#58;&amp;#50;&amp;#52;&amp;#53;&amp;#51;&amp;#53;&amp;#50;&amp;#52;&amp;#43;&amp;#109;&amp;#97;&amp;#108;&amp;#102;&amp;#101;&amp;#116;&amp;#64;&amp;#117;&amp;#115;&amp;#101;&amp;#114;&amp;#115;&amp;#46;&amp;#110;&amp;#111;&amp;#114;&amp;#101;&amp;#112;&amp;#108;&amp;#121;&amp;#46;&amp;#103;&amp;#105;&amp;#116;&amp;#104;&amp;#117;&amp;#98;&amp;#46;&amp;#99;&amp;#111;&amp;#109;"&gt;&amp;#50;&amp;#52;&amp;#53;&amp;#51;&amp;#53;&amp;#50;&amp;#52;&amp;#43;&amp;#109;&amp;#97;&amp;#108;&amp;#102;&amp;#101;&amp;#116;&amp;#64;&amp;#117;&amp;#115;&amp;#101;&amp;#114;&amp;#115;&amp;#46;&amp;#110;&amp;#111;&amp;#114;&amp;#101;&amp;#112;&amp;#108;&amp;#121;&amp;#46;&amp;#103;&amp;#105;&amp;#116;&amp;#104;&amp;#117;&amp;#98;&amp;#46;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/120767&lt;br /&gt;
Approved by: https://github.com/chuanqi129, https://github.com/jgong5, https://github.com/atalman&lt;/p&gt;</description>
      <pubDate>Mon, 11 Mar 2024 04:56:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d1510e01faceb7de9095caff75401955ab323310</guid>
    </item>
  </channel>
</rss>
