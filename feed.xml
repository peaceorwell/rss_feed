<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub Commits Feed</title>
    <link>https://github.com/username/repo/commits</link>
    <description>Recent commits from GitHub repo</description>
    <title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><item><title>[AOTInductor] Include build cmds at the end of wrapper file (#121872)</title><link>https://github.com/pytorch/pytorch/commit/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da</link><description><![CDATA[<p>[AOTInductor] Include build cmds at the end of wrapper file (#121872)</p>
<p>Summary:<br />
For easier debugging, include build commands at the end of codegen wrapper.</p>
<p>{F1468438991}</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D54882164</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/121872<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire</p>]]></description><pubDate>Thu, 14 Mar 2024 10:41:17 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da</guid></item><item><title>[sigmoid] Use deserializer from oss. (#121839)</title><link>https://github.com/pytorch/pytorch/commit/c40929219749f3c11be2220497f2a371cbe82ef7</link><description><![CDATA[<p>[sigmoid] Use deserializer from oss. (#121839)</p>
<p>Summary:<br />
Old path:<br />
thrift -&gt; thrift deserializer -&gt; graph module.<br />
new path:<br />
thrift -&gt; python dataclass -&gt; oss deserializer -&gt; graph_module</p>
<p>Test Plan:<br />
CI<br />
buck2 test mode/dev-nosan caffe2/test/inductor/fb:test_aot_inductor_pt2_inference</p>
<p>Reviewed By: SherlockNoMad</p>
<p>Differential Revision: D54855251</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/121839<br />
Approved by: https://github.com/angelayi</p>]]></description><pubDate>Thu, 14 Mar 2024 10:38:58 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c40929219749f3c11be2220497f2a371cbe82ef7</guid></item><item><title>[Inductor] Fix a dynamic shape problem when lowering diagonal (#121881)</title><link>https://github.com/pytorch/pytorch/commit/499136a4dd518775374bcdc8e2f2dc391cf43498</link><description><![CDATA[<p>[Inductor] Fix a dynamic shape problem when lowering diagonal (#121881)</p>
<p>Summary: when computing the diagonal size, we need to use correct symbolic min/max function.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54884899">D54884899</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/121881<br />
Approved by: https://github.com/aakhundov</p>]]></description><pubDate>Thu, 14 Mar 2024 10:36:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/499136a4dd518775374bcdc8e2f2dc391cf43498</guid></item><item><title>Add CUTLASS kernel as choice for _int_mm() Inductor autotuning (#119685)</title><link>https://github.com/pytorch/pytorch/commit/1251f0fa31aca84f6bbd4c32f1b9bfdf33774642</link><description><![CDATA[<p>Add CUTLASS kernel as choice for _int_mm() Inductor autotuning (#119685)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/119685<br />
Approved by: https://github.com/cpuhrsch, https://github.com/kadeng</p>]]></description><pubDate>Thu, 14 Mar 2024 05:25:23 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1251f0fa31aca84f6bbd4c32f1b9bfdf33774642</guid></item><item><title>Handle transitive replacements in Triton kernel mutation analysis (#121867)</title><link>https://github.com/pytorch/pytorch/commit/3eb322ff293169c19a3e7381039562e2b08f32be</link><description>&lt;p&gt;Handle transitive replacements in Triton kernel mutation analysis (#121867)&lt;/p&gt;
&lt;p&gt;Summary: Previously, we didn't handle transitive replacements in MLIR walk-based function info mining in the Triton kernel mutation analysis pass. As a result, for the TTIR below:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tt.func private @cumsum__fp32S1_16S__1cconstexpr_1__2cconstexpr_False_(%arg0: tensor&amp;lt;1x16xf32&amp;gt; loc("...":296:0)) -&amp;gt; tensor&amp;lt;1x16xf32&amp;gt; attributes {noinline = false} {
    %0 = "tt.scan"(%arg0) &amp;lt;{axis = 1 : i32, reverse = false}&amp;gt; ({
    ^bb0(%arg1: f32 loc(unknown), %arg2: f32 loc(unknown)):
      %1 = tt.call @_sum_combine__fp32_fp32__(%arg1, %arg2) : (f32, f32) -&amp;gt; f32 loc(#loc16)
      tt.scan.return %1 : f32 loc(#loc16)
    }) : (tensor&amp;lt;1x16xf32&amp;gt;) -&amp;gt; tensor&amp;lt;1x16xf32&amp;gt; loc(#loc16)
    tt.return %0 : tensor&amp;lt;1x16xf32&amp;gt; loc(#loc18)
  } loc(#loc15)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the mined function dict looked like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{Intermediate(idx=25): [Op(name='tt.call',
                           fn_call_name='_sum_combine__fp32_fp32__',
                           args=[Intermediate(idx=26),
                                 Intermediate(idx=26)])],
 Intermediate(idx=27): [Op(name='tt.scan.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=25)])],
 Intermediate(idx=-4): [Op(name='tt.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=27)])]}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;whereas it should look like this (not the &lt;code&gt;Param(idx=0)&lt;/code&gt; arguments of the &lt;code&gt;tt.call&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{Intermediate(idx=25): [Op(name='tt.call',
                           fn_call_name='_sum_combine__fp32_fp32__',
                           args=[Param(idx=0),
                                 Param(idx=0)])],
 Intermediate(idx=27): [Op(name='tt.scan.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=25)])],
 Intermediate(idx=-4): [Op(name='tt.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=27)])]}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This is fixed in the PR.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;p&gt;```&lt;br /&gt;
$ python test/inductor/test_triton_kernels.py -k test_cumsum&lt;br /&gt;
.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 1 test in 1.771s&lt;/p&gt;
&lt;p&gt;OK&lt;br /&gt;
```&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/121867&lt;br /&gt;
Approved by: https://github.com/oulgen&lt;/p&gt;</description><pubDate>Wed, 13 Mar 2024 20:06:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3eb322ff293169c19a3e7381039562e2b08f32be</guid></item><item><title>Enable FX graph cache for a batch of inductor tests (#121696)</title><link>https://github.com/pytorch/pytorch/commit/4cd503c1f3efdb80204e288237fce13025a18350</link><description>&lt;p&gt;Enable FX graph cache for a batch of inductor tests (#121696)&lt;/p&gt;
&lt;p&gt;Summary: Get more FX graph cache coverage by enabling it for these unit tests&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121696&lt;br /&gt;
Approved by: https://github.com/eellison&lt;/p&gt;</description><pubDate>Wed, 13 Mar 2024 19:39:59 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4cd503c1f3efdb80204e288237fce13025a18350</guid></item><item>
      <title>Update torchbench commit pin, add sam_fast benchmark (#121420)</title>
      <link>https://github.com/pytorch/pytorch/commit/ae22bdaefe2a0d1cc8d997d87ba5d88b2d8eafd1</link>
      <description>&lt;p&gt;Update torchbench commit pin, add sam_fast benchmark (#121420)&lt;/p&gt;
&lt;p&gt;After this, the sam_fast benchmark can now be run in the pytorch repo:&lt;br /&gt;
&lt;code&gt;SEGMENT_ANYTHING_FAST_USE_FLASH_4=0 benchmarks/dynamo/torchbench.py --inference --amp --performance --backend=inductor --explain --only sam_fast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;sam_fast is designed for inference only, with cuda and amp on. The code adds these restrictions to the benchmark.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121420&lt;br /&gt;
Approved by: https://github.com/oulgen, https://github.com/msaroufim&lt;/p&gt;</description>
      <pubDate>Mon, 11 Mar 2024 11:48:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ae22bdaefe2a0d1cc8d997d87ba5d88b2d8eafd1</guid>
    </item>
    <item>
      <title>Upgrade submodule onednn to v3.3.5 (#120767)</title>
      <link>https://github.com/pytorch/pytorch/commit/d1510e01faceb7de9095caff75401955ab323310</link>
      <description>&lt;p&gt;Upgrade submodule onednn to v3.3.5 (#120767)&lt;/p&gt;
&lt;p&gt;This upgrade contains the fixes to the known issues brought by oneDNN v3.3.2, including issues https://github.com/pytorch/pytorch/issues/115346, https://github.com/pytorch/pytorch/issues/120211 and https://github.com/pytorch/pytorch/issues/120406 and those listed in PR #112700.&lt;/p&gt;
&lt;p&gt;Issue https://github.com/pytorch/pytorch/issues/115346 (perf regression) was fixed by oneDNN v3.3.4. No new regression was found with v3.3.5. The detailed results of v3.3.4 are given below and compared with v3.1.1 (the oneDNN version in PyTorch before it was updated to v3.3.2).&lt;br /&gt;
1. A performance regression with 5.8% perf drop from &lt;code&gt;pytorch_stargan-train&lt;/code&gt; (see https://github.com/pytorch/benchmark/issues/2076#issuecomment-1847545843)&lt;br /&gt;
Validation results with this patch: Latency increased by 0.60%&lt;br /&gt;
&lt;code&gt;Tested on an Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz instance (IceLake)
oneDNN v3.1.1
metrics-1484287.json
{
    "name": "cpu",
    "environ": {
        "pytorch_git_version": "6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0"
    },
    "metrics": {
        "latency": 418.851717
    }
}
oneDNN v3.3.4
{
    "name": "cpu",
    "environ": {
        "pytorch_git_version": "6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0"
    },
    "metrics": {
        "latency": 421.381313
    }
}&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Performance regression of FP32 rexnet_100 with Inductor, dynamic shape, multi-threads (see https://github.com/pytorch/pytorch/issues/115346#issue-2030859592)&lt;br /&gt;
Validation results with this patch: Latency reduced by 3.23%&lt;br /&gt;
```&lt;br /&gt;
Tested on an Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz instance (IceLake)&lt;br /&gt;
oneDNN v3.1.1&lt;br /&gt;
(inductor speedup over eager mode) 2.876x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,rexnet_100,128,2.875904,113.314765,18.455283,0.990437,1302.636134,1315.212902,351,1,0,0&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;oneDNN v3.3.4&lt;br /&gt;
(inductor speedup over eager mode) 3.003x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,rexnet_100,128,3.003012,109.653012,91.547260,0.990048,1302.532506,1315.625370,351,1,0,0&lt;br /&gt;
```&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Performance regression of AMP hf_T5_generate and tinynet_a with Inductor, static shape, multi-threads (see https://github.com/pytorch/pytorch/issues/115346#issuecomment-1856029962)&lt;br /&gt;
Validation results with this patch: Latency reduced by 0.85%&lt;br /&gt;
```&lt;br /&gt;
Tested on an AWS spr metal instance&lt;br /&gt;
oneDNN v3.1.1&lt;br /&gt;
(inductor speedup over eager mode) 1.120x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,hf_T5_generate,1,1.120018,1197.807729,205.905466,0.442803,125.179904,282.698957,10550,48,8,4&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;oneDNN v3.3.4&lt;br /&gt;
(inductor speedup over eager mode) 1.134x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,hf_T5_generate,1,1.133594,1187.701514,205.855527,0.422012,128.405094,304.268493,10550,48,8,4&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;The following issues about functionality are fixed by this upgrade. Test cases are also added for these issues.&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120211&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120406&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120547&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Below are detailed data of torchbench CPU userbenchmark test and Inductor FP32/AMP inference tests. No regression of perf or functionality was found.&lt;br /&gt;
I.  &lt;em&gt;torchbench CPU userbenchmark test&lt;/em&gt;&lt;br /&gt;
Suite | Speedup&lt;br /&gt;
-- | --&lt;br /&gt;
eager_throughtput_bf16_infer | 1.001848&lt;br /&gt;
eager_throughtput_fp32_infer | 1.000257&lt;br /&gt;
eager_throughtput_fx_int8 | 1.003069&lt;br /&gt;
jit_llga_throughtput_amp_bf16 | 1.000682&lt;br /&gt;
jit_llga_throughtput_fp32 | 1.000313&lt;br /&gt;
eager_throughtput_bf16_train | 0.998222&lt;br /&gt;
eager_throughtput_fp32_train | 1.003384&lt;/p&gt;
&lt;p&gt;II. &lt;em&gt;Inductor FP32/AMP inference tests&lt;/em&gt;&lt;br /&gt;
i.  FP32 static default&lt;br /&gt;
suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.09&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.14&lt;/p&gt;
&lt;p&gt;ii.  FP32 dynamic default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | alexnet | multiple | 128 | 1.08&lt;br /&gt;
torchbench | basic_gnn_edgecnn | multiple | 1 | 0.98&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.08&lt;/p&gt;
&lt;p&gt;iii. AMP static default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | hf_distil_whisper | multiple | 1 | 1.18&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.32&lt;br /&gt;
huggingface | BartForConditionalGeneration | multiple | 2 | 1.19&lt;br /&gt;
timm_models | eca_halonext26ts | multiple | 128 | 1.13&lt;br /&gt;
timm_models | nfnet_l0 | multiple | 128 | 1.13&lt;br /&gt;
timm_models | rexnet_100 | multiple | 128 | 1.45&lt;br /&gt;
timm_models | spnasnet_100 | multiple | 128 | 1.15&lt;br /&gt;
timm_models | tf_efficientnet_b0 | multiple | 128 | 1.22&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.49&lt;br /&gt;
torchbench | hf_Bert_large | single | 1 | 1.16&lt;br /&gt;
huggingface | XLNetLMHeadModel | single | 1 | 1.07&lt;/p&gt;
&lt;p&gt;iv.  AMP dynamic default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.32&lt;br /&gt;
huggingface | PLBartForConditionalGeneration | multiple | 4 | 1.14&lt;br /&gt;
timm_models | nfnet_l0 | multiple | 128 | 1.15&lt;br /&gt;
timm_models | rexnet_100 | multiple | 128 | 1.45&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.34&lt;br /&gt;
huggingface | XLNetLMHeadModel | single | 1 | 1.09&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Co-authored-by: Nikita Shulga &lt;a href="&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#116;&amp;#111;&amp;#58;&amp;#50;&amp;#52;&amp;#53;&amp;#51;&amp;#53;&amp;#50;&amp;#52;&amp;#43;&amp;#109;&amp;#97;&amp;#108;&amp;#102;&amp;#101;&amp;#116;&amp;#64;&amp;#117;&amp;#115;&amp;#101;&amp;#114;&amp;#115;&amp;#46;&amp;#110;&amp;#111;&amp;#114;&amp;#101;&amp;#112;&amp;#108;&amp;#121;&amp;#46;&amp;#103;&amp;#105;&amp;#116;&amp;#104;&amp;#117;&amp;#98;&amp;#46;&amp;#99;&amp;#111;&amp;#109;"&gt;&amp;#50;&amp;#52;&amp;#53;&amp;#51;&amp;#53;&amp;#50;&amp;#52;&amp;#43;&amp;#109;&amp;#97;&amp;#108;&amp;#102;&amp;#101;&amp;#116;&amp;#64;&amp;#117;&amp;#115;&amp;#101;&amp;#114;&amp;#115;&amp;#46;&amp;#110;&amp;#111;&amp;#114;&amp;#101;&amp;#112;&amp;#108;&amp;#121;&amp;#46;&amp;#103;&amp;#105;&amp;#116;&amp;#104;&amp;#117;&amp;#98;&amp;#46;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/120767&lt;br /&gt;
Approved by: https://github.com/chuanqi129, https://github.com/jgong5, https://github.com/atalman&lt;/p&gt;</description>
      <pubDate>Mon, 11 Mar 2024 04:56:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d1510e01faceb7de9095caff75401955ab323310</guid>
    </item>
  </channel>
</rss>
