<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub Commits Feed</title>
    <link>https://github.com/username/repo/commits</link>
    <description>Recent commits from GitHub repo</description>
    <title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><title>GitHub Commits Feed</title><link>https://github.com/username/repo/commits</link><description>Recent commits from GitHub repo</description><item><title>[Inductor] Support custom op in JIT with cpp wrapper (#122554)</title><link>https://github.com/pytorch/pytorch/commit/537cd66e73e8a9b33c843d55d546471f3074a390</link><description><![CDATA[<p>[Inductor] Support custom op in JIT with cpp wrapper (#122554)</p>
<p>Summary:  To call custom ops in an ABI-compatible way requires doing boxed call with varargs across C shim. In the JIT mode, we can get around it by calling into Python.  https://gist.github.com/desertfire/be2a65b0a9b47780bb716b53ac2cd2b3 is an example of generated code.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D55326556">D55326556</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122554<br />
Approved by: https://github.com/jansel, https://github.com/chenyang78</p>]]></description><pubDate>Tue, 26 Mar 2024 10:48:45 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/537cd66e73e8a9b33c843d55d546471f3074a390</guid></item><item><title>Log autotune time in scuba (#122637)</title><link>https://github.com/pytorch/pytorch/commit/e61aaab72562fe95d76e77b01678d71467e1739e</link><description><![CDATA[<p>Log autotune time in scuba (#122637)</p>
<p>Summary:<br />
This diff<br />
* Refactors triton and autotune caches to be child classes of the original memcache based cache infra<br />
* Swaps scuba table for autotune<br />
* Adds autotune time spent/saved to scuba table</p>
<p>Test Plan:<br />
Local testing using:<br />
<code>buck run mode/opt fbcode//caffe2/test/inductor/:max_autotune -- -r test_max_autotune_remote_caching_dynamic_False</code><br />
and<br />
<code>TORCH_INDUCTOR_AUTOTUNE_REMOTE_CACHE=1 buck2 run mode/opt //scripts/oulgen:runner</code></p>
<p>Differential Revision: D55332620</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/122637<br />
Approved by: https://github.com/jamesjwu</p>]]></description><pubDate>Tue, 26 Mar 2024 09:51:33 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e61aaab72562fe95d76e77b01678d71467e1739e</guid></item><item><title>[Inductor] Run pattern matcher over the original graph (#122519)</title><link>https://github.com/pytorch/pytorch/commit/1f5fcb4e203eb343e8c53f6444015c98e8f68d60</link><description><![CDATA[<p>[Inductor] Run pattern matcher over the original graph (#122519)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/122519<br />
Approved by: https://github.com/jansel</p>]]></description><pubDate>Tue, 26 Mar 2024 09:30:32 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1f5fcb4e203eb343e8c53f6444015c98e8f68d60</guid></item><item><title>[ez] Add more files to trigger inductor (#122669)</title><link>https://github.com/pytorch/pytorch/commit/8bad7b63c8b1ba4e7d1a9d86023d5b29068c2cb4</link><description><![CDATA[<p>[ez] Add more files to trigger inductor (#122669)</p>
<p>To catch https://github.com/pytorch/pytorch/pull/122562/files<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122669<br />
Approved by: https://github.com/desertfire</p>]]></description><pubDate>Tue, 26 Mar 2024 07:19:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8bad7b63c8b1ba4e7d1a9d86023d5b29068c2cb4</guid></item><item><title>[BE][CPUInductor] Use C++17 helper templates (#122607)</title><link>https://github.com/pytorch/pytorch/commit/9d1de31634a5b9c8c990a6356ab8864bb4d9bede</link><description>&lt;p&gt;[BE][CPUInductor] Use C++17 helper templates (#122607)&lt;/p&gt;
&lt;p&gt;Such as &lt;code&gt;std::is_same_v&lt;/code&gt; ,&lt;code&gt;std::is_integral_v&lt;/code&gt; and C++14 one &lt;code&gt;std::enable_if_t&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122607&lt;br /&gt;
Approved by: https://github.com/jansel, https://github.com/Skylion007&lt;/p&gt;</description><pubDate>Mon, 25 Mar 2024 11:01:44 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9d1de31634a5b9c8c990a6356ab8864bb4d9bede</guid></item><item><title>[inductor] Improve error message for shape errors in slice_scatter (#122543)</title><link>https://github.com/pytorch/pytorch/commit/2db7d874a92f26ada3926b35de5716c1f457220e</link><description>&lt;p&gt;[inductor] Improve error message for shape errors in slice_scatter (#122543)&lt;/p&gt;
&lt;p&gt;Fixes #122291&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122543&lt;br /&gt;
Approved by: https://github.com/shunting314&lt;/p&gt;</description><pubDate>Mon, 25 Mar 2024 10:57:16 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2db7d874a92f26ada3926b35de5716c1f457220e</guid></item><item><title>GPT2 SDPA inference pattern-matching for Inductor-CPU (#121866)</title><link>https://github.com/pytorch/pytorch/commit/5152945441b2a1387f6e9fc8fe1531bfeb6cabe4</link><description>&lt;p&gt;GPT2 SDPA inference pattern-matching for Inductor-CPU (#121866)&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;With this PR, SDPA pattern of GPT2 is being mapped to &lt;code&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;.&lt;br /&gt;
While GPT2 supports both a causal mask &amp;amp; an attention mask, this PR considers the case of attention mask being absent.&lt;br /&gt;
TorchBench inference workload for GPT2 also doesn't use an attention-mask.&lt;/p&gt;
&lt;p&gt;This pattern's replacement is being disabled for CUDA because &lt;a href="https://github.com/pytorch/pytorch/actions/runs/8319111885/job/22762567770"&gt;CUDA AOT Inductor&lt;/a&gt; CI job's &lt;code&gt;GPT2ForSequenceClassification&lt;/code&gt; accuracy test failed, although all other trunk CUDA Inductor CI checks had passed.&lt;br /&gt;
Created #122429 to track that particular issue.&lt;/p&gt;
&lt;h3&gt;CPU performance data with TorchBench&lt;/h3&gt;
&lt;p&gt;|MODEL |BATCH SIZE | DTYPE | BEFORE: Speedup over eager-mode with the default Inductor implementation | AFTER: Speedup over eager mode with SDPA op mapped| Perf boost = (AFTER - BEFORE)/BEFORE * 100|&lt;br /&gt;
|--------------------------|-------------|---------|-----------------------------|--------------------------|------------|&lt;br /&gt;
|hf_GPT2| 1 | FP32 | 1.522x | 1.791x| 17.67%|&lt;br /&gt;
|hf_GPT2| 1 | BF16 (AMP) | 1.795x | 2.387x| 32.98%|&lt;br /&gt;
|hf_GPT2| 2 | FP32 |  1.313x |1.629x | 19.3%|&lt;br /&gt;
|hf_GPT2|2| BF16 (AMP) | 1.556x | 1.924x | 23.65%|&lt;br /&gt;
|hf_GPT2_large| 1 | FP32 | 1.380x |1.585x | 12.93%|&lt;br /&gt;
|hf_GPT2_large| 1 | BF16 (AMP) | 1.208x | 1.567x | 22.91%|&lt;br /&gt;
|hf_GPT2_large| 2 | FP32 | 1.188x | 1.490x | 25.42%|&lt;br /&gt;
|hf_GPT2_large|2| BF16 (AMP) | 0.991x | 1.575x | 58.93%|&lt;/p&gt;
&lt;p&gt;Machine - Intel(R) Xeon(R) Platinum 8468H (Xeon 4th gen Sapphire Rapids)&lt;br /&gt;
48 physical cores were used. Intel OpenMP &amp;amp; libtcmalloc were preloaded.&lt;/p&gt;
&lt;p&gt;Example command -&lt;br /&gt;
&lt;code&gt;OMP_NUM_THREADS=48 MKL_NUM_THREADS=48 numactl --membind=0 --cpunodebind=0 -C 0-47 python benchmarks/dynamo/torchbench.py --performance --inference --inductor --float32 -dcpu --only hf_GPT2_large --freezing --batch-size 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121866&lt;br /&gt;
Approved by: https://github.com/Valentine233, https://github.com/jgong5, https://github.com/desertfire&lt;/p&gt;</description><pubDate>Mon, 25 Mar 2024 07:04:03 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5152945441b2a1387f6e9fc8fe1531bfeb6cabe4</guid></item><item><title>[EZ][BE] Add missing `acosh` op to vec256_float_neon.h (#122513)</title><link>https://github.com/pytorch/pytorch/commit/bf40e3f880cc6820656d5d5fc32a21ca8334c520</link><description>&lt;p&gt;[EZ][BE] Add missing &lt;code&gt;acosh&lt;/code&gt; op to vec256_float_neon.h (#122513)&lt;/p&gt;
&lt;p&gt;As base class has it&lt;br /&gt;
https://github.com/pytorch/pytorch/blob/ed15370aabf951eec2ba0140de5ff71634868791/aten/src/ATen/cpu/vec/vec_base.h#L367-L369&lt;/p&gt;
&lt;p&gt;Discovered while attempting to enabling Inductor vectorization on ARM platform&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122513&lt;br /&gt;
Approved by: https://github.com/Skylion007&lt;/p&gt;</description><pubDate>Sat, 23 Mar 2024 06:18:02 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bf40e3f880cc6820656d5d5fc32a21ca8334c520</guid></item><item><title>[aoti] Add handling of ir.Constants in promote_constants (#122419)</title><link>https://github.com/pytorch/pytorch/commit/ed15370aabf951eec2ba0140de5ff71634868791</link><description>&lt;p&gt;[aoti] Add handling of ir.Constants in promote_constants (#122419)&lt;/p&gt;
&lt;p&gt;This issue popped up when enabling predispatch IR on the benchmarks (https://github.com/pytorch/pytorch/pull/122225)&lt;/p&gt;
&lt;p&gt;On the following model:&lt;br /&gt;
```&lt;br /&gt;
class M(torch.nn.Module):&lt;br /&gt;
    def &lt;strong&gt;init&lt;/strong&gt;(self, device):&lt;br /&gt;
        super().&lt;strong&gt;init&lt;/strong&gt;()&lt;br /&gt;
        self.device = device&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def forward(self, x):
    t = torch.tensor(x.size(-1), device=self.device, dtype=torch.float)
    t = torch.sqrt(t * 3)
    return x * t
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;We get the following error:&lt;br /&gt;
```&lt;br /&gt;
======================================================================&lt;br /&gt;
ERROR: test_constant_abi_compatible_cuda (&lt;strong&gt;main&lt;/strong&gt;.AOTInductorTestABICompatibleCuda)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Traceback (most recent call last):&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/testing/_internal/common_utils.py", line 2741, in wrapper&lt;br /&gt;
    method(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_torchinductor.py", line 9232, in new_test&lt;br /&gt;
    return value(self)&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;/em&gt;args, &lt;strong&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor.py", line 922, in test_constant&lt;br /&gt;
    self.check_model(M(self.device), (torch.randn(5, 5, device=self.device),))&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor.py", line 91, in check_model&lt;br /&gt;
    actual = AOTIRunnerUtil.run(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor_utils.py", line 102, in run&lt;br /&gt;
    so_path = AOTIRunnerUtil.compile(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/test/inductor/test_aot_inductor_utils.py", line 40, in compile&lt;br /&gt;
    so_path = torch._inductor.aot_compile_ep(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/&lt;strong&gt;init&lt;/strong&gt;.py", line 150, in aot_compile_ep&lt;br /&gt;
    return compile_fx_aot(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1005, in compile_fx_aot&lt;br /&gt;
    compiled_lib_path = compile_fx(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(*args, &lt;/strong&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1111, in compile_fx&lt;br /&gt;
    return compile_fx(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1145, in compile_fx&lt;br /&gt;
    return compile_fx(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;/em&gt;args, &lt;strong&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/&lt;em&gt;inductor/compile_fx.py", line 1336, in compile_fx&lt;br /&gt;
    return inference_compiler(unlifted_gm, example_inputs&lt;/em&gt;)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/utils.py", line 265, in time_wrapper&lt;br /&gt;
    r = func(*args, &lt;/strong&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 1266, in fw_compiler_base&lt;br /&gt;
    return inner_compile(&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper&lt;br /&gt;
    inner_compiled_fn = compiler_fn(gm, example_inputs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/debug.py", line 304, in inner&lt;br /&gt;
    return fn(&lt;/em&gt;args, &lt;strong&gt;kwargs)&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(*args, &lt;/strong&gt;kwds)&lt;br /&gt;
  File "/home/angelayi/.conda/envs/pytorch10/lib/python3.10/contextlib.py", line 79, in inner&lt;br /&gt;
    return func(&lt;em&gt;args, &lt;/em&gt;&lt;em&gt;kwds)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/utils.py", line 265, in time_wrapper&lt;br /&gt;
    r = func(&lt;/em&gt;args, &lt;strong&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 447, in compile_fx_inner&lt;br /&gt;
    compiled_graph = fx_codegen_and_compile(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/compile_fx.py", line 707, in fx_codegen_and_compile&lt;br /&gt;
    graph.run(&lt;em&gt;example_inputs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_dynamo/utils.py", line 265, in time_wrapper&lt;br /&gt;
    r = func(&lt;/em&gt;args, &lt;/strong&gt;kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 612, in run&lt;br /&gt;
    return super().run(&lt;em&gt;args)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/fx/interpreter.py", line 145, in run&lt;br /&gt;
    self.env[node] = self.run_node(node)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 957, in run_node&lt;br /&gt;
    result = super().run_node(n)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/fx/interpreter.py", line 202, in run_node&lt;br /&gt;
    return getattr(self, n.op)(n.target, args, kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 819, in call_function&lt;br /&gt;
    raise LoweringException(e, target, args, kwargs).with_traceback(&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/graph.py", line 816, in call_function&lt;br /&gt;
    out = lowerings&lt;a href="*args, **kwargs"&gt;target&lt;/a&gt;&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 298, in wrapped&lt;br /&gt;
    out = decomp_fn(&lt;/em&gt;args, **kwargs)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 5340, in mul&lt;br /&gt;
    return make_pointwise(fn)(a, b)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 409, in inner&lt;br /&gt;
    inputs = promote_constants(inputs, override_return_dtype)&lt;br /&gt;
  File "/data/users/angelayi/pytorch/torch/_inductor/lowering.py", line 373, in promote_constants&lt;br /&gt;
    ex = next(x for x in inputs if isinstance(x, (TensorBox, ExpandView)))&lt;br /&gt;
torch._inductor.exc.LoweringException: StopIteration:&lt;br /&gt;
  target: aten.mul.Tensor&lt;br /&gt;
  args[0]: Constant(value=5.0, dtype=torch.float32, device=device(type='cuda', index=0))&lt;br /&gt;
  args[1]: 3&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;So I added an additional casing in &lt;code&gt;promote_constants&lt;/code&gt; to handle the ir.Constants and now it works! Although please let me know if this is the wrong approach. Here's a paste of the full run with the inductor logs: P1198927007&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122419&lt;br /&gt;
Approved by: https://github.com/eellison, https://github.com/desertfire, https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 10:39:36 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ed15370aabf951eec2ba0140de5ff71634868791</guid></item><item><title>[inductor] device guard for max autotune benchmark (#122479)</title><link>https://github.com/pytorch/pytorch/commit/a3d4eaf2531a9aae974ed2eab157a3b3594e7e7b</link><description>&lt;p&gt;[inductor] device guard for max autotune benchmark (#122479)&lt;/p&gt;
&lt;p&gt;Internal users reported that they get failure for max-autotune if tensors are not on device 0. It turns out that we may use tensors on device say 6 and run kernel on them at device 0.&lt;/p&gt;
&lt;p&gt;This PR enforces that we do benchmarking for max-autotune on the correct device.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122479&lt;br /&gt;
Approved by: https://github.com/xintwfb, https://github.com/Chillee&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 09:27:53 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a3d4eaf2531a9aae974ed2eab157a3b3594e7e7b</guid></item><item><title>[aot_inductor] added runtime checks for input/output tensors in debug compile mode (#122047)</title><link>https://github.com/pytorch/pytorch/commit/86082f1fdc336ca89f9b6f8888fba9e87f3a4c06</link><description>&lt;p&gt;[aot_inductor] added runtime checks for input/output tensors in debug compile mode (#122047)&lt;/p&gt;
&lt;p&gt;This PR added runtime checks to guard the dtypes and shapes of input/output tensors.&lt;br /&gt;
Currently, we enable these only for debug compilation&lt;br /&gt;
(i.e. aot_inductor.debug_compile is True) in abi_compatible mode.&lt;/p&gt;
&lt;p&gt;Differential Revision: &lt;a href="https://our.internmc.facebook.com/intern/diff/D54993148"&gt;D54993148&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/122047&lt;br /&gt;
Approved by: https://github.com/desertfire&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 08:40:33 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/86082f1fdc336ca89f9b6f8888fba9e87f3a4c06</guid></item><item><title>Revert "[Quant] [PT2] Add SiLU  into X86InductorQuantizer Conv2d Unary Annotation (#122267)"</title><link>https://github.com/pytorch/pytorch/commit/60bc29aa0b22dcc06c5778589a9d2b5c4b9fc965</link><description>&lt;p&gt;Revert "[Quant] [PT2] Add SiLU  into X86InductorQuantizer Conv2d Unary Annotation (#122267)"&lt;/p&gt;
&lt;p&gt;This reverts commit 2c6eeb26d3f61fba352ad51fd8653120937a20f3.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122267 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/60bc29aa0b22dcc06c5778589a9d2b5c4b9fc965</guid></item><item><title>Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op SiLU (#122268)"</title><link>https://github.com/pytorch/pytorch/commit/b30b396d057f3462fbf2a1b46b8f480947dcb6d0</link><description>&lt;p&gt;Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op SiLU (#122268)"&lt;/p&gt;
&lt;p&gt;This reverts commit 99f0fec7d0873d627e8c7f2dec65818d725424b0.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122268 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b30b396d057f3462fbf2a1b46b8f480947dcb6d0</guid></item><item><title>Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardSwish with int8-mix-bf16 (#122373)"</title><link>https://github.com/pytorch/pytorch/commit/777ac511cc98a7edf916e61d0770afac4e37fccc</link><description>&lt;p&gt;Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardSwish with int8-mix-bf16 (#122373)"&lt;/p&gt;
&lt;p&gt;This reverts commit 783fd89ff1cf401e484c20d14b16823abf20d87d.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122373 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/777ac511cc98a7edf916e61d0770afac4e37fccc</guid></item><item><title>Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)"</title><link>https://github.com/pytorch/pytorch/commit/dbedc6bb7cf4cfaab5df85aff2e4dbaf049e6746</link><description>&lt;p&gt;Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)"&lt;/p&gt;
&lt;p&gt;This reverts commit 23a6d74f9352e0afb37750fee300d077c4ba9393.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122374 on behalf of https://github.com/jeanschmidt due to Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it (&lt;a href="https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 07:04:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dbedc6bb7cf4cfaab5df85aff2e4dbaf049e6746</guid></item><item><title>[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)</title><link>https://github.com/pytorch/pytorch/commit/23a6d74f9352e0afb37750fee300d077c4ba9393</link><description>&lt;p&gt;[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;br /&gt;
Enable the fusion pattern of &lt;code&gt;QConv2d -&amp;gt; hardtanh&lt;/code&gt; lowering for int8-mixed-bf16 case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Test Plan&lt;/strong&gt;&lt;br /&gt;
&lt;code&gt;python -m pytest test_mkldnn_pattern_matcher.py -k test_qconv2d_hardtanh_int8_mixed_bf16_cpu&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122374&lt;br /&gt;
Approved by: https://github.com/leslie-fang-intel, https://github.com/jgong5&lt;br /&gt;
ghstack dependencies: #122266, #122267, #122268, #122373&lt;/p&gt;</description><pubDate>Fri, 22 Mar 2024 05:13:14 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/23a6d74f9352e0afb37750fee300d077c4ba9393</guid></item><item><title>Precompile triton templates (#121998)</title><link>https://github.com/pytorch/pytorch/commit/b8df2f0ca530ebe01fa079c891c170a1f4b22823</link><description>&lt;p&gt;Precompile triton templates (#121998)&lt;/p&gt;
&lt;p&gt;Before this PR we were not precompiling triton templates in parallel. Compilation would occur during benchmarking.&lt;/p&gt;
&lt;p&gt;Triton benchmarking templates were emitted as :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;@triton.jit
def triton_mm(arg_A, arg_B, out_ptr0):&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In order to precompile we need to give the full kernel specification, as we do when we emit the template in the final output code generation.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'backend_hash': 'cdeecfeccd31ad7810f96b5752194b1c2406d0a81e39a6ca09c8ee150baae183'},
)
@triton.jit
def triton_mm(arg_A, arg_B, out_ptr0):&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121998&lt;br /&gt;
Approved by: https://github.com/jansel&lt;br /&gt;
ghstack dependencies: #121996, #120275, #121997&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 09:04:53 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b8df2f0ca530ebe01fa079c891c170a1f4b22823</guid></item><item><title>Introduce XPU implementation for PyTorch ATen operators (#120891)</title><link>https://github.com/pytorch/pytorch/commit/148a8de6397be6e4b4ca1508b03b82d117bfb03c</link><description>&lt;p&gt;Introduce XPU implementation for PyTorch ATen operators (#120891)&lt;/p&gt;
&lt;p&gt;As a follow-up to #114835 and #119682, we add limited ATen operators implementation for XPU. With this PR, the blocking issue for oneDNN operations and Inductor XPU backend will be resolved as the two components depend on these operations to support its basic features, respectively.&lt;/p&gt;
&lt;p&gt;The added ATen operators include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;copy_&lt;/code&gt;, &lt;code&gt;_to_copy&lt;/code&gt;, &lt;code&gt;_copy_from_and_resize&lt;/code&gt;, , &lt;code&gt;clone&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;view&lt;/code&gt;, &lt;code&gt;view_as_real&lt;/code&gt;, &lt;code&gt;view_as_complex&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;as_strided&lt;/code&gt;, &lt;code&gt;_reshape_alias&lt;/code&gt;, &lt;code&gt;resize_&lt;/code&gt;, &lt;code&gt;resize_as_&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;add&lt;/code&gt;/&lt;code&gt;add_&lt;/code&gt;, &lt;code&gt;sub&lt;/code&gt;/&lt;code&gt;sub_&lt;/code&gt;, &lt;code&gt;mul&lt;/code&gt;/&lt;code&gt;mul_&lt;/code&gt;, &lt;code&gt;div&lt;/code&gt;/&lt;code&gt;div_&lt;/code&gt;, &lt;code&gt;abs&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;empty&lt;/code&gt;, &lt;code&gt;empty_strided&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fill_&lt;/code&gt;, &lt;code&gt;zeros_&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Co-authored-by: Wang, Eikan &lt;a href="&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#116;&amp;#111;&amp;#58;&amp;#101;&amp;#105;&amp;#107;&amp;#97;&amp;#110;&amp;#46;&amp;#119;&amp;#97;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;"&gt;&amp;#101;&amp;#105;&amp;#107;&amp;#97;&amp;#110;&amp;#46;&amp;#119;&amp;#97;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/120891&lt;br /&gt;
Approved by: https://github.com/EikanWang, https://github.com/jgong5, https://github.com/gujinghui, https://github.com/atalman&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 07:42:20 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/148a8de6397be6e4b4ca1508b03b82d117bfb03c</guid></item><item><title>[BE] Enable torch inductor tests running on MacOS (#122360)</title><link>https://github.com/pytorch/pytorch/commit/f9996ed7643820d9b2191c5bfb8cf28962d41664</link><description>&lt;p&gt;[BE] Enable torch inductor tests running on MacOS (#122360)&lt;/p&gt;
&lt;p&gt;Original idea was limit the testing to just x86 Macs, but right now it will be skipped on all Apple Silicon ones, as all of them have Metal capable GPU&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122360&lt;br /&gt;
Approved by: https://github.com/jansel&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 06:47:05 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f9996ed7643820d9b2191c5bfb8cf28962d41664</guid></item><item><title>[inductor] Support non-Tensor predicate in torch.cond (#122378)</title><link>https://github.com/pytorch/pytorch/commit/456b112dca17a7d221f1d9c1a83e7b94cf915cbb</link><description>&lt;p&gt;[inductor] Support non-Tensor predicate in torch.cond (#122378)&lt;/p&gt;
&lt;p&gt;Summary: Previously, we only supported torch.Tensor boolean scalar predicate in &lt;code&gt;torch.cond&lt;/code&gt; in Inductor. This PR adds support for SymBool and Python bool predicate, to match the &lt;code&gt;torch.cond&lt;/code&gt; &lt;a href="https://pytorch.org/docs/stable/generated/torch.cond.html"&gt;sematics&lt;/a&gt; in Dynamo / Export.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;p&gt;```&lt;br /&gt;
$ python test/inductor/test_control_flow.py&lt;br /&gt;
...&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 34 tests in 56.980s&lt;/p&gt;
&lt;p&gt;OK&lt;/p&gt;
&lt;p&gt;$ python test/inductor/test_aot_inductor.py -k test_cond&lt;br /&gt;
...&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 54 tests in 460.093s&lt;/p&gt;
&lt;p&gt;OK (skipped=4)&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122378&lt;br /&gt;
Approved by: https://github.com/jansel, https://github.com/chenyang78&lt;/p&gt;</description><pubDate>Thu, 21 Mar 2024 06:35:01 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/456b112dca17a7d221f1d9c1a83e7b94cf915cbb</guid></item><item><title>Skip nonzero unbacked SymInt memo in inference mode (#122147)</title><link>https://github.com/pytorch/pytorch/commit/2e02e1efad957b86dbcc5b64748e03acfb8d330c</link><description>&lt;p&gt;Skip nonzero unbacked SymInt memo in inference mode (#122147)&lt;/p&gt;
&lt;p&gt;Summary: In &lt;code&gt;torch.inference_mode()&lt;/code&gt;, fake tensors don't have &lt;code&gt;_version&lt;/code&gt;s. This breaks unbacked SymInt memoization in &lt;code&gt;torch.nonzero&lt;/code&gt; tracing. Here we disable the latter in inference mode.&lt;/p&gt;
&lt;p&gt;Fixes https://github.com/pytorch/pytorch/issues/122127&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;p&gt;```&lt;br /&gt;
$ python test/inductor/test_unbacked_symints.py -k test_nonzero_in_inference_mode&lt;br /&gt;
...&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 2 tests in 14.060s&lt;/p&gt;
&lt;p&gt;OK&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;Reviewers:&lt;/p&gt;
&lt;p&gt;Subscribers:&lt;/p&gt;
&lt;p&gt;Tasks:&lt;/p&gt;
&lt;p&gt;Tags:&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/122147&lt;br /&gt;
Approved by: https://github.com/ezyang&lt;/p&gt;</description><pubDate>Wed, 20 Mar 2024 06:44:55 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2e02e1efad957b86dbcc5b64748e03acfb8d330c</guid></item><item><title>Revert "Skip nonzero unbacked SymInt memo in inference mode (#122147)"</title><link>https://github.com/pytorch/pytorch/commit/7673cb534ade295ffd0b8ebfa9f0528b82945f7d</link><description>&lt;p&gt;Revert "Skip nonzero unbacked SymInt memo in inference mode (#122147)"&lt;/p&gt;
&lt;p&gt;This reverts commit 5e2687391229cee6e4dc0214f9208b4ecbe058c1.&lt;/p&gt;
&lt;p&gt;Reverted https://github.com/pytorch/pytorch/pull/122147 on behalf of https://github.com/jeanschmidt due to Reverting to see if trunk error in inductor are related (&lt;a href="https://github.com/pytorch/pytorch/pull/122147#issuecomment-2007513000"&gt;comment&lt;/a&gt;)&lt;/p&gt;</description><pubDate>Tue, 19 Mar 2024 07:37:24 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7673cb534ade295ffd0b8ebfa9f0528b82945f7d</guid></item><item><title>[inductor] disable linear weight prepacking pass on double (#121478)</title><link>https://github.com/pytorch/pytorch/commit/6d9588a12b5834f29bd8970936749a9725e7f609</link><description>&lt;p&gt;[inductor] disable linear weight prepacking pass on double (#121478)&lt;/p&gt;
&lt;p&gt;Fix #121175&lt;/p&gt;
&lt;p&gt;Co-authored-by: Jiong Gong &lt;a href="&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#116;&amp;#111;&amp;#58;&amp;#106;&amp;#105;&amp;#111;&amp;#110;&amp;#103;&amp;#46;&amp;#103;&amp;#111;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;"&gt;&amp;#106;&amp;#105;&amp;#111;&amp;#110;&amp;#103;&amp;#46;&amp;#103;&amp;#111;&amp;#110;&amp;#103;&amp;#64;&amp;#105;&amp;#110;&amp;#116;&amp;#101;&amp;#108;&amp;#46;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/121478&lt;br /&gt;
Approved by: https://github.com/jgong5, https://github.com/desertfire&lt;/p&gt;</description><pubDate>Sat, 16 Mar 2024 05:24:21 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6d9588a12b5834f29bd8970936749a9725e7f609</guid></item><item><title>Enable FX graph caching in another batch of inductor tests (#121697)</title><link>https://github.com/pytorch/pytorch/commit/535bc71d037d44167270d99577b71b8d97db1d8f</link><description>&lt;p&gt;Enable FX graph caching in another batch of inductor tests (#121697)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121697&lt;br /&gt;
Approved by: https://github.com/eellison&lt;/p&gt;</description><pubDate>Fri, 15 Mar 2024 11:38:51 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/535bc71d037d44167270d99577b71b8d97db1d8f</guid></item><item><title>Fall back to eager mode when viewing with differing bitwidths (#120998) (#121786)</title><link>https://github.com/pytorch/pytorch/commit/3ee319c49ca5478a27aad00bf5d30946a5b784d2</link><description>&lt;p&gt;Fall back to eager mode when viewing with differing bitwidths (#120998) (#121786)&lt;/p&gt;
&lt;p&gt;The inductor lowering code for viewing a tensor as a type with a different bitwidth currently doesn't generate valid triton code. This change looks for a source and destination dtype and, if different sizes, falls back to the eager mode aten implementation.  Prior to this change, this condition would throw an exception.&lt;/p&gt;
&lt;p&gt;Fixes #120998.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121786&lt;br /&gt;
Approved by: https://github.com/peterbell10, https://github.com/bertmaher&lt;/p&gt;</description><pubDate>Fri, 15 Mar 2024 11:33:30 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3ee319c49ca5478a27aad00bf5d30946a5b784d2</guid></item><item><title>Disable inductor (default) and inductor (dynamic) by default in the perf run launcher (#121914)</title><link>https://github.com/pytorch/pytorch/commit/3c3d7455a3ef81fb90fb18199d7e0db316dbf159</link><description>&lt;p&gt;Disable inductor (default) and inductor (dynamic) by default in the perf run launcher (#121914)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121914&lt;br /&gt;
Approved by: https://github.com/desertfire&lt;/p&gt;</description><pubDate>Fri, 15 Mar 2024 08:46:24 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3c3d7455a3ef81fb90fb18199d7e0db316dbf159</guid></item><item><title>[AOTInductor] Include build cmds at the end of wrapper file (#121872)</title><link>https://github.com/pytorch/pytorch/commit/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da</link><description>&lt;p&gt;[AOTInductor] Include build cmds at the end of wrapper file (#121872)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
For easier debugging, include build commands at the end of codegen wrapper.&lt;/p&gt;
&lt;p&gt;{F1468438991}&lt;/p&gt;
&lt;p&gt;Test Plan: CI&lt;/p&gt;
&lt;p&gt;Differential Revision: D54882164&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121872&lt;br /&gt;
Approved by: https://github.com/chenyang78, https://github.com/desertfire&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 10:41:17 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da</guid></item><item><title>[sigmoid] Use deserializer from oss. (#121839)</title><link>https://github.com/pytorch/pytorch/commit/c40929219749f3c11be2220497f2a371cbe82ef7</link><description>&lt;p&gt;[sigmoid] Use deserializer from oss. (#121839)&lt;/p&gt;
&lt;p&gt;Summary:&lt;br /&gt;
Old path:&lt;br /&gt;
thrift -&amp;gt; thrift deserializer -&amp;gt; graph module.&lt;br /&gt;
new path:&lt;br /&gt;
thrift -&amp;gt; python dataclass -&amp;gt; oss deserializer -&amp;gt; graph_module&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;br /&gt;
CI&lt;br /&gt;
buck2 test mode/dev-nosan caffe2/test/inductor/fb:test_aot_inductor_pt2_inference&lt;/p&gt;
&lt;p&gt;Reviewed By: SherlockNoMad&lt;/p&gt;
&lt;p&gt;Differential Revision: D54855251&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121839&lt;br /&gt;
Approved by: https://github.com/angelayi&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 10:38:58 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c40929219749f3c11be2220497f2a371cbe82ef7</guid></item><item><title>[Inductor] Fix a dynamic shape problem when lowering diagonal (#121881)</title><link>https://github.com/pytorch/pytorch/commit/499136a4dd518775374bcdc8e2f2dc391cf43498</link><description>&lt;p&gt;[Inductor] Fix a dynamic shape problem when lowering diagonal (#121881)&lt;/p&gt;
&lt;p&gt;Summary: when computing the diagonal size, we need to use correct symbolic min/max function.&lt;/p&gt;
&lt;p&gt;Differential Revision: &lt;a href="https://our.internmc.facebook.com/intern/diff/D54884899"&gt;D54884899&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/121881&lt;br /&gt;
Approved by: https://github.com/aakhundov&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 10:36:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/499136a4dd518775374bcdc8e2f2dc391cf43498</guid></item><item><title>Add CUTLASS kernel as choice for _int_mm() Inductor autotuning (#119685)</title><link>https://github.com/pytorch/pytorch/commit/1251f0fa31aca84f6bbd4c32f1b9bfdf33774642</link><description>&lt;p&gt;Add CUTLASS kernel as choice for _int_mm() Inductor autotuning (#119685)&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/119685&lt;br /&gt;
Approved by: https://github.com/cpuhrsch, https://github.com/kadeng&lt;/p&gt;</description><pubDate>Thu, 14 Mar 2024 05:25:23 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1251f0fa31aca84f6bbd4c32f1b9bfdf33774642</guid></item><item><title>Handle transitive replacements in Triton kernel mutation analysis (#121867)</title><link>https://github.com/pytorch/pytorch/commit/3eb322ff293169c19a3e7381039562e2b08f32be</link><description>&lt;p&gt;Handle transitive replacements in Triton kernel mutation analysis (#121867)&lt;/p&gt;
&lt;p&gt;Summary: Previously, we didn't handle transitive replacements in MLIR walk-based function info mining in the Triton kernel mutation analysis pass. As a result, for the TTIR below:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tt.func private @cumsum__fp32S1_16S__1cconstexpr_1__2cconstexpr_False_(%arg0: tensor&amp;lt;1x16xf32&amp;gt; loc("...":296:0)) -&amp;gt; tensor&amp;lt;1x16xf32&amp;gt; attributes {noinline = false} {
    %0 = "tt.scan"(%arg0) &amp;lt;{axis = 1 : i32, reverse = false}&amp;gt; ({
    ^bb0(%arg1: f32 loc(unknown), %arg2: f32 loc(unknown)):
      %1 = tt.call @_sum_combine__fp32_fp32__(%arg1, %arg2) : (f32, f32) -&amp;gt; f32 loc(#loc16)
      tt.scan.return %1 : f32 loc(#loc16)
    }) : (tensor&amp;lt;1x16xf32&amp;gt;) -&amp;gt; tensor&amp;lt;1x16xf32&amp;gt; loc(#loc16)
    tt.return %0 : tensor&amp;lt;1x16xf32&amp;gt; loc(#loc18)
  } loc(#loc15)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the mined function dict looked like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{Intermediate(idx=25): [Op(name='tt.call',
                           fn_call_name='_sum_combine__fp32_fp32__',
                           args=[Intermediate(idx=26),
                                 Intermediate(idx=26)])],
 Intermediate(idx=27): [Op(name='tt.scan.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=25)])],
 Intermediate(idx=-4): [Op(name='tt.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=27)])]}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;whereas it should look like this (not the &lt;code&gt;Param(idx=0)&lt;/code&gt; arguments of the &lt;code&gt;tt.call&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{Intermediate(idx=25): [Op(name='tt.call',
                           fn_call_name='_sum_combine__fp32_fp32__',
                           args=[Param(idx=0),
                                 Param(idx=0)])],
 Intermediate(idx=27): [Op(name='tt.scan.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=25)])],
 Intermediate(idx=-4): [Op(name='tt.return',
                           fn_call_name=None,
                           args=[Intermediate(idx=27)])]}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This is fixed in the PR.&lt;/p&gt;
&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;p&gt;```&lt;br /&gt;
$ python test/inductor/test_triton_kernels.py -k test_cumsum&lt;br /&gt;
.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Ran 1 test in 1.771s&lt;/p&gt;
&lt;p&gt;OK&lt;br /&gt;
```&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/121867&lt;br /&gt;
Approved by: https://github.com/oulgen&lt;/p&gt;</description><pubDate>Wed, 13 Mar 2024 20:06:37 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3eb322ff293169c19a3e7381039562e2b08f32be</guid></item><item><title>Enable FX graph cache for a batch of inductor tests (#121696)</title><link>https://github.com/pytorch/pytorch/commit/4cd503c1f3efdb80204e288237fce13025a18350</link><description>&lt;p&gt;Enable FX graph cache for a batch of inductor tests (#121696)&lt;/p&gt;
&lt;p&gt;Summary: Get more FX graph cache coverage by enabling it for these unit tests&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121696&lt;br /&gt;
Approved by: https://github.com/eellison&lt;/p&gt;</description><pubDate>Wed, 13 Mar 2024 19:39:59 GMT</pubDate><guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4cd503c1f3efdb80204e288237fce13025a18350</guid></item><item>
      <title>Update torchbench commit pin, add sam_fast benchmark (#121420)</title>
      <link>https://github.com/pytorch/pytorch/commit/ae22bdaefe2a0d1cc8d997d87ba5d88b2d8eafd1</link>
      <description>&lt;p&gt;Update torchbench commit pin, add sam_fast benchmark (#121420)&lt;/p&gt;
&lt;p&gt;After this, the sam_fast benchmark can now be run in the pytorch repo:&lt;br /&gt;
&lt;code&gt;SEGMENT_ANYTHING_FAST_USE_FLASH_4=0 benchmarks/dynamo/torchbench.py --inference --amp --performance --backend=inductor --explain --only sam_fast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;sam_fast is designed for inference only, with cuda and amp on. The code adds these restrictions to the benchmark.&lt;/p&gt;
&lt;p&gt;Pull Request resolved: https://github.com/pytorch/pytorch/pull/121420&lt;br /&gt;
Approved by: https://github.com/oulgen, https://github.com/msaroufim&lt;/p&gt;</description>
      <pubDate>Mon, 11 Mar 2024 11:48:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ae22bdaefe2a0d1cc8d997d87ba5d88b2d8eafd1</guid>
    </item>
    <item>
      <title>Upgrade submodule onednn to v3.3.5 (#120767)</title>
      <link>https://github.com/pytorch/pytorch/commit/d1510e01faceb7de9095caff75401955ab323310</link>
      <description>&lt;p&gt;Upgrade submodule onednn to v3.3.5 (#120767)&lt;/p&gt;
&lt;p&gt;This upgrade contains the fixes to the known issues brought by oneDNN v3.3.2, including issues https://github.com/pytorch/pytorch/issues/115346, https://github.com/pytorch/pytorch/issues/120211 and https://github.com/pytorch/pytorch/issues/120406 and those listed in PR #112700.&lt;/p&gt;
&lt;p&gt;Issue https://github.com/pytorch/pytorch/issues/115346 (perf regression) was fixed by oneDNN v3.3.4. No new regression was found with v3.3.5. The detailed results of v3.3.4 are given below and compared with v3.1.1 (the oneDNN version in PyTorch before it was updated to v3.3.2).&lt;br /&gt;
1. A performance regression with 5.8% perf drop from &lt;code&gt;pytorch_stargan-train&lt;/code&gt; (see https://github.com/pytorch/benchmark/issues/2076#issuecomment-1847545843)&lt;br /&gt;
Validation results with this patch: Latency increased by 0.60%&lt;br /&gt;
&lt;code&gt;Tested on an Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz instance (IceLake)
oneDNN v3.1.1
metrics-1484287.json
{
    "name": "cpu",
    "environ": {
        "pytorch_git_version": "6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0"
    },
    "metrics": {
        "latency": 418.851717
    }
}
oneDNN v3.3.4
{
    "name": "cpu",
    "environ": {
        "pytorch_git_version": "6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0"
    },
    "metrics": {
        "latency": 421.381313
    }
}&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Performance regression of FP32 rexnet_100 with Inductor, dynamic shape, multi-threads (see https://github.com/pytorch/pytorch/issues/115346#issue-2030859592)&lt;br /&gt;
Validation results with this patch: Latency reduced by 3.23%&lt;br /&gt;
```&lt;br /&gt;
Tested on an Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz instance (IceLake)&lt;br /&gt;
oneDNN v3.1.1&lt;br /&gt;
(inductor speedup over eager mode) 2.876x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,rexnet_100,128,2.875904,113.314765,18.455283,0.990437,1302.636134,1315.212902,351,1,0,0&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;oneDNN v3.3.4&lt;br /&gt;
(inductor speedup over eager mode) 3.003x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,rexnet_100,128,3.003012,109.653012,91.547260,0.990048,1302.532506,1315.625370,351,1,0,0&lt;br /&gt;
```&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Performance regression of AMP hf_T5_generate and tinynet_a with Inductor, static shape, multi-threads (see https://github.com/pytorch/pytorch/issues/115346#issuecomment-1856029962)&lt;br /&gt;
Validation results with this patch: Latency reduced by 0.85%&lt;br /&gt;
```&lt;br /&gt;
Tested on an AWS spr metal instance&lt;br /&gt;
oneDNN v3.1.1&lt;br /&gt;
(inductor speedup over eager mode) 1.120x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,hf_T5_generate,1,1.120018,1197.807729,205.905466,0.442803,125.179904,282.698957,10550,48,8,4&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;oneDNN v3.3.4&lt;br /&gt;
(inductor speedup over eager mode) 1.134x&lt;br /&gt;
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks&lt;br /&gt;
cpu,hf_T5_generate,1,1.133594,1187.701514,205.855527,0.422012,128.405094,304.268493,10550,48,8,4&lt;br /&gt;
```&lt;/p&gt;
&lt;p&gt;The following issues about functionality are fixed by this upgrade. Test cases are also added for these issues.&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120211&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120406&lt;br /&gt;
- https://github.com/pytorch/pytorch/issues/120547&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Below are detailed data of torchbench CPU userbenchmark test and Inductor FP32/AMP inference tests. No regression of perf or functionality was found.&lt;br /&gt;
I.  &lt;em&gt;torchbench CPU userbenchmark test&lt;/em&gt;&lt;br /&gt;
Suite | Speedup&lt;br /&gt;
-- | --&lt;br /&gt;
eager_throughtput_bf16_infer | 1.001848&lt;br /&gt;
eager_throughtput_fp32_infer | 1.000257&lt;br /&gt;
eager_throughtput_fx_int8 | 1.003069&lt;br /&gt;
jit_llga_throughtput_amp_bf16 | 1.000682&lt;br /&gt;
jit_llga_throughtput_fp32 | 1.000313&lt;br /&gt;
eager_throughtput_bf16_train | 0.998222&lt;br /&gt;
eager_throughtput_fp32_train | 1.003384&lt;/p&gt;
&lt;p&gt;II. &lt;em&gt;Inductor FP32/AMP inference tests&lt;/em&gt;&lt;br /&gt;
i.  FP32 static default&lt;br /&gt;
suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.09&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.14&lt;/p&gt;
&lt;p&gt;ii.  FP32 dynamic default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | alexnet | multiple | 128 | 1.08&lt;br /&gt;
torchbench | basic_gnn_edgecnn | multiple | 1 | 0.98&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.08&lt;/p&gt;
&lt;p&gt;iii. AMP static default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | hf_distil_whisper | multiple | 1 | 1.18&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.32&lt;br /&gt;
huggingface | BartForConditionalGeneration | multiple | 2 | 1.19&lt;br /&gt;
timm_models | eca_halonext26ts | multiple | 128 | 1.13&lt;br /&gt;
timm_models | nfnet_l0 | multiple | 128 | 1.13&lt;br /&gt;
timm_models | rexnet_100 | multiple | 128 | 1.45&lt;br /&gt;
timm_models | spnasnet_100 | multiple | 128 | 1.15&lt;br /&gt;
timm_models | tf_efficientnet_b0 | multiple | 128 | 1.22&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.49&lt;br /&gt;
torchbench | hf_Bert_large | single | 1 | 1.16&lt;br /&gt;
huggingface | XLNetLMHeadModel | single | 1 | 1.07&lt;/p&gt;
&lt;p&gt;iv.  AMP dynamic default&lt;/p&gt;
&lt;p&gt;suite | name | thread | batch size | Ratio Speedup(New/old)&lt;br /&gt;
-- | -- | -- | -- | --&lt;br /&gt;
torchbench | timm_efficientnet | multiple | 64 | 1.32&lt;br /&gt;
huggingface | PLBartForConditionalGeneration | multiple | 4 | 1.14&lt;br /&gt;
timm_models | nfnet_l0 | multiple | 128 | 1.15&lt;br /&gt;
timm_models | rexnet_100 | multiple | 128 | 1.45&lt;br /&gt;
timm_models | tinynet_a | multiple | 128 | 1.34&lt;br /&gt;
huggingface | XLNetLMHeadModel | single | 1 | 1.09&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Co-authored-by: Nikita Shulga &lt;a href="&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#116;&amp;#111;&amp;#58;&amp;#50;&amp;#52;&amp;#53;&amp;#51;&amp;#53;&amp;#50;&amp;#52;&amp;#43;&amp;#109;&amp;#97;&amp;#108;&amp;#102;&amp;#101;&amp;#116;&amp;#64;&amp;#117;&amp;#115;&amp;#101;&amp;#114;&amp;#115;&amp;#46;&amp;#110;&amp;#111;&amp;#114;&amp;#101;&amp;#112;&amp;#108;&amp;#121;&amp;#46;&amp;#103;&amp;#105;&amp;#116;&amp;#104;&amp;#117;&amp;#98;&amp;#46;&amp;#99;&amp;#111;&amp;#109;"&gt;&amp;#50;&amp;#52;&amp;#53;&amp;#51;&amp;#53;&amp;#50;&amp;#52;&amp;#43;&amp;#109;&amp;#97;&amp;#108;&amp;#102;&amp;#101;&amp;#116;&amp;#64;&amp;#117;&amp;#115;&amp;#101;&amp;#114;&amp;#115;&amp;#46;&amp;#110;&amp;#111;&amp;#114;&amp;#101;&amp;#112;&amp;#108;&amp;#121;&amp;#46;&amp;#103;&amp;#105;&amp;#116;&amp;#104;&amp;#117;&amp;#98;&amp;#46;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt;&lt;br /&gt;
Pull Request resolved: https://github.com/pytorch/pytorch/pull/120767&lt;br /&gt;
Approved by: https://github.com/chuanqi129, https://github.com/jgong5, https://github.com/atalman&lt;/p&gt;</description>
      <pubDate>Mon, 11 Mar 2024 04:56:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d1510e01faceb7de9095caff75401955ab323310</guid>
    </item>
  </channel>
</rss>
