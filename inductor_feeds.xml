<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub Commits Feed</title>
    <link>https://github.com/username/repo/commits</link>
    <description>Recent commits from GitHub repo</description>
    <item>
      <title>[inductor] Support sympy.expr in user-defined Triton kernel grid fn (#119165)</title>
      <link>https://github.com/pytorch/pytorch/commit/7d7a3f0b37b44e06c76727e4166719790049ac10</link>
      <description><![CDATA[<p>[inductor] Support sympy.expr in user-defined Triton kernel grid fn (#119165)</p>
<h2>Problem</h2>
<p>A user-defined Triton kernel grid may use a sympy magic method like <code>Max</code>. This comes in the form of a form of a <code>sympy.Expr</code>, namely <code>sympy.core.function.FunctionClass</code>.</p>
<p>Handling this is not trivial since <code>user_defined_kernel_grid_fn_code</code> is used in Eager &amp; Inductor. Eager usage below.</p>
<h2>Approach</h2>
<p>Pass in wrapper when Inductor codegens grid with ints/sympy.Expr, so we can utilize wrapper functions, such as <code>codegen_shape_tuple()</code>.</p>
<p>Differential Revision: D53367012</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/119165<br />
Approved by: https://github.com/aakhundov</p>]]></description>
      <pubDate>Tue, 06 Feb 2024 00:39:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7d7a3f0b37b44e06c76727e4166719790049ac10</guid>
    </item>
    <item>
      <title>[inductor] Implementing missing magic methods on IR values. (#118933)</title>
      <link>https://github.com/pytorch/pytorch/commit/884b6d2a675756091ae7c19b49f0aec1a8931955</link>
      <description><![CDATA[<p>[inductor] Implementing missing magic methods on IR values. (#118933)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118933<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Mon, 05 Feb 2024 21:50:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/884b6d2a675756091ae7c19b49f0aec1a8931955</guid>
    </item>
    <item>
      <title>Fix dynamo benchmark runner for torchbench skip sets (#118615)</title>
      <link>https://github.com/pytorch/pytorch/commit/074f2bb5cebf84bf10e3c4daa76d7217905282dd</link>
      <description><![CDATA[<p>Fix dynamo benchmark runner for torchbench skip sets (#118615)</p>
<p>Fix dynamo benchmark runner for torchbench skip sets, which introduced by PR #118032</p>
<p>This runner.py script is still used in the <a href="https://github.com/pytorch/pytorch/issues/93531">Inductor CPU Performance Dashboard</a> regular test</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118615<br />
Approved by: https://github.com/jgong5, https://github.com/ysiraichi, https://github.com/ezyang</p>]]></description>
      <pubDate>Mon, 05 Feb 2024 18:06:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/074f2bb5cebf84bf10e3c4daa76d7217905282dd</guid>
    </item>
    <item>
      <title>[inductor] Support ProxyExecutor argument codegen for sympy.Expr (#119166)</title>
      <link>https://github.com/pytorch/pytorch/commit/3829b55416e115603d21073a8b3710324abae059</link>
      <description><![CDATA[<p>[inductor] Support ProxyExecutor argument codegen for sympy.Expr (#119166)</p>
<p>Differential Revision: D53398312</p>
<h2>Problem</h2>
<p>Currently, if a sympy expression that uses a magic method like <code>Max</code> is passed as an argument to ProxyExecutor, then C++ compilation will fail. We need to use std::max method instead.</p>
<p>```</p>
<h1>What we see</h1>
<p>aoti_torch_proxy_executor_call_function(..., std::vector<int64_t>{Max(1025, u1)}.data(), ...);</p>
<h1>What we want</h1>
<p>aoti_torch_proxy_executor_call_function(..., std::vector<int64_t>{std::max(1025L, u1)}.data(), ...)<br />
```</p>
<h2>Approach</h2>
<p>Use C++ wrapper's expression printer to handle this conversion</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/119166<br />
Approved by: https://github.com/aakhundov</p>]]></description>
      <pubDate>Mon, 05 Feb 2024 16:33:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3829b55416e115603d21073a8b3710324abae059</guid>
    </item>
    <item>
      <title>[inductor] make multi-kernel work with cpp-wrapper (#117813)</title>
      <link>https://github.com/pytorch/pytorch/commit/fd0bf96c2b9aea46f0597ba6fef9b896f5b874bb</link>
      <description><![CDATA[<p>[inductor] make multi-kernel work with cpp-wrapper (#117813)</p>
<p>Make multi-kernel work with cpp-wrapper. multi-kernel generates two equivalent variants for a reduction. At runtime the faster one is picked. But cpp-wrapper need save cubin file during codegen. They don't work with each other at the beginning.</p>
<p>Thanks Jason for suggesting a neat way to integrate these two. cpp-wrapper does 2 passes codegen right now. For the first pass, we still generate multi-kernel code and run it; for the second pass, we load the cubin file for the faster kernel directly. And multi-kernel python code is not generated for the second pass since they should not be needed.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117813<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 05 Feb 2024 15:35:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fd0bf96c2b9aea46f0597ba6fef9b896f5b874bb</guid>
    </item>
    <item>
      <title>Revert "[inductor] make multi-kernel work with cpp-wrapper (#117813)"</title>
      <link>https://github.com/pytorch/pytorch/commit/b964a1222cef0af3aebe04e25714aa08c97a6cd3</link>
      <description><![CDATA[<p>Revert "[inductor] make multi-kernel work with cpp-wrapper (#117813)"</p>
<p>This reverts commit c24ffc3f66b2270dfc65a404687b91b55ed580e9.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/117813 on behalf of https://github.com/atalman due to Failing internal tests (<a href="https://github.com/pytorch/pytorch/pull/117813#issuecomment-1927877102">comment</a>)</p>]]></description>
      <pubDate>Mon, 05 Feb 2024 11:25:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b964a1222cef0af3aebe04e25714aa08c97a6cd3</guid>
    </item>
    <item>
      <title>[aotinductor] Migrate fuse_split_linear_add from dper_pass to AOTI based on predispatch IR (#118983)</title>
      <link>https://github.com/pytorch/pytorch/commit/fa8d97776cb2051e1066f0f9426e25a1bfe7a1cf</link>
      <description><![CDATA[<p>[aotinductor] Migrate fuse_split_linear_add from dper_pass to AOTI based on predispatch IR (#118983)</p>
<p>Summary: As titled. Added support of fuse_split_linear_add in pregrad passes based on predispatch IR</p>
<p>Test Plan: TORCH_LOGS=inductor,aot   buck2 run  mode/opt mode/inplace caffe2/test/inductor/fb:test_split_cat_fx_passes_aten_fb</p>
<p>Differential Revision: D53302168</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118983<br />
Approved by: https://github.com/kflu, https://github.com/chenyang78</p>]]></description>
      <pubDate>Mon, 05 Feb 2024 09:58:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fa8d97776cb2051e1066f0f9426e25a1bfe7a1cf</guid>
    </item>
    <item>
      <title>make nanogpt work with both compiled autograd and _LazyGraphModule (#118981)</title>
      <link>https://github.com/pytorch/pytorch/commit/a72190fd51f19cbfb5c09ae3088729f94aef7141</link>
      <description><![CDATA[<p>make nanogpt work with both compiled autograd and _LazyGraphModule (#118981)</p>
<p>@xmfan and @fegin reported that _LazyGraphModule ( https://github.com/pytorch/pytorch/pull/117911 ) makes nanogpt training fail with compiled autograd.</p>
<p>We have a repro:  <code>python benchmarks/dynamo/torchbench.py --training --backend=inductor --disable-cudagraphs --accuracy --only nanogpt --repeat 1 --compiled-autograd</code><br />
but it's still mysterious how to trigger the issue with a toy model.</p>
<p>The error message for the failure is https://gist.github.com/shunting314/6402a6388b3539956090b6bc098952fb . In compile_fx we will call <code>detect_fake_mode</code>. This function will look for an active FakeTensorMode from both TracingContext and example inputs. The error is triggered because we find different FakeTensorMode from these 2 sources.</p>
<p>Although I don't know what really causes the discrepancy of FakeTensorMode above, the fix here is to force _LazyGraphModule recompilation if we have compiled autograd enabled. This does not hurt compilation time most of the time because we anyway will call the graph module here in the backward pass when compiled autograd is enabled: https://github.com/pytorch/pytorch/blob/855d5f144efc1db50316b9fcad1e62bf37caed10/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py#L705</p>
<p>Let me know if we can have a better fix.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118981<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 05 Feb 2024 02:40:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a72190fd51f19cbfb5c09ae3088729f94aef7141</guid>
    </item>
    <item>
      <title>[AOTI] Make abi_compatible as default for OSS CI (#119126)</title>
      <link>https://github.com/pytorch/pytorch/commit/b41f3e8df1b6ac2240d591f5e53b36589e90299b</link>
      <description><![CDATA[<p>[AOTI] Make abi_compatible as default for OSS CI (#119126)</p>
<p>Summary: Introduce an environment varible AOT_INDUCTOR_ABI_COMPATIBLE to control the ABI-compatible mode, and turn it on for OSS CI.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/119126<br />
Approved by: https://github.com/chenyang78<br />
ghstack dependencies: #119125</p>]]></description>
      <pubDate>Sun, 04 Feb 2024 07:48:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b41f3e8df1b6ac2240d591f5e53b36589e90299b</guid>
    </item>
    <item>
      <title>[AOTI] Support copy_, _fft_c2c and view_as_real in C shim (#119125)</title>
      <link>https://github.com/pytorch/pytorch/commit/79b20aec764c62e7a086aa2e2515b21731c9a831</link>
      <description><![CDATA[<p>[AOTI] Support copy_, _fft_c2c and view_as_real in C shim (#119125)</p>
<p>Summary: These ops exist in GoogleFnet. Also add a Complex fallback for convert_element_type. After this PR, we can enable ABI-compatible for AOTInductor OSS CI.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/119125<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Sun, 04 Feb 2024 07:48:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/79b20aec764c62e7a086aa2e2515b21731c9a831</guid>
    </item>
    <item>
      <title>[auto_functionalize] Remove mutated_args_name from args (#119050)</title>
      <link>https://github.com/pytorch/pytorch/commit/440b7d52793be046354c03521295ab7f2d3969f2</link>
      <description><![CDATA[<p>[auto_functionalize] Remove mutated_args_name from args (#119050)</p>
<p><code>auto_functionalize</code> currently takes a custom op, a list of mutated argument names, and inputs to the custom op as kwargs. The list of mutated argument names is computed from the schema, and gets created when we're tracing. However, it seems that having the list of mutated argument names is a little unnecessary since we can always recompute it from the schema during runtime.</p>
<p>This also prevents the case where users might incorrectly modify the inputs to this operator, as we will now just recompute it during the runtime. This probably won't affect things too much because inductor will decompose auto_functionalize.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/119050<br />
Approved by: https://github.com/zou3519</p>]]></description>
      <pubDate>Fri, 02 Feb 2024 16:27:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/440b7d52793be046354c03521295ab7f2d3969f2</guid>
    </item>
    <item>
      <title>Expose aggressive_recomputation as an inductor config (#118943)</title>
      <link>https://github.com/pytorch/pytorch/commit/de6a9060932f4541863130d4cb8e90267f22dd56</link>
      <description><![CDATA[<p>Expose aggressive_recomputation as an inductor config (#118943)</p>
<p>Summary:<br />
As title.</p>
<p>We found aggressive_recomputation shows memory savings (7% on APS COFFEE model) with 2% QPS loss.</p>
<p>It also gives very promising signal on our auto ac experiments: https://docs.google.com/document/d/1S2qgMg1CwAQ4U1Ffuk2epbEOx06ogZhioX2jKCwL7ZQ/edit</p>
<p>{F1426175073}</p>
<p>Test Plan:<br />
APS COFFEE from silverlakeli<br />
- Zoom of baseline job: https://www.internalfb.com/intern/zoomer/?profiling_run_fbid=927380488801910&amp;tab=overview<br />
- Zoom of job with aggressive_recomputation: https://www.internalfb.com/intern/zoomer/?profiling_run_fbid=1126815608217470&amp;tab=overview</p>
<p>APS 1100x shrunk version:<br />
- baseline: https://www.internalfb.com/mast/job/aps-yuzhenhuang-afe049505a<br />
- test: https://www.internalfb.com/mast/job/aps-yuzhenhuang-709e41bf0d<br />
Memory from 42.98% -&gt; 41.04%.</p>
<p>Reviewed By: yf225, yuxihu, silverlakeli, richqyz</p>
<p>Differential Revision: D53248057</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118943<br />
Approved by: https://github.com/anijain2305, https://github.com/yanboliang</p>]]></description>
      <pubDate>Fri, 02 Feb 2024 16:17:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/de6a9060932f4541863130d4cb8e90267f22dd56</guid>
    </item>
    <item>
      <title>[inductor] make multi-kernel work with cpp-wrapper (#117813)</title>
      <link>https://github.com/pytorch/pytorch/commit/c24ffc3f66b2270dfc65a404687b91b55ed580e9</link>
      <description><![CDATA[<p>[inductor] make multi-kernel work with cpp-wrapper (#117813)</p>
<p>Make multi-kernel work with cpp-wrapper. multi-kernel generates two equivalent variants for a reduction. At runtime the faster one is picked. But cpp-wrapper need save cubin file during codegen. They don't work with each other at the beginning.</p>
<p>Thanks Jason for suggesting a neat way to integrate these two. cpp-wrapper does 2 passes codegen right now. For the first pass, we still generate multi-kernel code and run it; for the second pass, we load the cubin file for the faster kernel directly. And multi-kernel python code is not generated for the second pass since they should not be needed.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117813<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 02 Feb 2024 16:06:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c24ffc3f66b2270dfc65a404687b91b55ed580e9</guid>
    </item>
    <item>
      <title>[Inductor] GEMM shape padding improvements (#118522)</title>
      <link>https://github.com/pytorch/pytorch/commit/cc46829f96dba05b9b46bae31a1e6d2a053f667e</link>
      <description><![CDATA[<p>[Inductor] GEMM shape padding improvements (#118522)</p>
<p>Improvements to shape padding logic in torch/_inductor/pad_mm.py</p>
<p>These changes could lead up to 14% perf improvement for certain Meta internal models in experiments.</p>
<p>Most notably:<br />
  * 1.) Use aten.const_pad_nd operation to pad Tensors in a single op instead of using multiple steps involving intermediate buffers. This appears to be more performant than the previous logic, confirmed by Profiling &amp; Benchmarking results ( Meta internal )<br />
 * 2.) Make many paddings unneccessary using explicitly transposed GEMM when either M or N dimension is properly aligned but the other is not, configurable via config.shape_pad_use_transpose (default: True).<br />
  * 3.) Enable shape padding for the Inductor CUDA  /  Cutlass backend for all GEMM ops where Cutlass would be enabled, without benchmarking in that case.<br />
  * Add config flag to always pad shapes (without benchmarking first), configurable via config.force_shape_pad (default: False )<br />
  * Added several new unit tests to ensure tensors are padded such that they meet all alignment requirements after padding.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118522<br />
Approved by: https://github.com/jansel, https://github.com/eellison</p>]]></description>
      <pubDate>Fri, 02 Feb 2024 00:50:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cc46829f96dba05b9b46bae31a1e6d2a053f667e</guid>
    </item>
    <item>
      <title>[inductor] skip launching kernels with zero grid in AOTInductor when using backed symints (#118654)</title>
      <link>https://github.com/pytorch/pytorch/commit/babd6c776dfdc5db0f0f1bc69e41a5a5d76ca719</link>
      <description><![CDATA[<p>[inductor] skip launching kernels with zero grid in AOTInductor when using backed symints (#118654)</p>
<p>Like #110312 but we also run this check when backed symints are in the grid (e.g. s1 / 512)</p>
<h3>Why?</h3>
<p>Let's say we lower a model and generate GPU kernel grid with symbolic shapes, for e.g. <code>s1 / 512</code>. If at some point later, we ran the lowered model with inputs s.t. <code>s1 = 0</code>, then we'll launch the kernel with a <code>0</code> sized grid. This surfaces as <code>CUDA driver error: invalid argument</code>.</p>
<p>To avoid this, we check for a <code>0</code> sized grid whenever there's symbolic shapes which includes backed and unbacked symints.</p>
<p>This adds non-zero overhead to the CPU. However, in return, we get better reliability when encountering this scenario. This scenario happened when serving an internal model.</p>
<h3>Test</h3>
<p>```<br />
$ python test/inductor/test_aot_inductor.py -k test_zero_grid_with_unbacked_symbols<br />
OK (skipped=3)</p>
<p>$ python test/inductor/test_aot_inductor.py -k test_zero_grid_with_backed_symbols</p>
<h1>Before</h1>
<p>Error: CUDA driver error: invalid argument<br />
FAILED (errors=2, skipped=3)</p>
<h1>Now</h1>
<p>OK (skipped=3)<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118654<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 01 Feb 2024 19:19:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/babd6c776dfdc5db0f0f1bc69e41a5a5d76ca719</guid>
    </item>
    <item>
      <title>[inductor] Fix an internal test issue (#118903)</title>
      <link>https://github.com/pytorch/pytorch/commit/946ea47a4fcf7883560743e3b290547dc506056a</link>
      <description><![CDATA[<p>[inductor] Fix an internal test issue (#118903)</p>
<p>Summary: test_add_complex4 that introduced in https://github.com/pytorch/pytorch/pull/117929  fails internally, because of a cpp compilation issue for cpu. Specify the right device in the test instead.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D53333919">D53333919</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/118903<br />
Approved by: https://github.com/clee2000</p>]]></description>
      <pubDate>Thu, 01 Feb 2024 19:18:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/946ea47a4fcf7883560743e3b290547dc506056a</guid>
    </item>
    <item>
      <title>Revert "[inductor] make multi-kernel work with cpp-wrapper (#117813)"</title>
      <link>https://github.com/pytorch/pytorch/commit/796278b57e95c9a6c2bdcdea413d61b31fb6344a</link>
      <description><![CDATA[<p>Revert "[inductor] make multi-kernel work with cpp-wrapper (#117813)"</p>
<p>This reverts commit 20484a193626ef72e0b3f35914f17deb2a89b8fc.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/117813 on behalf of https://github.com/atalman due to broke linux-focal-rocm5.7-py3.8 tests (<a href="https://github.com/pytorch/pytorch/pull/117813#issuecomment-1922613135">comment</a>)</p>]]></description>
      <pubDate>Thu, 01 Feb 2024 17:19:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/796278b57e95c9a6c2bdcdea413d61b31fb6344a</guid>
    </item>
    <item>
      <title>Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)</title>
      <link>https://github.com/pytorch/pytorch/commit/08d90a1ea97d17b9fb233a17c605415e13497922</link>
      <description><![CDATA[<p>Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)</p>
<p>Info about super in dynamic classes:<br />
https://stackoverflow.com/questions/71879642/how-to-pass-function-with-super-when-creating-class-dynamically<br />
https://stackoverflow.com/questions/43782944/super-does-not-work-together-with-type-supertype-obj-obj-must-be-an-i</p>
<p>Calling super(TestCase) actually calls TestCase's parent's functions, bypassing TestCase itself's functions</p>
<p>Mainly doing this because it's making disable bot spam</p>
<p>Test: checked locally and check that https://github.com/pytorch/pytorch/issues/117954 actually got skipped</p>
<p>Logs for <code>inductor/test_torchinductor_dynamic_shapes.py::TestInductorDynamicCUDA::test_unbacked_index_select_cuda</code><br />
https://ossci-raw-job-status.s3.amazonaws.com/log/21083466405<br />
Afaik this PR doesn't actually cause the test to fail, it just surfaces the error since the mem leak check wasn't running previously</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118586<br />
Approved by: https://github.com/huydhn</p>]]></description>
      <pubDate>Thu, 01 Feb 2024 16:40:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/08d90a1ea97d17b9fb233a17c605415e13497922</guid>
    </item>
    <item>
      <title>[inductor] more accurate throughput calculations for kernel benchmarks (#118858)</title>
      <link>https://github.com/pytorch/pytorch/commit/61b572ed5638dea4b47c16672eb7e7361e806bfc</link>
      <description><![CDATA[<p>[inductor] more accurate throughput calculations for kernel benchmarks (#118858)</p>
<p>Our current throughput calculations for kernel benchmarks have some issues,<br />
particularly when we slice inputs in the kernel. In such cases, we count<br />
the original inputs as part of the memory traffic passed across the kernel.<br />
This is incorrect because it may result in a much larger throughput<br />
calculation, which can even exceed the theoretical bandwidth.</p>
<p>Instead, we should only count the size of the "slices" that contribute to<br />
the actual memory traffic.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118858<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 01 Feb 2024 13:42:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/61b572ed5638dea4b47c16672eb7e7361e806bfc</guid>
    </item>
    <item>
      <title>[inductor] make multi-kernel work with cpp-wrapper (#117813)</title>
      <link>https://github.com/pytorch/pytorch/commit/20484a193626ef72e0b3f35914f17deb2a89b8fc</link>
      <description><![CDATA[<p>[inductor] make multi-kernel work with cpp-wrapper (#117813)</p>
<p>Make multi-kernel work with cpp-wrapper. multi-kernel generates two equivalent variants for a reduction. At runtime the faster one is picked. But cpp-wrapper need save cubin file during codegen. They don't work with each other at the beginning.</p>
<p>Thanks Jason for suggesting a neat way to integrate these two. cpp-wrapper does 2 passes codegen right now. For the first pass, we still generate multi-kernel code and run it; for the second pass, we load the cubin file for the faster kernel directly. And multi-kernel python code is not generated for the second pass since they should not be needed.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117813<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 01 Feb 2024 13:29:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/20484a193626ef72e0b3f35914f17deb2a89b8fc</guid>
    </item>
    <item>
      <title>[inductor] Handle special values correctly in ir.Scan codegen (#118788)</title>
      <link>https://github.com/pytorch/pytorch/commit/9c2b43cc5057cb840879224e3bdced58d6334c0a</link>
      <description><![CDATA[<p>[inductor] Handle special values correctly in ir.Scan codegen (#118788)</p>
<p>Special values (<code>NaN</code>/<code>+/-Inf</code>) are not correctly during codegen for <code>ir.Scan</code> nodes. This<br />
is a fairly minor bugfix that has not come up since the only two scan<br />
ops with lowerings use "normal" values.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118788<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Thu, 01 Feb 2024 06:54:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9c2b43cc5057cb840879224e3bdced58d6334c0a</guid>
    </item>
    <item>
      <title>[AOTInductor] Add Runtime Constant-folding for AOTInductor (#118765)</title>
      <link>https://github.com/pytorch/pytorch/commit/2b48891e62e5c4b57c8cac92cee5eb71228a203a</link>
      <description><![CDATA[<p>[AOTInductor] Add Runtime Constant-folding for AOTInductor (#118765)</p>
<p>Summary:<br />
Add Runtime Constant-folding for AOTInductor.<br />
This also include the invocation of constant folding at load time.</p>
<p>The constant folding lowering is a 2-step process.<br />
First, we split the graph into 2 modules, one of it is the constant module, which doesn't depend on any input and the whole module could be inferred (constant-folded) one-time and be reused. The constant module, is lowered, and being codegen-ed as usual and cached (let's call this constant code). The constant code reuses the whole lowering/profiling/etc. process, only difference is that we do not generate any headers or initialization for the constant code.<br />
Second, after handling the constant module, we take care of the main module (which is the part that would depend on the user input.) For the main module, we take in one additional component, the constant code, compare with a normal lowering. Addition step we do here is that, we inject the constant code into the codegen-ed main module, and create the caller for the main module to consume the result of the constant module.</p>
<p>Test Plan: Unit tests included in commit.</p>
<p>Differential Revision: D53274382</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118765<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Wed, 31 Jan 2024 20:54:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2b48891e62e5c4b57c8cac92cee5eb71228a203a</guid>
    </item>
    <item>
      <title>[Inductor] Skip triton templates for mixedmm on SM70- (#118591)</title>
      <link>https://github.com/pytorch/pytorch/commit/6c67f3333a539e8f29515375a87612897214f8f2</link>
      <description><![CDATA[<p>[Inductor] Skip triton templates for mixedmm on SM70- (#118591)</p>
<p>As it results in numerical errors, see https://github.com/pytorch/pytorch/issues/117144</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/117144</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118591<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 31 Jan 2024 15:30:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6c67f3333a539e8f29515375a87612897214f8f2</guid>
    </item>
    <item>
      <title>Revert "Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)"</title>
      <link>https://github.com/pytorch/pytorch/commit/483001e8468209911292aa7f23c2a25fbfb6e31b</link>
      <description><![CDATA[<p>Revert "Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)"</p>
<p>This reverts commit f2682e75e6fd735c4a84afe59eafd541f7643f4a.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/118586 on behalf of https://github.com/atalman due to Broke slow tests (<a href="https://github.com/pytorch/pytorch/pull/118586#issuecomment-1919810802">comment</a>)</p>]]></description>
      <pubDate>Wed, 31 Jan 2024 11:44:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/483001e8468209911292aa7f23c2a25fbfb6e31b</guid>
    </item>
    <item>
      <title>[AOTI] Support _embedding_bag in C shim (#118706)</title>
      <link>https://github.com/pytorch/pytorch/commit/1128cf96f078a76f27b038d7adbc2b72fe8927e2</link>
      <description><![CDATA[<p>[AOTI] Support _embedding_bag in C shim (#118706)</p>
<p>Summary: At some point I will stop manually adding ops to C shim, but use torchgen to generate those code. For the near term, I need to add a few more in order to switch the AOTInductor dashboard run.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D53249074">D53249074</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/118706<br />
Approved by: https://github.com/frank-wei, https://github.com/aakhundov<br />
ghstack dependencies: #118704, #118705</p>]]></description>
      <pubDate>Wed, 31 Jan 2024 07:02:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1128cf96f078a76f27b038d7adbc2b72fe8927e2</guid>
    </item>
    <item>
      <title>[inductor] Refactor ir.ComplexView (#118704)</title>
      <link>https://github.com/pytorch/pytorch/commit/dd52939438834545632ccc9e986029d63f68e064</link>
      <description><![CDATA[<p>[inductor] Refactor ir.ComplexView (#118704)</p>
<p>Summary: Make ir.ComplexView a subclass of ir.FallbackKernel, to unify the codegen logic</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D53248972">D53248972</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/118704<br />
Approved by: https://github.com/frank-wei</p>]]></description>
      <pubDate>Wed, 31 Jan 2024 06:42:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dd52939438834545632ccc9e986029d63f68e064</guid>
    </item>
    <item>
      <title>[Cutlass 3.3.0 submodule upgrade] (#118629)</title>
      <link>https://github.com/pytorch/pytorch/commit/35f3ccffd4dd836d71b1db6d560800597979ec5b</link>
      <description><![CDATA[<p>[Cutlass 3.3.0 submodule upgrade] (#118629)</p>
<p>Cutlass 3.3 offers the following improvements:</p>
<p>Adds support for mixed precision GEMMs On Hopper and Ampere Adds support for &lt; 16B aligned GEMMs on Hopper<br />
Enhancements to EVT<br />
Enhancements to Python interface<br />
Enhancements to Sub-byte type handling in CuTe<br />
Several other bug-fixes and performance improvements. minor doc update<br />
Test Plan:</p>
<p>CI ( ciflow/trunk, ciflow/inductor )<br />
pytest test/inductor/test_max_autotune.py</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118629<br />
Approved by: https://github.com/drisspg, https://github.com/Skylion007, https://github.com/khabinov</p>]]></description>
      <pubDate>Wed, 31 Jan 2024 05:53:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/35f3ccffd4dd836d71b1db6d560800597979ec5b</guid>
    </item>
    <item>
      <title>Reland: [aotinductor] Replicate split_cat from torch IR to predispatch IR" (#118590)</title>
      <link>https://github.com/pytorch/pytorch/commit/6fa162e68148c10626bc824bc3eb31d56db4c2d8</link>
      <description><![CDATA[<p>Reland: [aotinductor] Replicate split_cat from torch IR to predispatch IR" (#118590)</p>
<p>Summary:<br />
This is part the pass migration efforts. The final target is removing the acc tracer in AOTI.<br />
In this diff, I did a few things:<br />
1. copy and modify the <code>fx_passes/split_cat.py</code> passes based on predispatch IR.<br />
2. verify the correctness by copying the <code>test_split_cat_fx_passes.py</code> and create a new file <code>test_split_cat_fx_passes_aten_fb.py</code> which is executed in AOTI and checked the counters<br />
3. create a util function to execute the pass and compare the before/after graph to give user more information like pass effect and time spent. It will create logs like<br />
<code>[2024-01-25 20:26:48,997] torch._inductor.utils: [INFO] [Pre grad(predispatch IR)]Apply split_cat, index: 0, save before/after graph to /tmp/tmpvlpwrklp, graph before/after are the same = False, time elapsed = 0:00:00.001585
[2024-01-25 20:26:49,000] torch._inductor.utils: [INFO] [Pre grad(predispatch IR)]Apply split_cat, index: 1, save before/after graph to /tmp/tmpz_onjfeu, graph before/after are the same = False, time elapsed = 0:00:00.001873
[2024-01-25 20:26:49,002] torch._inductor.utils: [INFO] [Pre grad(predispatch IR)]Apply split_cat, index: 2, save before/after graph to /tmp/tmpgkck8yko, graph before/after are the same = True, time elapsed = 0:00:00.000269
[2024-01-25 20:26:49,007] torch._inductor.utils: [INFO] [Pre grad(predispatch IR)]Apply split_cat, index: 3, save before/after graph to /tmp/tmpquenq06y, graph before/after are the same = False, time elapsed = 0:00:00.003621
[2024-01-25 20:26:49,009] torch._inductor.utils: [INFO] [Pre grad(predispatch IR)]Apply split_cat, index: 4, save before/after graph to /tmp/tmpi8fia0dv, graph before/after are the same = True, time elapsed = 0:00:00.000190</code></p>
<p>Differential Revision: D53171027</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118590<br />
Approved by: https://github.com/kflu, https://github.com/khabinov, https://github.com/chenyang78</p>]]></description>
      <pubDate>Tue, 30 Jan 2024 16:09:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6fa162e68148c10626bc824bc3eb31d56db4c2d8</guid>
    </item>
    <item>
      <title>Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)</title>
      <link>https://github.com/pytorch/pytorch/commit/f2682e75e6fd735c4a84afe59eafd541f7643f4a</link>
      <description><![CDATA[<p>Workaround for super() calls in test_torchinductor_dynamic_shapes (#118586)</p>
<p>Info about super in dynamic classes:<br />
https://stackoverflow.com/questions/71879642/how-to-pass-function-with-super-when-creating-class-dynamically<br />
https://stackoverflow.com/questions/43782944/super-does-not-work-together-with-type-supertype-obj-obj-must-be-an-i</p>
<p>Calling super(TestCase) actually calls TestCase's parent's functions, bypassing TestCase itself's functions</p>
<p>Mainly doing this because it's making disable bot spam</p>
<p>Test: checked locally and check that https://github.com/pytorch/pytorch/issues/117954 actually got skipped</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118586<br />
Approved by: https://github.com/huydhn</p>]]></description>
      <pubDate>Tue, 30 Jan 2024 13:34:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f2682e75e6fd735c4a84afe59eafd541f7643f4a</guid>
    </item>
    <item>
      <title>[inductor] Use at::detail::empty_strided_* in cpp_wraper mode (#118490)</title>
      <link>https://github.com/pytorch/pytorch/commit/e332653eb38b6f97e8088b4d3adc5b112fe8f761</link>
      <description><![CDATA[<p>[inductor] Use at::detail::empty_strided_* in cpp_wraper mode (#118490)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118490<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 30 Jan 2024 13:03:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e332653eb38b6f97e8088b4d3adc5b112fe8f761</guid>
    </item>
    <item>
      <title>[inductor][cpp] support scalar value in vec reduction (#118511)</title>
      <link>https://github.com/pytorch/pytorch/commit/e5bb527d3e91b244404e142dfac8e9e55e02aa81</link>
      <description><![CDATA[<p>[inductor][cpp] support scalar value in vec reduction (#118511)</p>
<p>Fix https://github.com/pytorch/pytorch/issues/118379</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118511<br />
Approved by: https://github.com/leslie-fang-intel, https://github.com/lezcano, https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 30 Jan 2024 05:07:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e5bb527d3e91b244404e142dfac8e9e55e02aa81</guid>
    </item>
    <item>
      <title>[inductor] Fix codegen bug with Native Triton kernels with  ReinterpretView args (#118569)</title>
      <link>https://github.com/pytorch/pytorch/commit/8be6dee14b76f63dea1e9329d6cd0fdc9552a2d6</link>
      <description><![CDATA[<p>[inductor] Fix codegen bug with Native Triton kernels with  ReinterpretView args (#118569)</p>
<p>Summary:</p>
<h3>Context</h3>
<p>It's possible for the args of a user-defined Triton Kernel to be codegen-ed twiced. But this only happens if the arg is a <code>ReinterpretView</code>.<br />
* First via <code>arg.codegen_reference()</code> in <code>define_user_defined_triton_kernel()</code><br />
* Second in <code>self.codegen_kwargs()</code>.</p>
<p>When using <code>abi_compatible=True</code>, the duplicate codegen will look like the code below. The issue in the code is that one of the Tensors, internal to the graph, isn't properly freed. This scenario was eventually exposed as a memory leak when we re-ran an AOTInductor model many times and observed <code>memory.used</code> increase after each iteration.<br />
<code>auto tmp_tensor_handle_0 = reinterpret_tensor_wrapper(buf1, 2, int_array_0, int_array_1, 0L);
auto tmp_tensor_handle_1 = reinterpret_tensor_wrapper(buf1, 2, int_array_0, int_array_1, 0L);
...
// There's no wrap_with_raii_handle_if_needed() for tmp_tensor_handle_0.
// And there's no reference to tmp_tensor_handle_0.
// Thus, tmp_tensor_handle_0 is left as an AtenTensorHandle which isn't
// automatically cleaned-up like RAIIAtenTensorHandle
CUdeviceptr var_6;
aoti_torch_get_data_ptr(wrap_with_raii_handle_if_needed(tmp_tensor_handle_1), reinterpret_cast&lt;void**&gt;(&amp;var_6));
void* kernel_args_var_2[] = {..., &amp;var_6, ...};
launchKernel(kernels.add_kernel_0, ..., kernel_args_var_2);</code></p>
<h3>Solution</h3>
<p>We just need the arg's buffer name when creating the <code>TensorArg</code> in <code>define_user_defined_triton_kernel()</code>. Thus, just return the buffer's name and avoid any potential side-effects with <code>arg.codegen_reference()</code>.</p>
<p>Test Plan:</p>
<h3>Inspect device memory allocated</h3>
<p>```</p>
<h1>Before diff</h1>
<p>0 device memory 2048<br />
1 device memory 2560<br />
2 device memory 3072<br />
3 device memory 3584<br />
4 device memory 4096<br />
5 device memory 4608</p>
<h1>With diff (memory usage doesn't grow)</h1>
<p>0 device memory 1536<br />
1 device memory 1536<br />
2 device memory 1536<br />
3 device memory 1536<br />
4 device memory 1536<br />
5 device memory 1536<br />
```</p>
<p>Reviewed By: jingsh, tissue3</p>
<p>Differential Revision: D53190934</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118569<br />
Approved by: https://github.com/oulgen</p>]]></description>
      <pubDate>Mon, 29 Jan 2024 21:19:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8be6dee14b76f63dea1e9329d6cd0fdc9552a2d6</guid>
    </item>
    <item>
      <title>Reland PR117393 [inductor/fb] log config dict when compilation finishes (#118552)</title>
      <link>https://github.com/pytorch/pytorch/commit/5dfcf07449c9ffc7a2277c9782c88444ca11a77c</link>
      <description><![CDATA[<p>Reland PR117393 [inductor/fb] log config dict when compilation finishes (#118552)</p>
<p>Summary: Reverted due to merge conflict</p>
<p>Differential Revision: D53188124</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118552<br />
Approved by: https://github.com/mengluy0125</p>]]></description>
      <pubDate>Mon, 29 Jan 2024 20:34:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5dfcf07449c9ffc7a2277c9782c88444ca11a77c</guid>
    </item>
    <item>
      <title>[ez][inductor] fix a typo in should_pad_bench (#118598)</title>
      <link>https://github.com/pytorch/pytorch/commit/0d47f6a44f4a72324ee096c5bbdb107b379a55c2</link>
      <description><![CDATA[<p>[ez][inductor] fix a typo in should_pad_bench (#118598)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118598<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Mon, 29 Jan 2024 19:49:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0d47f6a44f4a72324ee096c5bbdb107b379a55c2</guid>
    </item>
    <item>
      <title>[inductor] Remove ROCm xfail on test_cum{sum,prod}_zero_dim (#118558)</title>
      <link>https://github.com/pytorch/pytorch/commit/f99adbb4ec7a738684925d111f81f6b523cc83d0</link>
      <description><![CDATA[<p>[inductor] Remove ROCm xfail on test_cum{sum,prod}_zero_dim (#118558)</p>
<p>Fixes #118540, fixes #118541</p>
<p>Since the zero-dim case reduces to a pointwise operation, we don't fallback on<br />
ROCm.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118558<br />
Approved by: https://github.com/malfet</p>]]></description>
      <pubDate>Mon, 29 Jan 2024 12:23:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f99adbb4ec7a738684925d111f81f6b523cc83d0</guid>
    </item>
    <item>
      <title>[inductor][cpp] enable vectorization with constant bool (#118380)</title>
      <link>https://github.com/pytorch/pytorch/commit/04c1df651aa58bea50977f4efcf19b09ce27cefd</link>
      <description><![CDATA[<p>[inductor][cpp] enable vectorization with constant bool (#118380)</p>
<p>Related model DebertaForQuestionAnswering etc. For DebertaForQuestionAnswering, single thread, measured on ICX:<br />
Before: 0.990x, After: 1.043x</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118380<br />
Approved by: https://github.com/leslie-fang-intel, https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 29 Jan 2024 05:31:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/04c1df651aa58bea50977f4efcf19b09ce27cefd</guid>
    </item>
    <item>
      <title>[Inductor] Fix Argmax codegen with Nan input (#118358)</title>
      <link>https://github.com/pytorch/pytorch/commit/ee3dfbbe470822a50516310384ecce071ec79f7c</link>
      <description><![CDATA[<p>[Inductor] Fix Argmax codegen with Nan input (#118358)</p>
<p><strong>Summary</strong><br />
Fix issue https://github.com/pytorch/pytorch/issues/118266, current <code>torch.argmax</code> and <code>torch.argmin</code> has different return values with eager and Inductor cpp backend when inputs has <code>Nan</code> value. Align cpp backend results to eager by reusing the compare function.</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_argmin_cpu_only
python -u -m pytest -s -v test_cpu_repro.py -k test_argmax_argmin_with_nan_value</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118358<br />
Approved by: https://github.com/lezcano, https://github.com/jgong5, https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 29 Jan 2024 01:09:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ee3dfbbe470822a50516310384ecce071ec79f7c</guid>
    </item>
    <item>
      <title>Add some type annotations to torch._inductor.codegen.wrapper (#118491)</title>
      <link>https://github.com/pytorch/pytorch/commit/2951bbf0f72dcbf8841dafbd6e764c07eb72edc6</link>
      <description><![CDATA[<p>Add some type annotations to torch._inductor.codegen.wrapper (#118491)</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118491<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Sun, 28 Jan 2024 22:17:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2951bbf0f72dcbf8841dafbd6e764c07eb72edc6</guid>
    </item>
    <item>
      <title>Unify MYPYINDUCTOR and MYPY (#118432)</title>
      <link>https://github.com/pytorch/pytorch/commit/d03173e88cf43544fbeb458e1b8d7122038bef5b</link>
      <description><![CDATA[<p>Unify MYPYINDUCTOR and MYPY (#118432)</p>
<p>The original motivation for MYPYINDUCTOR was a faster type checking configuration that only checked a subset of files. With the removal of <code>follow_imports = ignore</code>, we are now able to use dmypy to do fast incremental typechecking, eliminating the need for this.</p>
<p>Perhaps erroneously, when I tee'ed up this PR I elected to delete the <code>follow_imports = skip</code> designations in the mypy-inductor.ini. This lead to a number of extra type error suppressions that I manually edited. You will need to review.</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118432<br />
Approved by: https://github.com/Skylion007<br />
ghstack dependencies: #118414, #118418</p>]]></description>
      <pubDate>Sat, 27 Jan 2024 09:23:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d03173e88cf43544fbeb458e1b8d7122038bef5b</guid>
    </item>
    <item>
      <title>Replace follow_imports = silent with normal (#118414)</title>
      <link>https://github.com/pytorch/pytorch/commit/9bce208dfbdb71e38f9e9ee38a07d43645ffb82a</link>
      <description><![CDATA[<p>Replace follow_imports = silent with normal (#118414)</p>
<p>This is a lot of files changed! Don't panic! Here's how it works:</p>
<ul>
<li>Previously, we set <code>follow_imports = silent</code> for our mypy.ini configuration. Per https://mypy.readthedocs.io/en/stable/running_mypy.html#follow-imports, what this does is whenever we have an import to a module which is not listed as a file to be typechecked in mypy, we typecheck it as normal but suppress all errors that occurred in that file.</li>
<li>When mypy is run inside lintrunner, the list of files is precisely the files covered by the glob in lintrunner.toml, but with files in excludes excluded.</li>
<li>The top-level directive <code># mypy: ignore-errors</code> instructs mypy to typecheck the file as normal, but ignore all errors.</li>
<li>Therefore, it should be equivalent to set <code>follow_imports = normal</code>, if we put <code># mypy: ignore-errors</code> on all files that were previously excluded from the file list.</li>
<li>Having done this, we can remove the exclude list from .lintrunner.toml, since excluding a file from typechecking is baked into the files themselves.</li>
<li>torch/_dynamo and torch/_inductor were previously in the exclude list, because they were covered by MYPYINDUCTOR. It is not OK to mark these as <code># mypy: ignore-errors</code> as this will impede typechecking on the alternate configuration. So they are temporarily being checked twice, but I am suppressing the errors in these files as the configurations are not quite the same. I plan to unify the configurations so this is only a temporary state.</li>
<li>There were some straggler type errors after these changes somehow, so I fixed them as needed. There weren't that many.</li>
</ul>
<p>In the future, to start type checking a file, just remove the ignore-errors directive from the top of the file.</p>
<p>The codemod was done with this script authored by GPT-4:</p>
<p>```<br />
import glob</p>
<p>exclude_patterns = [<br />
    ...<br />
]</p>
<p>for pattern in exclude_patterns:<br />
    for filepath in glob.glob(pattern, recursive=True):<br />
        if filepath.endswith('.py'):<br />
            with open(filepath, 'r+') as f:<br />
                content = f.read()<br />
                f.seek(0, 0)<br />
                f.write('# mypy: ignore-errors\n\n' + content)<br />
```</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118414<br />
Approved by: https://github.com/thiagocrepaldi, https://github.com/albanD</p>]]></description>
      <pubDate>Fri, 26 Jan 2024 18:44:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9bce208dfbdb71e38f9e9ee38a07d43645ffb82a</guid>
    </item>
    <item>
      <title>[inductor] Handle cum{sum,prod} on zero-dim tensors (#117990)</title>
      <link>https://github.com/pytorch/pytorch/commit/f129e3fe034c0052d1d45856ac10fc4b3a1e9700</link>
      <description><![CDATA[<p>[inductor] Handle cum{sum,prod} on zero-dim tensors (#117990)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117990<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Fri, 26 Jan 2024 14:21:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f129e3fe034c0052d1d45856ac10fc4b3a1e9700</guid>
    </item>
    <item>
      <title>Fix several bugs related to unbacked SymInt codegen in inductor (#117862)</title>
      <link>https://github.com/pytorch/pytorch/commit/96d94f574e32c63421dc7badb0a5c42bb29324c7</link>
      <description><![CDATA[<p>Fix several bugs related to unbacked SymInt codegen in inductor (#117862)</p>
<p>Let me tell you, this was a <em>journey.</em></p>
<ul>
<li>When we repropagate through FX interpreter in AOTAutograd, this will reallocate unbacked SymInts. We can eliminate all of these fresh allocations by appropriately asserting equalities on them setting up replacements. See also https://github.com/pytorch/pytorch/issues/111950</li>
<li>The <code>inner_fn</code> of Loops can contain references to unbacked SymInts. We must collect them to prevent DCE.</li>
<li>Export naughtily accessed <code>_expr</code> when it should have accessed <code>expr</code> on SymNode. Fixed two sites of this.</li>
</ul>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117862<br />
Approved by: https://github.com/bdhirsh</p>]]></description>
      <pubDate>Fri, 26 Jan 2024 10:08:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/96d94f574e32c63421dc7badb0a5c42bb29324c7</guid>
    </item>
    <item>
      <title>[Inductor] Add Thread Number Checker in scatter_reduce_ fallback for CPP backend (#118278)</title>
      <link>https://github.com/pytorch/pytorch/commit/b66c4eda61f4e6be536a096c3fcdf464a3608aef</link>
      <description><![CDATA[<p>[Inductor] Add Thread Number Checker in scatter_reduce_ fallback for CPP backend (#118278)</p>
<p><strong>Summary</strong><br />
Follow up of https://github.com/pytorch/pytorch/pull/108220 which improves performance of <code>basic_gnn_gin</code>, <code>basic_gnn_sage</code> and <code>basic_gnn_gcn</code> in multi thread test cases. However, it causes performance regression of these 3 models in single thread test case as reported in https://github.com/pytorch/pytorch/issues/117740. Fix the single thread issues in this PR by adding the thread number check to decide whether fallback <code>scatter_reduce_</code> or not.</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_scatter_using_atomic_add</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118278<br />
Approved by: https://github.com/jansel, https://github.com/jgong5</p>]]></description>
      <pubDate>Fri, 26 Jan 2024 04:43:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b66c4eda61f4e6be536a096c3fcdf464a3608aef</guid>
    </item>
    <item>
      <title>[c10d_functional] fix an issue where mutation on views fails in inductor (#118333)</title>
      <link>https://github.com/pytorch/pytorch/commit/0857a3a753b5d04e01a57c313dec7e0b53f1bf2d</link>
      <description><![CDATA[<p>[c10d_functional] fix an issue where mutation on views fails in inductor (#118333)</p>
<p><code>_CollectiveKernel.create_inplace</code> expresses mutation with the newly introduced <code>MutationOutput</code> which requires the <code>layout</code> of the input. Currently, there's a bug where if the input is a view, <code>inp.layout</code> fails. This PR fixes the issue by unwrapping the input if it's a view.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118333<br />
Approved by: https://github.com/wanchaol</p>]]></description>
      <pubDate>Fri, 26 Jan 2024 03:13:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0857a3a753b5d04e01a57c313dec7e0b53f1bf2d</guid>
    </item>
    <item>
      <title>fix key error in pre_grad fx_passes_numeric_check (#118325)</title>
      <link>https://github.com/pytorch/pytorch/commit/4d0b471389dd38865a2ff214ba7df185d02b18b1</link>
      <description><![CDATA[<p>fix key error in pre_grad fx_passes_numeric_check (#118325)</p>
<p>Summary:<br />
<code>I0125 121749.865 pyper_config_utils.py:8225] torchdynamo pyper config = TorchDynamoConfig(backend='inductor', optimize_ddp=False, log_compile_graph=False, inductor_config=TorchInductorConfig(enable_cudagraph=False, max_autotune=False, max_autotune_pointwise=True, max_autotune_gemm=False, search_autotune_cache=False, autotune_in_subproc=False, aggressive_fusion=False, shape_padding=True, permute_fusion=False, epilogue_fusion_first=False, debug=True, triton=None, trace_enabled=False, log_kernel_source=False, split_cat_fx_passes=False, group_fusion=False, batch_fusion=False, coordinate_descent_tuning=False, coordinate_descent_check_all_directions=False, coordinate_descent_search_radius=1, layout_optimization=True, pre_grad_fusion_options={}, post_grad_fusion_options={}, max_pointwise_cat_inputs=4, fx_passes_numeric_check={}), automatic_dynamic_shapes=True)</code><br />
In trainer<br />
<code>I0125 12:58:51.832000 4011.139732263132160 torchdynamo_wrapper.py:291  trainer:0:1 ] [pt2] creating torchdynamo backend wrapper with settings TorchDynamoConfig(backend='inductor', optimize_ddp=False, log_compile_graph=False, inductor_config=TorchInductorConfig(enable_cudagraph=False, max_autotune=False, max_autotune_pointwise=True, max_autotune_gemm=False, search_autotune_cache=False, autotune_in_subproc=False, aggressive_fusion=False, shape_padding=True, permute_fusion=False, epilogue_fusion_first=False, debug=True, triton=None, trace_enabled=False, log_kernel_source=False, split_cat_fx_passes=False, group_fusion=False, batch_fusion=False, coordinate_descent_tuning=False, coordinate_descent_check_all_directions=False, coordinate_descent_search_radius=1, layout_optimization=True, pre_grad_fusion_options={}, post_grad_fusion_options={}, max_pointwise_cat_inputs=4, fx_passes_numeric_check={}), automatic_dynamic_shapes=True) #ai_training_job_id="febe34d9-b2fb-493e-a5cc-6a0b1dc85ad4" #ai_training_local_rank="1" #ai_training_role_rank="1" #mast_job_attempt="2" #mast_job_name="f525072920-TrainingApplication"
...
if config.fx_passes_numeric_check["pre_grad"]:</code></p>
<p>https://www.internalfb.com/diff/D52826442?dst_version_fbid=1115735309429172&amp;transaction_fbid=682438900759710</p>
<p>https://www.internalfb.com/diff/D51838043?dst_version_fbid=336373395892373&amp;transaction_fbid=349901787874069</p>
<p>This diff first fixes the key error to restore broken tests.  Its pyper changes can be addressed later.</p>
<p>https://www.internalfb.com/code/fbsource/[72c19313ed73]/fbcode/caffe2/torch/_inductor/config.py?lines=142-147</p>
<p>Test Plan: buck2 run //caffe2/torch/fb/training_toolkit/integration_tests/training_lifecycle/cogwheel_tests/pyper_release_v2:cogwheel_smallworld_mimo_cmf_deterministic_ne_pt2_training_platform__canary_offline_training-launcher -- --build-fbpkg --run-disabled --tests test</p>
<p>Reviewed By: yusuo</p>
<p>Differential Revision: D53102344</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118325<br />
Approved by: https://github.com/mengluy0125</p>]]></description>
      <pubDate>Fri, 26 Jan 2024 03:02:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4d0b471389dd38865a2ff214ba7df185d02b18b1</guid>
    </item>
    <item>
      <title>[Inductor] Use sleef implementation for CPP backend acosh codegen (#118350)</title>
      <link>https://github.com/pytorch/pytorch/commit/8dd1be49b7c17223ed6de18fac4471abc6735aae</link>
      <description><![CDATA[<p>[Inductor] Use sleef implementation for CPP backend acosh codegen (#118350)</p>
<p><strong>Summary</strong><br />
Fix https://github.com/pytorch/pytorch/issues/118267. Current cpp backend using <code>f"({x} + ({x}*{x} - {vec_one}).sqrt()).log()"</code> to calculate <code>acosh</code>, the issue happens when input is a large negative value like <code>-910685.8125</code>. In this case, <code>(x*x - 1).sqrt() + x</code> equals to 0, and <code>0.log()</code> returns <code>-inf</code>. However, based on the document: https://pytorch.org/docs/stable/generated/torch.acosh.html, negative inputs should returns <code>Nan</code>. Using acosh sleef implementation to fix this issue.</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_acosh_with_negative_large_input</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118350<br />
Approved by: https://github.com/jgong5, https://github.com/lezcano</p>]]></description>
      <pubDate>Fri, 26 Jan 2024 02:19:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8dd1be49b7c17223ed6de18fac4471abc6735aae</guid>
    </item>
    <item>
      <title>Fix mergeability check for ghstack PRs (#118258)</title>
      <link>https://github.com/pytorch/pytorch/commit/b599f5608c309331df7a599c3b7f21a74375b2fa</link>
      <description><![CDATA[<p>Fix mergeability check for ghstack PRs (#118258)</p>
<h1>Changes</h1>
<ul>
<li>introduce <code>--check-mergeability</code> trymerge flag that attempts to merge PR locally, using the same merge logic as the mergebot, but requires just a read-only <code>GITHUB_TOKEN</code> and git repo.</li>
<li>change mergeability workflow to utilize the new --check-mergeability logic</li>
</ul>
<h1>Alternatives considered</h1>
<p>1.</p>
<blockquote>
<p>Rewrite <code>https://github.com/pytorch/test-infra/actions/workflows/pr-dependencies-check.yml</code> to correctly support partially merged ghstacks.</p>
</blockquote>
<p>That would be a slightly better approach, but ROI is lower, as it requires reimplementing trymerge logic and additional effort to consolidate the codebase (trymerge lives in pytorch repo).</p>
<p><code>pr-dependencies-check.yml</code> still produces human-readable results for partially merged ghstack prs (even if it falsely reports them as non-mergeable).</p>
<p>2.</p>
<blockquote>
<p>Instead of introducing new trymerge flag, use existing flags, including <code>--dry-run</code>.</p>
</blockquote>
<p>That didn't work, as no combination of existing flags skips the rule checks and ROCKSET lookups.</p>
<h1>Testing</h1>
<ol>
<li>Manual testing  <code>trymerge.py --check-mergeability</code>  on the regular and ghstack PRs:</li>
</ol>
<p><code>``
export GITHUB_TOKEN=
export GIT_REPO_DIR=</code>pwd`<br />
export GITHUB_REPOSITORY=pytorch/pytorch<br />
export GIT_REMOTE_URL=https://github.com/pytorch/pytorch</p>
<h1>Test 1 (2 prs, 1 is closed)</h1>
<p>python3 ../pytorch/.github/scripts/trymerge.py --check-mergeability  117862<br />
Skipping 1 of 2 PR (#117859) as its already been merged</p>
<p>echo $?<br />
0</p>
<h1>Test 2 (3 prs, 1 is closed)</h1>
<p>python3 ../pytorch/.github/scripts/trymerge.py --check-mergeability  118125<br />
Skipping 1 of 3 PR (#117859) as its already been merged</p>
<p>echo $?<br />
0</p>
<h1>Test 3 (3 prs, intentional conflicts introduced into <code>main</code>):</h1>
<p>python3 ../pytorch/.github/scripts/trymerge.py --check-mergeability  118125<br />
Skipping 1 of 3 PR (#117859) as its already been merged<br />
stdout:<br />
Auto-merging torch/_inductor/ir.py<br />
Auto-merging torch/_inductor/lowering.py<br />
CONFLICT (content): Merge conflict in torch/_inductor/lowering.py<br />
error: could not apply 66ba5b8792f... Realize inputs to DynamicScalar before unwrapping<br />
...<br />
RuntimeError: Command <code>git -C /Users/ivanzaitsev/pytorch2 cherry-pick -x 66ba5b8792fa076c4e512d920651e5b6b7e466f4</code> returned non-zero exit code 1<br />
```</p>
<ol>
<li>Workflow run:<br />
https://github.com/pytorch/pytorch/actions/runs/7660736172/job/20878651852?pr=118258</li>
</ol>
<p><img width="516" alt="image" src="https://github.com/pytorch/pytorch/assets/108101595/28fbf0d2-ac2a-4518-b41d-b32b41373747"><br />
<img width="621" alt="image" src="https://github.com/pytorch/pytorch/assets/108101595/ddbf8566-a417-43ec-9d0e-f623f4a71313"></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118258<br />
Approved by: https://github.com/PaliC, https://github.com/huydhn</p>]]></description>
      <pubDate>Thu, 25 Jan 2024 19:15:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b599f5608c309331df7a599c3b7f21a74375b2fa</guid>
    </item>
    <item>
      <title>[inductor][easy] dump triton kernel names in the log (#118313)</title>
      <link>https://github.com/pytorch/pytorch/commit/3cdd4e236e2c58deeb39b620b5aa1c2bf70ec602</link>
      <description><![CDATA[<p>[inductor][easy] dump triton kernel names in the log (#118313)</p>
<p>This may help debugging.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118313<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 25 Jan 2024 18:00:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3cdd4e236e2c58deeb39b620b5aa1c2bf70ec602</guid>
    </item>
    <item>
      <title>[inductor] Slightly faster memory allocation on CUDA (#118255)</title>
      <link>https://github.com/pytorch/pytorch/commit/2de24c11f66e0a11ccd392983f865e1b216281e6</link>
      <description><![CDATA[<p>[inductor] Slightly faster memory allocation on CUDA (#118255)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118255<br />
Approved by: https://github.com/peterbell10<br />
ghstack dependencies: #118065, #118070, #118171</p>]]></description>
      <pubDate>Thu, 25 Jan 2024 12:49:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2de24c11f66e0a11ccd392983f865e1b216281e6</guid>
    </item>
    <item>
      <title>[inductor] correctly generate grid info for benchmark_kernel (#118202)</title>
      <link>https://github.com/pytorch/pytorch/commit/1565d58ad909dfa1b058cbe97cf5c51088721a9f</link>
      <description><![CDATA[<p>[inductor] correctly generate grid info for benchmark_kernel (#118202)</p>
<p>Previously, we generated the grid argument with tree.numel for<br />
a benchmark TritonKernel. This was not correct, because it<br />
didn't match the launch config used for profiling and running.</p>
<p>This PR fixed the issue by emitting the grid value computed<br />
by the kernel's grid_fn, which is used by the profiler and<br />
the kernel's runner.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118202<br />
Approved by: https://github.com/shunting314, https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 25 Jan 2024 12:37:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1565d58ad909dfa1b058cbe97cf5c51088721a9f</guid>
    </item>
    <item>
      <title>[mypy] Enable follow_imports = normal for mypy-torch.backends.* (#116311)</title>
      <link>https://github.com/pytorch/pytorch/commit/1f6aa4b336a7aac756092759fc54eacc0b84fbba</link>
      <description><![CDATA[<p>[mypy] Enable follow_imports = normal for mypy-torch.backends.* (#116311)</p>
<p>Summary:</p>
<p>Test Plan:</p>
<p>```<br />
lintrunner --take MYPYINDUCTOR --all-files<br />
ok No lint issues.</p>
<p>lintrunner -a<br />
ok No lint issues.<br />
Successfully applied all patches.<br />
```</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116311<br />
Approved by: https://github.com/int3</p>]]></description>
      <pubDate>Thu, 25 Jan 2024 12:17:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1f6aa4b336a7aac756092759fc54eacc0b84fbba</guid>
    </item>
    <item>
      <title>[inductor] Slightly faster memory allocation on CPU (#118171)</title>
      <link>https://github.com/pytorch/pytorch/commit/817debeb8976adede8f781c312d85595a4dbb83c</link>
      <description><![CDATA[<p>[inductor] Slightly faster memory allocation on CPU (#118171)</p>
<p>Based on <code>python benchmarks/dynamo/microbenchmarks/overheads.py</code>:<br />
- Before <code>12.2us</code><br />
- After <code>10.5us</code></p>
<p>This is inspired by https://github.com/pytorch/pytorch/commit/a2c17a2b00f7c41866bbde28d33b8c50e5632e01 -- but in Python rather than C++</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118171<br />
Approved by: https://github.com/jgong5, https://github.com/peterbell10<br />
ghstack dependencies: #118065, #118070</p>]]></description>
      <pubDate>Thu, 25 Jan 2024 08:54:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/817debeb8976adede8f781c312d85595a4dbb83c</guid>
    </item>
    <item>
      <title>Fix failure of test_dynamo_distributed &amp; test_inductor_collectives (#117741)</title>
      <link>https://github.com/pytorch/pytorch/commit/b5b36cf0c4e1958f1ff25120f5d4beeef3288187</link>
      <description><![CDATA[<p>Fix failure of test_dynamo_distributed &amp; test_inductor_collectives (#117741)</p>
<p>When CUDA is not available <code>c10d.init_process_group("nccl"...)</code> will fail with</p>
<blockquote>
<p>RuntimeError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!</p>
</blockquote>
<p>Hence add a corresponding skip marker to the classes deriving from DynamoDistributedSingleProcTestCase next to the <code>requires_nccl</code> marker.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117741<br />
Approved by: https://github.com/ezyang, https://github.com/malfet</p>]]></description>
      <pubDate>Thu, 25 Jan 2024 05:25:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b5b36cf0c4e1958f1ff25120f5d4beeef3288187</guid>
    </item>
    <item>
      <title>Revert "Update triton ROCm version to 6.0" (#118179)</title>
      <link>https://github.com/pytorch/pytorch/commit/e6288820e3a2ae385a0ad51b412c43f384827787</link>
      <description><![CDATA[<p>Revert "Update triton ROCm version to 6.0" (#118179)</p>
<p>Reverting <a href="https://github.com/pytorch/pytorch/pull/117433">this commit</a> due to failures observed in wheel environment e.g:<br />
<code>ImportError: /tmp/torchinductor_root/triton/0/ebfa57c0b7b95873c96cad6f9bca148d/hip_utils.so: undefined symbol: hipGetDevicePropertiesR0600`</code></p>
<p>Will revert for now and investigate and aim to re-land this as part of https://github.com/pytorch/pytorch/pull/116270</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118179<br />
Approved by: https://github.com/jeffdaily, https://github.com/malfet</p>]]></description>
      <pubDate>Wed, 24 Jan 2024 14:01:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e6288820e3a2ae385a0ad51b412c43f384827787</guid>
    </item>
    <item>
      <title>[AOTI] Enable for MacOS (#118076)</title>
      <link>https://github.com/pytorch/pytorch/commit/bd991152767c1241e9a658cfd6a98e6efa028872</link>
      <description><![CDATA[<p>[AOTI] Enable for MacOS (#118076)</p>
<ul>
<li>Add <code>darwin</code> to the list of supported platform</li>
<li>Add <code>#include &lt;sstream&gt;</code> to <code>aoti_runtime/model.h</code></li>
<li>Refactor Linux specific constant compilation logic to <code>_compile_consts_linux</code></li>
<li>Add <code>_compile_consts_darwin</code> that converts consts to .S file that is linked into a shared library</li>
<li>Patch file using magic to avoid converting bytes to large hexadecimal string</li>
<li>Generate integer constants with <code>LL</code> suffix on MacOS (corresponds to int64_t definition)</li>
<li>Enable test_aot_inductor.py tests on MacOS</li>
</ul>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118076<br />
Approved by: https://github.com/desertfire<br />
ghstack dependencies: #118077</p>]]></description>
      <pubDate>Wed, 24 Jan 2024 06:24:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bd991152767c1241e9a658cfd6a98e6efa028872</guid>
    </item>
    <item>
      <title>[Inductor] Fix `argument unused during compilation` warning (#118077)</title>
      <link>https://github.com/pytorch/pytorch/commit/7d396918c6acc84ceb134d99844c09484de8694f</link>
      <description><![CDATA[<p>[Inductor] Fix <code>argument unused during compilation</code> warning (#118077)</p>
<p>By not passing linker flag if <code>compile_only</code> is set to <code>True</code><br />
Before that change every invocation of AOTI compiler resulted in emitting at least 4 warnings:<br />
<code>clang: warning: -lomp: 'linker' input unused [-Wunused-command-line-argument]
clang: warning: argument unused during compilation: '-shared' [-Wunused-command-line-argument]
clang: warning: argument unused during compilation: '-undefined dynamic_lookup' [-Wunused-command-line-argument]
clang: warning: argument unused during compilation: '-L/Users/nshulga/miniforge3/lib' [-Wunused-command-line-argument]</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118077<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Wed, 24 Jan 2024 01:52:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7d396918c6acc84ceb134d99844c09484de8694f</guid>
    </item>
    <item>
      <title>[inductor][fx] Fix broadcast_tensors with unbacked symints when translation validation is off (#118066)</title>
      <link>https://github.com/pytorch/pytorch/commit/21e8546b1180d16425c82dbececc87e31d544328</link>
      <description><![CDATA[<p>[inductor][fx] Fix broadcast_tensors with unbacked symints when translation validation is off (#118066)</p>
<h2>Context</h2>
<p>This is an example that runs into an AssertionError while lowering in Inductor.<br />
```</p>
<h1>While lowering, b will be expanded because b.size(1) == 1.</h1>
<p>a = torch.zeros([u0, 512])<br />
b = torch.ones([u0, 1])<br />
return a * b<br />
```</p>
<p>Below's the tail-end of the stack trace. Here's the important bits:<br />
1. In _inductor/sizevars.py, we'll call <code>self.shape_env.defer_runtime_assert(expr, msg, fx_node=V.graph.current_node)</code>.<br />
2. This leads to the creation of a <code>ShapeEnvEvent</code> with an FX node via <code>kwargs={"fx_node": V.graph.current_node}</code> (<a href="https://github.com/pytorch/pytorch/blob/0c9b5134701ab04552bc1724c5f148d22f98f20d/torch/fx/experimental/recording.py#L245-L247">see</a>).<br />
3. Eventually, we try to call <code>maybe_convert_node()</code> but it expects translation validation to be on (<a href="https://github.com/pytorch/pytorch/blob/0c9b5134701ab04552bc1724c5f148d22f98f20d/torch/fx/experimental/recording.py#L118-L121">see</a>).<br />
<code>File "pytorch/torch/_inductor/lowering.py", line 221, in transform_args
    for i, x in zip(indices, broadcast_tensors(*[args[i] for i in indices])):
  File "pytorch/torch/_inductor/lowering.py", line 294, in wrapped
    out = decomp_fn(*args, **kwargs)
  File "pytorch/torch/_inductor/lowering.py", line 676, in broadcast_tensors
    x = expand(x, target)
  File "pytorch/torch/_inductor/lowering.py", line 294, in wrapped
    out = decomp_fn(*args, **kwargs)
  File "pytorch/torch/_inductor/lowering.py", line 793, in expand
    return TensorBox(ExpandView.create(x.data, tuple(sizes)))
  File "pytorch/torch/_inductor/ir.py", line 1871, in create
    new_size = cls._normalize_size(x, new_size)
  File "pytorch/torch/_inductor/ir.py", line 1862, in _normalize_size
    new_size[i] = V.graph.sizevars.expect_equals(
  File "pytorch/torch/_inductor/sizevars.py", line 338, in expect_equals
    self.expect_true(sympy.Eq(left, right), msg=msg)
  File "pytorch/torch/_inductor/sizevars.py", line 333, in expect_true
    self.shape_env.defer_runtime_assert(expr, msg, fx_node=V.graph.current_node)  # (1) is here
  File "pytorch/torch/fx/experimental/recording.py", line 257, in wrapper
    return event.run(self)   # (2) happens right before this
  File "pytorch/torch/fx/experimental/recording.py", line 155, in run
    replacearg(index=3, key="fx_node", fn=maybe_convert_node)
  File "pytorch/torch/fx/experimental/recording.py", line 138, in replacearg
    kwargs[key] = fn(kwargs[key])
  File "pytorch/torch/fx/experimental/recording.py", line 128, in maybe_convert_node
    assert hasattr(shape_env, "name_to_node")  # (3) is here</code></p>
<h2>Approach</h2>
<p>Since <a href="https://github.com/pytorch/pytorch/blob/c6be5d55a56cc12b7a004acdb6a7da92ee2142f7/torch/fx/experimental/validator.py#L574">translation validation</a> may not be on during Inductor lowering, we can check if that's True and return the FX node's name in this case.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/118066<br />
Approved by: https://github.com/ezyang, https://github.com/peterbell10</p>]]></description>
      <pubDate>Tue, 23 Jan 2024 19:07:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/21e8546b1180d16425c82dbececc87e31d544328</guid>
    </item>
    <item>
      <title>Remove optimizer.step patching for profiler hook (#115772)</title>
      <link>https://github.com/pytorch/pytorch/commit/13d2cdffa29803c73cf6a4282894d5c4ee42cf1b</link>
      <description><![CDATA[<p>Remove optimizer.step patching for profiler hook (#115772)</p>
<ol>
<li>I'd like to remove the patching that avoids the profiler hook, but it adds an additional graph break due to nested wrappers. #117767 if interested, see (internal only) paste for <a href="P996529232">before</a> and <a href="P997507449">after</a> this PR.</li>
</ol>
<p><code>I've locally run perf benchmarks for yolov3: Before the speedup is 4.183x, and after it is 4.208x.
I've also run it for resnet50: before, speedup is 3.706x and now it is 3.924x.</code></p>
<ol>
<li>@mlazos I now unwrap twice in the dynamo and inductor tests. This feels like we're testing deficiently--should we add tests to test that tracing through the profiler hook and the use_grad hook are functioning according to expectations (I know there's at least one graph break in one).</li>
<li>There's a strange memory thing going on...what is happening? This has been resolved with @voznesenskym's <a href="https://github.com/pytorch/pytorch/pull/116169">change</a>. (for details see below)</li>
</ol>
<details>
This PR will fail the test_static_address_finalizer test due to a mysterious thing that is happening (idk what, but maybe the dynamo cache or a frame _expecting_ the patching to have been done).

There is no Python refcycle, as the backrefs for `p_ref()` look like:
![image](https://github.com/pytorch/pytorch/assets/31798555/4d6cbf50-3924-4efe-b578-d93389eebec8)
(so 5 backrefs but none of them python)

And the refs:
![image](https://github.com/pytorch/pytorch/assets/31798555/25e01105-bcb9-44ca-997a-2cf1670a6d42)
</details>

<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115772<br />
Approved by: https://github.com/jansel, https://github.com/mlazos</p>]]></description>
      <pubDate>Tue, 23 Jan 2024 12:15:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/13d2cdffa29803c73cf6a4282894d5c4ee42cf1b</guid>
    </item>
    <item>
      <title>[inductor] Allow reinplacing functionalized scatter ops (#116899)</title>
      <link>https://github.com/pytorch/pytorch/commit/3ec4f00316707687dae78219f0bff0545f053253</link>
      <description><![CDATA[<p>[inductor] Allow reinplacing functionalized scatter ops (#116899)</p>
<p>This expands the reinplacing pass to allow reinplacing view-scatter operations.<br />
e.g. if our python code is:<br />
<code>a = view1(inp)
b = view2(a)
b.copy_(src)</code><br />
this generates a functionalized graph like:<br />
<code>python
a = view1(inp)
a_updated = view2_scatter(a, src)
inp_updated = view1_scatter(inp, a_updated)</code></p>
<p>First, the <code>canonicalize_view_scatter_ops</code> step rewrites the functionalized graph<br />
in the form:<br />
<code>python
inp_updated = _generalized_scatter(inp, src, [view1, view2])
a_updated = view1(inp_updated)</code></p>
<p>I then register <code>_generalized_scatter</code> as a normal inplacable op which can be<br />
handled by the pre-existing mechanism. Since we've fused the two scatter ops into one,<br />
the reinplacing pass sees only one user of <code>inp</code> which allows the entire operation to be<br />
reinplaced  if desired (and I add heuristics that sometimes choose not to reinplace).</p>
<p>Finally, there is a decomposition step which decomposes out-of-place or in-place<br />
<code>_generalized_scatter</code> operations either back into view_scatter operations, or<br />
into the version with mutations. When introducing mutations, the reinplaced<br />
version is equivalent to the original mutation:<br />
<code>a = view1(inp)
b = view2(a)
b.copy_(src)</code></p>
<p>Or when out-of-place we end up with a minor restructuring of the graph:<br />
<code>a = view1(inp)
tmp = view2_scatter(a, src)
inp_updated = view1_scatter(inp, tmp)
a_updated = view1(inp_updated)</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116899<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #116898, #117121</p>]]></description>
      <pubDate>Tue, 23 Jan 2024 07:31:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3ec4f00316707687dae78219f0bff0545f053253</guid>
    </item>
    <item>
      <title>[inductor] Allow reinplacing before meta-only users (#117121)</title>
      <link>https://github.com/pytorch/pytorch/commit/5502a63b222635f1e65e4f934a410cc7552d5680</link>
      <description><![CDATA[<p>[inductor] Allow reinplacing before meta-only users (#117121)</p>
<p>Currently if you have the code:<br />
<code>python
idx = torch.arange(10, device=x.device)
src = torch.ones(10, dtype=x.dtype, device=x.device)
x.index_put_((idx,), src)
expand = x.expand((2, x.shape[0]))</code></p>
<p>The <code>index_put_</code> cannot be reinplaced under dynamic shapes due to the user<br />
<code>aten.sym_size(x, 0)</code> however since this function only looks at the tensor<br />
metadata, it is actually fine to reinplace.</p>
<p>Here I ignore these operators in the analysis of the reinplacing pass, so<br />
reinplacing can happen under dynamic shapes as well. I also handle cases<br />
where views are created just to be fed to <code>sym_size</code>.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117121<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #116898</p>]]></description>
      <pubDate>Tue, 23 Jan 2024 07:31:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5502a63b222635f1e65e4f934a410cc7552d5680</guid>
    </item>
    <item>
      <title>[inductor] Move reinplace pass to its own file (#116898)</title>
      <link>https://github.com/pytorch/pytorch/commit/eb0fcab421fa99855ebe4760d3384204c96728b5</link>
      <description><![CDATA[<p>[inductor] Move reinplace pass to its own file (#116898)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116898<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Tue, 23 Jan 2024 07:31:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/eb0fcab421fa99855ebe4760d3384204c96728b5</guid>
    </item>
    <item>
      <title>[AOTI] Add missing include to `model.h` (#118075)</title>
      <link>https://github.com/pytorch/pytorch/commit/bff348b28f905c96ceae5d11ae8ae713c63fc767</link>
      <description><![CDATA[<p>[AOTI] Add missing include to <code>model.h</code> (#118075)</p>
<p>At lest if one tries to compile the AOTI code on Darwin, compilation<br />
fails with implicit instantiation of undefined template error:<br />
<code>In file included from /Users/nshulga/git/pytorch/pytorch/torch/include/torch/csrc/inductor/aoti_runtime/arrayref_tensor.h:3:
/Users/nshulga/git/pytorch/pytorch/torch/include/torch/csrc/inductor/aoti_runtime/model.h:69:21: error: implicit instantiation of undefined template 'std::basic_stringstream&lt;char&gt;'
  std::stringstream ss;</code><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/118075<br />
Approved by: https://github.com/desertfire<br />
ghstack dependencies: #118074</p>]]></description>
      <pubDate>Tue, 23 Jan 2024 06:34:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bff348b28f905c96ceae5d11ae8ae713c63fc767</guid>
    </item>
    <item>
      <title>[Inductor] optimize transpose_mxn with bf16 data type (#117958)</title>
      <link>https://github.com/pytorch/pytorch/commit/4cfd16cb6da3c9cc7ba09d47456e850065dd5187</link>
      <description><![CDATA[<p>[Inductor] optimize transpose_mxn with bf16 data type (#117958)</p>
<p><strong>Summary</strong><br />
Add the vectorization implementation of <code>transpose_mxn</code> with BFloat16 data type when matrix size is 16X16 or 32X32 which observed in Stable Diffusion BF16.</p>
<p><strong>TestPlan</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_transpose_mxn_16_16_bf16_fp16
python -u -m pytest -s -v test_cpu_repro.py -k test_transpose_mxn_32_32_bf16_fp16</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117958<br />
Approved by: https://github.com/jgong5, https://github.com/lezcano</p>]]></description>
      <pubDate>Tue, 23 Jan 2024 01:43:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4cfd16cb6da3c9cc7ba09d47456e850065dd5187</guid>
    </item>
    <item>
      <title>[Inductor][Reliability] Add runtime numeric check for pt2 Optimus in the pre grad pass (#115142)</title>
      <link>https://github.com/pytorch/pytorch/commit/0036385b55795e727bd9ec4fdcb8f354f1fa5434</link>
      <description><![CDATA[<p>[Inductor][Reliability] Add runtime numeric check for pt2 Optimus in the pre grad pass (#115142)</p>
<p>Summary: Titled</p>
<p>Test Plan:</p>
<h1>local reproduce</h1>
<p>Patch ``icfg.fx_passes_numeric_check["pre_fx_passes"] = True"<br />
<code>buck2 run @mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split_batch</code><br />
P965217137</p>
<h1>MC candidates</h1>
<h3>FIRST + CMF</h3>
<p>f520754604<br />
P1056796962</p>
<h3>ICVR</h3>
<p>f520816217<br />
P1056839342</p>
<h3>IG_CTR</h3>
<p>f520819178<br />
P1056903302</p>
<h3>MAI</h3>
<p>f520823559<br />
P1057712009</p>
<h3>AFOC</h3>
<p>f520822438<br />
P1057760058</p>
<h3>DPA</h3>
<p>f520826815<br />
P1057808574</p>
<h3>How the runtime numeric check to catch <a href="https://docs.google.com/document/d/1WOtlbgCBbmU1klK1LiGSO0lYf_7mtSP4nAdvhQHM0JE/edit#heading=h.k61fy2rhaijp">SEVs</a></h3>
<p>bug fix diff: D51378532</p>
<h3>CMF+(FIRST)</h3>
<p>f509587388<br />
P1058305139<br />
by running the numeric check, we can catch the forward loss differences (e.g., diffing(https://www.internalfb.com/intern/diffing/?paste_number=1058293804))<br />
https://pxl.cl/4bQDG</p>
<p>f501760099<br />
P1058400691<br />
by running the numeric check, we can catch the forward loss differences (e.g., diffing(https://www.internalfb.com/intern/diffing/?paste_number=1058412054))<br />
https://pxl.cl/4bQMw</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115142<br />
Approved by: https://github.com/jackiexu1992, https://github.com/yanboliang</p>]]></description>
      <pubDate>Mon, 22 Jan 2024 19:56:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0036385b55795e727bd9ec4fdcb8f354f1fa5434</guid>
    </item>
    <item>
      <title>Update passrate calculation script to skip inductor and export (#118030)</title>
      <link>https://github.com/pytorch/pytorch/commit/dc1b9d758e27a6d155374f9fde646b803a259bdc</link>
      <description><![CDATA[<p>Update passrate calculation script to skip inductor and export (#118030)</p>
<p>We don't want to count running test/inductor/ and test/export/ with<br />
PYTORCH_TEST_WITH_DYNAMO=1 as a part of the pass rate.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/118030<br />
Approved by: https://github.com/ydwu4<br />
ghstack dependencies: #117998</p>]]></description>
      <pubDate>Mon, 22 Jan 2024 18:33:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dc1b9d758e27a6d155374f9fde646b803a259bdc</guid>
    </item>
    <item>
      <title>enable fp8 cast for inductor CPU (#117737)</title>
      <link>https://github.com/pytorch/pytorch/commit/d01ba4e94e6318e4eb99edd6f4347df9cf445345</link>
      <description><![CDATA[<p>enable fp8 cast for inductor CPU (#117737)</p>
<p>Enable FP8 cast for this issue https://github.com/pytorch/pytorch/issues/117119.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117737<br />
Approved by: https://github.com/jgong5, https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 22 Jan 2024 17:16:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d01ba4e94e6318e4eb99edd6f4347df9cf445345</guid>
    </item>
    <item>
      <title>[inductor] For View.create(x, sizes) call realize_input() instead of realize() when handling unbacked symints (#117013)</title>
      <link>https://github.com/pytorch/pytorch/commit/b90199935078b509c4a721893ea7d478878e7734</link>
      <description><![CDATA[<p>[inductor] For View.create(x, sizes) call realize_input() instead of realize() when handling unbacked symints (#117013)</p>
<h1>Context</h1>
<p>Let's say we do <code>View.create(x, sizes)</code> where <code>x</code> is a <code>SliceView</code> and <code>sizes</code> contains unbacked symints e.g. <code>sizes = [i14, 256]</code>. Then, this we'll run (<a href="https://github.com/pytorch/pytorch/blob/7e37f63e5e77a217a08d9261af49e63ddb51d306/torch/_inductor/ir.py#L2058-L2071">this code</a>) where we.<br />
1. Call <code>x.realize()</code> -- SliceView(Pointwise) -&gt; SliceView(ComputedBuffer).<br />
2. Retrieve storage &amp; layout via <code>as_storage_and_layout(x)</code><br />
3. Calculate <code>new_layout</code> based off layout &amp; <code>new_sizes</code><br />
3. <code>return ReinterpretView(storage, new_layout)</code><br />
However, (2) will raise <code>NotImplementedError</code> (<a href="https://github.com/pytorch/pytorch/blob/7e37f63e5e77a217a08d9261af49e63ddb51d306/torch/_inductor/ir.py#L1704-L1731">see</a>) since <code>x</code> is a <code>SliceView</code> and that isn't supported.</p>
<p>Thus, I tried adding support for <code>SliceView</code> in <code>as_storage_and_layout</code>. This worked for my case, but if instead <code>sizes</code> had backed symints e.g. <code>sizes=[s0, 256]</code> then some benchmarked models lost accuracy.<br />
<code>if isinstance(x, SliceView):
        return as_storage_and_layout(
            x.data,
            freeze=freeze,
            want_contiguous=want_contiguous,
            stride_order=stride_order,
        )</code></p>
<p>So instead of the above, I tried unwrapping the <code>SliceView</code> via <code>x = x.unwrap_view()</code>. This works for my usecase and passes CI but I'm not entirely sure why. If we unwrap our <code>SliceView</code> and create a <code>ReinterpretView</code>, I'd assume we'd lose the reindexer from <code>SliceView</code>. ~~But maybe we can re-create the same indexing from the <code>ReinterpretView</code>'s strides?~~ edit: we do lose vital information (like offset) when you release your <code>SliceView</code> and create a <code>ReinterpretView</code> so that's a no-go.</p>
<p>Moving onto the final version of this PR. We call <code>ExternKernel.realize_input()</code> (feels a bit weird to use <code>ExternKernel</code> but it's exactly what I need). It will go ahead and handle our <code>SliceView</code> case (<a href="https://github.com/pytorch/pytorch/blob/a468b9fbdf75b2da128e5fac4284ebbb981cd31a/torch/_inductor/ir.py#L3733-L3739">see</a>) by converting it to a <code>ReinterpretView</code> with the correct offset.</p>
<h1>Test</h1>
<p>```<br />
$ python test/inductor/test_unbacked_symints.py<br />
..</p>
<hr />
<p>Ran 10 tests in 20.813s</p>
<p>OK<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117013<br />
Approved by: https://github.com/jansel, https://github.com/ezyang</p>]]></description>
      <pubDate>Mon, 22 Jan 2024 14:34:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b90199935078b509c4a721893ea7d478878e7734</guid>
    </item>
    <item>
      <title>Replace `constraints` with `dynamic_shapes` in scripts/sijiac/prototypes and test/inductor (#117915)</title>
      <link>https://github.com/pytorch/pytorch/commit/b14d57cedaeb6cfb653190f7dcf73a6f8c1d82f4</link>
      <description><![CDATA[<p>Replace <code>constraints</code> with <code>dynamic_shapes</code> in scripts/sijiac/prototypes and test/inductor (#117915)</p>
<p>Summary: <code>constraints</code> argument for <code>torch.export</code> has been deprecated in favor of the <code>dynamic_shapes</code> argument. This PR updates the use of the deprecated API in <code>scripts/sijiac/prototypes</code> and <code>test/inductor</code>.</p>
<p>Test Plan: buck test mode/dev-nosan fbcode//caffe2/test/inductor:test_aot_inductor</p>
<p>Differential Revision: D52931743</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117915<br />
Approved by: https://github.com/angelayi</p>]]></description>
      <pubDate>Mon, 22 Jan 2024 13:24:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b14d57cedaeb6cfb653190f7dcf73a6f8c1d82f4</guid>
    </item>
    <item>
      <title>Rename unbacked SymInt prefix to u (#117859)</title>
      <link>https://github.com/pytorch/pytorch/commit/903e1913ff9156ade6838ddfd84d488729f21076</link>
      <description><![CDATA[<p>Rename unbacked SymInt prefix to u (#117859)</p>
<p>Currently, it conflicts with Inductor's naming convention for index<br />
variables</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117859<br />
Approved by: https://github.com/lezcano, https://github.com/jansel, https://github.com/avikchaudhuri</p>]]></description>
      <pubDate>Mon, 22 Jan 2024 12:53:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/903e1913ff9156ade6838ddfd84d488729f21076</guid>
    </item>
    <item>
      <title>[inductor][custom ops] Add tag to custom ops to preserve stride orders in inductor (#117298)</title>
      <link>https://github.com/pytorch/pytorch/commit/afabed6ae608fcf05eac62c1888ffd556572d8cf</link>
      <description><![CDATA[<p>[inductor][custom ops] Add tag to custom ops to preserve stride orders in inductor (#117298)</p>
<p>fixes #116715</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117298<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Sun, 21 Jan 2024 10:47:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/afabed6ae608fcf05eac62c1888ffd556572d8cf</guid>
    </item>
    <item>
      <title>[cpp_wrapper] Change CppWrapperCodeCache to use faster python binding (#117693)</title>
      <link>https://github.com/pytorch/pytorch/commit/41556324a9da7e3a8cf5fd75cb13766f5d2d8b22</link>
      <description><![CDATA[<p>[cpp_wrapper] Change CppWrapperCodeCache to use faster python binding (#117693)</p>
<p>Summary: Using faster binding following https://github.com/pytorch/pytorch/pull/117500. torch.utils.cpp_extension.load_inline builds a lot of things and is very slow. With this change, later we can further reduce the included header files using the ABI-compatible mode and thus further speed up the compilation.</p>
<p>Result:<br />
```<br />
python test/inductor/test_cuda_cpp_wrapper.py -k test_relu_cuda_cuda_wrapper</p>
<p>Before: Ran 1 test in 32.843s<br />
After: Ran 1 test in 26.229s<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117693<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Sun, 21 Jan 2024 08:07:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/41556324a9da7e3a8cf5fd75cb13766f5d2d8b22</guid>
    </item>
    <item>
      <title>[inductor] Fix CPP wrapper codegen for ExternKernel args (#117931)</title>
      <link>https://github.com/pytorch/pytorch/commit/fbd1d567ed788e5f9c2d0e3ed46c632682ab967e</link>
      <description><![CDATA[<p>[inductor] Fix CPP wrapper codegen for ExternKernel args (#117931)</p>
<p>Summary: We see IR nodes <code>repr</code>-ed directly in the CPP wrapper codegen. Recently, this issue has been fixed for the Python wrapper codegen in D52899373 (https://github.com/pytorch/pytorch/pull/117838). Here we extend the fix to CPP wrapper codegen / AOTInductor.</p>
<p>Test Plan:<br />
New unit tests. In OSS:</p>
<p><code>python test/inductor/test_aot_inductor.py -k test_triton_kernel_multi_output_arg</code></p>
<p><code>python test/inductor/test_aot_inductor.py -k test_triton_kernel_extern_kernel_arg</code></p>
<p>Differential Revision: D52936248</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117931<br />
Approved by: https://github.com/oulgen, https://github.com/chenyang78, https://github.com/desertfire</p>]]></description>
      <pubDate>Sat, 20 Jan 2024 20:58:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fbd1d567ed788e5f9c2d0e3ed46c632682ab967e</guid>
    </item>
    <item>
      <title>[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op hardswish (#117489)</title>
      <link>https://github.com/pytorch/pytorch/commit/bad5e1e0bb17452f26cf0527704a95892509e3ca</link>
      <description><![CDATA[<p>[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op hardswish (#117489)</p>
<p><strong>Summary</strong><br />
Enable the fusion pattern of <code>QConv2d -&gt; hardswish</code> lowering to <code>hardswish</code> as <code>QConv2d</code> post operator.</p>
<p><strong>Test Plan</strong><br />
<code>python -m pytest test_mkldnn_pattern_matcher.py -k test_qconv2d_hardswish_cpu
python -m pytest test_mkldnn_pattern_matcher.py -k test_qat_qconv2d_hardswish</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117489<br />
Approved by: https://github.com/leslie-fang-intel, https://github.com/jgong5<br />
ghstack dependencies: #117487, #117488</p>]]></description>
      <pubDate>Sat, 20 Jan 2024 16:01:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bad5e1e0bb17452f26cf0527704a95892509e3ca</guid>
    </item>
    <item>
      <title>Fix inductor pattern match error for qlinear with bmm (#117633)</title>
      <link>https://github.com/pytorch/pytorch/commit/4bf481fb1ba372f1ed977bb6a1092f3967811db0</link>
      <description><![CDATA[<p>Fix inductor pattern match error for qlinear with bmm (#117633)</p>
<p>Summary:</p>
<p>PR https://github.com/pytorch/pytorch/pull/116599 convert <code>bmm</code> when input dim exceeds 2 and not contiguous to <code>qlinear</code>. However, there is an error when check weight size because of not considering the permute op.</p>
<p>Test Plan:<br />
python test_mkldnn_pattern_matcher.py -k test_qlinear_input_dim_exceeds_2_and_not_contiguous</p>
<p>Fixes: -</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117633<br />
Approved by: https://github.com/jgong5, https://github.com/leslie-fang-intel</p>]]></description>
      <pubDate>Sat, 20 Jan 2024 04:26:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4bf481fb1ba372f1ed977bb6a1092f3967811db0</guid>
    </item>
    <item>
      <title>Revert "[inductor][custom ops] Add tag to custom ops to preserve stride orders in inductor (#117298)"</title>
      <link>https://github.com/pytorch/pytorch/commit/10923f87201c799c047e7e8d85dfc68f9b2cae89</link>
      <description><![CDATA[<p>Revert "[inductor][custom ops] Add tag to custom ops to preserve stride orders in inductor (#117298)"</p>
<p>This reverts commit 1967394690f144a7ba1717eccec977286cafe2da.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/117298 on behalf of https://github.com/huydhn due to Sorry for reverting you change but it is failing in MacOS https://hud.pytorch.org/pytorch/pytorch/commit/1967394690f144a7ba1717eccec977286cafe2da, may be due to a landrace (<a href="https://github.com/pytorch/pytorch/pull/117298#issuecomment-1901594120">comment</a>)</p>]]></description>
      <pubDate>Fri, 19 Jan 2024 18:14:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/10923f87201c799c047e7e8d85dfc68f9b2cae89</guid>
    </item>
    <item>
      <title>[Quant] [PT2] Add Hardswish into X86InductorQuantizer Conv2d Unary Annotation (#117488)</title>
      <link>https://github.com/pytorch/pytorch/commit/94f047257928d1f29c3cecb4eeb3e6abeb1320f6</link>
      <description><![CDATA[<p>[Quant] [PT2] Add Hardswish into X86InductorQuantizer Conv2d Unary Annotation (#117488)</p>
<p><strong>Summary</strong><br />
Add <code>hardswish</code>  into X86InductorQuantizer Conv2d Unary Annotation</p>
<p><strong>TestPlan</strong><br />
<code>python -m pytest test_x86inductor_quantizer.py -k test_conv2d_unary
python -m pytest test_x86inductor_quantizer.py -k test_qat_conv2d_unary</code><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/117488<br />
Approved by: https://github.com/leslie-fang-intel, https://github.com/jgong5<br />
ghstack dependencies: #117487</p>]]></description>
      <pubDate>Fri, 19 Jan 2024 17:37:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/94f047257928d1f29c3cecb4eeb3e6abeb1320f6</guid>
    </item>
    <item>
      <title>[inductor][custom ops] Add tag to custom ops to preserve stride orders in inductor (#117298)</title>
      <link>https://github.com/pytorch/pytorch/commit/1967394690f144a7ba1717eccec977286cafe2da</link>
      <description><![CDATA[<p>[inductor][custom ops] Add tag to custom ops to preserve stride orders in inductor (#117298)</p>
<p>fixes #116715</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117298<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Fri, 19 Jan 2024 17:37:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1967394690f144a7ba1717eccec977286cafe2da</guid>
    </item>
    <item>
      <title>[Inductor] Use codegen reference for buffer to string (#117838)</title>
      <link>https://github.com/pytorch/pytorch/commit/15d568d62142f207734c29a370d6e7ce0e7753de</link>
      <description><![CDATA[<p>[Inductor] Use codegen reference for buffer to string (#117838)</p>
<p>Summary: The added test case ends up emitting an inductor IR as the buffer string, lets properly emit the buffer name instead.</p>
<p>Test Plan: added new test</p>
<p>Differential Revision: D52899373</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117838<br />
Approved by: https://github.com/aakhundov</p>]]></description>
      <pubDate>Fri, 19 Jan 2024 12:18:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/15d568d62142f207734c29a370d6e7ce0e7753de</guid>
    </item>
    <item>
      <title>[inductor] multi-kernel support (#103469)</title>
      <link>https://github.com/pytorch/pytorch/commit/e432b2e607093a2e0e80fec1ed0ac63c516bffcd</link>
      <description><![CDATA[<p>[inductor] multi-kernel support (#103469)</p>
<p>For a persistent reduction, we generate 2 flavor of 'equivalant' kernels at the same time<br />
- persistent reduction<br />
- regular reduction</p>
<p>A MultiKernel wraps these 2 kernels and pick the one with better performance at runtime.</p>
<p>Here I talk more about implementation details:<br />
- Inductor maintains states for generating kernels. E.g. the wrapper code.  After we generate code for one kernel, we need restore the inductor state before we can generate the counterpart.</p>
<p><strong><em>There is one thing I need some comments from others</em></strong>:<br />
There is one tricky thing about kernel arguments. In general, inductor removes a buffer from the argument list if it's only used inside the kernel.  But somehow a buffer removed by persistent reduction kernel may still be kept by the regular (non-persistent) reduction kernel because of some CSE invalidation rule. My current implementation avoid removing buffers if multi_kernel is enabled. This makes sure both flavors of reduction has consistent argument list.  Another idea I have is, we generate the multi-kernel definition with the union of arguments from both sub-kernels. Let each sub-kernel pick the subset of arguments it wants. But this will make the code-gen or multi-kernel much complex.</p>
<p>I'm not sure if there is some easy and clean way to resolve this.</p>
<p>Testing command:<br />
```</p>
<p>TORCHINDUCTOR_MULTI_KERNEL=1 TORCH_LOGS=+torch._inductor.graph TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 python benchmarks/dynamo/huggingface.py --backend inductor --amp --performance --only BertForMaskedLM --training</p>
<p>```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/103469<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 18 Jan 2024 15:16:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e432b2e607093a2e0e80fec1ed0ac63c516bffcd</guid>
    </item>
    <item>
      <title>Revert "[inductor] allow mm template to accumulate with float16 dtype (#117479)"</title>
      <link>https://github.com/pytorch/pytorch/commit/def4959662a23bc307fa961a912683beb9dd60f7</link>
      <description><![CDATA[<p>Revert "[inductor] allow mm template to accumulate with float16 dtype (#117479)"</p>
<p>This reverts commit a7fbbc2a4a05fa4863f9d0e2adabcdc5e276c675.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/117479 on behalf of https://github.com/PaliC due to breaking tests internally (<a href="https://github.com/pytorch/pytorch/pull/117479#issuecomment-1899032973">comment</a>)</p>]]></description>
      <pubDate>Thu, 18 Jan 2024 10:53:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/def4959662a23bc307fa961a912683beb9dd60f7</guid>
    </item>
    <item>
      <title>Don't run inductor tests in Dynamo shard (#117747)</title>
      <link>https://github.com/pytorch/pytorch/commit/5aa895e53e56bb5fee5afb53fe5f32ea51d1dee9</link>
      <description><![CDATA[<p>Don't run inductor tests in Dynamo shard (#117747)</p>
<p>In theory we could, but these get really slow once we turn on strict<br />
mode, so we're not going to for now.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/117747<br />
Approved by: https://github.com/bdhirsh<br />
ghstack dependencies: #117729</p>]]></description>
      <pubDate>Thu, 18 Jan 2024 09:43:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5aa895e53e56bb5fee5afb53fe5f32ea51d1dee9</guid>
    </item>
    <item>
      <title>[inductor] Faster C++ kernel python bindings (#117500)</title>
      <link>https://github.com/pytorch/pytorch/commit/a669319450c2bbceaeb6036131bdef38e415e03c</link>
      <description><![CDATA[<p>[inductor] Faster C++ kernel python bindings (#117500)</p>
<p>Calling C++ from Python via ctypes is notoriously slow.  This switches to generating our own C++ bindings directly, which is a &gt;5x speedup on this kernel-launch-bound microbenchmark:<br />
```python<br />
from ctypes import c_void_p<br />
import torch<br />
from torch import empty<br />
from torch._inductor.codecache import AsyncCompile<br />
from torch._dynamo.testing import rand_strided<br />
from torch._inductor.utils import print_performance<br />
from torch._inductor.wrapper_benchmark import compiled_module_main</p>
<p>async_compile = AsyncCompile()</p>
<p>src = '''</p>
<h1>include "/tmp/torchinductor_jansel/gb/cgbau5vlj6cetmcjbjbtw6x4rrivaln6f45s5d72gy2bfx5foz3k.h"</h1>
<p>extern "C" void kernel(const float<em> in_ptr0,<br />
                       float</em> out_ptr0)<br />
{<br />
    {<br />
        auto tmp0 = in_ptr0[static_cast<long>(0L)];<br />
        auto tmp1 = static_cast<float>(1.0);<br />
        auto tmp2 = decltype(tmp0)(tmp0 + tmp1);<br />
        out_ptr0[static_cast<long>(0L)] = tmp2;<br />
    }<br />
}<br />
'''</p>
<p>cpp_fused_add_ctypes = async_compile.cpp(src)<br />
cpp_fused_add_cpython = async_compile.cpp_pybinding(["const float<em>", "float</em>"], src)</p>
<p>async_compile.wait(globals())<br />
del async_compile</p>
<p>def call(arg0_1):<br />
    buf0 = empty((1,), device='cpu', dtype=torch.float32)<br />
    if use_ctypes:<br />
        for _ in range(100):<br />
            cpp_fused_add_ctypes(c_void_p(arg0_1.data_ptr()), c_void_p(buf0.data_ptr()))<br />
    else:<br />
        for _ in range(100):<br />
            cpp_fused_add_cpython(arg0_1, buf0)<br />
    del arg0_1<br />
    return (buf0,)</p>
<p>def benchmark_compiled_module(times=1000, repeat=100):<br />
    arg0_1 = rand_strided((1,), (1,), device='cpu', dtype=torch.float32)<br />
    return print_performance(lambda: call(arg0_1), times=times, repeat=repeat)</p>
<p>print("old ctypes bindings: ", end='')<br />
use_ctypes = True<br />
compiled_module_main('None', benchmark_compiled_module)<br />
print("new bindings:        ", end='')<br />
use_ctypes = False<br />
compiled_module_main('None', benchmark_compiled_module)<br />
<code>Output:</code><br />
old ctypes bindings: 0.000073<br />
new bindings:        0.000013<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117500<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 18 Jan 2024 08:20:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a669319450c2bbceaeb6036131bdef38e415e03c</guid>
    </item>
    <item>
      <title>[AOTI] Add torch._export.aot_load (#117610)</title>
      <link>https://github.com/pytorch/pytorch/commit/26956980c6953b42e57ad889b87444c4abccec1c</link>
      <description><![CDATA[<p>[AOTI] Add torch._export.aot_load (#117610)</p>
<p>Summary: Add a torch._export.aot_load API that can load an AOTInductor-compiled model.so into a python executable.</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D52825456</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117610<br />
Approved by: https://github.com/angelayi, https://github.com/khabinov, https://github.com/chenyang78</p>]]></description>
      <pubDate>Thu, 18 Jan 2024 07:02:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/26956980c6953b42e57ad889b87444c4abccec1c</guid>
    </item>
    <item>
      <title>[Inductor Intel GPU backend Upstream] Generalize part of Inductor test case (#117513)</title>
      <link>https://github.com/pytorch/pytorch/commit/e60bc502b4d9cffcd00ed70d03117e45b672d6f8</link>
      <description><![CDATA[<p>[Inductor Intel GPU backend Upstream] Generalize part of Inductor test case (#117513)</p>
<p>Following the RFC https://github.com/pytorch/pytorch/issues/114856, before upstream Intel XPU Inductor Backend, we need to preapre corresponding Inductor test cases. This PR aims to generalize part of Inductor test case so that a new GPU backend can reuse the existing test case with minimal code change.</p>
<p>This Pull Request preferentially generalizes the test cases that cover Inductor's base functionality as follow:<br />
- test/inductor/test_codecache.py<br />
- test/inductor/test_codegen_triton.py<br />
- test/inductor/test_kernel_benchmark.py<br />
- test/inductor/test_torchinductor.py<br />
- test/inductor/test_torchinductor_codegen_dynamic_shapes.py<br />
- test/inductor/test_torchinductor_dynamic_shapes.py<br />
- test/inductor/test_torchinductor_opinfo.py<br />
- test/inductor/test_triton_heuristics.py<br />
- test/inductor/test_triton_wrapper.py</p>
<p>Feature request: https://github.com/pytorch/pytorch/issues/114856</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117513<br />
Approved by: https://github.com/EikanWang, https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 18 Jan 2024 00:26:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e60bc502b4d9cffcd00ed70d03117e45b672d6f8</guid>
    </item>
    <item>
      <title>Use wait stream instead of synchronize() in cudagraph warmup (#117578)</title>
      <link>https://github.com/pytorch/pytorch/commit/41153542ae92ee10fbb43d1e68215d9b3f7e951a</link>
      <description><![CDATA[<p>Use wait stream instead of synchronize() in cudagraph warmup (#117578)</p>
<p>Fix for https://github.com/pytorch/pytorch/issues/113895</p>
<p>There are three phases to cudagraph trees. Warmup, recording, and execution. On recording and execution we are executing under the current_stream. In warmup we execute under a side stream that we also use for cudagraph recording so as to reuse memory.</p>
<p>After we execute on the side stream we need to sync the current stream to the side stream. Previously there was a <code>torch.cuda.synchronize</code> but not a <code>torch.cuda.current_stream().wait_stream(stream)</code>. This PR removes the global sync and adds a wait_stream. I have confirmed that it fixes https://github.com/pytorch/pytorch/issues/113895.</p>
<p>It's not entirely clear me why torch.cuda.synchronize would be insufficient - I would have thought the global sync would encompass the stream to stream sync. However, we do have a number of <a href="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/compile_fx.py#L748-L749">instances</a> throughout the code base where we do a stream-&gt;stream sync after the global sync so clearly I am missing something here. In any case the stream-&gt;stream sync is better perf than a global synchronize.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117578<br />
Approved by: https://github.com/zdevito</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 19:33:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/41153542ae92ee10fbb43d1e68215d9b3f7e951a</guid>
    </item>
    <item>
      <title>Document and type torch._inductor.virtualized (#117658)</title>
      <link>https://github.com/pytorch/pytorch/commit/634ce3c913952d44fc9e7e3dcd9ef3377ace471f</link>
      <description><![CDATA[<p>Document and type torch._inductor.virtualized (#117658)</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117658<br />
Approved by: https://github.com/eellison, https://github.com/peterbell10<br />
ghstack dependencies: #117650</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 19:03:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/634ce3c913952d44fc9e7e3dcd9ef3377ace471f</guid>
    </item>
    <item>
      <title>Revert "[inductor] Faster C++ kernel python bindings (#117500)"</title>
      <link>https://github.com/pytorch/pytorch/commit/a1afd1b195f7d46c0d203c5f63b871405b607e21</link>
      <description><![CDATA[<p>Revert "[inductor] Faster C++ kernel python bindings (#117500)"</p>
<p>It should have never been landed, but was landed again, thanks to<br />
ghstack grafting/ungrafting see discussion on https://github.com/pytorch/pytorch/pull/116910</p>
<p>This reverts commit e457b6fb18782425661e8a09d0222d0b29518ad1.</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 17:06:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a1afd1b195f7d46c0d203c5f63b871405b607e21</guid>
    </item>
    <item>
      <title>Replace `constraints` with `dynamic_shapes` in deeplearning/aot_inductor test (#117573)</title>
      <link>https://github.com/pytorch/pytorch/commit/31148133144dbaa9b7c75b0618ef87a4093041dd</link>
      <description><![CDATA[<p>Replace <code>constraints</code> with <code>dynamic_shapes</code> in deeplearning/aot_inductor test (#117573)</p>
<p>Summary: <code>constraints</code> argument for <code>torch.export</code> has been deprecated in favor of the <code>dynamic_shapes</code> argument. This PR updates the use of the deprecated API in <code>deeplearning/aot_inductor/test/test_custom_ops.py</code>.</p>
<p>Test Plan: buck test mode/dev-nosan fbcode//deeplearning/aot_inductor/test:test_custom_ops -- test_export_extern_fallback_nodes_dynamic_shape</p>
<p>Differential Revision: D52790332</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117573<br />
Approved by: https://github.com/angelayi</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 15:50:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/31148133144dbaa9b7c75b0618ef87a4093041dd</guid>
    </item>
    <item>
      <title>[inductor] Faster C++ kernel python bindings (#117500)</title>
      <link>https://github.com/pytorch/pytorch/commit/e457b6fb18782425661e8a09d0222d0b29518ad1</link>
      <description><![CDATA[<p>[inductor] Faster C++ kernel python bindings (#117500)</p>
<p>Calling C++ from Python via ctypes is notoriously slow.  This switches to generating our own C++ bindings directly, which is a &gt;5x speedup on this kernel-launch-bound microbenchmark:<br />
```python<br />
from ctypes import c_void_p<br />
import torch<br />
from torch import empty<br />
from torch._inductor.codecache import AsyncCompile<br />
from torch._dynamo.testing import rand_strided<br />
from torch._inductor.utils import print_performance<br />
from torch._inductor.wrapper_benchmark import compiled_module_main</p>
<p>async_compile = AsyncCompile()</p>
<p>src = '''</p>
<h1>include "/tmp/torchinductor_jansel/gb/cgbau5vlj6cetmcjbjbtw6x4rrivaln6f45s5d72gy2bfx5foz3k.h"</h1>
<p>extern "C" void kernel(const float<em> in_ptr0,<br />
                       float</em> out_ptr0)<br />
{<br />
    {<br />
        auto tmp0 = in_ptr0[static_cast<long>(0L)];<br />
        auto tmp1 = static_cast<float>(1.0);<br />
        auto tmp2 = decltype(tmp0)(tmp0 + tmp1);<br />
        out_ptr0[static_cast<long>(0L)] = tmp2;<br />
    }<br />
}<br />
'''</p>
<p>cpp_fused_add_ctypes = async_compile.cpp(src)<br />
cpp_fused_add_cpython = async_compile.cpp_pybinding(["const float<em>", "float</em>"], src)</p>
<p>async_compile.wait(globals())<br />
del async_compile</p>
<p>def call(arg0_1):<br />
    buf0 = empty((1,), device='cpu', dtype=torch.float32)<br />
    if use_ctypes:<br />
        for _ in range(100):<br />
            cpp_fused_add_ctypes(c_void_p(arg0_1.data_ptr()), c_void_p(buf0.data_ptr()))<br />
    else:<br />
        for _ in range(100):<br />
            cpp_fused_add_cpython(arg0_1, buf0)<br />
    del arg0_1<br />
    return (buf0,)</p>
<p>def benchmark_compiled_module(times=1000, repeat=100):<br />
    arg0_1 = rand_strided((1,), (1,), device='cpu', dtype=torch.float32)<br />
    return print_performance(lambda: call(arg0_1), times=times, repeat=repeat)</p>
<p>print("old ctypes bindings: ", end='')<br />
use_ctypes = True<br />
compiled_module_main('None', benchmark_compiled_module)<br />
print("new bindings:        ", end='')<br />
use_ctypes = False<br />
compiled_module_main('None', benchmark_compiled_module)<br />
<code>Output:</code><br />
old ctypes bindings: 0.000073<br />
new bindings:        0.000013<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117500<br />
Approved by: https://github.com/desertfire<br />
ghstack dependencies: #117409, #116667, #117591</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 15:03:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e457b6fb18782425661e8a09d0222d0b29518ad1</guid>
    </item>
    <item>
      <title>[inductor] allow mm template to accumulate with float16 dtype (#117479)</title>
      <link>https://github.com/pytorch/pytorch/commit/a7fbbc2a4a05fa4863f9d0e2adabcdc5e276c675</link>
      <description><![CDATA[<p>[inductor] allow mm template to accumulate with float16 dtype (#117479)</p>
<p>Fixes #108621</p>
<p>replace #108637 and #108982</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117479<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 13:01:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a7fbbc2a4a05fa4863f9d0e2adabcdc5e276c675</guid>
    </item>
    <item>
      <title>Revert "[inductor] Faster C++ kernel python bindings (#117500)"</title>
      <link>https://github.com/pytorch/pytorch/commit/da6abaeeacedc872ada6577fa1dd0c6c8024188a</link>
      <description><![CDATA[<p>Revert "[inductor] Faster C++ kernel python bindings (#117500)"</p>
<p>This reverts commit bb0fd1bd3ca145b77159427bc5bacf5f98ec3896.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/117500 on behalf of https://github.com/PaliC due to breaking internal discussed with author offline (<a href="https://github.com/pytorch/pytorch/pull/117500#issuecomment-1896516512">comment</a>)</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 11:34:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/da6abaeeacedc872ada6577fa1dd0c6c8024188a</guid>
    </item>
    <item>
      <title>[AOTInductor] Allow user to explicitly specify Device to run on (#117413)</title>
      <link>https://github.com/pytorch/pytorch/commit/89cf1ddb5c96423bb5f6bbe0b5c5a6d3e432cbed</link>
      <description><![CDATA[<p>[AOTInductor] Allow user to explicitly specify Device to run on (#117413)</p>
<p>Summary:<br />
AOTInductor currently infer cuda device index by <code>cudaGetDevice()</code>. This assumes outer runtime calls <code>cudaSetDevice()</code> somewhere, before invoking AOTInductor run.</p>
<p>This diff adds an explicit argument for specifying target Device. e.g. compiled on "cuda:0", run on "cuda:1".</p>
<p>todo:<br />
- Are the changes in interface.h BC breaking? as it changes the function signatures in .so file. Might just need introduce a new "Create" function.</p>
<p>Test Plan: CI</p>
<p>Differential Revision:<br />
D52747132</p>
<p>Privacy Context Container: 368960445142440</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117413<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire, https://github.com/khabinov</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 11:28:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/89cf1ddb5c96423bb5f6bbe0b5c5a6d3e432cbed</guid>
    </item>
    <item>
      <title>Add inductor-specific testing strict mode denylist (#117553)</title>
      <link>https://github.com/pytorch/pytorch/commit/ca0abf8606555bd17f159369da13838894adbaee</link>
      <description><![CDATA[<p>Add inductor-specific testing strict mode denylist (#117553)</p>
<p>We have one for Dynamo that currently applies to all "compile"<br />
configurations (PYTORCH_TEST_WITH_DYNAMO, PYTORCH_TEST_WITH_INDUCTOR). I<br />
don't want to figure out the inductor situation right now, so we're<br />
going to add another denylist for inductor and work through it later.</p>
<p>Test Plan:<br />
- existing tests<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/117553<br />
Approved by: https://github.com/voznesenskym<br />
ghstack dependencies: #117409, #116667, #117591, #117500, #116910</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 11:12:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ca0abf8606555bd17f159369da13838894adbaee</guid>
    </item>
    <item>
      <title>[inductor] Faster C++ kernel python bindings (#117500)</title>
      <link>https://github.com/pytorch/pytorch/commit/bb0fd1bd3ca145b77159427bc5bacf5f98ec3896</link>
      <description><![CDATA[<p>[inductor] Faster C++ kernel python bindings (#117500)</p>
<p>Calling C++ from Python via ctypes is notoriously slow.  This switches to generating our own C++ bindings directly, which is a &gt;5x speedup on this kernel-launch-bound microbenchmark:<br />
```python<br />
from ctypes import c_void_p<br />
import torch<br />
from torch import empty<br />
from torch._inductor.codecache import AsyncCompile<br />
from torch._dynamo.testing import rand_strided<br />
from torch._inductor.utils import print_performance<br />
from torch._inductor.wrapper_benchmark import compiled_module_main</p>
<p>async_compile = AsyncCompile()</p>
<p>src = '''</p>
<h1>include "/tmp/torchinductor_jansel/gb/cgbau5vlj6cetmcjbjbtw6x4rrivaln6f45s5d72gy2bfx5foz3k.h"</h1>
<p>extern "C" void kernel(const float<em> in_ptr0,<br />
                       float</em> out_ptr0)<br />
{<br />
    {<br />
        auto tmp0 = in_ptr0[static_cast<long>(0L)];<br />
        auto tmp1 = static_cast<float>(1.0);<br />
        auto tmp2 = decltype(tmp0)(tmp0 + tmp1);<br />
        out_ptr0[static_cast<long>(0L)] = tmp2;<br />
    }<br />
}<br />
'''</p>
<p>cpp_fused_add_ctypes = async_compile.cpp(src)<br />
cpp_fused_add_cpython = async_compile.cpp_pybinding(["const float<em>", "float</em>"], src)</p>
<p>async_compile.wait(globals())<br />
del async_compile</p>
<p>def call(arg0_1):<br />
    buf0 = empty((1,), device='cpu', dtype=torch.float32)<br />
    if use_ctypes:<br />
        for _ in range(100):<br />
            cpp_fused_add_ctypes(c_void_p(arg0_1.data_ptr()), c_void_p(buf0.data_ptr()))<br />
    else:<br />
        for _ in range(100):<br />
            cpp_fused_add_cpython(arg0_1, buf0)<br />
    del arg0_1<br />
    return (buf0,)</p>
<p>def benchmark_compiled_module(times=1000, repeat=100):<br />
    arg0_1 = rand_strided((1,), (1,), device='cpu', dtype=torch.float32)<br />
    return print_performance(lambda: call(arg0_1), times=times, repeat=repeat)</p>
<p>print("old ctypes bindings: ", end='')<br />
use_ctypes = True<br />
compiled_module_main('None', benchmark_compiled_module)<br />
print("new bindings:        ", end='')<br />
use_ctypes = False<br />
compiled_module_main('None', benchmark_compiled_module)<br />
<code>Output:</code><br />
old ctypes bindings: 0.000073<br />
new bindings:        0.000013<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117500<br />
Approved by: https://github.com/desertfire<br />
ghstack dependencies: #117409, #116667, #117591</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 11:12:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bb0fd1bd3ca145b77159427bc5bacf5f98ec3896</guid>
    </item>
    <item>
      <title>Revert "[inductor] Faster C++ kernel python bindings (#117500)"</title>
      <link>https://github.com/pytorch/pytorch/commit/9da01affd3a98d5aeab90a283883ae5da63a1743</link>
      <description><![CDATA[<p>Revert "[inductor] Faster C++ kernel python bindings (#117500)"</p>
<p>This reverts commit 3a52147cc59b240737602d3d046080bbf6f567f1.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/117500 on behalf of https://github.com/PaliC due to breaking internal discussed with author offline (<a href="https://github.com/pytorch/pytorch/pull/117500#issuecomment-1896426304">comment</a>)</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 10:42:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9da01affd3a98d5aeab90a283883ae5da63a1743</guid>
    </item>
    <item>
      <title>Revert "Add inductor-specific testing strict mode denylist (#117553)"</title>
      <link>https://github.com/pytorch/pytorch/commit/e877c2e6ffdb9db72ee40ac61de3de8da86a84b7</link>
      <description><![CDATA[<p>Revert "Add inductor-specific testing strict mode denylist (#117553)"</p>
<p>This reverts commit ab6207a34248fdf2d2766d0062f358b63380e151.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/117553 on behalf of https://github.com/PaliC due to breaking internal discussed with author offline (<a href="https://github.com/pytorch/pytorch/pull/117500#issuecomment-1896426304">comment</a>)</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 10:42:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e877c2e6ffdb9db72ee40ac61de3de8da86a84b7</guid>
    </item>
    <item>
      <title>Add inductor-specific testing strict mode denylist (#117553)</title>
      <link>https://github.com/pytorch/pytorch/commit/ab6207a34248fdf2d2766d0062f358b63380e151</link>
      <description><![CDATA[<p>Add inductor-specific testing strict mode denylist (#117553)</p>
<p>We have one for Dynamo that currently applies to all "compile"<br />
configurations (PYTORCH_TEST_WITH_DYNAMO, PYTORCH_TEST_WITH_INDUCTOR). I<br />
don't want to figure out the inductor situation right now, so we're<br />
going to add another denylist for inductor and work through it later.</p>
<p>Test Plan:<br />
- existing tests<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/117553<br />
Approved by: https://github.com/voznesenskym</p>]]></description>
      <pubDate>Tue, 16 Jan 2024 15:04:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ab6207a34248fdf2d2766d0062f358b63380e151</guid>
    </item>
    <item>
      <title>[inductor] Faster C++ kernel python bindings (#117500)</title>
      <link>https://github.com/pytorch/pytorch/commit/3a52147cc59b240737602d3d046080bbf6f567f1</link>
      <description><![CDATA[<p>[inductor] Faster C++ kernel python bindings (#117500)</p>
<p>Calling C++ from Python via ctypes is notoriously slow.  This switches to generating our own C++ bindings directly, which is a &gt;5x speedup on this kernel-launch-bound microbenchmark:<br />
```python<br />
from ctypes import c_void_p<br />
import torch<br />
from torch import empty<br />
from torch._inductor.codecache import AsyncCompile<br />
from torch._dynamo.testing import rand_strided<br />
from torch._inductor.utils import print_performance<br />
from torch._inductor.wrapper_benchmark import compiled_module_main</p>
<p>async_compile = AsyncCompile()</p>
<p>src = '''</p>
<h1>include "/tmp/torchinductor_jansel/gb/cgbau5vlj6cetmcjbjbtw6x4rrivaln6f45s5d72gy2bfx5foz3k.h"</h1>
<p>extern "C" void kernel(const float<em> in_ptr0,<br />
                       float</em> out_ptr0)<br />
{<br />
    {<br />
        auto tmp0 = in_ptr0[static_cast<long>(0L)];<br />
        auto tmp1 = static_cast<float>(1.0);<br />
        auto tmp2 = decltype(tmp0)(tmp0 + tmp1);<br />
        out_ptr0[static_cast<long>(0L)] = tmp2;<br />
    }<br />
}<br />
'''</p>
<p>cpp_fused_add_ctypes = async_compile.cpp(src)<br />
cpp_fused_add_cpython = async_compile.cpp_pybinding(["const float<em>", "float</em>"], src)</p>
<p>async_compile.wait(globals())<br />
del async_compile</p>
<p>def call(arg0_1):<br />
    buf0 = empty((1,), device='cpu', dtype=torch.float32)<br />
    if use_ctypes:<br />
        for _ in range(100):<br />
            cpp_fused_add_ctypes(c_void_p(arg0_1.data_ptr()), c_void_p(buf0.data_ptr()))<br />
    else:<br />
        for _ in range(100):<br />
            cpp_fused_add_cpython(arg0_1, buf0)<br />
    del arg0_1<br />
    return (buf0,)</p>
<p>def benchmark_compiled_module(times=1000, repeat=100):<br />
    arg0_1 = rand_strided((1,), (1,), device='cpu', dtype=torch.float32)<br />
    return print_performance(lambda: call(arg0_1), times=times, repeat=repeat)</p>
<p>print("old ctypes bindings: ", end='')<br />
use_ctypes = True<br />
compiled_module_main('None', benchmark_compiled_module)<br />
print("new bindings:        ", end='')<br />
use_ctypes = False<br />
compiled_module_main('None', benchmark_compiled_module)<br />
<code>Output:</code><br />
old ctypes bindings: 0.000073<br />
new bindings:        0.000013<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117500<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 16 Jan 2024 14:30:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3a52147cc59b240737602d3d046080bbf6f567f1</guid>
    </item>
    <item>
      <title>[ROCm] Fix NHWC related tests in test_inductor_freezing (#117158)</title>
      <link>https://github.com/pytorch/pytorch/commit/2a3fb7dbb6375369f30487426fdbe8fefd919791</link>
      <description><![CDATA[<p>[ROCm] Fix NHWC related tests in test_inductor_freezing (#117158)</p>
<p>Fixes #ISSUE_NUMBER</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117158<br />
Approved by: https://github.com/eellison, https://github.com/pruthvistony</p>]]></description>
      <pubDate>Tue, 16 Jan 2024 12:48:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2a3fb7dbb6375369f30487426fdbe8fefd919791</guid>
    </item>
    <item>
      <title>[inductor] add C-shim for index_put (#116667)</title>
      <link>https://github.com/pytorch/pytorch/commit/4712c7dac802917e74c2ad6c5bb94924220e59e2</link>
      <description><![CDATA[<p>[inductor] add C-shim for index_put (#116667)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116667<br />
Approved by: https://github.com/desertfire, https://github.com/chenyang78</p>]]></description>
      <pubDate>Tue, 16 Jan 2024 12:29:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4712c7dac802917e74c2ad6c5bb94924220e59e2</guid>
    </item>
    <item>
      <title>[fx][inductor] Add statically_known_true utility for SymBool (#117359)</title>
      <link>https://github.com/pytorch/pytorch/commit/001585f4464be3aa6c41568b4a31a08d6562fd49</link>
      <description><![CDATA[<p>[fx][inductor] Add statically_known_true utility for SymBool (#117359)</p>
<p>This adds a function <code>statically_known_true</code> for <code>SymBool</code> that works<br />
like inductor's <code>is_expr_static_and_true</code>. That is, it tries to simplify the<br />
expression to a constant or returns <code>False</code> if it cannot be simplified.</p>
<p>This is useful in cases that can be optimized if the condition is met,<br />
otherwise it doesn't effect correctness so we can avoid adding guards.</p>
<p>I also use this new function in inductor for <code>FakeTensorUpdater</code> and<br />
<code>remove_noop_pass</code> which both generated unexpected guards previously.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117359<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Mon, 15 Jan 2024 10:01:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/001585f4464be3aa6c41568b4a31a08d6562fd49</guid>
    </item>
    <item>
      <title>[inductor] Handle more edge cases in slice and slice_scatter (#117377)</title>
      <link>https://github.com/pytorch/pytorch/commit/7a8013fbfa43f7bdf850b13c350febd7419aed2e</link>
      <description><![CDATA[<p>[inductor] Handle more edge cases in slice and slice_scatter (#117377)</p>
<p>Fixes #117110</p>
<p>When slicing we can end up with start and end which are out of bounds, which is<br />
handled in python slicing by clamping to the correct bounds. There is also the<br />
case where end &lt; start which should result in an empty slice.</p>
<p>In the isoneutral_mixing failure we have the second case, with <code>start=2, end=0</code><br />
which in <code>slice_scatter</code> became <code>src_size[dim] = -2</code>.</p>
<p>This PR improves slice's edge case handling and factors the start and end<br />
normalization code out so it can be shared with slice_scatter.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117377<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Mon, 15 Jan 2024 09:05:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7a8013fbfa43f7bdf850b13c350febd7419aed2e</guid>
    </item>
    <item>
      <title>Properly preserve SymInt input invariant when splitting graphs (#117406)</title>
      <link>https://github.com/pytorch/pytorch/commit/5c700f60a5ee588d32975fc0acd80dfc06e14425</link>
      <description><![CDATA[<p>Properly preserve SymInt input invariant when splitting graphs (#117406)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/111636<br />
Fixes https://github.com/pytorch/pytorch/issues/108877<br />
Fixes https://github.com/pytorch/pytorch/issues/116956</p>
<p>Inductor has an invariant that every dynamic shape symbol s0, s1, etc. which is referenced by an input tensor must also be passed in explicitly as an argument. It has some capability of reverse engineering symbols if it's obvious how to get them (e.g., if you pass in <code>arg: f32[s0, 4]</code> it will know that it can retrieve <code>s0 = arg.size(0)</code>) but in full generality it is not always possible to derive this (e.g., if the only mention of s0 is in <code>arg2: f32[s0 + s1, 4]</code>).  However, the graph splitter used by optimize_ddp did not respect this invariant. This PR makes it respect it.</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117406<br />
Approved by: https://github.com/wconstab</p>]]></description>
      <pubDate>Mon, 15 Jan 2024 07:04:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5c700f60a5ee588d32975fc0acd80dfc06e14425</guid>
    </item>
    <item>
      <title>[inductor][cpp] apply simplify_index_in_vec_range to vector store and vector transpose (#117263)</title>
      <link>https://github.com/pytorch/pytorch/commit/2b56d80460aa0056e111150d7ff5a7e6faf0abd9</link>
      <description><![CDATA[<p>[inductor][cpp] apply simplify_index_in_vec_range to vector store and vector transpose (#117263)</p>
<p>As the title, this PR extends the <code>simplify_index_in_vec_range</code> to store and transpose.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117263<br />
Approved by: https://github.com/jansel<br />
ghstack dependencies: #117221, #117260</p>]]></description>
      <pubDate>Mon, 15 Jan 2024 00:41:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2b56d80460aa0056e111150d7ff5a7e6faf0abd9</guid>
    </item>
    <item>
      <title>[inductor][cpp] apply simplify_index_in_vec_range in select_tiling_indices to enable more contiguous vec load (#117260)</title>
      <link>https://github.com/pytorch/pytorch/commit/3b00dd5843346dd861404729dedc9f46cccd3bbd</link>
      <description><![CDATA[<p>[inductor][cpp] apply simplify_index_in_vec_range in select_tiling_indices to enable more contiguous vec load (#117260)</p>
<p>For the one of the kernels in the UT <code>test_vec_contiguous_ModularIndexing</code>:<br />
Before:<br />
<code>c++
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(28L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(16L); x1+=static_cast&lt;long&gt;(16L))
                {
                    {
                        #pragma omp declare reduction(welford:Welford&lt;float&gt;:omp_out = welford_combine(omp_out, omp_in)) initializer(omp_priv={Welford&lt;float&gt;()})
                        #pragma omp declare reduction(welford:Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;:omp_out = welford_combine(omp_out, omp_in)) initializer(omp_priv={Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;()})
                        Welford&lt;float&gt; tmp_acc0 = Welford&lt;float&gt;();
                        Welford&lt;at::vec::Vectorized&lt;float&gt;&gt; tmp_acc0_vec = Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;();
                        for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(512L); x2+=static_cast&lt;long&gt;(1L))
                        {
                            auto tmp0 =
                            [&amp;]
                            {
                                __at_align__ std::array&lt;float, 16&gt; tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner &lt; 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr0[static_cast&lt;long&gt;((128L*(c10::div_floor_integer(x2, 256L))) + (256L*x1) + (256L*x1_inner) + (7168L*(static_cast&lt;long&gt;(c10::div_floor_integer(x2, 128L)) % static_cast&lt;long&gt;(2L))) + (14336L*x0) + (static_cast&lt;long&gt;(x2) % static_cast&lt;long&gt;(128L)))];
                                }
                                return at::vec::Vectorized&lt;float&gt;::loadu(tmpbuf.data());
                            }
                            ()
                            ;
                            tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp0);
                        }
                        tmp_acc0_vec.mean.store(out_ptr0 + static_cast&lt;long&gt;(x1 + (28L*x0)));
                        tmp_acc0_vec.m2.store(out_ptr1 + static_cast&lt;long&gt;(x1 + (28L*x0)));
                    }
                }
                #pragma omp simd simdlen(8)
                for(long x1=static_cast&lt;long&gt;(16L); x1&lt;static_cast&lt;long&gt;(28L); x1+=static_cast&lt;long&gt;(1L))
                {
                    {
                        #pragma omp declare reduction(    welford:Welford&lt;float&gt;:    omp_out = welford_combine(omp_out, omp_in))     initializer(omp_priv={Welford&lt;float&gt;()})
                        Welford&lt;float&gt; tmp_acc0 = Welford&lt;float&gt;();
                        for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(512L); x2+=static_cast&lt;long&gt;(1L))
                        {
                            auto tmp0 = in_ptr0[static_cast&lt;long&gt;((128L*(c10::div_floor_integer(x2, 256L))) + (256L*x1) + (7168L*(static_cast&lt;long&gt;(c10::div_floor_integer(x2, 128L)) % static_cast&lt;long&gt;(2L))) + (14336L*x0) + (static_cast&lt;long&gt;(x2) % static_cast&lt;long&gt;(128L)))];
                            tmp_acc0 = welford_combine(tmp_acc0, tmp0);
                        }
                        out_ptr0[static_cast&lt;long&gt;(x1 + (28L*x0))] = tmp_acc0.mean;
                        out_ptr1[static_cast&lt;long&gt;(x1 + (28L*x0))] = tmp_acc0.m2;
                    }
                }</code></p>
<p>After:<br />
<code>c++
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(28L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(28L); x1+=static_cast&lt;long&gt;(1L))
                {
                    {
                        #pragma omp declare reduction(welford:Welford&lt;float&gt;:omp_out = welford_combine(omp_out, omp_in)) initializer(omp_priv={Welford&lt;float&gt;()})
                        #pragma omp declare reduction(welford:Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;:omp_out = welford_combine(omp_out, omp_in)) initializer(omp_priv={Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;()})
                        Welford&lt;float&gt; tmp_acc0 = Welford&lt;float&gt;();
                        Welford&lt;at::vec::Vectorized&lt;float&gt;&gt; tmp_acc0_vec = Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;();
                        for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(512L); x2+=static_cast&lt;long&gt;(16L))
                        {
                            auto tmp0 = at::vec::Vectorized&lt;float&gt;::loadu(in_ptr0 + static_cast&lt;long&gt;((128L*(c10::div_floor_integer(x2, 256L))) + (256L*x1) + (7168L*(static_cast&lt;long&gt;(c10::div_floor_integer(x2, 128L)) % static_cast&lt;long&gt;(2L))) + (14336L*x0) + (static_cast&lt;long&gt;(x2) % static_cast&lt;long&gt;(128L))));
                            tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp0);
                        }
                        tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                        out_ptr0[static_cast&lt;long&gt;(x1 + (28L*x0))] = static_cast&lt;float&gt;(tmp_acc0.mean);
                        out_ptr1[static_cast&lt;long&gt;(x1 + (28L*x0))] = static_cast&lt;float&gt;(tmp_acc0.m2);
                    }
                }
            }</code></p>
<p>This PR also further speeds up the model <code>swin_base_patch4_window7_224</code> from 1.25x to 1.28x.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117260<br />
Approved by: https://github.com/jansel<br />
ghstack dependencies: #117221</p>]]></description>
      <pubDate>Sun, 14 Jan 2024 22:57:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3b00dd5843346dd861404729dedc9f46cccd3bbd</guid>
    </item>
    <item>
      <title>Some basic support for uint{16,32,64} codegen in CPU inductor (#116810)</title>
      <link>https://github.com/pytorch/pytorch/commit/7a7535283f3f9985beeb54bb12aecddaba20d7f7</link>
      <description><![CDATA[<p>Some basic support for uint{16,32,64} codegen in CPU inductor (#116810)</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116810<br />
Approved by: https://github.com/chenyang78, https://github.com/eellison, https://github.com/desertfire</p>]]></description>
      <pubDate>Fri, 12 Jan 2024 15:13:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7a7535283f3f9985beeb54bb12aecddaba20d7f7</guid>
    </item>
    <item>
      <title>Make auto_functionalized HOP fallback in inductor (#117084)</title>
      <link>https://github.com/pytorch/pytorch/commit/cb42bc705b8840269ae6e8eece9e7a6b87a54a2d</link>
      <description><![CDATA[<p>Make auto_functionalized HOP fallback in inductor (#117084)</p>
<p>It looks like the inductor fallback previously worked with HOPs but no longer<br />
does, so I fixed that:<br />
- all HOPs are exposed under torch.ops.higher_order, so I changed how<br />
  inductor looks them up<br />
- the inductor fallback assumed that an operator's signature was (<em>args,<br />
</em>*kwargs). This is true for all the OpOverloads but not HOPs. I<br />
  rewrote the code to not rely on this.</p>
<p>Test Plan:<br />
- existing tests<br />
- new test for auto_functionalized HOP.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117084<br />
Approved by: https://github.com/williamwen42</p>]]></description>
      <pubDate>Fri, 12 Jan 2024 09:57:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cb42bc705b8840269ae6e8eece9e7a6b87a54a2d</guid>
    </item>
    <item>
      <title>[inductor][cpp] improve vector contiguous checks for FloorDiv and ModularIndexing (#117221)</title>
      <link>https://github.com/pytorch/pytorch/commit/172dd13ecff965a549bf4f2a58f5b2900a00497a</link>
      <description><![CDATA[<p>[inductor][cpp] improve vector contiguous checks for FloorDiv and ModularIndexing (#117221)</p>
<p>Fix https://github.com/pytorch/pytorch/issues/114488</p>
<p>The PR tries to enable contiguous vector loads for cases where we can reduce <code>FloorDiv</code> and <code>ModularIndexing</code> in the vectorized loop.</p>
<p>Take the index expression in test case <code>test_vec_contiguous_ModularIndexing</code> for example.<br />
<code>14336*x0 + 256*x1 + 128*((x2//256)) + ModularIndexing(x2, 1, 128) + 7168*ModularIndexing(x2, 128, 2)</code> can be reduced to <code>14336*x0 + 256*x1 + x2 + 128*x2_div_c0 + 7168*x2_mod_c0 + x2_mod_c1</code> where <code>x2</code> is a vectorized loop variable and the vector length is 16. This means we can do vectorized load for this index. Check the code comment for more details:<br />
https://github.com/pytorch/pytorch/pull/117221/files#diff-5ab7b0235e2076a5fc6629ba0b109208940f5b94f5c13babc3e0f87cf4fcec82R317-R329</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117221<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 12 Jan 2024 07:20:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/172dd13ecff965a549bf4f2a58f5b2900a00497a</guid>
    </item>
    <item>
      <title>[CPU] Disable floating-point contraction when compiling (#116318)</title>
      <link>https://github.com/pytorch/pytorch/commit/6c624aad377723bde32026c8ddd58a3091c9efc4</link>
      <description><![CDATA[<p>[CPU] Disable floating-point contraction when compiling (#116318)</p>
<p>Fixes #100775.</p>
<p>For CPU inductor path, disable -ffp-contract, such as fma, from optimization flags to fix functional issues.</p>
<h3>Validation</h3>
<p>Validation on 3 benchmark suites.</p>
<ul>
<li>[x] FP32: Negligible geomean change; No outlier models.</li>
</ul>
<p><img width="582" alt="image" src="https://github.com/pytorch/pytorch/assets/23010269/7c14a8b8-eb6c-4794-bff9-2e1ae3a22781"></p>
<ul>
<li>[x] BF16: Negligible geomean change; No outlier models.</li>
</ul>
<p><img width="589" alt="image" src="https://github.com/pytorch/pytorch/assets/23010269/cf558737-8cb2-411f-8761-27b9f8fc43af"></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116318<br />
Approved by: https://github.com/jgong5, https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 12 Jan 2024 06:09:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6c624aad377723bde32026c8ddd58a3091c9efc4</guid>
    </item>
    <item>
      <title>[inductor] Iterative percolate tags (#117306)</title>
      <link>https://github.com/pytorch/pytorch/commit/f7d90478643ada84679b7a9228659381e72ad6d7</link>
      <description><![CDATA[<p>[inductor] Iterative percolate tags (#117306)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/116581</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117306<br />
Approved by: https://github.com/aorenste, https://github.com/eellison</p>]]></description>
      <pubDate>Thu, 11 Jan 2024 23:52:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f7d90478643ada84679b7a9228659381e72ad6d7</guid>
    </item>
    <item>
      <title>Change predispatch tracing API (#117278)</title>
      <link>https://github.com/pytorch/pytorch/commit/40f12cec938fc84a2438d8cf918004c330d099ff</link>
      <description><![CDATA[<p>Change predispatch tracing API (#117278)</p>
<p>Summary: Change the API used in export for aotinductor</p>
<p>Test Plan: buck2 run mode/opt mode/inplace caffe2/test/inductor/fb:test_group_batch_fusion_fb</p>
<p>Differential Revision: D52678653</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117278<br />
Approved by: https://github.com/angelayi, https://github.com/khabinov</p>]]></description>
      <pubDate>Thu, 11 Jan 2024 22:10:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/40f12cec938fc84a2438d8cf918004c330d099ff</guid>
    </item>
    <item>
      <title>enable fp16 mkldnn fusion/prepack in inductor (#117206)</title>
      <link>https://github.com/pytorch/pytorch/commit/ec443089c7bb0389ddfa5f6191204de59b40126a</link>
      <description><![CDATA[<p>enable fp16 mkldnn fusion/prepack in inductor (#117206)</p>
<ul>
<li>Extend <code>linear/conv/rnn</code> packable with <code>float16</code>.</li>
<li>Extend <code>Unary fusion</code> to support <code>float16</code>.</li>
</ul>
<p>Test Case:<br />
    Extend bfloat16 related test in <code>test_cpu_repro.py</code> and <code>test_mkldnn_pattern_matcher.py</code> to test both <code>fp16</code> and <code>bf16</code>.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117206<br />
Approved by: https://github.com/jgong5, https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 11 Jan 2024 22:08:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ec443089c7bb0389ddfa5f6191204de59b40126a</guid>
    </item>
    <item>
      <title>[inductor] check nan/inf for graph inputs (#117189)</title>
      <link>https://github.com/pytorch/pytorch/commit/04604eea8ad201777a11c80c9dc64112e21cfb3c</link>
      <description><![CDATA[<p>[inductor] check nan/inf for graph inputs (#117189)</p>
<p>This is split out from #103469</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117189<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 11 Jan 2024 16:59:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/04604eea8ad201777a11c80c9dc64112e21cfb3c</guid>
    </item>
    <item>
      <title>[benchmark] add --compile-autograd to dynamo benchmarks (#117196)</title>
      <link>https://github.com/pytorch/pytorch/commit/88bf84f1067008b8568b5349e5107cddb5de5c43</link>
      <description><![CDATA[<p>[benchmark] add --compile-autograd to dynamo benchmarks (#117196)</p>
<p>Adds <code>--compile-autograd</code> flag to benchmark suite to run accuracy and performance tests. Also adds autograd_captures and autograd_compiles to dynamo stats</p>
<p>e.g. accuracy_inductor.csv<br />
<code>dev,name,batch_size,accuracy,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles
cuda,BERT_pytorch,4,pass,2655,2,8,7,1,1
cuda,Background_Matting,4,pass_due_to_skip,0,0,0,0,0,0
cuda,DALLE2_pytorch,0,eager_fail_to_run,0,0,0,0,0,0
cuda,LearningToPaint,4,pass,639,2,8,7,1,1
...</code></p>
<p>e.g. speedup_inductor.csv<br />
<code>dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles
cuda,hf_T5,8,1.214311,136.236793,88.350570,0.751322,18.754706,24.962275,3298,2,8,8,1,1
cuda,hf_T5,8,1.226645,135.431856,52.461461,1.040973,18.754706,18.016508,795,1,7,7,0,0
...</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117196<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 11 Jan 2024 12:12:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/88bf84f1067008b8568b5349e5107cddb5de5c43</guid>
    </item>
    <item>
      <title>Add Support For Symbolic Shapes in Register_replacement, SDPA Pattern Matching (#115441)</title>
      <link>https://github.com/pytorch/pytorch/commit/4c7b602645ca285bd9ebbc40c59b91773ffe38e1</link>
      <description><![CDATA[<p>Add Support For Symbolic Shapes in Register_replacement, SDPA Pattern Matching (#115441)</p>
<p>Many of our pattern matching replacements are specified as a <code>search_fn</code> and a <code>replacment_fn</code>. The search_fn's are traced out once with static shapes, converted to a pattern, and then matched on every graph compiled with inductor.</p>
<p>The static shape patterns would not match with graphs that are traced out with dynamic shapes because SymInts would be added to the graph as <code>sym_size</code> fx nodes which added additional uses and prevented matching. The previous PR partially addresses this by deduping SymInts that are resolvable to graph inputs, as is the calling convention in aot autograd.</p>
<p>This PR adjusts our matching of the <code>search_fn</code> by adding SymInts to the arguments we trace out the search_fn with so that their symint accesses are deduped. Later, if we have a match, we will trace out the replacement graph with the correct Tensors and corresponding symbolic shapes that will get added to the graph.</p>
<p>Note: the replacement patterns will insert sym_size uses which could potentially be removed, but I'll leave that for follow up.</p>
<p>Fix for https://github.com/pytorch/pytorch/issues/111190.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115441<br />
Approved by: https://github.com/jansel<br />
ghstack dependencies: #116158</p>]]></description>
      <pubDate>Thu, 11 Jan 2024 07:58:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4c7b602645ca285bd9ebbc40c59b91773ffe38e1</guid>
    </item>
    <item>
      <title>[dynamo] Added dyn shapes support for math trigo ops: sin(h), cos(h), tan(h) ... (#114866)</title>
      <link>https://github.com/pytorch/pytorch/commit/7005a4bcb63f595b93d35f175339152600799905</link>
      <description><![CDATA[<p>[dynamo] Added dyn shapes support for math trigo ops: sin(h), cos(h), tan(h) ... (#114866)</p>
<p>Description:<br />
- Added dynamic shapes support for math trigo ops: sin(h), cos(h), tan(h) ...</p>
<p>```python<br />
import math<br />
import torch</p>
<p>def func(x, a, b):<br />
    c = 0<br />
    c = c + math.sqrt(a)<br />
    c = c + math.cos(a)<br />
    c = c + math.cosh(a)<br />
    c = c + math.sin(a)<br />
    c = c + math.sinh(a)<br />
    c = c + math.tan(a)<br />
    c = c + math.tanh(a)<br />
    c = c + math.asin(b)<br />
    c = c + math.acos(b)<br />
    c = c + math.atan(a)<br />
    y = x + c<br />
    return y</p>
<p>cfunc = torch.compile(func, dynamic=True, fullgraph=True)</p>
<p>device = "cpu"  # or "cuda"<br />
x = torch.tensor([0, 1, 2, 3], dtype=torch.float32, device=device)<br />
a = 12<br />
b = 1</p>
<p>out = cfunc(x, a, b)<br />
expected = func(x, a, b)<br />
torch.testing.assert_close(out, expected)<br />
```</p>
<p>and the graph <code>TORCH_LOGS=+graph_code python check_math_ops.py</code>:</p>
<details>
<summary>
graph code
</summary>

```
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]  ===== __compiled_fn_0 =====
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]  <eval_with_key>.0 class GraphModule(torch.nn.Module):
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]     def forward(self, L_a_ : torch.SymInt, s1 : torch.SymInt, L_x_ : torch.Tensor):
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         l_a_ = L_a_
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         l_x_ = L_x_
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: check_math_ops.py:57, code: c = c + math.sqrt(a)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         sym_sqrt = torch.sym_sqrt(l_a_)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         add = 0 + sym_sqrt;  sym_sqrt = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: check_math_ops.py:58, code: c = c + math.cos(a)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         sym_cos = torch.sym_cos(l_a_)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         add_1 = add + sym_cos;  add = sym_cos = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: check_math_ops.py:59, code: c = c + math.cosh(a)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         sym_cosh = torch.sym_cosh(l_a_)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         add_2 = add_1 + sym_cosh;  add_1 = sym_cosh = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: check_math_ops.py:60, code: c = c + math.sin(a)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         sym_sin = torch.sym_sin(l_a_)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         add_3 = add_2 + sym_sin;  add_2 = sym_sin = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: check_math_ops.py:61, code: c = c + math.sinh(a)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         sym_sinh = torch.sym_sinh(l_a_)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         add_4 = add_3 + sym_sinh;  add_3 = sym_sinh = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: check_math_ops.py:62, code: c = c + math.tan(a)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         sym_tan = torch.sym_tan(l_a_)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         add_5 = add_4 + sym_tan;  add_4 = sym_tan = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: check_math_ops.py:63, code: c = c + math.tanh(a)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         sym_tanh = torch.sym_tanh(l_a_)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         add_6 = add_5 + sym_tanh;  add_5 = sym_tanh = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: check_math_ops.py:64, code: c = c + math.asin(b)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         add_7 = add_6 + 1.5707963267948966;  add_6 = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: check_math_ops.py:65, code: c = c + math.acos(b)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         add_8 = add_7 + 0.0;  add_7 = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: check_math_ops.py:66, code: c = c + math.atan(a)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         sym_atan = torch.sym_atan(l_a_);  l_a_ = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         add_9 = add_8 + sym_atan;  add_8 = sym_atan = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: check_math_ops.py:67, code: y = x + c
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         y = l_x_ + add_9;  l_x_ = add_9 = None
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         return (y,)
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
[2023-11-30 22:16:10,654] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]
```
</details>

<p>Generated code with <code>TORCH_LOGS=+output_code python check_math_ops.py</code>:</p>
<details>
<summary>
C++ code
</summary>

```
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG] cpp_fused_add_0 = async_compile.cpp('''
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG] #include "/tmp/torchinductor_root/2l/c2ljzlm4sosod7u6lyrroqdba6hmfcyijrric6p4t3fhbcmw6osp.h"
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG] extern "C" void kernel(const float* in_ptr0,
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]                        float* out_ptr0,
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]                        const long ks0,
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]                        const long ks1)
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG] {
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]     {
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]         #pragma GCC ivdep
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]         for(long x0=static_cast<long>(0L); x0<static_cast<long>(ks0); x0+=static_cast<long>(1L))
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]         {
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]             auto tmp0 = in_ptr0[static_cast<long>(x0)];
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]             auto tmp1 = c10::convert<float>(1.57079632679490 + (std::sqrt(ks1)) + (std::atan(ks1)) + (std::cos(ks1)) + (std::cosh(ks1)) + (std::sin(ks1)) + (std::sinh(ks1)) + (std::tan(ks1)) + (std::tanh(ks1)));
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]             auto tmp2 = decltype(tmp0)(tmp0 + tmp1);
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]             out_ptr0[static_cast<long>(x0)] = tmp2;
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]         }
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG]     }
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG] }
[2023-11-30 22:19:09,709] [0/0] torch._inductor.graph.__output_code: [DEBUG] ''')
```

</details>

<details>
<summary>
Triton code
</summary>

```
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG] @pointwise(
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     size_hints=[4],
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     filename=__file__,
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), i
ds_of_folded_args=(), divisible_by_8=())]},
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_0', 'mutated_arg_names': []},
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     min_elem_per_thread=0
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG] )
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG] @triton.jit
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG] def triton_(in_ptr0, out_ptr0, ks0, xnumel, XBLOCK : tl.constexpr):
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     xoffset = tl.program_id(0) * XBLOCK
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     xmask = xindex < xnumel
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     x0 = xindex
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     tmp0 = tl.load(in_ptr0 + (x0), xmask)
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     tmp1 = 1.57079632679490 + (tl.math.sqrt(ks0.to(tl.float32))) + (tl.math.atan((ks0).to(tl.float32))) + (tl.math.cos((ks0).to(tl.float32))) + (tl.math.cosh((ks0).to(tl.float32))) + (tl.math.sin((ks0)
.to(tl.float32))) + (tl.math.sinh((ks0).to(tl.float32))) + (tl.math.tan((ks0).to(tl.float32))) + (tl.math.tanh((ks0).to(tl.float32)))
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     tmp2 = tmp1.to(tl.float32)
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     tmp3 = tmp0 + tmp2
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG]     tl.store(out_ptr0 + (x0), tmp3, xmask)
[2023-11-30 22:20:00,383] [0/0] torch._inductor.graph.__output_code: [DEBUG] ''')
```

</details>

<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114866<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Thu, 11 Jan 2024 03:52:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7005a4bcb63f595b93d35f175339152600799905</guid>
    </item>
    <item>
      <title>[inductor] don't access cluster_dims for too old version of triton (#117192)</title>
      <link>https://github.com/pytorch/pytorch/commit/0e1f43c44d18d2fb38c8d39dae7c482ef5143f3f</link>
      <description><![CDATA[<p>[inductor] don't access cluster_dims for too old version of triton (#117192)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/117192<br />
Approved by: https://github.com/masnesral</p>]]></description>
      <pubDate>Thu, 11 Jan 2024 00:37:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0e1f43c44d18d2fb38c8d39dae7c482ef5143f3f</guid>
    </item>
    <item>
      <title>[inductor][cpu]disable pointwise_cat on CPU (#116313)</title>
      <link>https://github.com/pytorch/pytorch/commit/9f57cf502fba060285f0a6cab7516066fee40d8f</link>
      <description><![CDATA[<p>[inductor][cpu]disable pointwise_cat on CPU (#116313)</p>
<p>We observed negative performance impact of pointwise_cat optimization on CPU so disabled it. We will revisit this later after enabling vectorization on index_expr.</p>
<p>This PR fix the following three regression issues:<br />
https://github.com/pytorch/pytorch/issues/115827<br />
https://github.com/pytorch/pytorch/issues/112139<br />
https://github.com/pytorch/pytorch/issues/114495</p>
<p>and cause performance regression of pytorch_unet again. Related issue: https://github.com/pytorch/pytorch/issues/115343</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116313<br />
Approved by: https://github.com/jgong5, https://github.com/leslie-fang-intel, https://github.com/eellison</p>]]></description>
      <pubDate>Thu, 11 Jan 2024 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9f57cf502fba060285f0a6cab7516066fee40d8f</guid>
    </item>
    <item>
      <title>Revert "Add _assert_scalar and teach Inductor to codegen it (#114148)"</title>
      <link>https://github.com/pytorch/pytorch/commit/1174e82bde74711eb0936e63dd896d76510af5dc</link>
      <description><![CDATA[<p>Revert "Add _assert_scalar and teach Inductor to codegen it (#114148)"</p>
<p>This reverts commit b6028acfa46363c1d3262a1522741a06c307843f.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/114148 on behalf of https://github.com/osalpekar due to Going to revert this given the broken torchrec PT2 tests internally: <a href="https://www.internalfb.com/diff/D52648865">D52648865</a>. Logs aren't too clear but @dstaay-fb can help debug as well (<a href="https://github.com/pytorch/pytorch/pull/114148#issuecomment-1886100368">comment</a>)</p>]]></description>
      <pubDate>Wed, 10 Jan 2024 18:30:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1174e82bde74711eb0936e63dd896d76510af5dc</guid>
    </item>
    <item>
      <title>[inductor] Add support for tl.make_block_ptr (#116079)</title>
      <link>https://github.com/pytorch/pytorch/commit/6f8fc42dba651923f26599bf8c231d9ced3480d8</link>
      <description><![CDATA[<p>[inductor] Add support for tl.make_block_ptr (#116079)</p>
<p>On A100 this is a small regression:<br />
<img alt="image" src="https://github.com/pytorch/pytorch/assets/533820/b30eee9d-c0fe-4123-99da-d554fc5d0171" /></p>
<p>So I will leave it disabled by default.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116079<br />
Approved by: https://github.com/shunting314</p>]]></description>
      <pubDate>Wed, 10 Jan 2024 12:02:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6f8fc42dba651923f26599bf8c231d9ced3480d8</guid>
    </item>
    <item>
      <title>Compiled autograd: Lift autograd functions' backward and provide default key for custom autograd functions (#115573)</title>
      <link>https://github.com/pytorch/pytorch/commit/9eb842cbd6f80b3d8c3e88bf70e389465c86eb6f</link>
      <description><![CDATA[<p>Compiled autograd: Lift autograd functions' backward and provide default key for custom autograd functions (#115573)</p>
<p>This PR adds support for torch.autograd.Function subclasses in compiled autograd. We do this by:<br />
- Creating a uid for all torch.autograd.Function via its metaclass. This uid is used in the compiled autograd key, which is a subset of the cache key to the compiled graph<br />
- "Lifting" the backward/saved_tensors, having them as input arguments in the compiled graph<br />
  - Creating proxies to track the backward's inputs and outputs. Since the backward's outputs (grads) have to match the forward's inputs, we pass the node's <code>input_info</code> (forward's input sizes) to build the proxies tracking the backward's outputs.<br />
  - Use a <code>FakeContext</code> class as a replacement for the autograd node's context object (<code>BackwardCFunction</code>) during tracing, only support passing saved_tensors from the forward to the backward<br />
  - Index each backward, to support multiple torch.autograd.Functions in the same graph<br />
  - Special case for <code>CompiledFunctionBackward</code>, lifting CompiledFunction will fail 4 tests and requires some skipfiles changes that I'd rather do that in a separate PR</p>
<p>Example graph: test_custom_fn_saved_multiple_tensors (eager fw + compiled autograd)<br />
```python<br />
class MyFn(torch.autograd.Function):<br />
    @staticmethod<br />
    def forward(ctx, x, y):<br />
        ctx.save_for_backward(x, y)<br />
        return torch.sin(x), torch.sin(y)</p>
<pre><code>@staticmethod
def backward(ctx, gO_x, gO_y):
    (x, y) = ctx.saved_tensors
    return gO_x * torch.cos(x), gO_y * torch.cos(y)
</code></pre>
<p><code>The backwards is lifted via `getitem_5` and `call_backward`</code>python</p>
<h1>Compiled autograd graph</h1>
<p>===== Compiled autograd graph =====<br />
 <eval_with_key>.0 class CompiledAutograd(torch.nn.Module):<br />
    def forward(self, inputs, sizes, hooks):<br />
        # No stacktrace found for following nodes<br />
        getitem: "f32[]" = inputs[0]<br />
        getitem_1: "f32[10]" = inputs[1]<br />
        getitem_2: "f32[10]" = inputs[2]<br />
        getitem_3: "f32[10]" = inputs[3]<br />
        getitem_4: "f32[10]" = inputs[4];  inputs = None<br />
        expand: "f32[10]" = torch.ops.aten.expand.default(getitem, [10]);  getitem = None<br />
        mul: "f32[10]" = torch.ops.aten.mul.Tensor(expand, getitem_2);  getitem_2 = None<br />
        mul_1: "f32[10]" = torch.ops.aten.mul.Tensor(expand, getitem_1);  expand = getitem_1 = None<br />
        getitem_5 = hooks[0];  hooks = None<br />
        call_backward = torch__dynamo_external_utils_call_backward(getitem_5, (getitem_3, getitem_4), mul_1, mul);  getitem_5 = mul_1 = mul = None<br />
        getitem_6: "f32[10]" = call_backward[0]<br />
        getitem_7: "f32[10]" = call_backward[1];  call_backward = None<br />
        accumulate_grad_ = torch.ops.inductor.accumulate_grad_.default(getitem_4, getitem_7);  getitem_4 = getitem_7 = None<br />
        accumulate_grad__1 = torch.ops.inductor.accumulate_grad_.default(getitem_3, getitem_6);  getitem_3 = getitem_6 = None<br />
        return []<br />
```</p>
<p>then is later inlined by dynamo<br />
```python</p>
<h1>Dynamo graph</h1>
<p>===== __compiled_fn_0 =====<br />
 <eval_with_key>.1 class GraphModule(torch.nn.Module):<br />
    def forward(self, L_inputs_0_ : torch.Tensor, L_inputs_1_ : torch.Tensor, L_inputs_2_ : torch.Tensor, L_inputs_3_ : torch.Tensor, L_inputs_4_ : torch.Tensor):<br />
        getitem = L_inputs_0_<br />
        getitem_1 = L_inputs_1_<br />
        getitem_2 = L_inputs_2_<br />
        x = L_inputs_3_<br />
        y = L_inputs_4_</p>
<pre><code>    # File: &lt;eval_with_key&gt;.0:10, code: expand = torch.ops.aten.expand.default(getitem, [10]);  getitem = None
    expand = torch.ops.aten.expand.default(getitem, [10]);  getitem = None

    # File: &lt;eval_with_key&gt;.0:11, code: mul = torch.ops.aten.mul.Tensor(expand, getitem_2);  getitem_2 = None
    mul = torch.ops.aten.mul.Tensor(expand, getitem_2);  getitem_2 = None

    # File: &lt;eval_with_key&gt;.0:12, code: mul_1 = torch.ops.aten.mul.Tensor(expand, getitem_1);  expand = getitem_1 = None
    mul_1 = torch.ops.aten.mul.Tensor(expand, getitem_1);  expand = getitem_1 = None

    # File: /data/users/xmfan/core/pytorch/test/inductor/test_compiled_autograd.py:412, code: return gO_x * torch.cos(x), gO_y * torch.cos(y)
    cos = torch.cos(x)
    getitem_6 = mul_1 * cos;  mul_1 = cos = None
    cos_1 = torch.cos(y)
    getitem_7 = mul * cos_1;  mul = cos_1 = None

    # File: &lt;eval_with_key&gt;.0:17, code: accumulate_grad_ = torch.ops.inductor.accumulate_grad_.default(getitem_4, getitem_7);  getitem_4 = getitem_7 = None
    accumulate_grad__default = torch.ops.inductor.accumulate_grad_.default(y, getitem_7);  y = getitem_7 = None

    # File: &lt;eval_with_key&gt;.0:18, code: accumulate_grad__1 = torch.ops.inductor.accumulate_grad_.default(getitem_3, getitem_6);  getitem_3 = getitem_6 = None
    accumulate_grad__default_1 = torch.ops.inductor.accumulate_grad_.default(x, getitem_6);  x = getitem_6 = None
    return ()
</code></pre>
<p>```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115573<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 10 Jan 2024 10:01:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9eb842cbd6f80b3d8c3e88bf70e389465c86eb6f</guid>
    </item>
    <item>
      <title>[ROCm] Add opt-in option for inductor's layout optimisation on ROCm (#116329)</title>
      <link>https://github.com/pytorch/pytorch/commit/5046b4981dd3edec4e1f61a986d86f447f19ee25</link>
      <description><![CDATA[<p>[ROCm] Add opt-in option for inductor's layout optimisation on ROCm (#116329)</p>
<p>Disabling layout optimisation in inductor for ROCm (https://github.com/pytorch/pytorch/pull/111474) was a bit shortsighted.</p>
<p>If there are workloads that heavily use NHWC we will see a perf drop from additional transpose ops. Instead of disabling this entirely on ROCm this is now an opt-in feature.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116329<br />
Approved by: https://github.com/jansel, https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 10 Jan 2024 05:56:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5046b4981dd3edec4e1f61a986d86f447f19ee25</guid>
    </item>
    <item>
      <title>[Quant] Add dynamic quantization config for x86 inductor backend (#115337)</title>
      <link>https://github.com/pytorch/pytorch/commit/94db6578ccee2551c986d92c245e0a0729b99449</link>
      <description><![CDATA[<p>[Quant] Add dynamic quantization config for x86 inductor backend (#115337)</p>
<p><strong>Description</strong><br />
Add dynamic quantization config for x86 inductor backend.<br />
To support the QKV structure in self-attention, we removed an assertion in port-metadata-pass that requires single dequantize node after quantize node.</p>
<p><strong>Test plan</strong><br />
<code>python test/test_quantization.py -k TestQuantizePT2EX86Inductor.test_dynamic_quant_linear
python test/test_quantization.py -k TestQuantizePT2EX86Inductor.test_qat_dynamic_quant_linear</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115337<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168</p>]]></description>
      <pubDate>Wed, 10 Jan 2024 03:33:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/94db6578ccee2551c986d92c245e0a0729b99449</guid>
    </item>
    <item>
      <title>Add _assert_scalar and teach Inductor to codegen it (#114148)</title>
      <link>https://github.com/pytorch/pytorch/commit/b6028acfa46363c1d3262a1522741a06c307843f</link>
      <description><![CDATA[<p>Add _assert_scalar and teach Inductor to codegen it (#114148)</p>
<p>Inductor codegen for <code>_assert_async</code> is currently disabled because we don't really understand how to codegen <code>scalar_to_tensor</code> on a Sympy expression. I initially tried to see if I could get this to work, but I got into some weird problem involving stride sorting, so I decided to fix it properly by not going through a tensor.</p>
<p>So we introduce an <code>_assert_scalar</code> which takes a scalar as an argument, avoiding needing to turn a SymBool into a tensor before asserting on it. I also add <code>_functional_assert_scalar</code> for good luck, although this doesn't do anything right now because https://github.com/pytorch/pytorch/pull/104203 still hasn't been landed.</p>
<p>I need to customize the codegen for this operator, so I decide to directly implement it in Inductor, rather than trying to treat it as a generic ExternKernel. This leads to the new AssertScalar IR node. This is written carefully so that it doesn't get DCE'd by Inductor.</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114148<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 09 Jan 2024 15:21:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b6028acfa46363c1d3262a1522741a06c307843f</guid>
    </item>
    <item>
      <title>add predispatch_pass to hold pass functions to be run when config.is_predispatch is true (#116788)</title>
      <link>https://github.com/pytorch/pytorch/commit/8a6c43fbe57b069dd76103f6d82217977e88d5fd</link>
      <description><![CDATA[<p>add predispatch_pass to hold pass functions to be run when config.is_predispatch is true (#116788)</p>
<p>Summary:<br />
config.is_predispatch is a config to instruct inductor to enable predispatch<br />
tracing (high level pre-dispatch IR).  Currently, there is no dedicated pass<br />
for this config.</p>
<p>In this commit, for better pass function management, we created<br />
<code>predispatch_pass</code> to hold the pass functions to be run on the high level<br />
pre-dispatch IR-based graphs.</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D52491332</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116788<br />
Approved by: https://github.com/frank-wei</p>]]></description>
      <pubDate>Tue, 09 Jan 2024 14:42:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8a6c43fbe57b069dd76103f6d82217977e88d5fd</guid>
    </item>
    <item>
      <title>Revert "[inductor] Add support for tl.make_block_ptr (#116079)"</title>
      <link>https://github.com/pytorch/pytorch/commit/39ae4d8cd7a19177896f50283288273a3dfdc092</link>
      <description><![CDATA[<p>Revert "[inductor] Add support for tl.make_block_ptr (#116079)"</p>
<p>This reverts commit d527df707acce59bd432763c94399aa7b3fe38cf.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/116079 on behalf of https://github.com/huydhn due to Sorry for reverting your change but the new test is failing on ROCm (<a href="https://github.com/pytorch/pytorch/pull/116079#issuecomment-1883890254">comment</a>)</p>]]></description>
      <pubDate>Tue, 09 Jan 2024 14:19:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/39ae4d8cd7a19177896f50283288273a3dfdc092</guid>
    </item>
    <item>
      <title>[inductor] Add support for tl.make_block_ptr (#116079)</title>
      <link>https://github.com/pytorch/pytorch/commit/d527df707acce59bd432763c94399aa7b3fe38cf</link>
      <description><![CDATA[<p>[inductor] Add support for tl.make_block_ptr (#116079)</p>
<p>On A100 this is a small regression:<br />
<img alt="image" src="https://github.com/pytorch/pytorch/assets/533820/b30eee9d-c0fe-4123-99da-d554fc5d0171" /></p>
<p>So I will leave it disabled by default.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116079<br />
Approved by: https://github.com/shunting314<br />
ghstack dependencies: #116078</p>]]></description>
      <pubDate>Tue, 09 Jan 2024 11:06:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d527df707acce59bd432763c94399aa7b3fe38cf</guid>
    </item>
    <item>
      <title>[inductor] Indexing refactors (#116078)</title>
      <link>https://github.com/pytorch/pytorch/commit/94363cee41aeefbf65ec95020afce3da12acfc07</link>
      <description><![CDATA[<p>[inductor] Indexing refactors (#116078)</p>
<p>Perf differences seems to be noise:<br />
<img alt="image" src="https://github.com/pytorch/pytorch/assets/533820/d7a36574-0388-46e4-bd4d-b274d37cab2b" /></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116078<br />
Approved by: https://github.com/aakhundov</p>]]></description>
      <pubDate>Tue, 09 Jan 2024 11:06:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/94363cee41aeefbf65ec95020afce3da12acfc07</guid>
    </item>
    <item>
      <title>[ROCm] Enable aot_inductor tests (#116713)</title>
      <link>https://github.com/pytorch/pytorch/commit/84b04e42a19590b573192ce23e9ec6bbaa8ef2d8</link>
      <description><![CDATA[<p>[ROCm] Enable aot_inductor tests (#116713)</p>
<p>Fixes #ISSUE_NUMBER</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116713<br />
Approved by: https://github.com/jithunnair-amd, https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 09 Jan 2024 11:05:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/84b04e42a19590b573192ce23e9ec6bbaa8ef2d8</guid>
    </item>
    <item>
      <title>[ROCm] Enabling additional UTs on ROCm (#115738)</title>
      <link>https://github.com/pytorch/pytorch/commit/db79ceb110f6646523019a59bbd7b838f43d4a86</link>
      <description><![CDATA[<p>[ROCm] Enabling additional UTs on ROCm (#115738)</p>
<p>Unskips mostly for dynamo/inductor UT.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115738<br />
Approved by: https://github.com/jithunnair-amd, https://github.com/malfet</p>]]></description>
      <pubDate>Tue, 09 Jan 2024 00:36:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/db79ceb110f6646523019a59bbd7b838f43d4a86</guid>
    </item>
    <item>
      <title>[AOTInductor] Small refactor so both Meta internal and OSS can deal with misplaced args and kwargs for Extern Fallback kernels (#116779)</title>
      <link>https://github.com/pytorch/pytorch/commit/f0bbc2fcf54a47d5b76ddec48efba6b7c431b034</link>
      <description><![CDATA[<p>[AOTInductor] Small refactor so both Meta internal and OSS can deal with misplaced args and kwargs for Extern Fallback kernels (#116779)</p>
<p>Summary:<br />
In torch/_inductor/lowering.py (https://fburl.com/code/jd58vxpw), we are using<br />
<code>fallback_cumsum(x, dim=axis, dtype=dtype)</code><br />
so this will treat <code>x</code> as args, <code>dim</code> and <code>dtype</code> as kwargs from https://fburl.com/code/cikchxp9</p>
<p>The issue has been fixed from D52530506 for OSS but not Meta internal. This diff address the Meta internal issue by some refactoring so both Meta internal and OSS can use the same helper function. The diff also added some debug log.</p>
<p>Test Plan:<br />
before<br />
<code>aoti_torch_proxy_executor_call_function(proxy_executor, 2, 1, std::vector&lt;int64_t&gt;{torch.int64}.data(), 2, std::vector&lt;AtenTensorHandle&gt;{buf702, buf708}.data());</code><br />
after<br />
<code>aoti_torch_proxy_executor_call_function(proxy_executor, 2, 1, std::vector&lt;int64_t&gt;{0}.data(), 2, std::vector&lt;AtenTensorHandle&gt;{buf702, buf708}.data());</code><br />
so <code>torch.int64</code> changed to <code>0</code></p>
<p>Differential Revision: D52532031</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116779<br />
Approved by: https://github.com/desertfire, https://github.com/chenyang78</p>]]></description>
      <pubDate>Mon, 08 Jan 2024 23:57:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f0bbc2fcf54a47d5b76ddec48efba6b7c431b034</guid>
    </item>
    <item>
      <title>[cpu][vec512] improve bf16/fp16 load/store with mask for inductor (#116961)</title>
      <link>https://github.com/pytorch/pytorch/commit/a0bd7dfec1a7f2fdaf05d483955178ab907e135b</link>
      <description><![CDATA[<p>[cpu][vec512] improve bf16/fp16 load/store with mask for inductor (#116961)</p>
<p>Improve perf of vec512 bfloat16 (and also float16) load and store with partial vector lanes using masked load/store instead of via <code>memcpy</code> with aux buffer. In inductor CPU backend, we do load/store half (16) vector lanes for bfloat16 and float16.</p>
<p>Using the following micro-benchmark script for <code>layernorm + add</code>:<br />
```python<br />
import torch<br />
import torch.nn as nn<br />
from benchmark_helper import time_with_torch_timer</p>
<p>class AddLayernorm(nn.Module):<br />
    def <strong>init</strong>(self, hidden_size):<br />
        super().<strong>init</strong>()<br />
        self.ln = nn.LayerNorm(hidden_size)</p>
<pre><code>def forward(self, hidden_states):
    return hidden_states + self.ln(hidden_states)
</code></pre>
<p>hidden_states = torch.randn(1, 512, 1024).to(torch.bfloat16)</p>
<p>with torch.no_grad():<br />
    compiled_add_ln = torch.compile(add_ln)<br />
    print(time_with_torch_timer(compiled_add_ln, hidden_states, iters=10000))<br />
```</p>
<p>Measured on single-core <code>Intel(R) Xeon(R) Platinum 8358 CPU</code>.<br />
Before: 1.39 ms<br />
After: 498.66 us</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116961<br />
Approved by: https://github.com/sanchitintel, https://github.com/leslie-fang-intel</p>]]></description>
      <pubDate>Mon, 08 Jan 2024 20:18:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a0bd7dfec1a7f2fdaf05d483955178ab907e135b</guid>
    </item>
    <item>
      <title>[ROCm] Add minimal inductor test to rocm-test workflow (#115425)</title>
      <link>https://github.com/pytorch/pytorch/commit/bac0de160c86a249d8be55ec81bbb20e5a4568bc</link>
      <description><![CDATA[<p>[ROCm] Add minimal inductor test to rocm-test workflow (#115425)</p>
<p>Adds the <code>inductor/test_torchinductor</code> to tests-to-include so we can have some PR-level test coverage for inductor tests on ROCm. This should help catch issues before merging (e.g. https://github.com/pytorch/pytorch/pull/114772)</p>
<p>This unit test takes ~6minutes</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115425<br />
Approved by: https://github.com/jithunnair-amd, https://github.com/huydhn, https://github.com/malfet</p>]]></description>
      <pubDate>Mon, 08 Jan 2024 19:54:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bac0de160c86a249d8be55ec81bbb20e5a4568bc</guid>
    </item>
    <item>
      <title>Inductor qlinear int8_bf16 with bmm (#116604)</title>
      <link>https://github.com/pytorch/pytorch/commit/14be2ee27175794fb89b96cfd62e3decb81c7e25</link>
      <description><![CDATA[<p>Inductor qlinear int8_bf16 with bmm (#116604)</p>
<p><strong>Summary</strong><br />
Fix issue: https://github.com/pytorch/pytorch/issues/116492, <code>linear</code> will be decomposed into <code>bmm</code> when input dim exceeds 2 and not contiguous. Fix this issue by convert the pattern back into <code>qlinear</code>. This PR focus on int8_bf16 case following of https://github.com/pytorch/pytorch/pull/116599.</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_mkldnn_pattern_matcher.py -k test_qlinear_int8_mixed_bf16_input_dim_exceeds_2_and_not_contiguous</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116604<br />
Approved by: https://github.com/jgong5<br />
ghstack dependencies: #116937, #116599</p>]]></description>
      <pubDate>Mon, 08 Jan 2024 17:36:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/14be2ee27175794fb89b96cfd62e3decb81c7e25</guid>
    </item>
    <item>
      <title>Inductor qlinear int8_fp32 with bmm (#116599)</title>
      <link>https://github.com/pytorch/pytorch/commit/153b3a0996c3a30a89bf591bf258a89aac723654</link>
      <description><![CDATA[<p>Inductor qlinear int8_fp32 with bmm (#116599)</p>
<p><strong>Summary</strong><br />
Fix issue: https://github.com/pytorch/pytorch/issues/116492, <code>linear</code> will be decomposed into <code>bmm</code> when input dim exceeds 2 and not contiguous. Fix this issue by convert the pattern back into <code>qlinear</code>. This PR focus on int8_fp32 case, will follow up int8_bf16 case in next PR.</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_mkldnn_pattern_matcher.py -k test_qlinear_input_dim_exceeds_2_and_not_contiguous</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116599<br />
Approved by: https://github.com/jgong5<br />
ghstack dependencies: #116937</p>]]></description>
      <pubDate>Mon, 08 Jan 2024 17:33:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/153b3a0996c3a30a89bf591bf258a89aac723654</guid>
    </item>
    <item>
      <title>[CI] Add inductor workflow for rocm (#110544)</title>
      <link>https://github.com/pytorch/pytorch/commit/6ca31ae1d3821c7019611ebd03991aa684f2ce61</link>
      <description><![CDATA[<p>[CI] Add inductor workflow for rocm (#110544)</p>
<p>This PR is to create a separate CI job for inductor UTs on ROCm. You will need to add <code>ciflow/inductor</code> tag on PRs to trigger this job. However, the job will run on its own on any commit merged in main. This job takes around 1.5 hours to run and it is run in parallel to other rocm jobs. It is run only on the MI210 CI runners to ensure maximum inductor functionality is tested.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110544<br />
Approved by: https://github.com/jithunnair-amd, https://github.com/jansel, https://github.com/huydhn</p>]]></description>
      <pubDate>Mon, 08 Jan 2024 17:32:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6ca31ae1d3821c7019611ebd03991aa684f2ce61</guid>
    </item>
    <item>
      <title>[Inductor] [Quant] Add remaining user check for qconv binary fusion (#115809)</title>
      <link>https://github.com/pytorch/pytorch/commit/227579d6a0cb0a3774dbd164bea1b1a9d61d0bdb</link>
      <description><![CDATA[<p>[Inductor] [Quant] Add remaining user check for qconv binary fusion (#115809)</p>
<p><strong>Summary</strong><br />
Similar as https://github.com/pytorch/pytorch/pull/115153, when we do the <code>qconv_binary</code> fusion with post op sum, we also need to ensure that: all users of the extra input in this pattern should be ancestor nodes of the compute node, except for the binary node connected to the compute node.</p>
<p>Also change some variable names in this diff as:</p>
<ul>
<li>Change name of <code>qconv2d_node_after_weight_prepack</code> to <code>compute_node</code></li>
<li>Change name of <code>extra_input_node</code> to <code>extra_input_of_binary_node</code></li>
</ul>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_mkldnn_pattern_matcher.py -k test_qconv2d_add_3</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115809<br />
Approved by: https://github.com/jgong5<br />
ghstack dependencies: #115153</p>]]></description>
      <pubDate>Mon, 08 Jan 2024 17:26:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/227579d6a0cb0a3774dbd164bea1b1a9d61d0bdb</guid>
    </item>
    <item>
      <title>Merge merging rules of CPU inductor and x86 CPU quantization (#116937)</title>
      <link>https://github.com/pytorch/pytorch/commit/7073dc604e220e1690dc21c77b52da1e63a534e1</link>
      <description><![CDATA[<p>Merge merging rules of CPU inductor and x86 CPU quantization (#116937)</p>
<p><strong>Summary</strong><br />
Following the discussion at https://github.com/pytorch/pytorch/pull/116599#issuecomment-1878757581, due to the limitation of the current merging rules that prevent cross-checking all files among different merge groups, it is proposed to merge the groups <code>x86 CPU quantization</code> and <code>CPU inductor</code> since they are closely related.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116937<br />
Approved by: https://github.com/jgong5, https://github.com/atalman</p>]]></description>
      <pubDate>Mon, 08 Jan 2024 07:32:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7073dc604e220e1690dc21c77b52da1e63a534e1</guid>
    </item>
    <item>
      <title>[aot_inductor] Retrieve original FQNs for weights (#116157)</title>
      <link>https://github.com/pytorch/pytorch/commit/5377b994da7ff4c2c5543e74dfcbf8b4faa701bb</link>
      <description><![CDATA[<p>[aot_inductor] Retrieve original FQNs for weights (#116157)</p>
<p>Differential Revision: D52303882</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116157<br />
Approved by: https://github.com/frank-wei</p>]]></description>
      <pubDate>Fri, 05 Jan 2024 13:30:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5377b994da7ff4c2c5543e74dfcbf8b4faa701bb</guid>
    </item>
    <item>
      <title>[ROCm] Use MI210 CI runners for all trunk commits (#116797)</title>
      <link>https://github.com/pytorch/pytorch/commit/0a0209e8a1230ad6d756f430a80a6e3a3b9971f2</link>
      <description><![CDATA[<p>[ROCm] Use MI210 CI runners for all trunk commits (#116797)</p>
<p>As a follow-up to https://github.com/pytorch/pytorch/pull/115981</p>
<p>To make sure we catch any regressions/breakages related to flash attention/inductor/etc. functionality that is only enabled for MI210s, we would like to switch the trunk commit CI jobs to always run on MI210 runners. This should help us accurately identify the breaking commits for ROCm CI on the HUD.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116797<br />
Approved by: https://github.com/jeffdaily, https://github.com/pruthvistony</p>]]></description>
      <pubDate>Fri, 05 Jan 2024 09:46:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0a0209e8a1230ad6d756f430a80a6e3a3b9971f2</guid>
    </item>
    <item>
      <title>[AOTI] Add pybind for AOTIModelContainerRunnerCpu and AOTIModelContainerRunnerCuda (#116269)</title>
      <link>https://github.com/pytorch/pytorch/commit/70f3a530d7290d9eb010ad789787bec4dcbfbeb9</link>
      <description><![CDATA[<p>[AOTI] Add pybind for AOTIModelContainerRunnerCpu and AOTIModelContainerRunnerCuda (#116269)</p>
<p>Summary: Now we can allocate an AOTIModelContainerRunner object instead of relying on torch.utils.cpp_extension.load_inline. Also renamed AOTInductorModelRunner to AOTIRunnerUtil in this PR.</p>
<p>Test Plan: CI</p>
<p>Reviewed By: khabinov</p>
<p>Differential Revision: D52339116</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116269<br />
Approved by: https://github.com/khabinov</p>]]></description>
      <pubDate>Thu, 04 Jan 2024 10:58:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/70f3a530d7290d9eb010ad789787bec4dcbfbeb9</guid>
    </item>
    <item>
      <title>[inductor] Use max sm clock when calculating device tflops (#116754)</title>
      <link>https://github.com/pytorch/pytorch/commit/39f885331394e6baea6220f7e22bf1cda17bc361</link>
      <description><![CDATA[<p>[inductor] Use max sm clock when calculating device tflops (#116754)</p>
<p>See openai/triton#2801</p>
<p>Current SM clocks may fluctuate at runtime and change the result of<br />
<code>get_device_tflops</code>.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116754<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Thu, 04 Jan 2024 09:38:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/39f885331394e6baea6220f7e22bf1cda17bc361</guid>
    </item>
    <item>
      <title>[Inductor] Fix Conv Binary Inplace Fusion issue (#115153)</title>
      <link>https://github.com/pytorch/pytorch/commit/4926146537df5a39846dae0e551d394001588b13</link>
      <description><![CDATA[<p>[Inductor] Fix Conv Binary Inplace Fusion issue (#115153)</p>
<p><strong>Summary</strong><br />
Take this Pattern as example<br />
<code>#      ReLU
  #     /   \
  #  Conv1
  #   /      \
  # Conv2
  #   \      /
  #      Add</code><br />
The current <code>ConvBinaryInplace</code> check will fail to perform Inplace fusion (using outplace fusion instead) due to <code>ReLU</code> having 2 users. However, if all users of <code>ReLU</code> are ancestor nodes of <code>Conv2</code>, we should be able to proceed with the <code>ConvBinaryInplace</code> fusion. This diff relaxes the <code>ConvBinaryInplace</code> check accordingly.</p>
<p><strong>TestPlan</strong><br />
<code>python -m pytest test_mkldnn_pattern_matcher.py -k test_conv2d_binary_inplace_fusion_pass_cpu
python -m pytest test_mkldnn_pattern_matcher.py -k test_conv2d_binary_inplace_fusion_failed_cpu</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115153<br />
Approved by: https://github.com/CaoE, https://github.com/jgong5</p>]]></description>
      <pubDate>Wed, 03 Jan 2024 17:06:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4926146537df5a39846dae0e551d394001588b13</guid>
    </item>
    <item>
      <title>[inductor] Add ABI shim function for torch.scatter_reduce (#116700)</title>
      <link>https://github.com/pytorch/pytorch/commit/4e330882da44f97b12dff6bf6f8a53d8276dff54</link>
      <description><![CDATA[<p>[inductor] Add ABI shim function for torch.scatter_reduce (#116700)</p>
<p>Ran into the following exception during C++ file compilation.<br />
<code>error: use of undeclared identifier 'aoti_torch_scatter_reduce_out'
    aoti_torch_scatter_reduce_out(buf12, buf12,0,buf13,buf14, "sum",1);
    ^</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116700<br />
Approved by: https://github.com/aakhundov</p>]]></description>
      <pubDate>Wed, 03 Jan 2024 15:43:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4e330882da44f97b12dff6bf6f8a53d8276dff54</guid>
    </item>
    <item>
      <title>[inductor] Add shape checks to ExpandView (#113839)</title>
      <link>https://github.com/pytorch/pytorch/commit/f6be25bae60853c6654d0efea096a459afa72c01</link>
      <description><![CDATA[<p>[inductor] Add shape checks to ExpandView (#113839)</p>
<p>Currently <code>ExpandView</code> doesn't check that the expanded shape is valid which may<br />
allow bugs to slip through which cause silent correctness issues.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113839<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Wed, 03 Jan 2024 14:31:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f6be25bae60853c6654d0efea096a459afa72c01</guid>
    </item>
    <item>
      <title>Revert "[Dynamo] Trace autograd.function in dynamo when inputs require grad (#116358)"</title>
      <link>https://github.com/pytorch/pytorch/commit/68105da229b4907207b55f50121389c8f570cc28</link>
      <description><![CDATA[<p>Revert "[Dynamo] Trace autograd.function in dynamo when inputs require grad (#116358)"</p>
<p>This reverts commit 97891b184c12763f335fbe1ff63fab843edafab5.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/116358 on behalf of https://github.com/izaitsevfb due to Breaks internal accuracy test, see D52491095, pytorch/benchmark/fb/test_gpu:run_test_gpu - test_train_ig_feed_over_inductor_accuracy  (<a href="https://github.com/pytorch/pytorch/pull/116358#issuecomment-1875779697">comment</a>)</p>]]></description>
      <pubDate>Wed, 03 Jan 2024 10:20:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/68105da229b4907207b55f50121389c8f570cc28</guid>
    </item>
    <item>
      <title>add cpu inductor merge rule (#116679)</title>
      <link>https://github.com/pytorch/pytorch/commit/3407541b0c58315b6252811eeecc600acf0798b7</link>
      <description><![CDATA[<p>add cpu inductor merge rule (#116679)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116679<br />
Approved by: https://github.com/huydhn</p>]]></description>
      <pubDate>Wed, 03 Jan 2024 07:09:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3407541b0c58315b6252811eeecc600acf0798b7</guid>
    </item>
    <item>
      <title>Inductor cpp wrapper: fix cumsum codegen (#116171)</title>
      <link>https://github.com/pytorch/pytorch/commit/1ae39a372e000f22c5a30a3668535f0ca5d2b1e5</link>
      <description><![CDATA[<p>Inductor cpp wrapper: fix cumsum codegen (#116171)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/115829</p>
<p>For <code>cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -&gt; Tensor</code>, <code>dim</code> is not a <code>kwarg_only</code> argument, but it could be provided as a kwarg when calling this OP.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116171<br />
Approved by: https://github.com/jgong5, https://github.com/desertfire, https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 02 Jan 2024 21:33:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1ae39a372e000f22c5a30a3668535f0ca5d2b1e5</guid>
    </item>
    <item>
      <title>[inductor] Control the cpp_wrapper mode with an env variable (#116615)</title>
      <link>https://github.com/pytorch/pytorch/commit/640d46f823cbd5d2d08c0807646926816cf6786b</link>
      <description><![CDATA[<p>[inductor] Control the cpp_wrapper mode with an env variable (#116615)</p>
<p>Summary: also add one model test for the cpp_wrapper mode on CI</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116615<br />
Approved by: https://github.com/angelayi</p>]]></description>
      <pubDate>Tue, 02 Jan 2024 13:50:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/640d46f823cbd5d2d08c0807646926816cf6786b</guid>
    </item>
    <item>
      <title>[Quant] Add int8 linear op gelu for quantization PT2E with Inductor. input is an int8 CPU tensor; weight is an int8 MdkldnnCPU tensor (#114852)</title>
      <link>https://github.com/pytorch/pytorch/commit/95a86ed9ca107329151e0dc172386d50dd3471c6</link>
      <description><![CDATA[<p>[Quant] Add int8 linear op gelu for quantization PT2E with Inductor. input is an int8 CPU tensor; weight is an int8 MdkldnnCPU tensor (#114852)</p>
<p><strong>Summary</strong><br />
Enable Int8 Linear Gelu post operator fusions for Stock PyTorch Inductor. The input is an int8 CPU tensor and weight is an int8 MkldnnCPU tensor.</p>
<p><strong>Test plan</strong><br />
python test/test_quantization.py -k test_qlinear_gelu_pt2e</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114852<br />
Approved by: https://github.com/leslie-fang-intel, https://github.com/jgong5</p>]]></description>
      <pubDate>Tue, 02 Jan 2024 00:11:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/95a86ed9ca107329151e0dc172386d50dd3471c6</guid>
    </item>
    <item>
      <title>[inductor] Fix cpp_wrapper codegen for ir.ComplexView (#116481)</title>
      <link>https://github.com/pytorch/pytorch/commit/a81edf9f230452feb62bae30f8e92f131370571e</link>
      <description><![CDATA[<p>[inductor] Fix cpp_wrapper codegen for ir.ComplexView (#116481)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116481<br />
Approved by: https://github.com/htyu</p>]]></description>
      <pubDate>Mon, 01 Jan 2024 21:38:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a81edf9f230452feb62bae30f8e92f131370571e</guid>
    </item>
    <item>
      <title>[Inductor] Decompose bmm if batch2's last dim size is 1 and coordinate_descent_tuning is enabled (#116582)</title>
      <link>https://github.com/pytorch/pytorch/commit/abd80cbb15bdc67e829687ed99604c554b7ac1d2</link>
      <description><![CDATA[<p>[Inductor] Decompose bmm if batch2's last dim size is 1 and coordinate_descent_tuning is enabled (#116582)</p>
<p>We found this perf optimization opportunity at https://github.com/pytorch-labs/gpt-fast/pull/71. This would bring 5%+ perf gain for Mixtral 8x7B on gpt-fast.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116582<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Mon, 01 Jan 2024 13:24:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/abd80cbb15bdc67e829687ed99604c554b7ac1d2</guid>
    </item>
    <item>
      <title>[Inductor][Observability] Add logging for split cat pass (#116442)</title>
      <link>https://github.com/pytorch/pytorch/commit/df85a920cf4c893a138c1f2fa215feceec025e38</link>
      <description><![CDATA[<p>[Inductor][Observability] Add logging for split cat pass (#116442)</p>
<p>Summary: Add logs for both in the pre and post grad passes</p>
<p>Test Plan:<br />
<code>buck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split_batch</code><br />
[2023-12-26 17:14:24,203] [0/0] torch._inductor.fx_passes.post_grad: [INFO] counters of inductor dict after apply the split cat in the post grad pass: Counter({'pattern_matcher_nodes': 4076, 'pattern_matcher_count': 2917, 'remove_split_with_size_one': 1322, 'split_cat_norm': 461, 'consecutive_split_merged': 371, 'scmerge_cat_removed': 41, 'scmerge_cat_added': 32, 'scmerge_split_removed': 28, 'getitem_cat_merged': 11, 'batch_fusion': 7, 'scmerge_split_sections_removed': 3, 'scmerge_split_added': 2, 'split_squeeze_replaced': 2})</p>
<p>[2023-12-26 17:16:28,437] torch._inductor.fx_passes.post_grad: [INFO] counters of inductor dict after apply the split cat in the post grad pass: Counter({'pattern_matcher_nodes': 4122, 'pattern_matcher_count': 2935, 'remove_split_with_size_one': 1322, 'split_cat_norm': 461, 'consecutive_split_merged': 371, 'scmerge_cat_removed': 41, 'batch_fusion': 39, 'scmerge_cat_added': 32, 'scmerge_split_removed': 28, 'getitem_cat_merged': 11, 'scmerge_split_sections_removed': 3, 'scmerge_split_added': 2, 'split_squeeze_replaced': 2})</p>
<p>Differential Revision: D52425400</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116442<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Thu, 28 Dec 2023 21:10:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/df85a920cf4c893a138c1f2fa215feceec025e38</guid>
    </item>
    <item>
      <title> [Inductor Intel GPU backend Upstream]  Step 2: Register and add Intel GPU Inductor backend (#116330)</title>
      <link>https://github.com/pytorch/pytorch/commit/cab79ceb51e1a7c608dafc209ad3059fa9b15de4</link>
      <description><![CDATA[<p>[Inductor Intel GPU backend Upstream]  Step 2: Register and add Intel GPU Inductor backend (#116330)</p>
<p>Right after the first PR https://github.com/pytorch/pytorch/pull/116020, this PR forcus on generalizing device-bias runtime code that used in the basic workflow including triton kernel generation, codecache, autotuning.</p>
<p>Feature request: #114856</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116330<br />
Approved by: https://github.com/EikanWang, https://github.com/jgong5, https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 28 Dec 2023 18:49:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cab79ceb51e1a7c608dafc209ad3059fa9b15de4</guid>
    </item>
    <item>
      <title>[RelEng] Missing signal for release branches (#116516)</title>
      <link>https://github.com/pytorch/pytorch/commit/439f2a6c1fc31b5235b9f712bc8fb81af5f06f16</link>
      <description><![CDATA[<p>[RelEng] Missing signal for release branches (#116516)</p>
<p>Run slow/periodic and inductor workflows on push to release branches</p>
<p>Right now there are no signal from those jobs on release branches at all.<br />
This will run periodic jobs on every commit to release branch, which is fine, as they are short lived and have a much lower traffic that a regular jobs</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116516<br />
Approved by: https://github.com/clee2000</p>]]></description>
      <pubDate>Thu, 28 Dec 2023 12:19:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/439f2a6c1fc31b5235b9f712bc8fb81af5f06f16</guid>
    </item>
    <item>
      <title>Add setUserEnabledNNPACK config (#116152)</title>
      <link>https://github.com/pytorch/pytorch/commit/6b91e6907ef3677456780e887a45893fa7b2d279</link>
      <description><![CDATA[<p>Add setUserEnabledNNPACK config (#116152)</p>
<p>When exporting a model with a convolution kernel on cpu, if mkldnn is disabled and nnpack is enabled, export will go down the nnpack optimized convolution kernel for certain shapes ((code pointer)[https://github.com/pytorch/pytorch/blob/cd449e260c830c9ce0f06ed4833b46aa638f1529/aten/src/ATen/native/Convolution.cpp#L542-L552]). This means that we will automatically create a guard on that certain shape. If users want to export without any restrictions, one option is to disable nnpack. However, no config function exists for this, so this PR is adding a config function, similar to the <code>set_mkldnn_enabled</code> function.</p>
<p>Original context is in https://fb.workplace.com/groups/1075192433118967/posts/1349589822345892/?comment_id=1349597102345164&amp;reply_comment_id=1349677642337110.</p>
<p>To test the flag, the following script runs successfully:<br />
```<br />
import os</p>
<p>import torch<br />
from torchvision.models import ResNet18_Weights, resnet18</p>
<p>torch.set_float32_matmul_precision("high")</p>
<p>model = resnet18(weights=ResNet18_Weights.DEFAULT)<br />
model.eval()</p>
<p>with torch.no_grad():<br />
    # device = "cuda" if torch.cuda.is_available() else "cpu"<br />
    torch.backends.mkldnn.set_flags(False)<br />
    torch.backends.nnpack.set_flags(False)   # &lt;--- Added config<br />
    device = "cpu"<br />
    model = model.to(device=device)<br />
    example_inputs = (torch.randn(2, 3, 224, 224, device=device),)<br />
    batch_dim = torch.export.Dim("batch", min=2, max=32)<br />
    so_path = torch._export.aot_compile(<br />
        model,<br />
        example_inputs,<br />
        # Specify the first dimension of the input x as dynamic<br />
        dynamic_shapes={"x": {0: batch_dim}},<br />
        # Specify the generated shared library path<br />
        options={<br />
            "aot_inductor.output_path": os.path.join(os.getcwd(), "resnet18_pt2.so"),<br />
            "max_autotune": True,<br />
        },<br />
    )</p>
<p>```</p>
<p>I'm not sure who to add as reviewer, so please feel free to add whoever is relevant!</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116152<br />
Approved by: https://github.com/malfet</p>]]></description>
      <pubDate>Tue, 26 Dec 2023 22:00:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6b91e6907ef3677456780e887a45893fa7b2d279</guid>
    </item>
    <item>
      <title>[inductor] Sort unbacked symbols before iterating on them (#116421)</title>
      <link>https://github.com/pytorch/pytorch/commit/2c89e5a5e5e18f65569a246428deff6efb95d734</link>
      <description><![CDATA[<p>[inductor] Sort unbacked symbols before iterating on them (#116421)</p>
<p>get_unbacked_symbol_defs and get_unbacked_symbol_uses inconsistently return dicts vs. sets. The majority of the use cases of these methods use them for set membership, which is deterministic, but set iteration is non deterministic. Therefore, in the one place where we iterate through unbacked symbols, we sort by the symbol name before iterating to preserve determinism.</p>
<p>Another approach would be to have these functions consistently return dictionaries, where the key of the dictionary is the name of the symbol. I'm happy to do that approach if we think it's likely future code will forget to sort before iteration.</p>
<p>Fixes #113130</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116421<br />
Approved by: https://github.com/oulgen, https://github.com/aakhundov</p>]]></description>
      <pubDate>Tue, 26 Dec 2023 19:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2c89e5a5e5e18f65569a246428deff6efb95d734</guid>
    </item>
    <item>
      <title>[inductor] fix cpp_wrapper inputs mismatch (#116197)</title>
      <link>https://github.com/pytorch/pytorch/commit/e5bcfe205e4495e3d42a2800441283fae5060278</link>
      <description><![CDATA[<p>[inductor] fix cpp_wrapper inputs mismatch (#116197)</p>
<p>Summary: fixes https://github.com/pytorch/pytorch/issues/115035, where in the cpp_wrapper JIT inductor, the input args should contain the lifted parameters.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116197<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 26 Dec 2023 13:41:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e5bcfe205e4495e3d42a2800441283fae5060278</guid>
    </item>
    <item>
      <title>[inductor] More tweaks to fusion logs (#115084)</title>
      <link>https://github.com/pytorch/pytorch/commit/7571511af9bdfe2e6b2f822c5e9d15173fb787cd</link>
      <description><![CDATA[<p>[inductor] More tweaks to fusion logs (#115084)</p>
<p>I think it's more useful to print out actual fusions rather than<br />
possible fusions.</p>
<p>I also updated <code>speedup_by_fusion</code>'s logs to include the node names in<br />
the log output.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115084<br />
Approved by: https://github.com/jansel, https://github.com/aakhundov</p>]]></description>
      <pubDate>Tue, 26 Dec 2023 12:25:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7571511af9bdfe2e6b2f822c5e9d15173fb787cd</guid>
    </item>
    <item>
      <title>[inductor] Remove the float16 restriction for cpu cpp_wrapper (#116205)</title>
      <link>https://github.com/pytorch/pytorch/commit/f4230ec9fda12c12f0703223308ab067b1adf7c3</link>
      <description><![CDATA[<p>[inductor] Remove the float16 restriction for cpu cpp_wrapper (#116205)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116205<br />
Approved by: https://github.com/jgong5, https://github.com/chunyuan-w, https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 26 Dec 2023 08:01:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f4230ec9fda12c12f0703223308ab067b1adf7c3</guid>
    </item>
    <item>
      <title>[inductor][cpp] load as scalar for the index invariant in the vector range (#116387)</title>
      <link>https://github.com/pytorch/pytorch/commit/4c6e842496da636123f83ef868ca1974631f1f1e</link>
      <description><![CDATA[<p>[inductor][cpp] load as scalar for the index invariant in the vector range (#116387)</p>
<p>For the test <code>test_expr_vec_non_contiguous</code>. The index_expr <code>31L + (63L*(c10::div_floor_integer(x1, 32L))) + (c10::div_floor_integer(x2, 32L))</code> is invariant under the vector range of <code>x2</code>.<br />
Before change<br />
<code>c++
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(4L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(1024L); x1+=static_cast&lt;long&gt;(16L))
                {
                    {
                        #pragma omp declare reduction(max:at::vec::Vectorized&lt;float&gt;:omp_out = at::vec::maximum(omp_out, omp_in)) initializer(omp_priv={at::vec::Vectorized&lt;float&gt;(-std::numeric_limits&lt;float&gt;::infinity())})
                        float tmp_acc0 = -std::numeric_limits&lt;float&gt;::infinity();
                        at::vec::Vectorized&lt;float&gt; tmp_acc0_vec = at::vec::Vectorized&lt;float&gt;(-std::numeric_limits&lt;float&gt;::infinity());
                        for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(1024L); x2+=static_cast&lt;long&gt;(1L))
                        {
                            auto tmp0 =
                            [&amp;]
                            {
                                __at_align__ std::array&lt;int, 16&gt; tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner &lt; 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = static_cast&lt;long&gt;(31L + (63L*(c10::div_floor_integer((x1 + x1_inner), 32L))) + (c10::div_floor_integer(x2, 32L)));
                                }
                                return at::vec::Vectorized&lt;int&gt;::loadu(tmpbuf.data());
                            }
                            ()
                            ;
                            auto tmp1 = static_cast&lt;int&gt;(2048);
                            auto tmp2 = at::vec::Vectorized&lt;int&gt;(tmp1);
                            auto tmp3 = to_float_mask(tmp0 &lt; tmp2);
                            auto tmp4 = [&amp;]
                            {
                                auto tmp5 =
                                [&amp;]
                                {
                                    __at_align__ std::array&lt;float, 16&gt; tmpbuf;
                                    #pragma GCC unroll 16
                                    for (long x1_inner = 0; x1_inner &lt; 16; x1_inner++)
                                    {
                                        if (vector_lane_mask_check(tmp3, x1_inner))
                                        {
                                            tmpbuf[x1_inner] = in_ptr0[static_cast&lt;long&gt;(31L + (63L*(c10::div_floor_integer((x1 + x1_inner), 32L))) + (2048L*(static_cast&lt;long&gt;((x1 + x1_inner)) % static_cast&lt;long&gt;(32L))) + (65536L*x0) + (c10::div_floor_integer(x2, 32L)))];
                                        }
                                    }
                                    return at::vec::Vectorized&lt;float&gt;::loadu(tmpbuf.data());
                                }
                                ()
                                ;
                                return tmp5;
                            }
                            ;
                            auto tmp6 =
                            [&amp;]
                            {
                                if (all_zero(to_float_mask(tmp3)))
                                {
                                    return at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(0.0));
                                }
                                else
                                {
                                    return decltype(tmp4())::blendv(at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(0.0)), tmp4(), to_float_mask(tmp3));
                                }
                            }
                            ()
                            ;
                            tmp_acc0_vec = at::vec::maximum(tmp_acc0_vec, tmp6);
                        }
                        tmp_acc0_vec.store(out_ptr0 + static_cast&lt;long&gt;(x1 + (1024L*x0)));
                    }
                }
            }
        }</code><br />
After change<br />
<code>c++
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(4L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(1024L); x1+=static_cast&lt;long&gt;(16L))
                {
                    {
                        #pragma omp declare reduction(max:at::vec::Vectorized&lt;float&gt;:omp_out = at::vec::maximum(omp_out, omp_in)) initializer(omp_priv={at::vec::Vectorized&lt;float&gt;(-std::numeric_limits&lt;float&gt;::infinity())})
                        float tmp_acc0 = -std::numeric_limits&lt;float&gt;::infinity();
                        at::vec::Vectorized&lt;float&gt; tmp_acc0_vec = at::vec::Vectorized&lt;float&gt;(-std::numeric_limits&lt;float&gt;::infinity());
                        for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(1024L); x2+=static_cast&lt;long&gt;(1L))
                        {
                            auto tmp0 = c10::convert&lt;int&gt;(31L + (63L*(c10::div_floor_integer(x1, 32L))) + (c10::div_floor_integer(x2, 32L)));
                            auto tmp1 = static_cast&lt;int&gt;(2048);
                            auto tmp2 = tmp0 &lt; tmp1;
                            auto tmp3 = [&amp;]
                            {
                                auto tmp4 =
                                [&amp;]
                                {
                                    __at_align__ std::array&lt;float, 16&gt; tmpbuf;
                                    #pragma GCC unroll 16
                                    for (long x1_inner = 0; x1_inner &lt; 16; x1_inner++)
                                    {
                                        if (tmp2 != 0)
                                        {
                                            tmpbuf[x1_inner] = in_ptr0[static_cast&lt;long&gt;(31L + (63L*(c10::div_floor_integer((x1 + x1_inner), 32L))) + (2048L*(static_cast&lt;long&gt;((x1 + x1_inner)) % static_cast&lt;long&gt;(32L))) + (65536L*x0) + (c10::div_floor_integer(x2, 32L)))];
                                        }
                                    }
                                    return at::vec::Vectorized&lt;float&gt;::loadu(tmpbuf.data());
                                }
                                ()
                                ;
                                return tmp4;
                            }
                            ;
                            auto tmp5 =
                            [&amp;]
                            {
                                if (all_zero(to_float_mask(tmp2)))
                                {
                                    return at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(0.0));
                                }
                                else
                                {
                                    return decltype(tmp3())::blendv(at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(0.0)), tmp3(), to_float_mask(tmp2));
                                }
                            }
                            ()
                            ;
                            tmp_acc0_vec = at::vec::maximum(tmp_acc0_vec, tmp5);
                        }
                        tmp_acc0_vec.store(out_ptr0 + static_cast&lt;long&gt;(x1 + (1024L*x0)));
                    }
                }
            }
        }</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116387<br />
Approved by: https://github.com/EikanWang, https://github.com/lezcano<br />
ghstack dependencies: #114545</p>]]></description>
      <pubDate>Tue, 26 Dec 2023 00:45:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4c6e842496da636123f83ef868ca1974631f1f1e</guid>
    </item>
    <item>
      <title>[inductor cpp] support vectorization for index_expr that depends on tiling itervar or with indirect indexing (#114545)</title>
      <link>https://github.com/pytorch/pytorch/commit/ffe6f9ac916d5bec4e07064ec73706989c842919</link>
      <description><![CDATA[<p>[inductor cpp] support vectorization for index_expr that depends on tiling itervar or with indirect indexing (#114545)</p>
<p>As the title, this PR enables vectorization for the situation when the the index_expr depends on vectorized itervar. There are two cases here:<br />
1. The vectorized itervar has constant stride in the index_expr. We vectorize the index_expr with <code>Vectorized&lt;int32&gt;::arange</code> for this case.<br />
2. Otherwise, we load the index_expr vector in a non-contiguous way with a loop.</p>
<p>Below is the generated code for the first case from the test <code>test_concat_inner_vec</code>. Here <code>x1</code> is the index_expr and depends on the vectorized itervar <code>x1</code>. It has constant stride 1. We vectorized it with arange. We use <code>all_zero</code> to implement a short-cut for masks to avoid unnecessary execution of nested masked regions which are invalid.<br />
Before:<br />
<code>c++
            #pragma omp for  collapse(2)
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(32L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(155L); x1+=static_cast&lt;long&gt;(1L))
                {
                    auto tmp0 = c10::convert&lt;long&gt;(x1);
                    auto tmp1 = static_cast&lt;long&gt;(0);
                    auto tmp2 = tmp0 &gt;= tmp1;
                    auto tmp3 = static_cast&lt;long&gt;(35);
                    auto tmp4 = tmp0 &lt; tmp3;
                    auto tmp5 = [&amp;]
                    {
                        auto tmp6 = in_ptr0[static_cast&lt;long&gt;(x1 + (35L*x0))];
                        return tmp6;
                    }
                    ;
                    auto tmp7 = tmp4 ? tmp5() : static_cast&lt;decltype(tmp5())&gt;(0.0);
                    auto tmp8 = tmp0 &gt;= tmp3;
                    auto tmp9 = static_cast&lt;long&gt;(155);
                    auto tmp10 = tmp0 &lt; tmp9;
                    auto tmp11 = [&amp;]
                    {
                        auto tmp12 = in_ptr1[static_cast&lt;long&gt;((-35L) + x1 + (120L*x0))];
                        return tmp12;
                    }
                    ;
...</code><br />
After:<br />
<code>c++
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(32L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(144L); x1+=static_cast&lt;long&gt;(16L))
                {
                    auto tmp0 = c10::convert&lt;int&gt;(x1);
                    auto tmp1 = at::vec::Vectorized&lt;int32_t&gt;::arange(tmp0, 1);
                    auto tmp2 = static_cast&lt;int&gt;(0);
                    auto tmp3 = at::vec::Vectorized&lt;int&gt;(tmp2);
                    auto tmp4 = to_float_mask(tmp1 &gt;= tmp3);
                    auto tmp5 = static_cast&lt;int&gt;(35);
                    auto tmp6 = at::vec::Vectorized&lt;int&gt;(tmp5);
                    auto tmp7 = to_float_mask(tmp1 &lt; tmp6);
                    auto tmp8 = [&amp;]
                    {
                        auto tmp9 = masked_load(in_ptr0 + static_cast&lt;long&gt;(x1 + (35L*x0)), to_float_mask(tmp7));
                        return tmp9;
                    }
                    ;
                    auto tmp10 =
                    [&amp;]
                    {
                        if (all_zero(to_float_mask(tmp7)))
                        {
                            return at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(0.0));
                        }
                        else
                        {
                            return decltype(tmp8())::blendv(at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(0.0)), tmp8(), to_float_mask(tmp7));
                        }
                    }
                    ()
                    ;
...</code></p>
<p>Below is the generated code for the second case from the test case <code>test_expr_vec_non_contiguous</code>. Here, the index_expr is <code>31L + (63L*(c10::div_floor_integer(x1, 32L))) + (c10::div_floor_integer(x2, 32L))</code> which depends on the vectorized itervar <code>x2</code> and doesn't have constant stride. So, we load the index_expr vector with a loop. (In fact, this can be further optimized since the index_expr is invariant with the data points in the range [x2, x2+16). So it can be regarded as a scalar. This will be optimized in the follow-up PR.) The code uses <code>vector_lane_mask_check</code> to implement the masked version of non-contiguous load.<br />
Before:<br />
<code>c++
            #pragma omp for  collapse(2)
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(4L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(1024L); x1+=static_cast&lt;long&gt;(1L))
                {
                    {
                        float tmp_acc0 = -std::numeric_limits&lt;float&gt;::infinity();
                        for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(1024L); x2+=static_cast&lt;long&gt;(1L))
                        {
                            auto tmp0 = c10::convert&lt;long&gt;(31L + (63L*(c10::div_floor_integer(x1, 32L))) + (c10::div_floor_integer(x2, 32L)));
                            auto tmp1 = static_cast&lt;long&gt;(2048);
                            auto tmp2 = tmp0 &lt; tmp1;
                            auto tmp3 = [&amp;]
                            {
                                auto tmp4 = in_ptr0[static_cast&lt;long&gt;(31L + (63L*(c10::div_floor_integer(x1, 32L))) + (2048L*(static_cast&lt;long&gt;(x1) % static_cast&lt;long&gt;(32L))) + (65536L*x0) + (c10::div_floor_integer(x2, 32L)))];
                                return tmp4;
                            }
                            ;
                            auto tmp5 = tmp2 ? tmp3() : static_cast&lt;decltype(tmp3())&gt;(0.0);
                            tmp_acc0 = max_propagate_nan(tmp_acc0, tmp5);
                        }
                        out_ptr0[static_cast&lt;long&gt;(x1 + (1024L*x0))] = tmp_acc0;
                    }
                }
            }</code><br />
After:<br />
<code>c++
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(4L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(1024L); x1+=static_cast&lt;long&gt;(16L))
                {
                    {
                        #pragma omp declare reduction(max:at::vec::Vectorized&lt;float&gt;:omp_out = at::vec::maximum(omp_out, omp_in)) initializer(omp_priv={at::vec::Vectorized&lt;float&gt;(-std::numeric_limits&lt;float&gt;::infinity())})
                        float tmp_acc0 = -std::numeric_limits&lt;float&gt;::infinity();
                        at::vec::Vectorized&lt;float&gt; tmp_acc0_vec = at::vec::Vectorized&lt;float&gt;(-std::numeric_limits&lt;float&gt;::infinity());
                        for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(1024L); x2+=static_cast&lt;long&gt;(1L))
                        {
                            auto tmp0 =
                            [&amp;]
                            {
                                __at_align__ std::array&lt;int, 16&gt; tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner &lt; 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = static_cast&lt;long&gt;(31L + (63L*(c10::div_floor_integer((x1 + x1_inner), 32L))) + (c10::div_floor_integer(x2, 32L)));
                                }
                                return at::vec::Vectorized&lt;int&gt;::loadu(tmpbuf.data());
                            }
                            ()
                            ;
                            auto tmp1 = static_cast&lt;int&gt;(2048);
                            auto tmp2 = at::vec::Vectorized&lt;int&gt;(tmp1);
                            auto tmp3 = to_float_mask(tmp0 &lt; tmp2);
                            auto tmp4 = [&amp;]
                            {
                                auto tmp5 =
                                [&amp;]
                                {
                                    __at_align__ std::array&lt;float, 16&gt; tmpbuf;
                                    #pragma GCC unroll 16
                                    for (long x1_inner = 0; x1_inner &lt; 16; x1_inner++)
                                    {
                                        if (vector_lane_mask_check(tmp3, x1_inner))
                                        {
                                            tmpbuf[x1_inner] = in_ptr0[static_cast&lt;long&gt;(31L + (63L*(c10::div_floor_integer((x1 + x1_inner), 32L))) + (2048L*(static_cast&lt;long&gt;((x1 + x1_inner)) % static_cast&lt;long&gt;(32L))) + (65536L*x0) + (c10::div_floor_integer(x2, 32L)))];
                                        }
                                    }
                                    return at::vec::Vectorized&lt;float&gt;::loadu(tmpbuf.data());
                                }
                                ()
                                ;
                                return tmp5;
                            }
                            ;
                            auto tmp6 =
                            [&amp;]
                            {
                                if (all_zero(to_float_mask(tmp3)))
                                {
                                    return at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(0.0));
                                }
                                else
                                {
                                    return decltype(tmp4())::blendv(at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(0.0)), tmp4(), to_float_mask(tmp3));
                                }
                            }
                            ()
                            ;
                            tmp_acc0_vec = at::vec::maximum(tmp_acc0_vec, tmp6);
                        }
                        tmp_acc0_vec.store(out_ptr0 + static_cast&lt;long&gt;(x1 + (1024L*x0)));
                    }
                }
            }
        }</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114545<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Mon, 25 Dec 2023 21:36:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ffe6f9ac916d5bec4e07064ec73706989c842919</guid>
    </item>
    <item>
      <title>[Inductor] [Quant] Fix QConv Binary Inplace Layout Issue (#115613)</title>
      <link>https://github.com/pytorch/pytorch/commit/81cebca3d26b144ec0d4a98dcc11b0dd89af9b63</link>
      <description><![CDATA[<p>[Inductor] [Quant] Fix QConv Binary Inplace Layout Issue (#115613)</p>
<p>This pull request primarily addresses two issues to resolve the <code>QConvPointWiseBinaryPT2E</code> layout problem:</p>
<ul>
<li>
<p>As the changes made in https://github.com/pytorch/pytorch/commit/611a7457cad52d1aa81291f772a49dd280c17ba1, for <code>QConvPointWiseBinaryPT2E</code> with post-op <code>sum</code>, we should also utilize <code>NoneLayout</code> and return <code>accum</code> instead of <code>QConvPointWiseBinaryPT2E</code>.</p>
</li>
<li>
<p>Additionally, this pull request fixes an issue in the <code>_quantized_convolution_onednn</code> implementation. Given that we expect <code>accum</code> to be inplace changed, we should avoid copying <code>accum</code> by changing the memory format or data type inside the kernel implementation. Instead, we have moved the necessary changes of memory format or data type to the lowering of <code>QConvPointWiseBinaryPT2E</code>.</p>
</li>
</ul>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115613<br />
Approved by: https://github.com/jgong5, https://github.com/oulgen<br />
ghstack dependencies: #116172</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 00:04:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/81cebca3d26b144ec0d4a98dcc11b0dd89af9b63</guid>
    </item>
    <item>
      <title>[inductor] Do variance calculation in opmath type (#115181)</title>
      <link>https://github.com/pytorch/pytorch/commit/4f4b931aba66ae438aae8daca1dcbebeabb947e4</link>
      <description><![CDATA[<p>[inductor] Do variance calculation in opmath type (#115181)</p>
<p>Fixes #114903</p>
<p>Previously large split variance reductions stored the intermediates as float16<br />
precision, which may lead to overflow as the intermediate result is<br />
unnormalized.</p>
<p>In #114903 we see two different <code>num_split</code> decisions made based on the<br />
hardware capabilities, one of which has large enough intermediates to cause<br />
overflows.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115181<br />
Approved by: https://github.com/shunting314</p>]]></description>
      <pubDate>Fri, 22 Dec 2023 17:06:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4f4b931aba66ae438aae8daca1dcbebeabb947e4</guid>
    </item>
    <item>
      <title>[sigmoid] Remove workaround for constant output. (#116288)</title>
      <link>https://github.com/pytorch/pytorch/commit/65c5eed01d3bf1b971f97c143beaa9dd8b0be0df</link>
      <description><![CDATA[<p>[sigmoid] Remove workaround for constant output. (#116288)</p>
<p>Summary: no more workaround_export_bug_constant_buffer_output</p>
<p>Test Plan:<br />
buck2 run mode/dev-nosan //scripts/ads_pt2_inference:pt2_cli -- --src_model manifold://ads_storage_fblearner/tree/user/facebook/fblearner/predictor/473164617/6/gpu_lowering/input.predictor.disagg.gpu.merge</p>
<p>buck2 run mode/opt caffe2/torch/fb/model_transform/fx2trt/packaging:generate_merge_net_file -- --action=generate --lower_backend=aot_inductor_ep --input_file=/data/users/zhxchen17/fbsource/fbcode/input.predictor.disagg.gpu.merge --output_file=/tmp/409501788_66.predictor.disagg.gpu.merge</p>
<p>buck2 run mode/opt -c fbcode.nvcc_arch=a100 caffe2/torch/fb/model_transform/fx2trt/packaging:load_merge_net_predictor -- --loadMode=Normal --inputMergeNetFile=/tmp/409501788_66.predictor.disagg.gpu.merge --pytorch_predictor_sigmoid_enabled=true</p>
<p>Reviewed By: khabinov, SherlockNoMad</p>
<p>Differential Revision: D52210429</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116288<br />
Approved by: https://github.com/tugsbayasgalan</p>]]></description>
      <pubDate>Fri, 22 Dec 2023 12:33:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/65c5eed01d3bf1b971f97c143beaa9dd8b0be0df</guid>
    </item>
    <item>
      <title>[Inductor Intel GPU backend Upstream] Step 1/3:  Generalize device-bias code in code generation. (#116020)</title>
      <link>https://github.com/pytorch/pytorch/commit/7a6cb9fdfb01ab6e9ce55f1fb86348f1a6be8735</link>
      <description><![CDATA[<p>[Inductor Intel GPU backend Upstream] Step 1/3:  Generalize device-bias code in code generation. (#116020)</p>
<p>As the <a href="https://github.com/pytorch/pytorch/issues/114856">RFC</a> mentions, this is the step 1 to add Intel GPU backend as an alternative inductor backend.</p>
<h3>Design</h3>
<p>Typically, in order to integrate Intel GPU backend into Inductor, we need to inherit from <code>WrapperCodegen</code> and <code>TritonScheduling</code> and implement the corresponding subclasses respectively. However, since <code>WrapperCodegen</code> and <code>TritonScheduling</code> have some device-bias code generation <strong>scattered</strong> in their methods, overriding them in subclasses would introduce a lot of duplicated parent class code.<br />
For example:<br />
https://github.com/pytorch/pytorch/blob/2a440348958b3f0a2b09458bd76fe5959b371c0c/torch/_inductor/codegen/wrapper.py#L487</p>
<p>https://github.com/pytorch/pytorch/blob/2a440348958b3f0a2b09458bd76fe5959b371c0c/torch/_inductor/codegen/triton.py#L1996</p>
<p>So we abstract the device-bias code scattered in WrapperCodegen and TritonScheduling and provide a unified interface "DeviceOpOverrides". This way, when integrating a new backend, we can  maximize the reuse of <code>WrapperCodegen</code> and <code>TritonScheduling</code> code by inherit and implement this interface for device flexibility.</p>
<p>Currently the <code>DeviceOpOverrides</code> only cover Python wrapper code generation. We can futher extend it to cover Cpp wrapper code generation on demand.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116020<br />
Approved by: https://github.com/jgong5, https://github.com/EikanWang, https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 22 Dec 2023 00:42:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7a6cb9fdfb01ab6e9ce55f1fb86348f1a6be8735</guid>
    </item>
    <item>
      <title>Make native c10d_functional ops work with AOTInductor (#113735)</title>
      <link>https://github.com/pytorch/pytorch/commit/7d0ad6e87008e96c3edc33b8d46e1d2553e3deb9</link>
      <description><![CDATA[<p>Make native c10d_functional ops work with AOTInductor (#113735)</p>
<p>Summary:<br />
- Revised <code>c10d_functional</code> ops to conform to https://github.com/pytorch/pytorch/tree/main/aten/src/ATen/native#func<br />
- Modifed <code>get_cpp_op_schema()</code> to handle mutable args and aliasing returns</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113735<br />
Approved by: https://github.com/desertfire<br />
ghstack dependencies: #113438</p>]]></description>
      <pubDate>Fri, 22 Dec 2023 00:12:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7d0ad6e87008e96c3edc33b8d46e1d2553e3deb9</guid>
    </item>
    <item>
      <title>Port all_to_all_single to native c10d_functional (#113438)</title>
      <link>https://github.com/pytorch/pytorch/commit/718b576e2cbb7388d6ff7b0b94188944f836bff8</link>
      <description><![CDATA[<p>Port all_to_all_single to native c10d_functional (#113438)</p>
<p>Summary:<br />
- Ported <code>all_to_all_single</code> to native c10d_functional<br />
- Added Inductor support for the native <code>all_to_all_single</code> via the new collective IR's <code>create_out_of_place()</code><br />
- Since the new collective IR derives from <code>FallbackKernel</code> which implements a generic <code>free_unbacked_symbols</code>, no additional unbacked symbol handling for all_to_all_single is required</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113438<br />
Approved by: https://github.com/yf225, https://github.com/ezyang</p>]]></description>
      <pubDate>Fri, 22 Dec 2023 00:12:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/718b576e2cbb7388d6ff7b0b94188944f836bff8</guid>
    </item>
    <item>
      <title>[inductor] make inductor work with new triton compile interface (#115878)</title>
      <link>https://github.com/pytorch/pytorch/commit/99f7e721fef301b02fca2bb88c155136b3d24f0d</link>
      <description><![CDATA[<p>[inductor] make inductor work with new triton compile interface (#115878)</p>
<p>Recent 2 triton PRs (https://github.com/openai/triton/pull/2701, https://github.com/openai/triton/pull/2756) change the interface for triton.compile, this PR added the necessary change on inductor side to work with both old and new compile API.</p>
<p>Also there is some simplification between compilation call in subprocess and the one in main process<br />
- previously we pass warm_cache_only=True if the compilation happens in subprocess. But triton never use that argument in the currently used pin. So I removed that<br />
- previously we only pass compute_capability if compilation happens in subprocess. The PR change that to always passing compute_capability to triton.compile no matter if the compilation happens in main or sub process.</p>
<p>Updated:<br />
There are more interface change from triton side. E.g.<br />
- tl.math.{min, max} now requires a propagate_nan argument<br />
- JITFunction.run now requires a warmup argument. This affect the benchmarking phase of matmul max-autotune; on the other hand, JITFunction.run forbids stream argument now. Simply removing passing this in when benchmarking matmul triton kernel will work for both old and new version of triton.<br />
- triton Autotuner change attribute name from 'warmup' to 'num_warmup' and from 'rep' to 'num_rep'. This cause dynamo failed to handle triton Autotuner object since dynamo TritonKernelVariable makes assumption about attribute names. It's used in some test cases that a model call triton Autotuner directly.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115878<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 21 Dec 2023 16:09:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/99f7e721fef301b02fca2bb88c155136b3d24f0d</guid>
    </item>
    <item>
      <title>Preserve strides of custom Triton kernel args (#116219)</title>
      <link>https://github.com/pytorch/pytorch/commit/247f9c3de488e0206b5f99e19f4bdb94f90d4928</link>
      <description><![CDATA[<p>Preserve strides of custom Triton kernel args (#116219)</p>
<p>Summary: Currently, we <a href="https://github.com/pytorch/pytorch/blob/19207b9183baced683d675c341b7da051cd16440/torch/_inductor/lowering.py#L5273"><code>clone</code></a> every <code>TensorBox</code> argument of custom Triton kernels while lowering them to the Inductor IR, during which the stride information of the kernel inputs is lost. This is problematic in the common case when the strides of a <code>torch.Tensor</code> argument are passed as scalars to a custom Triton kernel alongside the tensor itself (due to the underlying Triton code interpreting the tensors as raw pointers, so the contained stride semantics of the <code>torch.Tensor</code> is lost).</p>
<p>In this PR, we add an extended version of the existing <a href="https://github.com/pytorch/pytorch/blob/19207b9183baced683d675c341b7da051cd16440/torch/_inductor/lowering.py#L2289"><code>clone</code> lowering</a>---<code>clone_preserve_reinterpret_view</code>---which carries over the <code>ir.ReinterpretVew</code> layers (if any) from the source <code>TensorBox</code> to the cloned one. The rationale behind adding a new function (and switching to it in the <code>triton_kernel_wrap</code> only for now) as opposed to extending the existing <code>clone</code> is keeping the semantics of the latter untouched, as it is a lowering of <code>torch.clone</code> (albeit incomplete, as the <code>memory_format</code> is currently ignored). Changing the existing <code>clone</code> would change the semantics which is not necessarily desirable in general. Open to suggestions, though.</p>
<p>Test Plan:</p>
<p>```<br />
$ python test/dynamo/test_functions.py -k test_triton_kernel_strided_input<br />
...</p>
<hr />
<p>Ran 1 test in 5.568s</p>
<p>OK<br />
```</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116219<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 21 Dec 2023 14:46:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/247f9c3de488e0206b5f99e19f4bdb94f90d4928</guid>
    </item>
    <item>
      <title>[dynamo / DDP] Add optimize_ddp_lazy_compile config to control lazy compile for DDPOptimizer (False by default) (#116292)</title>
      <link>https://github.com/pytorch/pytorch/commit/a27ed4d36446597ea38c56786465abdb03181168</link>
      <description><![CDATA[<p>[dynamo / DDP] Add optimize_ddp_lazy_compile config to control lazy compile for DDPOptimizer (False by default) (#116292)</p>
<p>We want to enable <code>optimize_ddp_lazy_compile</code> by default as soon as possible, becuase it will fix stride mismatch errors (see motivation: https://github.com/pytorch/pytorch/pull/114154).</p>
<p>However, lazy compile currently causes shape mismatch in other cases (<code>test_graph_split_inductor_transpose</code>) and we need to fix them before we can enable it by default.</p>
<p>Differential Revision: D52373445</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116292<br />
Approved by: https://github.com/williamwen42, https://github.com/wconstab</p>]]></description>
      <pubDate>Thu, 21 Dec 2023 14:34:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a27ed4d36446597ea38c56786465abdb03181168</guid>
    </item>
    <item>
      <title>Revert "[innductor] make inductor work with new triton compile interface (#115878)"</title>
      <link>https://github.com/pytorch/pytorch/commit/db35ccf46334164d0b02e5be9450193d881edb40</link>
      <description><![CDATA[<p>Revert "[innductor] make inductor work with new triton compile interface (#115878)"</p>
<p>This reverts commit bbded928b3556cf5678edf8fa41109d418312bcc.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/115878 on behalf of https://github.com/kit1980 due to Broke ROCm https://github.com/pytorch/pytorch/actions/runs/7282149837/job/19844618618 (<a href="https://github.com/pytorch/pytorch/pull/115878#issuecomment-1865369349">comment</a>)</p>]]></description>
      <pubDate>Wed, 20 Dec 2023 18:00:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/db35ccf46334164d0b02e5be9450193d881edb40</guid>
    </item>
    <item>
      <title>[Quant] [Inductor] add input shape check for quantized conv binary lowering (#115247)</title>
      <link>https://github.com/pytorch/pytorch/commit/c1d960aadd23bb261724c11f34d4a852ffaacfd6</link>
      <description><![CDATA[<p>[Quant] [Inductor] add input shape check for quantized conv binary lowering (#115247)</p>
<p>Add inputs shape check for quantized conv binary lowering, since qconv2d_pointwise.binary does not yet support the case of broadcasting shape inputs.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115247<br />
Approved by: https://github.com/leslie-fang-intel, https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 20 Dec 2023 17:36:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c1d960aadd23bb261724c11f34d4a852ffaacfd6</guid>
    </item>
    <item>
      <title>[innductor] make inductor work with new triton compile interface (#115878)</title>
      <link>https://github.com/pytorch/pytorch/commit/bbded928b3556cf5678edf8fa41109d418312bcc</link>
      <description><![CDATA[<p>[innductor] make inductor work with new triton compile interface (#115878)</p>
<p>Recent 2 triton PRs (https://github.com/openai/triton/pull/2701, https://github.com/openai/triton/pull/2756) change the interface for triton.compile, this PR added the necessary change on inductor side to work with both old and new compile API.</p>
<p>Also there is some simplification between compilation call in subprocess and the one in main process<br />
- previously we pass warm_cache_only=True if the compilation happens in subprocess. But triton never use that argument in the currently used pin. So I removed that<br />
- previously we only pass compute_capability if compilation happens in subprocess. The PR change that to always passing compute_capability to triton.compile no matter if the compilation happens in main or sub process.</p>
<p>Updated:<br />
There are more interface change from triton side. E.g.<br />
- tl.math.{min, max} now requires a propagate_nan argument<br />
- JITFunction.run now requires a warmup argument. This affect the benchmarking phase of matmul max-autotune; on the other hand, JITFunction.run forbids stream argument now. Simply removing passing this in when benchmarking matmul triton kernel will work for both old and new version of triton.<br />
- triton Autotuner change attribute name from 'warmup' to 'num_warmup' and from 'rep' to 'num_rep'. This cause dynamo failed to handle triton Autotuner object since dynamo TritonKernelVariable makes assumption about attribute names. It's used in some test cases that a model call triton Autotuner directly.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115878<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 20 Dec 2023 16:03:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bbded928b3556cf5678edf8fa41109d418312bcc</guid>
    </item>
    <item>
      <title>[inductor] Some tests have both CPU and CUDA variants running with CPU tensors (#116131)</title>
      <link>https://github.com/pytorch/pytorch/commit/897600eb350997f77553d537b93051ec31ff262b</link>
      <description><![CDATA[<p>[inductor] Some tests have both CPU and CUDA variants running with CPU tensors (#116131)</p>
<p>I don't think that's intended.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116131<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 20 Dec 2023 16:00:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/897600eb350997f77553d537b93051ec31ff262b</guid>
    </item>
    <item>
      <title>[inductor] Support sym exprs in lowering constant promotion (#116196)</title>
      <link>https://github.com/pytorch/pytorch/commit/b72127cd4b56a4e6978ec3d46bc8daf7c75e0c08</link>
      <description><![CDATA[<p>[inductor] Support sym exprs in lowering constant promotion (#116196)</p>
<p>Follow-up to https://github.com/pytorch/pytorch/pull/115920</p>
<p>This PR fixes the error with symbolic expression in aten.div:<br />
```python<br />
import torch<br />
aten = torch.ops.aten</p>
<p>def func(x, a):<br />
    return aten.div(x * 0.5, a, rounding_mode=None)</p>
<p>cfunc = torch.compile(func, dynamic=True, fullgraph=True)<br />
device = "cpu"<br />
x = 124<br />
a = 33<br />
out = cfunc(x, a)<br />
expected = func(x, a)<br />
torch.testing.assert_close(out, expected)<br />
<code>Error message:</code><br />
  File "/pytorch/torch/_inductor/graph.py", line 700, in call_function<br />
    out = lowerings<a href="*args, **kwargs">target</a><br />
  File "/pytorch/torch/_inductor/lowering.py", line 293, in wrapped<br />
    out = decomp_fn(<em>args, </em><em>kwargs)<br />
  File "/pytorch/torch/_inductor/lowering.py", line 4823, in div_mode<br />
    return div(a, b)<br />
  File "/pytorch/torch/_inductor/lowering.py", line 293, in wrapped<br />
    out = decomp_fn(</em>args, *<em>kwargs)<br />
  File "/pytorch/torch/_inductor/lowering.py", line 4857, in div<br />
    a, b = promote_constants(<br />
  File "/pytorch/torch/_inductor/lowering.py", line 368, in promote_constants<br />
    ex = next(x for x in inputs if isinstance(x, (TensorBox, ExpandView)))<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
LoweringException: StopIteration:<br />
  target: aten.div.Tensor_mode<br />
  args[0]: 1.0</em>s0<br />
  args[1]: s1<br />
  kwargs: {'rounding_mode': None}</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116196<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Wed, 20 Dec 2023 13:59:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b72127cd4b56a4e6978ec3d46bc8daf7c75e0c08</guid>
    </item>
    <item>
      <title>Revert "[inductor] Avoid bool being upcast to int (#109913)"</title>
      <link>https://github.com/pytorch/pytorch/commit/c215e59bf270334f9c17472ce77ec3a3b7d76d9e</link>
      <description><![CDATA[<p>Revert "[inductor] Avoid bool being upcast to int (#109913)"</p>
<p>This reverts commit 92998693a9455af6259cae468265f01cfff8810e.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/109913 on behalf of https://github.com/jeanschmidt due to causing performance regression in relevant metrics, @malfet I believe you are the correct person to help identify and fix the issues. More details check internal OPS count for ads metricsnin the internal related diff (<a href="https://github.com/pytorch/pytorch/pull/109913#issuecomment-1864397407">comment</a>)</p>]]></description>
      <pubDate>Wed, 20 Dec 2023 04:33:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c215e59bf270334f9c17472ce77ec3a3b7d76d9e</guid>
    </item>
    <item>
      <title>[Inductor] Deduplicate grid wrapper statements for user defined triton kernels (#115849)</title>
      <link>https://github.com/pytorch/pytorch/commit/c55210b4f0b3de7f7d9e57b2e298880f45a11bba</link>
      <description><![CDATA[<p>[Inductor] Deduplicate grid wrapper statements for user defined triton kernels (#115849)</p>
<p>Noticed that on many MRS kernels the grid wrapper for autotuning is huge with a bunch of duplicates due to num_warps and num_stages not being needed for grid calculation. Lets deduplicate these entries.</p>
<p>Previously, we would see wrapper like<br />
<code>def grid_wrapper_for_add_kernel_2d_autotuned_0(meta):
        if meta['BLOCK_SIZE_X'] == 128 and meta['BLOCK_SIZE_Y'] == 128: return (4, 2, 1)
        if meta['BLOCK_SIZE_X'] == 128 and meta['BLOCK_SIZE_Y'] == 128: return (4, 2, 1)
        if meta['BLOCK_SIZE_X'] == 64 and meta['BLOCK_SIZE_Y'] == 64: return (8, 4, 1)
        if meta['BLOCK_SIZE_X'] == 64 and meta['BLOCK_SIZE_Y'] == 64: return (8, 4, 1)</code><br />
now it looks like<br />
<code>def grid_wrapper_for_add_kernel_2d_autotuned_0(meta):
        if meta['BLOCK_SIZE_X'] == 128 and meta['BLOCK_SIZE_Y'] == 128: return (4, 2, 1)
        if meta['BLOCK_SIZE_X'] == 64 and meta['BLOCK_SIZE_Y'] == 64: return (8, 4, 1)</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115849<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 19 Dec 2023 16:25:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c55210b4f0b3de7f7d9e57b2e298880f45a11bba</guid>
    </item>
    <item>
      <title>Reset stepcurrent cache if file succeeds (#115775)</title>
      <link>https://github.com/pytorch/pytorch/commit/7f7a7b0b482a21760d5005fbf83082ee3cf871ed</link>
      <description><![CDATA[<p>Reset stepcurrent cache if file succeeds (#115775)</p>
<p>Attempt to surface the segfault that happens on exit by resetting the "pytest last run" cache if pytest succeeds.  CI does not rerun on success so we won't hit an infinite loop anywhere, and I don't expect people to rerun on success (unless they're looking for flakes? Either way I highly doubt any one is using the --sc/--scs flag locally).</p>
<p>This ensures that if pytest succeeds but the process gets a non zero exit code, the rerun will start at beginning instead of skipping all the "succeeding" tests.</p>
<p>This only applies if the --sc/--scs flags are used, custom to pytorch and probably not used anywhere other than CI, not to be confused with --stepwise, which pytest has by default</p>
<p>Here's a list of segfaulting inductor/test_aot_inductor tests, which I added skips for:<br />
<code>inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocation::test_duplicated_params_abi_compatible_cpu_with_stack_allocation
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocation::test_fqn_abi_compatible_cpu_with_stack_allocation
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocation::test_no_args_abi_compatible_cpu_with_stack_allocation
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocation::test_output_misaligned_abi_compatible_cpu_with_stack_allocation
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocation::test_pytree_inputs_abi_compatible_cpu_with_stack_allocation
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocation::test_seq_abi_compatible_cpu_with_stack_allocation
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocation::test_simple_split_abi_compatible_cpu_with_stack_allocation
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_addmm_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_aliased_buffer_reuse_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_buffer_reuse_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_convolution_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_duplicated_params_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_empty_graph_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_fqn_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_large_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_missing_output_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_no_args_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_output_misaligned_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_output_path_1_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_pytree_inputs_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_repeat_interleave_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_return_constant_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_reuse_kernel_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_seq_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_simple_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_simple_split_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_small_constant_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_with_no_triton_profiler_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_with_offset_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_with_profiler_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface
inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpuWithStackAllocationAndMinimalArrayRefInterface::test_zero_size_weight_abi_compatible_cpu_with_stack_allocation_and_minimal_arrayref_interface</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115775<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 19 Dec 2023 14:19:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7f7a7b0b482a21760d5005fbf83082ee3cf871ed</guid>
    </item>
    <item>
      <title>[Inductor UT] fix unreachable code (#116094)</title>
      <link>https://github.com/pytorch/pytorch/commit/71bedc3a69e3203fd8f76a68ecf2bd7c58d2e13e</link>
      <description><![CDATA[<p>[Inductor UT] fix unreachable code (#116094)</p>
<p>The testcase test_uint4x2_mixed_mm has indentation error. This pr make testcode reachable.</p>
<p>test result:<br />
```<br />
pytest test_torchinductor.py -k test_uint4x2_mixed_mm -v<br />
=========================================================================================== test session starts ===========================================================================================<br />
platform linux -- Python 3.10.12, pytest-7.4.2, pluggy-1.3.0 -- /usr/bin/python<br />
cachedir: .pytest_cache<br />
hypothesis profile 'default' -&gt; database=DirectoryBasedExampleDatabase('/workspace/pytorch/test/inductor/.hypothesis/examples')<br />
rootdir: /workspace/pytorch<br />
configfile: pytest.ini<br />
plugins: shard-0.1.2, xdoctest-1.0.2, flakefinder-1.1.0, xdist-3.3.1, rerunfailures-12.0, hypothesis-5.35.1<br />
collected 964 items / 962 deselected / 2 selected<br />
Running 2 items in this shard: test/inductor/test_torchinductor.py::CpuTests::test_uint4x2_mixed_mm_cpu, test/inductor/test_torchinductor.py::CudaTests::test_uint4x2_mixed_mm_cuda</p>
<p>test_torchinductor.py::CpuTests::test_uint4x2_mixed_mm_cpu PASSED [2.2136s]                                                                                                                         [ 50%]<br />
test_torchinductor.py::CudaTests::test_uint4x2_mixed_mm_cuda PASSED [1.9466s]                                                                                                                       [100%]</p>
<p>=================================================================================== 2 passed, 962 deselected in 15.70s ====================================================================================</p>
<p>```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116094<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Tue, 19 Dec 2023 09:14:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/71bedc3a69e3203fd8f76a68ecf2bd7c58d2e13e</guid>
    </item>
    <item>
      <title>Revert "[Inductor] Deduplicate grid wrapper statements for user defined triton kernels (#115849)"</title>
      <link>https://github.com/pytorch/pytorch/commit/c539f7df10bb114df77e074bd04bcce9bf024c6d</link>
      <description><![CDATA[<p>Revert "[Inductor] Deduplicate grid wrapper statements for user defined triton kernels (#115849)"</p>
<p>This reverts commit 21b8127f1c9f31c02145d906aae2db1ada703067.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/115849 on behalf of https://github.com/jeanschmidt due to Breaking internal tests, please check internal diff for more details (<a href="https://github.com/pytorch/pytorch/pull/115849#issuecomment-1863012933">comment</a>)</p>]]></description>
      <pubDate>Tue, 19 Dec 2023 07:47:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c539f7df10bb114df77e074bd04bcce9bf024c6d</guid>
    </item>
    <item>
      <title>[inductor] Avoid bool being upcast to int (#109913)</title>
      <link>https://github.com/pytorch/pytorch/commit/92998693a9455af6259cae468265f01cfff8810e</link>
      <description><![CDATA[<p>[inductor] Avoid bool being upcast to int (#109913)</p>
<p>Currently the inductor code for <code>x.any(-1)</code> does a this strange dance:<br />
<code>python
tmp0 = tl.load(in_ptr0 + (r1 + (128*x0)), rmask &amp; xmask)
tmp1 = tmp0.to(tl.int64)
tmp2 = (tmp1 != 0)</code></p>
<p>This happens because <code>register_lowering</code> is doing type promotion with the<br />
dimension argument, and so promotes to <code>int64</code> which we then cast back to bool.<br />
A better fix would be to fix <code>register_lowering</code> but for now I just remove<br />
the unnecessary type promotion from <code>aten.any</code>.</p>
<p>In the current code we also see:<br />
<code>python
     tmp5 = tl.where(rmask &amp; xmask, tmp3, 0)</code><br />
which promotes the boolean value to int since <code>0</code> is an int32 in triton.<br />
This fixes it to generate a boolean constant instead.</p>
<p>Finally there is also a triton bug where the <code>tl.load</code> itself upcasts to<br />
<code>tl.int8</code>. I fix this by adding an explicit cast to <code>tl.int1</code>. The final<br />
kernel code looks like:</p>
<p>```python<br />
tmp0 = tl.load(in_ptr0 + (r1 + (128*x0)), rmask &amp; xmask).to(tl.int1)<br />
tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])<br />
tmp3 = tl.full([1, 1], 0, tl.int1)<br />
tmp4 = tl.where(rmask &amp; xmask, tmp1, tmp3)<br />
tmp5 = triton_helpers.any(tmp4, 1)[:, None]</p>
<p>```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109913<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Tue, 19 Dec 2023 06:16:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/92998693a9455af6259cae468265f01cfff8810e</guid>
    </item>
    <item>
      <title>[Inductor] Fix constant folding and extern kernel mutation tracking bugs (#115908)</title>
      <link>https://github.com/pytorch/pytorch/commit/01b979fc9ae615405665e90a70c50a173539ea20</link>
      <description><![CDATA[<p>[Inductor] Fix constant folding and extern kernel mutation tracking bugs (#115908)</p>
<p>This PR fixes two bugs<br />
1) Constant folding a triton kernel results in the kernel's inputs to be returned back without any modification. Disable constant folding for triton kernels. Need more investigation<br />
2) NoneLayout buffers should not be deleted as they do not exist</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115908<br />
Approved by: https://github.com/aakhundov, https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 18 Dec 2023 18:06:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/01b979fc9ae615405665e90a70c50a173539ea20</guid>
    </item>
    <item>
      <title>Adds allreduce to inductor remap (#115950)</title>
      <link>https://github.com/pytorch/pytorch/commit/8452f41305cf1e35e5e666b02c36b4fec589283d</link>
      <description><![CDATA[<p>Adds allreduce to inductor remap (#115950)</p>
<p>Fixes #115728</p>
<p>Implements a rewrite path for allreduce</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115950<br />
Approved by: https://github.com/wconstab</p>]]></description>
      <pubDate>Mon, 18 Dec 2023 14:00:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8452f41305cf1e35e5e666b02c36b4fec589283d</guid>
    </item>
    <item>
      <title>Take 2 of "Add an option to log the source of the Triton kernels generated by torch._inductor (#115979)</title>
      <link>https://github.com/pytorch/pytorch/commit/3b70bd3970b5f464ae02585f0de89e88395936cb</link>
      <description><![CDATA[<p>Take 2 of "Add an option to log the source of the Triton kernels generated by torch._inductor (#115979)</p>
<p>Summary: This is useful the comparing the Triton kernels generated by two different invocations of torch.compile on the same model (e.g., checking of serial compile and parallel compile generate identical Triton kernels).</p>
<p>Test Plan:<br />
Unit test:<br />
buck2 test mode/opt //caffe2/torch/fb/module_factory/sync_sgd/tests:test_torchdynamo_wrapper -- --print-passing-details &gt;&amp; ~/tmp/log.test<br />
PyPer Mast job:<br />
https://www.internalfb.com/mast/job/sw-951074659-OfflineTraining_87587a4e<br />
See the *.py files generated in:<br />
pyper_traces/tree/torchinductor_traces/sw-951074659-OfflineTraining_87587a4e/4623</p>
<p>Differential Revision: D52221500</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115979<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Mon, 18 Dec 2023 10:16:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3b70bd3970b5f464ae02585f0de89e88395936cb</guid>
    </item>
    <item>
      <title>[AOTInductor] Add updaing constant buffer to active buffer. (#116001)</title>
      <link>https://github.com/pytorch/pytorch/commit/c285ca791666623b1352a1abfd0c8148c4c8877f</link>
      <description><![CDATA[<p>[AOTInductor] Add updaing constant buffer to active buffer. (#116001)</p>
<p>Summary:<br />
Refactor update inactive constant buffer to allow updating with active<br />
buffer.</p>
<p>Test Plan:<br />
Existing test to test inactive buffer updates.<br />
UpdateConstantsCuda in cpp test for active buffer updates.</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/116001<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Mon, 18 Dec 2023 03:49:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c285ca791666623b1352a1abfd0c8148c4c8877f</guid>
    </item>
    <item>
      <title>[Inductor][Observability] Change to log.debug to avoid excessive long of logs (#115474)</title>
      <link>https://github.com/pytorch/pytorch/commit/bc4115ffcfabba6fbaa899bb228586e805c69a27</link>
      <description><![CDATA[<p>[Inductor][Observability] Change to log.debug to avoid excessive long of logs (#115474)</p>
<p>Summary: Titled</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D52003825</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115474<br />
Approved by: https://github.com/jackiexu1992, https://github.com/yanboliang</p>]]></description>
      <pubDate>Sat, 16 Dec 2023 16:25:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bc4115ffcfabba6fbaa899bb228586e805c69a27</guid>
    </item>
    <item>
      <title>[TEST] Increase numerical tolerances in test_torchinductor_opinfo:test_comprehensive (#115768)</title>
      <link>https://github.com/pytorch/pytorch/commit/8283491eff569979eae9abc4695e0e9ac9a79bd2</link>
      <description><![CDATA[<p>[TEST] Increase numerical tolerances in test_torchinductor_opinfo:test_comprehensive (#115768)</p>
<p>There are numerical mismatches that causes some tests of <code>test_comprehensive</code> to fail. I propose to just increase tolerances a bit to make them pass.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115768<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 15 Dec 2023 19:00:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8283491eff569979eae9abc4695e0e9ac9a79bd2</guid>
    </item>
    <item>
      <title>[inductor] Fixed issue with true div on integer input with dyn shapes (#115920)</title>
      <link>https://github.com/pytorch/pytorch/commit/2a2f2e454a2d0e91e4fb2602d519bfe1ec02cab7</link>
      <description><![CDATA[<p>[inductor] Fixed issue with true div on integer input with dyn shapes (#115920)</p>
<p>Related to https://github.com/pytorch/pytorch/issues/115742, <code>Cpu/CudaTests.test_div8</code></p>
<p>Description:<br />
- Fixed issue with true div on integer input with dyn shapes</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115920<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Fri, 15 Dec 2023 18:06:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2a2f2e454a2d0e91e4fb2602d519bfe1ec02cab7</guid>
    </item>
    <item>
      <title>[inductor] split test_cpp_wrapper.py into cpu and cuda test files (#115479)</title>
      <link>https://github.com/pytorch/pytorch/commit/715d663794e2b9ce25061216bc5cca3477e0571a</link>
      <description><![CDATA[<p>[inductor] split test_cpp_wrapper.py into cpu and cuda test files (#115479)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115479<br />
Approved by: https://github.com/atalman<br />
ghstack dependencies: #115167</p>]]></description>
      <pubDate>Fri, 15 Dec 2023 13:21:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/715d663794e2b9ce25061216bc5cca3477e0571a</guid>
    </item>
    <item>
      <title>Fixed some failing inductor tests with exact_dtype=True (#115828)</title>
      <link>https://github.com/pytorch/pytorch/commit/6c2103bdf7791b7b2b92f1b6edd53c9f53e3add7</link>
      <description><![CDATA[<p>Fixed some failing inductor tests with exact_dtype=True (#115828)</p>
<p>Addresses point 1 from #115742: fixing  CPUReproTest.test_embedding_vec_bf16</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115828<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Fri, 15 Dec 2023 12:02:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6c2103bdf7791b7b2b92f1b6edd53c9f53e3add7</guid>
    </item>
    <item>
      <title>Revert "markDynamoStrictTest on more tests (#115879)"</title>
      <link>https://github.com/pytorch/pytorch/commit/91b848bf8190747421fa206fa60baebaef638714</link>
      <description><![CDATA[<p>Revert "markDynamoStrictTest on more tests (#115879)"</p>
<p>This reverts commit 8b650cdd3cdd1174b399f312ec2f7955551a2f5d.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/115879 on behalf of https://github.com/atalman due to OSSCI oncall, broke inductor (<a href="https://github.com/pytorch/pytorch/pull/115879#issuecomment-1858418921">comment</a>)</p>]]></description>
      <pubDate>Fri, 15 Dec 2023 12:00:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/91b848bf8190747421fa206fa60baebaef638714</guid>
    </item>
    <item>
      <title>Revert "markDynamoStrictTest some more (#115885)"</title>
      <link>https://github.com/pytorch/pytorch/commit/c006c8b50e4f68351955f0f24766c1c9312c81aa</link>
      <description><![CDATA[<p>Revert "markDynamoStrictTest some more (#115885)"</p>
<p>This reverts commit 55ce4693ff2c0b6e50b8af323f36ecc7ff929638.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/115885 on behalf of https://github.com/atalman due to OSSCI oncall, broke inductor (<a href="https://github.com/pytorch/pytorch/pull/115885#issuecomment-1858409669">comment</a>)</p>]]></description>
      <pubDate>Fri, 15 Dec 2023 11:51:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c006c8b50e4f68351955f0f24766c1c9312c81aa</guid>
    </item>
    <item>
      <title>Revert "[inductor] split test_cpp_wrapper.py into cpu and cuda test files (#115479)"</title>
      <link>https://github.com/pytorch/pytorch/commit/66994bca5f52fcb3b6e5eec965ac7a3a9b2f09a4</link>
      <description><![CDATA[<p>Revert "[inductor] split test_cpp_wrapper.py into cpu and cuda test files (#115479)"</p>
<p>This reverts commit 653acd8fe1d0a7b4a084a47ee022f163015fee64.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/115479 on behalf of https://github.com/desertfire due to will cause land race in fbcode because https://github.com/pytorch/pytorch/pull/115831 is already landed internally (<a href="https://github.com/pytorch/pytorch/pull/115479#issuecomment-1857979948">comment</a>)</p>]]></description>
      <pubDate>Fri, 15 Dec 2023 06:35:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/66994bca5f52fcb3b6e5eec965ac7a3a9b2f09a4</guid>
    </item>
    <item>
      <title>Back out "[aotinductor] replace lld with the default ld linker (#115478)" (#115875)</title>
      <link>https://github.com/pytorch/pytorch/commit/c1c9b739e2426884687f34a866bafc4cb6a86529</link>
      <description><![CDATA[<p>Back out "[aotinductor] replace lld with the default ld linker (#115478)" (#115875)</p>
<p>Summary:<br />
Back out the diff</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115875<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Thu, 14 Dec 2023 21:56:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c1c9b739e2426884687f34a866bafc4cb6a86529</guid>
    </item>
    <item>
      <title>[inductor] split test_cpp_wrapper.py into cpu and cuda test files (#115479)</title>
      <link>https://github.com/pytorch/pytorch/commit/653acd8fe1d0a7b4a084a47ee022f163015fee64</link>
      <description><![CDATA[<p>[inductor] split test_cpp_wrapper.py into cpu and cuda test files (#115479)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115479<br />
Approved by: https://github.com/atalman<br />
ghstack dependencies: #115167</p>]]></description>
      <pubDate>Thu, 14 Dec 2023 20:04:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/653acd8fe1d0a7b4a084a47ee022f163015fee64</guid>
    </item>
    <item>
      <title>[Inductor] Deduplicate grid wrapper statements for user defined triton kernels (#115849)</title>
      <link>https://github.com/pytorch/pytorch/commit/21b8127f1c9f31c02145d906aae2db1ada703067</link>
      <description><![CDATA[<p>[Inductor] Deduplicate grid wrapper statements for user defined triton kernels (#115849)</p>
<p>Noticed that on many MRS kernels the grid wrapper for autotuning is huge with a bunch of duplicates due to num_warps and num_stages not being needed for grid calculation. Lets deduplicate these entries.</p>
<p>Previously, we would see wrapper like<br />
<code>def grid_wrapper_for_add_kernel_2d_autotuned_0(meta):
        if meta['BLOCK_SIZE_X'] == 128 and meta['BLOCK_SIZE_Y'] == 128: return (4, 2, 1)
        if meta['BLOCK_SIZE_X'] == 128 and meta['BLOCK_SIZE_Y'] == 128: return (4, 2, 1)
        if meta['BLOCK_SIZE_X'] == 64 and meta['BLOCK_SIZE_Y'] == 64: return (8, 4, 1)
        if meta['BLOCK_SIZE_X'] == 64 and meta['BLOCK_SIZE_Y'] == 64: return (8, 4, 1)</code><br />
now it looks like<br />
<code>def grid_wrapper_for_add_kernel_2d_autotuned_0(meta):
        if meta['BLOCK_SIZE_X'] == 128 and meta['BLOCK_SIZE_Y'] == 128: return (4, 2, 1)
        if meta['BLOCK_SIZE_X'] == 64 and meta['BLOCK_SIZE_Y'] == 64: return (8, 4, 1)</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115849<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 14 Dec 2023 15:26:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/21b8127f1c9f31c02145d906aae2db1ada703067</guid>
    </item>
    <item>
      <title>[aotinductor] add no weight change version of fuse_parallel_linear (#115791)</title>
      <link>https://github.com/pytorch/pytorch/commit/87547a26b86848f23fc2a7785fef17a07853c16f</link>
      <description><![CDATA[<p>[aotinductor] add no weight change version of fuse_parallel_linear (#115791)</p>
<p>Summary: We need a new version of fuse_parallel_linear w/o creating new weights for real-time update.</p>
<p>Reviewed By: khabinov</p>
<p>Differential Revision: D52128296</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115791<br />
Approved by: https://github.com/khabinov</p>]]></description>
      <pubDate>Thu, 14 Dec 2023 10:36:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/87547a26b86848f23fc2a7785fef17a07853c16f</guid>
    </item>
    <item>
      <title>Revert "[inductor] Do variance calculation in opmath type (#115181)"</title>
      <link>https://github.com/pytorch/pytorch/commit/ca4caf4eac97138e6bc10e8d158bb0f7be7af476</link>
      <description><![CDATA[<p>Revert "[inductor] Do variance calculation in opmath type (#115181)"</p>
<p>This reverts commit 42390a097b987cd3384511c3df3747699f2281f4.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/115181 on behalf of https://github.com/atalman due to OSSCI oncall, broke periodic tests (<a href="https://github.com/pytorch/pytorch/pull/115181#issuecomment-1856360644">comment</a>)</p>]]></description>
      <pubDate>Thu, 14 Dec 2023 10:21:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ca4caf4eac97138e6bc10e8d158bb0f7be7af476</guid>
    </item>
    <item>
      <title>[inductor] label cpp test files with oncall: cpu inductor (#115167)</title>
      <link>https://github.com/pytorch/pytorch/commit/b618869208358112442aa44d9e7d18f36e385a8d</link>
      <description><![CDATA[<p>[inductor] label cpp test files with oncall: cpu inductor (#115167)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115167<br />
Approved by: https://github.com/atalman</p>]]></description>
      <pubDate>Thu, 14 Dec 2023 09:39:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b618869208358112442aa44d9e7d18f36e385a8d</guid>
    </item>
    <item>
      <title>[fbcode] consolidate usage of fp8 linears for inference models (#115808)</title>
      <link>https://github.com/pytorch/pytorch/commit/c80e2d5bb23b48410dd73bc0698df8552f3dfaba</link>
      <description><![CDATA[<p>[fbcode] consolidate usage of fp8 linears for inference models (#115808)</p>
<p>Summary:<br />
ATT, this will use implementation of D51812709 for fp8 linears.</p>
<p>Meanwhile, it also adds use-case of delay quantization</p>
<p>Test Plan:<br />
<code>CUDA_VISIBLE_DEVICES=7 buck run mode/opt  -c fbcode.platform010_cuda_version=12 -c fbcode.nvcc_arch=h100 -c fbcode.use_link_groups=false caffe2/torch/fb/model_transform/experimental/benchmark:mts_gpu_benchmark -- --local-model /home/xiaoruichao/test_models/463113248.input.predictor.disagg.gpu.merge --lower-backend AOT_INDUCTOR --fp8-linear-quantization-type delay_quantization --disable-acc-tracer-aot-inductor</code></p>
<p><code>CUDA_VISIBLE_DEVICES=7 buck run mode/opt  -c fbcode.platform010_cuda_version=12 -c fbcode.nvcc_arch=h100 -c fbcode.use_link_groups=false caffe2/torch/fb/model_transform/experimental/benchmark:mts_gpu_benchmark -- --local-model /home/xiaoruichao/test_models/463113248.input.predictor.disagg.gpu.merge --lower-backend AOT_INDUCTOR --fp8-linear-quantization-type delay_quantization --disable-acc-tracer-aot-inductor</code></p>
<p>Reviewed By: tter1</p>
<p>Differential Revision: D51840344</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115808<br />
Approved by: https://github.com/ipiszy</p>]]></description>
      <pubDate>Thu, 14 Dec 2023 08:59:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c80e2d5bb23b48410dd73bc0698df8552f3dfaba</guid>
    </item>
    <item>
      <title>[inductor] Updated upsample_bilinear2d decomposition (#104182)</title>
      <link>https://github.com/pytorch/pytorch/commit/f727bed2e614884a49b3cbc1075028954a3e9e61</link>
      <description><![CDATA[<p>[inductor] Updated upsample_bilinear2d decomposition (#104182)</p>
<p>Description:<br />
- Updated upsample_bilinear2d decomposition<br />
  - added support for uint8 dtype support<br />
  - code improvements<br />
- Added uint8 dtype tests</p>
<p>Perf considerations:<br />
- There is minor perf regression (speed-up ~0.7) on cases uint8, align_corners=True when output is smaller/equal (256, 256)<br />
- For cases, when output is larger (256, 256) and input dtype uint8, nightly output is wrong, so IMO large perf regression (speed-up around ~0.2) should not be taken into account.</p>
<h2>Perfs benchmarks</h2>
<p>```<br />
[--------------------------------------------------------------------------------------------------------------------------------------------------------- Interpolate, cpu --------------------------------------------------------------------------------------------------------------------------------------------------------]<br />
                                                                                                                                                    |  Eager (2.3.0a0+gitafcfdb1) PR  |  Compiled (2.3.0a0+gitafcfdb1) PR  |  Compiled (2.3.0a0+gitde89a53) Nightly  |  speed-up PR vs Nightly  |  Eager (2.3.0a0+gitde89a53) Nightly<br />
1 threads: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------<br />
      Input (1, 3, 500, 400), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (256, 256)       |        565.212 (+-3.548)        |        1384.210 (+-10.798)         |           1230.996 (+-32.930)           |     0.889 (+-0.000)      |          566.253 (+-1.526)<br />
      Input (1, 3, 500, 400), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (256, 256)      |        565.404 (+-1.614)        |         1491.649 (+-7.763)         |            2974.959 (+-6.006)           |     1.994 (+-0.000)      |          566.476 (+-1.742)<br />
      Input (1, 3, 500, 400), torch.uint8, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (256, 256)           |        270.761 (+-0.861)        |         1557.777 (+-4.699)         |            1080.919 (+-4.243)           |     0.694 (+-0.000)      |          269.829 (+-0.986)<br />
      Input (1, 3, 500, 400), torch.uint8, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (256, 256)          |        270.960 (+-0.995)        |        1723.913 (+-12.433)         |            3191.938 (+-6.194)           |     1.852 (+-0.000)      |          269.962 (+-1.657)<br />
      Input (1, 3, 500, 400), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (256, 256)     |        1555.884 (+-5.169)       |         1178.753 (+-4.957)         |            1910.445 (+-5.988)           |     1.621 (+-0.000)      |          1560.804 (+-6.793)<br />
      Input (1, 3, 500, 400), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (256, 256)    |        1651.193 (+-6.952)       |         1323.466 (+-6.059)         |            3374.842 (+-8.168)           |     2.550 (+-0.000)      |          1653.497 (+-8.018)<br />
      Input (1, 3, 500, 400), torch.float32, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (256, 256)         |        978.482 (+-10.183)       |         1383.768 (+-4.341)         |            2147.841 (+-6.581)           |     1.552 (+-0.000)      |          979.983 (+-1.499)<br />
      Input (1, 3, 500, 400), torch.float32, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (256, 256)        |        1074.472 (+-5.031)       |         1414.912 (+-5.754)         |           3590.968 (+-10.042)           |     2.538 (+-0.000)      |          1074.589 (+-3.948)<br />
      Input (4, 3, 500, 400), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (256, 256)       |        2168.703 (+-8.964)       |        5400.528 (+-26.628)         |           4777.299 (+-11.891)           |     0.885 (+-0.000)      |          2168.133 (+-7.667)<br />
      Input (4, 3, 500, 400), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (256, 256)      |       2169.132 (+-12.618)       |        6583.866 (+-28.959)         |           11986.894 (+-45.838)          |     1.821 (+-0.000)      |         2174.488 (+-10.317)<br />
      Input (4, 3, 500, 400), torch.uint8, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (256, 256)           |        992.808 (+-6.086)        |         5985.028 (+-9.532)         |            4334.158 (+-9.423)           |     0.724 (+-0.000)      |          989.604 (+-5.499)<br />
      Input (4, 3, 500, 400), torch.uint8, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (256, 256)          |        987.618 (+-6.350)        |        6963.044 (+-28.885)         |           15441.096 (+-55.324)          |     2.218 (+-0.000)      |          985.573 (+-5.159)<br />
      Input (4, 3, 500, 400), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (256, 256)     |       6695.557 (+-35.067)       |        4657.603 (+-14.220)         |           8058.708 (+-41.684)           |     1.730 (+-0.000)      |         6714.996 (+-38.626)<br />
      Input (4, 3, 500, 400), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (256, 256)    |       7040.481 (+-39.486)       |        5445.704 (+-16.659)         |           13906.618 (+-53.298)          |     2.554 (+-0.000)      |         7034.453 (+-44.626)<br />
      Input (4, 3, 500, 400), torch.float32, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (256, 256)         |       3926.186 (+-10.660)       |        5741.433 (+-12.748)         |           9356.036 (+-40.848)           |     1.630 (+-0.000)      |         3930.598 (+-17.086)<br />
      Input (4, 3, 500, 400), torch.float32, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (256, 256)        |        4308.536 (+-9.607)       |        6122.755 (+-47.278)         |           15637.567 (+-54.392)          |     2.554 (+-0.000)      |         4307.463 (+-11.268)<br />
      Input (1, 3, 1200, 1300), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (200, 300)     |       2512.740 (+-10.860)       |         1573.590 (+-5.061)         |            451.355 (+-1.210)            |     0.287 (+-0.000)      |         2511.727 (+-10.930)<br />
      Input (1, 3, 1200, 1300), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (200, 300)    |       2489.926 (+-11.915)       |         1537.233 (+-4.212)         |            2501.470 (+-7.446)           |     1.627 (+-0.000)      |         2500.000 (+-12.155)<br />
      Input (1, 3, 1200, 1300), torch.uint8, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (200, 300)         |        632.032 (+-2.108)        |         1496.994 (+-4.194)         |            404.759 (+-1.064)            |     0.270 (+-0.000)      |          630.122 (+-4.086)<br />
      Input (1, 3, 1200, 1300), torch.uint8, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (200, 300)        |        629.174 (+-4.386)        |         1708.935 (+-8.817)         |            2643.296 (+-9.723)           |     1.547 (+-0.000)      |          628.388 (+-1.326)<br />
      Input (1, 3, 1200, 1300), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (200, 300)   |        4409.941 (+-8.016)       |         1160.133 (+-4.698)         |            1897.089 (+-9.392)           |     1.635 (+-0.000)      |         4450.959 (+-10.438)<br />
      Input (1, 3, 1200, 1300), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (200, 300)  |       4493.427 (+-11.703)       |         1329.226 (+-4.740)         |           2835.872 (+-12.241)           |     2.133 (+-0.000)      |          4506.973 (+-9.914)<br />
      Input (1, 3, 1200, 1300), torch.float32, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (200, 300)       |        901.712 (+-4.071)        |         1320.739 (+-5.197)         |            2207.605 (+-8.219)           |     1.671 (+-0.000)      |          904.757 (+-4.558)<br />
      Input (1, 3, 1200, 1300), torch.float32, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (200, 300)      |        990.080 (+-3.922)        |         1702.563 (+-7.909)         |           3074.196 (+-10.478)           |     1.806 (+-0.000)      |          990.482 (+-4.444)<br />
      Input (4, 3, 1200, 1300), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (200, 300)     |       9785.550 (+-58.445)       |        6135.680 (+-33.569)         |           1628.572 (+-19.770)           |     0.265 (+-0.000)      |         9893.606 (+-62.377)<br />
      Input (4, 3, 1200, 1300), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (200, 300)    |       9710.191 (+-57.597)       |        6066.824 (+-36.364)         |           10469.110 (+-42.775)          |     1.726 (+-0.000)      |         9919.022 (+-72.190)<br />
      Input (4, 3, 1200, 1300), torch.uint8, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (200, 300)         |       2790.356 (+-12.188)       |        6134.101 (+-28.694)         |            1576.832 (+-6.030)           |     0.257 (+-0.000)      |         2761.122 (+-11.503)<br />
      Input (4, 3, 1200, 1300), torch.uint8, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (200, 300)        |       2778.711 (+-13.603)       |        6608.528 (+-37.776)         |           10841.549 (+-49.429)          |     1.641 (+-0.000)      |         2753.037 (+-10.995)<br />
      Input (4, 3, 1200, 1300), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (200, 300)   |      45533.868 (+-102.618)      |         4962.994 (+-8.215)         |           9003.968 (+-38.179)           |     1.814 (+-0.000)      |        43531.261 (+-102.951)<br />
      Input (4, 3, 1200, 1300), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (200, 300)  |       45932.699 (+-81.207)      |        5595.682 (+-11.482)         |           12302.907 (+-50.254)          |     2.199 (+-0.000)      |         43916.455 (+-80.468)<br />
      Input (4, 3, 1200, 1300), torch.float32, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (200, 300)       |        3827.804 (+-8.057)       |        6311.580 (+-25.021)         |           11760.614 (+-51.531)          |     1.863 (+-0.000)      |         3849.959 (+-10.848)<br />
      Input (4, 3, 1200, 1300), torch.float32, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (200, 300)      |        4169.007 (+-8.452)       |        6820.716 (+-35.310)         |           15264.633 (+-49.982)          |     2.238 (+-0.000)      |         4183.875 (+-19.104)<br />
      Input (1, 3, 300, 400), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (600, 700)       |        1306.914 (+-7.470)       |        10598.101 (+-38.410)        |           2678.031 (+-11.051)           |     0.253 (+-0.000)      |          1307.470 (+-8.519)<br />
      Input (1, 3, 300, 400), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (600, 700)      |        1307.268 (+-8.197)       |        10161.123 (+-45.643)        |           17148.842 (+-55.402)          |     1.688 (+-0.000)      |          1308.077 (+-8.553)<br />
      Input (1, 3, 300, 400), torch.uint8, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (600, 700)           |        548.574 (+-2.157)        |        10072.806 (+-41.368)        |            2408.971 (+-6.997)           |     0.239 (+-0.000)      |          547.726 (+-1.721)<br />
      Input (1, 3, 300, 400), torch.uint8, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (600, 700)          |        546.664 (+-1.484)        |        11123.694 (+-43.636)        |           18058.070 (+-48.552)          |     1.623 (+-0.000)      |          547.151 (+-1.627)<br />
      Input (1, 3, 300, 400), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (600, 700)     |       7935.051 (+-71.022)       |        7654.533 (+-29.512)         |           12414.194 (+-87.450)          |     1.622 (+-0.000)      |         7900.056 (+-53.997)<br />
      Input (1, 3, 300, 400), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (600, 700)    |       8546.732 (+-53.118)       |        8583.572 (+-35.656)         |          19111.824 (+-166.978)          |     2.227 (+-0.000)      |         8515.433 (+-63.300)<br />
      Input (1, 3, 300, 400), torch.float32, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (600, 700)         |       6202.642 (+-34.355)       |        8915.622 (+-62.293)         |           14327.295 (+-52.188)          |     1.607 (+-0.000)      |         6213.329 (+-39.740)<br />
      Input (1, 3, 300, 400), torch.float32, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (600, 700)        |       6811.128 (+-33.747)       |        9647.316 (+-50.837)         |           20830.594 (+-62.979)          |     2.159 (+-0.000)      |         6822.512 (+-37.092)<br />
      Input (4, 3, 300, 400), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (600, 700)       |       5079.586 (+-19.067)       |        42238.442 (+-87.643)        |           11282.141 (+-42.477)          |     0.267 (+-0.000)      |         5104.234 (+-17.706)<br />
      Input (4, 3, 300, 400), torch.uint8, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (600, 700)      |       5079.575 (+-16.306)       |        41512.995 (+-83.710)        |          68789.816 (+-440.001)          |     1.657 (+-0.000)      |         5097.446 (+-21.724)<br />
      Input (4, 3, 300, 400), torch.uint8, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (600, 700)           |        2039.974 (+-8.614)       |       42322.773 (+-111.866)        |           10399.237 (+-43.140)          |     0.246 (+-0.000)      |         2043.808 (+-10.707)<br />
      Input (4, 3, 300, 400), torch.uint8, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (600, 700)          |       2036.214 (+-10.083)       |        44353.281 (+-71.548)        |          73340.412 (+-324.780)          |     1.654 (+-0.000)      |          2039.000 (+-9.554)<br />
      Input (4, 3, 300, 400), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (600, 700)     |       33821.523 (+-96.639)      |        30552.094 (+-65.023)        |          49494.486 (+-872.916)          |     1.620 (+-0.000)      |         33844.404 (+-92.466)<br />
      Input (4, 3, 300, 400), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (600, 700)    |      36196.104 (+-128.169)      |        34038.432 (+-79.697)        |          75761.226 (+-905.194)          |     2.226 (+-0.000)      |         36260.473 (+-94.642)<br />
      Input (4, 3, 300, 400), torch.float32, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (600, 700)         |       24827.821 (+-77.335)      |        37006.218 (+-86.318)        |          61297.625 (+-898.192)          |     1.656 (+-0.000)      |         24823.275 (+-80.945)<br />
      Input (4, 3, 300, 400), torch.float32, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (600, 700)        |       27266.138 (+-70.262)      |        40109.475 (+-94.248)        |          92086.075 (+-404.922)          |     2.296 (+-0.000)      |         27287.992 (+-89.507)</p>
<p>Times are in microseconds (us).</p>
<p>[--------------------------------------------------------------------------------------------------------------------------------------------------------- Interpolate, cuda ---------------------------------------------------------------------------------------------------------------------------------------------------------]<br />
                                                                                                                                                      |  Eager (2.3.0a0+gitafcfdb1) PR  |  Compiled (2.3.0a0+gitafcfdb1) PR  |  Compiled (2.3.0a0+gitde89a53) Nightly  |  speed-up PR vs Nightly  |  Eager (2.3.0a0+gitde89a53) Nightly<br />
1 threads: ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------<br />
      Input (1, 3, 2345, 2456), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (1234, 1345)   |         98.259 (+-0.014)        |          97.156 (+-0.008)          |             97.443 (+-0.031)            |     1.003 (+-0.000)      |           98.248 (+-0.021)<br />
      Input (1, 3, 2345, 2456), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (1234, 1345)  |         97.048 (+-0.016)        |          97.480 (+-0.018)          |             96.819 (+-0.126)            |     0.993 (+-0.000)      |           97.045 (+-0.015)<br />
      Input (1, 3, 2345, 2456), torch.float32, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (1234, 1345)       |         97.944 (+-0.028)        |          91.686 (+-0.411)          |             93.894 (+-1.011)            |     1.024 (+-0.000)      |           97.933 (+-0.008)<br />
      Input (1, 3, 2345, 2456), torch.float32, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (1234, 1345)      |         98.008 (+-0.011)        |          91.205 (+-0.346)          |             96.854 (+-0.058)            |     1.062 (+-0.000)      |           97.203 (+-0.010)<br />
      Input (4, 3, 2345, 2456), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (1234, 1345)   |        384.318 (+-0.011)        |         382.793 (+-0.007)          |            382.472 (+-0.011)            |     0.999 (+-0.000)      |          384.701 (+-0.012)<br />
      Input (4, 3, 2345, 2456), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (1234, 1345)  |        384.266 (+-0.009)        |         385.333 (+-0.024)          |            382.554 (+-0.022)            |     0.993 (+-0.000)      |          384.386 (+-0.016)<br />
      Input (4, 3, 2345, 2456), torch.float32, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (1234, 1345)       |        383.924 (+-0.011)        |         570.071 (+-0.030)          |            545.615 (+-0.051)            |     0.957 (+-0.000)      |          384.044 (+-0.012)<br />
      Input (4, 3, 2345, 2456), torch.float32, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (1234, 1345)      |        384.184 (+-0.016)        |         560.857 (+-0.026)          |            552.447 (+-0.040)            |     0.985 (+-0.000)      |          384.063 (+-0.016)<br />
      Input (1, 3, 1234, 1345), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (2345, 2456)   |        122.188 (+-0.053)        |         116.744 (+-1.006)          |            163.762 (+-0.015)            |     1.403 (+-0.000)      |          121.874 (+-0.015)<br />
      Input (1, 3, 1234, 1345), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (2345, 2456)  |        122.156 (+-0.012)        |         182.692 (+-0.013)          |            161.653 (+-0.018)            |     0.885 (+-0.000)      |          121.926 (+-0.014)<br />
      Input (1, 3, 1234, 1345), torch.float32, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (2345, 2456)       |        105.852 (+-0.324)        |         119.545 (+-0.294)          |            190.527 (+-0.023)            |     1.594 (+-0.000)      |          105.999 (+-0.446)<br />
      Input (1, 3, 1234, 1345), torch.float32, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (2345, 2456)      |        106.507 (+-0.282)        |         120.060 (+-0.257)          |            162.330 (+-0.012)            |     1.352 (+-0.000)      |          106.567 (+-0.385)<br />
      Input (4, 3, 1234, 1345), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: True, antialias: False, osize: (2345, 2456)   |        447.907 (+-0.015)        |         463.863 (+-1.779)          |            650.492 (+-0.331)            |     1.402 (+-0.000)      |          446.596 (+-0.017)<br />
      Input (4, 3, 1234, 1345), torch.float32, torch.contiguous_format | mode: bilinear, align_corners: False, antialias: False, osize: (2345, 2456)  |        447.750 (+-0.017)        |         723.832 (+-0.170)          |            641.539 (+-0.075)            |     0.886 (+-0.000)      |          446.467 (+-0.019)<br />
      Input (4, 3, 1234, 1345), torch.float32, torch.channels_last | mode: bilinear, align_corners: True, antialias: False, osize: (2345, 2456)       |        439.549 (+-0.031)        |         507.772 (+-2.879)          |            758.795 (+-0.482)            |     1.494 (+-0.000)      |          440.372 (+-0.025)<br />
      Input (4, 3, 1234, 1345), torch.float32, torch.channels_last | mode: bilinear, align_corners: False, antialias: False, osize: (2345, 2456)      |        439.538 (+-0.029)        |         509.260 (+-2.704)          |            654.195 (+-2.621)            |     1.285 (+-0.000)      |          440.362 (+-0.026)</p>
<p>Times are in microseconds (us).<br />
```</p>
<p><a href="https://github.com/vfdev-5/pth-inductor-dev/blob/f4751a3196d96531c016cb9a1617be18a931d3b1/perf_interp_mode.py">Source</a>, <a href="https://github.com/vfdev-5/pth-inductor-dev/blob/899f34c024d5e6d6e6c7af9702c49cb28ecf13e3/output/20231213-214209-upsample-bilinear-pr_vs_nightly-speedup.md">Output</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/104182<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Thu, 14 Dec 2023 06:50:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f727bed2e614884a49b3cbc1075028954a3e9e61</guid>
    </item>
    <item>
      <title>[inductor] Added non-integer expr support for floordiv in triton codegen (#115751)</title>
      <link>https://github.com/pytorch/pytorch/commit/c7ae2c170f1e5e85badec0a0679abad136ecf8b7</link>
      <description><![CDATA[<p>[inductor] Added non-integer expr support for floordiv in triton codegen (#115751)</p>
<p>Description:<br />
- Added non-integer expr support for floordiv in triton codegen<br />
- Added a test<br />
  - cpp test is skipped as failing and https://github.com/pytorch/pytorch/pull/115647 may fix it</p>
<p>This PR is fixing compilation error with the following code:<br />
```python<br />
import torch</p>
<p>def func(x, a):<br />
    n = (a * 1.234) // 8.234<br />
    y = x + n<br />
    return y</p>
<p>cfunc = torch.compile(func, dynamic=True, fullgraph=True)</p>
<p>device = "cuda"<br />
x = torch.tensor(0, dtype=torch.float32, device=device)<br />
a = 33</p>
<p>out = cfunc(x, a)<br />
expected = func(x, a)<br />
torch.testing.assert_close(out, expected)<br />
<code>Error message on Nightly:</code><br />
  File "/usr/lib/python3.8/concurrent/futures/<em>base.py", line 389, in __get_result<br />
    raise self._exception<br />
torch._dynamo.exc.BackendCompilerFailed: backend='compile_fx_wrapper' raised:<br />
CompilationError: at 7:38:def triton</em>(in_ptr0, out_ptr0, ks0, xnumel, XBLOCK : tl.constexpr):<br />
    xoffset = tl.program_id(0) * XBLOCK<br />
    xindex = xoffset + tl.arange(0, XBLOCK)[:]<br />
    xmask = xindex &lt; xnumel<br />
    x0 = xindex<br />
    tmp0 = tl.load(in_ptr0 + (x0), xmask)<br />
    tmp1 = ((1.23400000000000*ks0) // 8.23400000000000)<br />
                                      ^<br />
AssertionError()<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115751<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Wed, 13 Dec 2023 15:17:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c7ae2c170f1e5e85badec0a0679abad136ecf8b7</guid>
    </item>
    <item>
      <title>[inductor] Allow sympy expressions to participate in type promotion (#115676)</title>
      <link>https://github.com/pytorch/pytorch/commit/ad76a4e1e79463681f8546b2be6ece33b39b34c3</link>
      <description><![CDATA[<p>[inductor] Allow sympy expressions to participate in type promotion (#115676)</p>
<p>In the test example we have <code>add(i64[10], sympy.Expr)</code> where<br />
<code>sympy.Expr</code> is not considered a promoting arg so isn't factored into<br />
the type promotion. However, in eager it would promote to float32.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115676<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #115677, #115699, #115700</p>]]></description>
      <pubDate>Wed, 13 Dec 2023 14:22:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ad76a4e1e79463681f8546b2be6ece33b39b34c3</guid>
    </item>
    <item>
      <title>[inductor] Do variance calculation in opmath type (#115181)</title>
      <link>https://github.com/pytorch/pytorch/commit/42390a097b987cd3384511c3df3747699f2281f4</link>
      <description><![CDATA[<p>[inductor] Do variance calculation in opmath type (#115181)</p>
<p>Fixes #114903</p>
<p>Previously large split variance reductions stored the intermediates as float16<br />
precision, which may lead to overflow as the intermediate result is<br />
unnormalized.</p>
<p>In #114903 we see two different <code>num_split</code> decisions made based on the<br />
hardware capabilities, one of which has large enough intermediates to cause<br />
overflows.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115181<br />
Approved by: https://github.com/shunting314</p>]]></description>
      <pubDate>Wed, 13 Dec 2023 10:40:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/42390a097b987cd3384511c3df3747699f2281f4</guid>
    </item>
    <item>
      <title>add sm80orlater check to test_sdpa (#115702)</title>
      <link>https://github.com/pytorch/pytorch/commit/95de4f5764f1736d654c8d49842b8837f3e1154a</link>
      <description><![CDATA[<p>add sm80orlater check to test_sdpa (#115702)</p>
<p>test_sdpa and test_sdpa2 in test_aot_inductor.py use bfloat16 which is not supported by sm &lt; 80, so skip test if sm &lt; 80</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115702<br />
Approved by: https://github.com/soulitzer</p>]]></description>
      <pubDate>Wed, 13 Dec 2023 10:21:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/95de4f5764f1736d654c8d49842b8837f3e1154a</guid>
    </item>
    <item>
      <title>[inductor] Fix angle decomposition return type (#115700)</title>
      <link>https://github.com/pytorch/pytorch/commit/fb80f05ee2e1cba17892980701bfd5dbce58349f</link>
      <description><![CDATA[<p>[inductor] Fix angle decomposition return type (#115700)</p>
<p>The current decomposition always returns float32 when the input isn't complex.<br />
Instead, we should do proper type promotion.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115700<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #115677, #115699</p>]]></description>
      <pubDate>Wed, 13 Dec 2023 06:16:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fb80f05ee2e1cba17892980701bfd5dbce58349f</guid>
    </item>
    <item>
      <title>[inductor] Fix torch.bernoulli decomposition return type (#115699)</title>
      <link>https://github.com/pytorch/pytorch/commit/9cdc80d5818b330edbf03ec157ee352d4ecfc28c</link>
      <description><![CDATA[<p>[inductor] Fix torch.bernoulli decomposition return type (#115699)</p>
<p>Strangely enough, <code>torch.bernoulli</code> doesn't return a boolean and instead<br />
it matches the output type of the inplace bernoulli.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115699<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #115677</p>]]></description>
      <pubDate>Wed, 13 Dec 2023 06:16:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9cdc80d5818b330edbf03ec157ee352d4ecfc28c</guid>
    </item>
    <item>
      <title>[PyTorch] AOTI: add minimal arrayref interface (#112800)</title>
      <link>https://github.com/pytorch/pytorch/commit/f9cf6ae889625ced5ebea824716dc74d6efb0de7</link>
      <description><![CDATA[<p>[PyTorch] AOTI: add minimal arrayref interface (#112800)</p>
<p>This implements an optional alternate interface to the AOTI<br />
generated DSO, intended to increase efficiency for models running on<br />
CPU and requiring minimal overhead. See comment in config.py for more<br />
explanation.</p>
<p>This took a while to get right (e.g., I initially required 1-D<br />
MiniArrayRef<T> for the inputs, but found that multi-dimensional<br />
ArrayRefTensor<T> ended up simplifying the implementation and allowed<br />
test_aot_inductor.py to run) and is somewhat intricate, so I am<br />
anticipating that review will require some back-and-forth.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50699890/">D50699890</a></p>
<p><strong>NOTE FOR REVIEWERS</strong>: This PR has internal Meta-specific changes or comments, please review them on <a href="https://our.internmc.facebook.com/intern/diff/D50699890/">Phabricator</a>!</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112800<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Wed, 13 Dec 2023 04:06:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f9cf6ae889625ced5ebea824716dc74d6efb0de7</guid>
    </item>
    <item>
      <title>[export] Preserve FQN in export_to_torch_ir (#115462)</title>
      <link>https://github.com/pytorch/pytorch/commit/dd42201cb8fdb46fb08f32144f98f73766749c01</link>
      <description><![CDATA[<p>[export] Preserve FQN in export_to_torch_ir (#115462)</p>
<p>AOTInductor currently relies of export_to_torch_ir to generate a graph, and passes it to inductor to generate the .so. They would like the FQN to be consistent so that they can easily find/update the weights in the .so.</p>
<p>Note that since export flattens all modules in to a single computational graph, we will change the FQNs in the original module by replacing all periods with underscores. For example, <code>foo.child1param</code>, which points to a submodule named <code>foo</code>'s parameter named <code>child1param</code>, will be renamed to <code>foo_child1param</code> since we no longer have the submodule <code>foo</code>. This is done just by doing <code>name.replace(".", "_")</code>.</p>
<p>Outputted AOTInductor c++ code: https://www.internalfb.com/phabricator/paste/view/P900120950?lines=377-355%2C354</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115462<br />
Approved by: https://github.com/tugsbayasgalan</p>]]></description>
      <pubDate>Tue, 12 Dec 2023 20:58:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dd42201cb8fdb46fb08f32144f98f73766749c01</guid>
    </item>
    <item>
      <title>[inductor] make sure bitcast input and target type have the same bitwidth (#115619)</title>
      <link>https://github.com/pytorch/pytorch/commit/1392843e7bec4f412935c013c1733e1f213fb6b8</link>
      <description><![CDATA[<p>[inductor] make sure bitcast input and target type have the same bitwidth (#115619)</p>
<p>This PR fixed #104791</p>
<p>bitcast requires the source and target have the bitwidth.<br />
Because the input tensor's dtype could be promoted, e.g. from float16 to<br />
float, we have to cast the tensor to its original source dtype before<br />
invoking bitcast in such cases. After that, we also need to convert<br />
the bit-casted tensor back to float to make sure we keep using higher<br />
precision values for the rest of the computation.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115619<br />
Approved by: https://github.com/jansel, https://github.com/eellison</p>]]></description>
      <pubDate>Tue, 12 Dec 2023 16:53:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1392843e7bec4f412935c013c1733e1f213fb6b8</guid>
    </item>
    <item>
      <title>[Inductor] Implement a deduplist data structure for name to user tracking (#115609)</title>
      <link>https://github.com/pytorch/pytorch/commit/af09fe256a8357725e63b487c9c2064bebba804b</link>
      <description><![CDATA[<p>[Inductor] Implement a deduplist data structure for name to user tracking (#115609)</p>
<p>Summary:<br />
An internal MRS model was taking over a day's worth of time to compile due to many duplicates in dependency tracking. This PR replaces the list with a custom dedup list.<br />
Normally one could use a set/dict for this purpose however the list in question gets elements appended as it is being iterated over which means that we need to keep the list semantics.</p>
<p>Test Plan: ad hoc testing</p>
<p>Differential Revision: D52060659</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115609<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 12 Dec 2023 14:28:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/af09fe256a8357725e63b487c9c2064bebba804b</guid>
    </item>
    <item>
      <title>[inductor] Don't print disable_cudagraphs_reason when cudagraphs is disabled (#115489)</title>
      <link>https://github.com/pytorch/pytorch/commit/36b513627005a86d544cdace24d4e30bb6038e95</link>
      <description><![CDATA[<p>[inductor] Don't print disable_cudagraphs_reason when cudagraphs is disabled (#115489)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115489<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Tue, 12 Dec 2023 09:50:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/36b513627005a86d544cdace24d4e30bb6038e95</guid>
    </item>
    <item>
      <title>[inductor] De-duplicate triton helper functions (#115546)</title>
      <link>https://github.com/pytorch/pytorch/commit/40dc0580a69565b06ec5263efe5d87cecc8200f7</link>
      <description><![CDATA[<p>[inductor] De-duplicate triton helper functions (#115546)</p>
<p>Previously if two calls to cumsum were generated in the same triton kernel<br />
we would generate identical helper functions with different names. Now this<br />
recognizes identical functions and only defines it once. To do this I defer<br />
choosing the name until after codegen.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115546<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #109132</p>]]></description>
      <pubDate>Tue, 12 Dec 2023 08:30:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/40dc0580a69565b06ec5263efe5d87cecc8200f7</guid>
    </item>
    <item>
      <title>[inductor] Parameterize ir.Scan on combine_fn (#109132)</title>
      <link>https://github.com/pytorch/pytorch/commit/02196c21ac7a07ec3863f11a1dfc0e0a6aa4be2b</link>
      <description><![CDATA[<p>[inductor] Parameterize ir.Scan on combine_fn (#109132)</p>
<p>This replaces <code>tl.cumsum</code> and <code>tl.cumprod</code> with calls to <code>tl.associative_scan</code><br />
where the combine function is generated from inductor IR.</p>
<p>So before we had:<br />
<code>python
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 20
    rnumel = 30
    RBLOCK: tl.constexpr = 32
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex &lt; xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex &lt; rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (30*x0)), rmask &amp; xmask, other=0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp2 = tl.where(rmask &amp; xmask, tmp1, 0)
    tmp3 = tl.cumsum(tmp2, 1)
    tl.store(out_ptr0 + (r1 + (30*x0)), tmp3, rmask &amp; xmask)</code></p>
<p>Now we have:<br />
```python<br />
@triton.jit<br />
def _triton_helper_fn0(arg0, arg1):<br />
    tmp0 = tmp0 + tmp1<br />
    return tmp0</p>
<p>@triton.jit<br />
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):<br />
    xnumel = 20<br />
    rnumel = 30<br />
    RBLOCK: tl.constexpr = 32<br />
    xoffset = tl.program_id(0) * XBLOCK<br />
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]<br />
    xmask = xindex &lt; xnumel<br />
    rindex = tl.arange(0, RBLOCK)[None, :]<br />
    rmask = rindex &lt; rnumel<br />
    r1 = rindex<br />
    x0 = xindex<br />
    tmp0 = tl.load(in_ptr0 + (r1 + (30<em>x0)), rmask &amp; xmask, other=0).to(tl.float32)<br />
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])<br />
    tmp2 = tl.where(rmask &amp; xmask, tmp1, 0)<br />
    tmp3 = tl.associative_scan(tmp2, 1, _triton_helper_fn0)<br />
    tl.store(out_ptr0 + (r1 + (30</em>x0)), tmp3, rmask &amp; xmask)<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109132<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Tue, 12 Dec 2023 08:30:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/02196c21ac7a07ec3863f11a1dfc0e0a6aa4be2b</guid>
    </item>
    <item>
      <title>[inductor][optimus] enable smart fusion (#115471)</title>
      <link>https://github.com/pytorch/pytorch/commit/744d74c456cdc188530b33d45a3da8e854a9d8ac</link>
      <description><![CDATA[<p>[inductor][optimus] enable smart fusion (#115471)</p>
<p>Summary: Enable gmm smart fusion in D51698686</p>
<p>Test Plan: buck test</p>
<p>Differential Revision: D52002137</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115471<br />
Approved by: https://github.com/mengluy0125</p>]]></description>
      <pubDate>Mon, 11 Dec 2023 21:04:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/744d74c456cdc188530b33d45a3da8e854a9d8ac</guid>
    </item>
    <item>
      <title>[inductor] Fix an aliased output bug (#115373)</title>
      <link>https://github.com/pytorch/pytorch/commit/0fc04e274de9d6a52172674e294c1b73d43a50e9</link>
      <description><![CDATA[<p>[inductor] Fix an aliased output bug (#115373)</p>
<p>Summary: for https://github.com/pytorch/pytorch/issues/97083, when</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115373<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 11 Dec 2023 17:18:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0fc04e274de9d6a52172674e294c1b73d43a50e9</guid>
    </item>
    <item>
      <title>Revert "[inductor] Fix an aliased output bug (#115373)"</title>
      <link>https://github.com/pytorch/pytorch/commit/5fe2b138e389a7ded434e0139a46945faf12f32a</link>
      <description><![CDATA[<p>Revert "[inductor] Fix an aliased output bug (#115373)"</p>
<p>This reverts commit 1310f0bf38293b68a781287d1de8cf699a76974d.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/115373 on behalf of https://github.com/atalman due to Sorry for reverting your change it broke inductor tests (<a href="https://github.com/pytorch/pytorch/pull/115373#issuecomment-1850792869">comment</a>)</p>]]></description>
      <pubDate>Mon, 11 Dec 2023 12:02:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5fe2b138e389a7ded434e0139a46945faf12f32a</guid>
    </item>
    <item>
      <title>[benchmarking] Reduce box_detections_per_img for vision_maskrcnn (#115487)</title>
      <link>https://github.com/pytorch/pytorch/commit/de89a53df8222148460e8d6fc49009c0e1c90900</link>
      <description><![CDATA[<p>[benchmarking] Reduce box_detections_per_img for vision_maskrcnn (#115487)</p>
<p>This fixes a failure on the <a href="https://hud.pytorch.org/benchmark/compilers">perf dashboard</a> with <code>--amp</code> mode.  I believe boxes 5 and 6 were getting swapped.  The existing comment explains the issue.</p>
<p>Before<br />
<code>$ ./benchmarks/dynamo/torchbench.py --training  --accuracy --no-translation-validatio --amp --backend=inductor --disable-cudagraphs --only vision_maskrcnn
...
[2023-12-09 13:21:27,292] torch._dynamo.utils: [ERROR] RMSE (res-fp64): 0.00171, (ref-fp64): 0.00054 and shape=torch.Size([256, 256, 3, 3])
[2023-12-09 13:21:27,292] torch._dynamo.utils: [ERROR] Accuracy failed for key name backbone.fpn.layer_blocks.2.0.weight.grad
fail_accuracy</code></p>
<p>After<br />
<code>$ ./benchmarks/dynamo/torchbench.py --training  --accuracy --no-translation-validatio --amp --backend=inductor --disable-cudagraphs --only vision_maskrcnn
...
pass</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115487<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Mon, 11 Dec 2023 00:42:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/de89a53df8222148460e8d6fc49009c0e1c90900</guid>
    </item>
    <item>
      <title>[aotinductor] replace lld with the default ld linker (#115478)</title>
      <link>https://github.com/pytorch/pytorch/commit/fe01605830145b5aa204120b90361021a2952ac1</link>
      <description><![CDATA[<p>[aotinductor] replace lld with the default ld linker (#115478)</p>
<p>Currently, we place constants in the .so. To avoid cases<br />
where constants are too large (i.e. &gt;2G), we put the<br />
constants into .lrodata, which allows doesn't have 2G limit.<br />
Not sure why, lld still issues errors like beow even if<br />
those large constants data are stored in .lrodata section:</p>
<p>"relocation R_X86_64_PC32 out of range: 5459191920 is not in<br />
[-2147483648, 2147483647]"</p>
<p>In constrast, the default gnu ld linker works fine. Let's<br />
switch back to use ld to unblock some internal models.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115478<br />
Approved by: https://github.com/desertfire, https://github.com/htyu</p>]]></description>
      <pubDate>Sun, 10 Dec 2023 18:35:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fe01605830145b5aa204120b90361021a2952ac1</guid>
    </item>
    <item>
      <title>[inductor] Fix an aliased output bug (#115373)</title>
      <link>https://github.com/pytorch/pytorch/commit/1310f0bf38293b68a781287d1de8cf699a76974d</link>
      <description><![CDATA[<p>[inductor] Fix an aliased output bug (#115373)</p>
<p>Summary: for https://github.com/pytorch/pytorch/issues/97083, when</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115373<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Sun, 10 Dec 2023 15:52:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1310f0bf38293b68a781287d1de8cf699a76974d</guid>
    </item>
    <item>
      <title>[CI][Inductor] Skip CPU tests when running on GPU (#115430)</title>
      <link>https://github.com/pytorch/pytorch/commit/100c466bff0bf4d330d24c957267cd5f6a537b49</link>
      <description><![CDATA[<p>[CI][Inductor] Skip CPU tests when running on GPU (#115430)</p>
<p>This is just follows the standard practice for CI, when one specifies <code>PYTORCH_TESTING_DEVICE_ONLY_FOR=cuda</code>, only tests targeting the device should be run</p>
<p>Do it by refactoring part of <code>instantiate_device_type_tests</code> into <code>get_desired_device_type_test_bases</code> and using it from test_torchinductor.py to skip CPU tests</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/115423</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115430<br />
Approved by: https://github.com/seemethere</p>]]></description>
      <pubDate>Sun, 10 Dec 2023 07:21:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/100c466bff0bf4d330d24c957267cd5f6a537b49</guid>
    </item>
    <item>
      <title>Allow `preserve_rng_state=True` when torch.compile + selective checkpointing + CUDA (#113718)</title>
      <link>https://github.com/pytorch/pytorch/commit/495054545ccd7f0d277b5d464183a35a1028d38e</link>
      <description><![CDATA[<p>Allow <code>preserve_rng_state=True</code> when torch.compile + selective checkpointing + CUDA (#113718)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/113717.</p>
<p>When <code>preserve_rng_state=True</code>, we let AOTAutograd trace through <code>torch.random.fork_rng</code> op, and the tracing doesn't work under CUDA, hence the original error reported in the issue.</p>
<p>But since we are already doing RNG functionalization at Inductor level, we don't actually need to trace this <code>fork_rng</code> op. So we should just rewrite <code>preserve_rng_state</code> to False when we are using torch.compile (and let Inductor do its RNG functionalization which it's already been doing).</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113718<br />
Approved by: https://github.com/wanchaol</p>]]></description>
      <pubDate>Fri, 08 Dec 2023 17:47:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/495054545ccd7f0d277b5d464183a35a1028d38e</guid>
    </item>
    <item>
      <title>[Indcutor][fx pass] Add sub and div pointwise ops to the post grad fusion (#115389)</title>
      <link>https://github.com/pytorch/pytorch/commit/12d7ea19af6b2efabd1fef77bd5ea7ffa7d98bf6</link>
      <description><![CDATA[<p>[Indcutor][fx pass] Add sub and div pointwise ops to the post grad fusion (#115389)</p>
<p>Summary: Titled</p>
<p>Test Plan:</p>
<h1>unit test</h1>
<p><code>buck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:group_batch_fusion</code><br />
Buck UI: https://www.internalfb.com/buck2/792c58db-c369-487d-9a42-b5da471657c0<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/2814749981661407<br />
Network: Up: 74KiB  Down: 29KiB  (reSessionID-b47c266b-12d6-4e88-8dc3-4af1dd7ecbb4)<br />
Jobs completed: 20. Time elapsed: 2:09.6s.<br />
Cache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)<br />
Tests finished: Pass 7. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<h1>local reproduce</h1>
<p>OC: P899142918<br />
MAI: P899175452</p>
<h1>e2e (oc)</h1>
<p>Differential Revision: D51957242</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115389<br />
Approved by: https://github.com/dshi7, https://github.com/jackiexu1992, https://github.com/xuzhao9</p>]]></description>
      <pubDate>Fri, 08 Dec 2023 13:07:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/12d7ea19af6b2efabd1fef77bd5ea7ffa7d98bf6</guid>
    </item>
    <item>
      <title>[inductor] Remove hashing of tensor data for constants (#115356)</title>
      <link>https://github.com/pytorch/pytorch/commit/c370450f020a0bf40e1f19e8a4e4d2f162d864a0</link>
      <description><![CDATA[<p>[inductor] Remove hashing of tensor data for constants (#115356)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115356<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Fri, 08 Dec 2023 10:05:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c370450f020a0bf40e1f19e8a4e4d2f162d864a0</guid>
    </item>
    <item>
      <title>[OAT] toggle for forcing matmul precision matching (#115326)</title>
      <link>https://github.com/pytorch/pytorch/commit/c06ab369e829971e0ecc661604ed07b4f6f83b69</link>
      <description><![CDATA[<p>[OAT] toggle for forcing matmul precision matching (#115326)</p>
<p>Summary: Add a toggle to inductor config that will force matmul precision dtypes to match between cublas and triton backends for addmm, bmm, and mm operations.</p>
<p>Test Plan: CI + model launches</p>
<p>Reviewed By: jansel</p>
<p>Differential Revision: D51442001</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115326<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 07 Dec 2023 12:22:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c06ab369e829971e0ecc661604ed07b4f6f83b69</guid>
    </item>
    <item>
      <title>[inductor] enable mkldnn op weight pre-packing on aarch64 (#115037)</title>
      <link>https://github.com/pytorch/pytorch/commit/7faa67f6efd4453befc6822b829a3a79fc31f163</link>
      <description><![CDATA[<p>[inductor] enable mkldnn op weight pre-packing on aarch64 (#115037)</p>
<p>This PR enables the fx passes and mkldnn optimizations for aarch64 It improved the bert inference performance up to 5.8x on AWS c7g instance when compared torch.compile() vs no compile path. This is enabled when pytorch is built with USE_MKLDNN_ACL option for aarch64.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115037<br />
Approved by: https://github.com/jgong5, https://github.com/malfet</p>]]></description>
      <pubDate>Thu, 07 Dec 2023 11:58:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7faa67f6efd4453befc6822b829a3a79fc31f163</guid>
    </item>
    <item>
      <title>[Inductor][fx pass] Fuse pointwise operators in the post grad (#114778)</title>
      <link>https://github.com/pytorch/pytorch/commit/b0a9641815fbe52918890bfb1e4b07daf6d97593</link>
      <description><![CDATA[<p>[Inductor][fx pass] Fuse pointwise operators in the post grad (#114778)</p>
<p>Summary: We construct a unified API that can be easily add pointwise ops to be batched in the post grad</p>
<p>Test Plan:</p>
<h1>unit test</h1>
<p><code>buck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:group_batch_fusion</code><br />
Buck UI: https://www.internalfb.com/buck2/19b3f641-782f-4f94-a953-3ff9ce2cfa7b<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/1125900251953016<br />
Network: Up: 67KiB  Down: 32KiB  (reSessionID-c2a80f26-8227-4f78-89fc-bcbda0ae8353)<br />
Jobs completed: 18. Time elapsed: 1:19.8s.<br />
Cache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)<br />
Tests finished: Pass 6. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<h1>local reproduce</h1>
<h3>cmf</h3>
<p>P881792289</p>
<h3>igctr</h3>
<h3>dsnn</h3>
<h3>icvr</h3>
<p>Reviewed By: xuzhao9</p>
<p>Differential Revision: D51332067</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114778<br />
Approved by: https://github.com/xuzhao9</p>]]></description>
      <pubDate>Thu, 07 Dec 2023 11:04:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b0a9641815fbe52918890bfb1e4b07daf6d97593</guid>
    </item>
    <item>
      <title>[inductor] adapt to the get_max_simd_tflops Triton API change (#115288)</title>
      <link>https://github.com/pytorch/pytorch/commit/7457a5f4beeba84c423dcb3b2cb5e9166e6eeaed</link>
      <description><![CDATA[<p>[inductor] adapt to the get_max_simd_tflops Triton API change (#115288)</p>
<p>Differential Revision: D51907617</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115288<br />
Approved by: https://github.com/hl475, https://github.com/chenyang78</p>]]></description>
      <pubDate>Wed, 06 Dec 2023 16:22:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7457a5f4beeba84c423dcb3b2cb5e9166e6eeaed</guid>
    </item>
    <item>
      <title>[aot_inductor][pass] fuse parallel linear based on pre grad aten IR (#114776)</title>
      <link>https://github.com/pytorch/pytorch/commit/fcf6a76108be8e7b6db528b631a7b8ebdc7470ac</link>
      <description><![CDATA[<p>[aot_inductor][pass] fuse parallel linear based on pre grad aten IR (#114776)</p>
<p>Summary:<br />
This work is for PT2 inference. Since the IR from Export will change to pre-grad aten IR in a few months. We need to start this work from now on. Here is what I do in this diff:<br />
1) Copy the fuse parallel linear pass to fb folder and adapt it to aten IR. We still want to keep the original <code>group_batch_fusion.py</code> because it is still used in training. In future at certain time point when PT2 training decided to retire the torch IR based group_batch_fusion, we can remove it. But right now, it's better to have torch IR and aten IR version seperately.</p>
<p>Our plan is to gradually transform the existing and important pre-grad passes to aten IR based passes.</p>
<p>Differential Revision: D51017854</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114776<br />
Approved by: https://github.com/zhxchen17</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 21:48:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fcf6a76108be8e7b6db528b631a7b8ebdc7470ac</guid>
    </item>
    <item>
      <title>[inductor] avoid inplace for ComplexView (#115166)</title>
      <link>https://github.com/pytorch/pytorch/commit/534f25887bafefa2680aeab5b02ee73ce2551ba6</link>
      <description><![CDATA[<p>[inductor] avoid inplace for ComplexView (#115166)</p>
<p>Fix https://github.com/pytorch/pytorch/issues/115071<br />
A regression introduced by https://github.com/pytorch/pytorch/pull/112875/files#diff-d2539c9c8dc6a3d7e457767a880612e96d3c85752a77ead49a9e4e00a3e4c3c7R335</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115166<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 20:52:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/534f25887bafefa2680aeab5b02ee73ce2551ba6</guid>
    </item>
    <item>
      <title>[Quant] [Inductor] Enable QLinear weight prepack when input dimension size exceeds 2 (#113928)</title>
      <link>https://github.com/pytorch/pytorch/commit/f6291a5e9390d606aec7540bbac50c6eb28facfd</link>
      <description><![CDATA[<p>[Quant] [Inductor] Enable QLinear weight prepack when input dimension size exceeds 2 (#113928)</p>
<p><strong>Summary</strong><br />
Enable the qlinear weight prepack when input dimension size exceeds 2. There are extra reshape node before and after the <code>addmm</code> or <code>mm</code> node if input dimension size exceeds 2.</p>
<p><strong>Test Plan</strong><br />
<code>python -m pytest test_mkldnn_pattern_matcher.py -k input_dim_exceeds_2</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113928<br />
Approved by: https://github.com/jgong5, https://github.com/eellison<br />
ghstack dependencies: #113733, #113912</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 17:24:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f6291a5e9390d606aec7540bbac50c6eb28facfd</guid>
    </item>
    <item>
      <title>[Quant] [Inductor] Enable Dequant Promotion when Linear input dimension size exceeds 2 (#113912)</title>
      <link>https://github.com/pytorch/pytorch/commit/6d0cf26c3ac705c00168de59926b6afdaf9effa9</link>
      <description><![CDATA[<p>[Quant] [Inductor] Enable Dequant Promotion when Linear input dimension size exceeds 2 (#113912)</p>
<p><strong>Summary</strong><br />
When decomposing <code>Linear</code> to <code>addmm</code> or <code>mm</code> within Inductor, if the input dimension size exceeds 2, <code>reshape</code> nodes are introduced to convert the input into a 2-dimensional form before and after the <code>addmm</code> or <code>mm</code> node. It is essential to identify and match this pattern during quantization for dequantization promotion. For instance,<br />
<code>#            quant
        #      + - - - | - - - +
        #      |    dequant    |
        #      |       |       |
        #      |    reshape    |
        #      |    /     \    |
        #      |  node1  node2 |
        #      + - | - - - | - +
        #        reshape reshape
        #      + - | - - - | - +
        #        quant    quant</code><br />
In this PR, we mainly do 2 things:</p>
<ul>
<li>Extend support for the dequantization pattern in QLinear when the input dimension size exceeds 2.</li>
<li>Revise the implementation of the dequant promotion pass, as it now needs to accommodate the matching of four different patterns.</li>
</ul>
<p><strong>Test Plan</strong><br />
<code>python -m pytest test_mkldnn_pattern_matcher.py -k input_dim_exceeds_2</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113912<br />
Approved by: https://github.com/jgong5, https://github.com/eellison<br />
ghstack dependencies: #113733</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 17:20:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6d0cf26c3ac705c00168de59926b6afdaf9effa9</guid>
    </item>
    <item>
      <title>[PyTorch] Change test_aot_inductor CPU test failures syntax (#115180)</title>
      <link>https://github.com/pytorch/pytorch/commit/ae457a2c4a86b9e789635414520ac06a26e80499</link>
      <description><![CDATA[<p>[PyTorch] Change test_aot_inductor CPU test failures syntax (#115180)</p>
<p>This portion of D50416438 is extremely subject to merge conflicts. It can also be safely landed without full CI round trip because it changes just one test file that we can simply run to make sure it works.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D51856943/">D51856943</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115180<br />
Approved by: https://github.com/mikekgfb, https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 15:55:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ae457a2c4a86b9e789635414520ac06a26e80499</guid>
    </item>
    <item>
      <title>remove aot_config.keep_inference_input_mutations from assert_functional_graph (#115195)</title>
      <link>https://github.com/pytorch/pytorch/commit/1102d37958d269e929294ead2254e496da2cde03</link>
      <description><![CDATA[<p>remove aot_config.keep_inference_input_mutations from assert_functional_graph (#115195)</p>
<p>We technically allow backends to aot_autograd to pass a config saying "yes I am ok with seeing input mutations in my graph".</p>
<p>With https://github.com/pytorch/pytorch/pull/112906 though, there can be input mutations that show up in the backward (that we need to handle for correctness), that are a large pain to keep out of the graph. The meta-point is that it's been ~a year since we added the config, and it almost always makes sense for backends to support input mutations for performance reasons (inductor does). So I just allow these input mutations in the graph in this rare backward situation, even if the backend didn't explicitly use the config.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115195<br />
Approved by: https://github.com/drisspg</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 15:36:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1102d37958d269e929294ead2254e496da2cde03</guid>
    </item>
    <item>
      <title>[inductor] Add ir.Scan and lower aten.cumsum on CUDA (#106581)</title>
      <link>https://github.com/pytorch/pytorch/commit/7aac689b199fb3fd557a4c3ddccc5c619eb6e273</link>
      <description><![CDATA[<p>[inductor] Add ir.Scan and lower aten.cumsum on CUDA (#106581)</p>
<p>This adds the <code>ir.Scan</code> node (currently only supported on CUDA) which re-uses the existing reduction kernel machinery to support different kinds of non-pointwise ops. Just like reductions it supports prologue and epilogue fusions and has both persistent and non-persistent kernel generation.</p>
<p>Currently this doesn't support the equivalent of <code>Reduction.create_multilayer</code> and will instead fall back to eager in those cases. This is because splitting into multiple kernel invocations ends up being far slower than cub's single kernel strategy which matches the performance of a copy kernel.</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/93631</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/106581<br />
Approved by: https://github.com/lezcano, https://github.com/atalman</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 15:31:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7aac689b199fb3fd557a4c3ddccc5c619eb6e273</guid>
    </item>
    <item>
      <title>[AOTInductor] Double buffering for Weights (#114446)</title>
      <link>https://github.com/pytorch/pytorch/commit/80527c0cf24e3e7255b92bbaf0fe85a2e83467f2</link>
      <description><![CDATA[<p>[AOTInductor] Double buffering for Weights (#114446)</p>
<p>Summary:<br />
This adds function to model container doing weight swapping with double buffering.</p>
<p>There are 2 parts for double buffering<br />
a) Write constants into inactive buffer<br />
b) Swap active buffer</p>
<p>For (a), we write the constants into the buffer that's currently not in use, and store the information in both constants map and the corresponding constant array to read.<br />
For (b), we obtain the lock, and activate the constant map/constant array that is inactive, and flag the one that's currently in use to inactive.</p>
<p>Test Plan:<br />
test/cpp/aot_inductor/test.cpp</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D51543732">D51543732</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/114446<br />
Approved by: https://github.com/chenyang78, https://github.com/eellison</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 14:31:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/80527c0cf24e3e7255b92bbaf0fe85a2e83467f2</guid>
    </item>
    <item>
      <title>[Inductor][fx pass] Fix a bug in batch linear fusion in the post grad (#115061) (#115131)</title>
      <link>https://github.com/pytorch/pytorch/commit/f09e8381b724d497b389ecadabcdc7fc938596d3</link>
      <description><![CDATA[<p>[Inductor][fx pass] Fix a bug in batch linear fusion in the post grad (#115061) (#115131)</p>
<p>Summary:</p>
<p>Titled</p>
<p>Test Plan:<br />
<code>buck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:group_batch_fusion</code><br />
Buck UI: https://www.internalfb.com/buck2/ab4b918c-9ffa-4d00-a747-880521a27851<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/16607023638890043<br />
Network: Up: 11MiB  Down: 117MiB  (reSessionID-079402d0-8fd7-4797-9ed5-dd0f778dce1a)<br />
Jobs completed: 189430. Time elapsed: 2:02.5s.<br />
Cache hits: 99%. Commands: 77000 (cached: 76995, remote: 5, local: 0)<br />
Tests finished: Pass 7. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<p>Reviewed By: mengluy0125</p>
<p>Differential Revision: D51796899</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115131<br />
Approved by: https://github.com/mengluy0125</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 13:20:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f09e8381b724d497b389ecadabcdc7fc938596d3</guid>
    </item>
    <item>
      <title>[AOTI] Handle empty input args (#114682)</title>
      <link>https://github.com/pytorch/pytorch/commit/e06bff8bbea84ce672285bc34690b2e45a1b63ab</link>
      <description><![CDATA[<p>[AOTI] Handle empty input args (#114682)</p>
<p>Summary: When the model takes no inputs, AOTInductor relies on checking weights to figure out which device to compile the model into. Currently recording buffer device type happens too late, and this PR fixes that.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114682<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 07:02:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e06bff8bbea84ce672285bc34690b2e45a1b63ab</guid>
    </item>
    <item>
      <title>torch.compile should auto-functionalize certain mutable ops (#114955)</title>
      <link>https://github.com/pytorch/pytorch/commit/cfa4370c079ddc8641dabfc7576a5b4d79a7d286</link>
      <description><![CDATA[<p>torch.compile should auto-functionalize certain mutable ops (#114955)</p>
<p>Users may wish to torch.compile custom ops that mutate their inputs<br />
and return nothing (this is a common class of operators).<br />
torch.compile will automatically support this op without anyone needing<br />
to provide a functionalization kernel for it. Here's how.</p>
<p>Let's say we have a hypothetical mylib::sin_(Tensor(a!) x) -&gt; ()<br />
op. First, when FakeTensor sees this op, it can just return None.<br />
This is the case because custom ops are not allowed to mutate input<br />
metadata, so the FakeTensor rule for one that returns nothing is trivial.</p>
<p>Next, when Python FunctionalTensor sees the op, it will functionalize<br />
it by emitting a call to an auto_functionalize(op, ["x"], {"x": ...})<br />
HOP and replacing the mutated inputs with the outputs of this HOP.<br />
This HOP effectively runs the functional version of the op when<br />
called: it clones inputs that will be mutated, runs the op, and<br />
then returns Tensors with the new values.</p>
<p>In the future we can teach Inductor how to do re-inplacing when it sees<br />
this HOP (like how triton kernels do it) but this isn't urgent (and is<br />
more of a performance problem).</p>
<p>Test Plan:<br />
- new tests</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114955<br />
Approved by: https://github.com/bdhirsh</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 06:53:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cfa4370c079ddc8641dabfc7576a5b4d79a7d286</guid>
    </item>
    <item>
      <title>[Inductor][Optimus]Move group/batch fusion logic out of inductor (#115128)</title>
      <link>https://github.com/pytorch/pytorch/commit/58809e8914c20b4c73075eb1ab49b7b5cdbdd0d7</link>
      <description><![CDATA[<p>[Inductor][Optimus]Move group/batch fusion logic out of inductor (#115128)</p>
<p>Summary:<br />
As discussed D51695982, fusion may not be always good. We want to let the user customize the fx passes.</p>
<p>Some example for new configs:<br />
* Use batch_fusion config: this will automatically use the following batch fusions, including batch linear, layernorm, relu, tanh, sigmoid and post grad batch linear fusion<br />
* use config:<br />
<code>"pre_grad_fusion_options": {
            "batch_linear": {"min_fuse_set_size": 10},
            "batch_linear_lhs": {},
            "batch_layernorm": {"max_fuse_search_depth": 100},
            "batch_tanh": {},
            "batch_relu": {},
            "batch_sigmoid": {}
          },</code></p>
<p>Test Plan:<br />
with flag: f509168388</p>
<p>with config: f509168595</p>
<p>Reviewed By: frank-wei, mengluy0125</p>
<p>Differential Revision: D51817314</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115128<br />
Approved by: https://github.com/mengluy0125</p>]]></description>
      <pubDate>Tue, 05 Dec 2023 00:19:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/58809e8914c20b4c73075eb1ab49b7b5cdbdd0d7</guid>
    </item>
    <item>
      <title>Add get_mutation_names to ir.Wait (#115104)</title>
      <link>https://github.com/pytorch/pytorch/commit/1d0e70ad654451fa2d88599047de6e7fbf62ae5e</link>
      <description><![CDATA[<p>Add get_mutation_names to ir.Wait (#115104)</p>
<p><code>ir.Wait</code> generates the last 2 lines of this code:<br />
```python<br />
buf1_work = dist.all_gather_into_tensor(buf1[0], buf1_inputs[0], async_op=True, group=buf1_pg)<br />
fun_col_impl._register_tensor_work(buf1, buf1_work)<br />
buf2 = buf1[0]<br />
del buf1</p>
<p>buf2 = _wait_tensor(buf2)  #  &lt;- generated by ir.Wait<br />
buf3 = buf2;  # reuse  &lt;- generated by ir.Wait<br />
<code>``</code>_wait_tensor<code>technically is a "mutation" op that changes</code>buf2<code>in place. So we should mark</code>ir.Wait<code>as a mutation op (by overriding its</code>get_mutation_names()`).</p>
<p>This fixes a very peculiar issue when inductor comm reordering is used for llama model: downstream nodes that uses the all-gather comm output sometimes takes dependency on <code>buf2</code> (the node before <code>ir.Wait</code>) instead of on <code>buf3</code> (<code>ir.Wait</code>) (it's still unclear why it behaves like this). To work around the issue, we add the missing annotation that <code>buf3</code> is a mutation of <code>buf2</code>, so that the scheduler knows to schedule <code>buf3</code> before any of the <code>buf2</code> users.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115104<br />
Approved by: https://github.com/wanchaol</p>]]></description>
      <pubDate>Mon, 04 Dec 2023 19:54:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1d0e70ad654451fa2d88599047de6e7fbf62ae5e</guid>
    </item>
    <item>
      <title>[inductor] Replace rand[n].generator with inductor prim if generator=None (#115051)</title>
      <link>https://github.com/pytorch/pytorch/commit/3cf5348239be353c4ce06fc82a2c64fd364fb2cd</link>
      <description><![CDATA[<p>[inductor] Replace rand[n].generator with inductor prim if generator=None (#115051)</p>
<p>This fixes the "should have been handled in replace_random.py" error<br />
raised during lowering.</p>
<p>I also fixed <code>test_randn_generator</code> to catch any regressions.<br />
Previously, it did not use the result of randn(), so dynamo tracing<br />
omitted that node entirely.</p>
<p>Fixes #114203.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115051<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Mon, 04 Dec 2023 17:53:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3cf5348239be353c4ce06fc82a2c64fd364fb2cd</guid>
    </item>
    <item>
      <title>Fix `torch.inductor._utils.get_device_tflops` on ROCm  (#115102)</title>
      <link>https://github.com/pytorch/pytorch/commit/f6b6fad1367804cebdfdd5f5b7f92b2fb1cefe79</link>
      <description><![CDATA[<p>Fix <code>torch.inductor._utils.get_device_tflops</code> on ROCm  (#115102)</p>
<p>That caused numerous test regressions after https://github.com/pytorch/pytorch/pull/114772 changed triton APIs a bit to use <code>nvsmi</code> function, which is not available on <code>hip</code> platform</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/115087</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115102<br />
Approved by: https://github.com/desertfire, https://github.com/huydhn</p>]]></description>
      <pubDate>Mon, 04 Dec 2023 16:56:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f6b6fad1367804cebdfdd5f5b7f92b2fb1cefe79</guid>
    </item>
    <item>
      <title>FF inductor failure (#114980)</title>
      <link>https://github.com/pytorch/pytorch/commit/a9e9590934795c969840306e1a7117f04f873848</link>
      <description><![CDATA[<p>FF inductor failure (#114980)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114980<br />
Approved by: https://github.com/eellison, https://github.com/bdhirsh</p>]]></description>
      <pubDate>Mon, 04 Dec 2023 10:26:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a9e9590934795c969840306e1a7117f04f873848</guid>
    </item>
    <item>
      <title>[inductor][cpp] avoid redundant lowp type cast for direct load/store (#115006)</title>
      <link>https://github.com/pytorch/pytorch/commit/bfa2c844a86202930137492f91ea36b2e6e05384</link>
      <description><![CDATA[<p>[inductor][cpp] avoid redundant lowp type cast for direct load/store (#115006)</p>
<p>Fix https://github.com/pytorch/pytorch/issues/114879. See https://github.com/pytorch/pytorch/issues/114879#issuecomment-1836977610 for details.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115006<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Sun, 03 Dec 2023 22:39:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bfa2c844a86202930137492f91ea36b2e6e05384</guid>
    </item>
    <item>
      <title>[Inductor] Do not promote int to float for torch.mm (#115043)</title>
      <link>https://github.com/pytorch/pytorch/commit/3da67ffad1ab0f1a65e8838de315d36fd7f21d9e</link>
      <description><![CDATA[<p>[Inductor] Do not promote int to float for torch.mm (#115043)</p>
<p>This PR fixes inductor silently promoting int to float and causing behavior difference</p>
<p>Fixes #98978</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115043<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Sun, 03 Dec 2023 22:36:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3da67ffad1ab0f1a65e8838de315d36fd7f21d9e</guid>
    </item>
    <item>
      <title>[Inductor] We re-enable the batch_fusion and group_fusion flags in order not to disturb the current production model implementation (#114841)</title>
      <link>https://github.com/pytorch/pytorch/commit/50833021dd39eece1d0c0cf268d7b36dd787c338</link>
      <description><![CDATA[<p>[Inductor] We re-enable the batch_fusion and group_fusion flags in order not to disturb the current production model implementation (#114841)</p>
<p>Summary:<br />
We did two things:<br />
1. We add back the batch_fusion and group_fusion flags to keep the current production model implementation</p>
<ol>
<li>We tell batch and group fusion in the post grad since group need fbgemm.</li>
</ol>
<p>Test Plan:<br />
<code>buck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:group_batch_fusion</code><br />
Buck UI: https://www.internalfb.com/buck2/13d152d2-5d4d-4c7a-ab88-51f8e8218942<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/1125900253044737<br />
Network: Up: 376KiB  Down: 44KiB  (reSessionID-c508aedc-8cc2-434a-8c17-bbe075a05562)<br />
Jobs completed: 17. Time elapsed: 1:23.1s.<br />
Cache hits: 0%. Commands: 1 (cached: 0, remote: 0, local: 1)<br />
Tests finished: Pass 6. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<p>Differential Revision: D51695982</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114841<br />
Approved by: https://github.com/jackiexu1992</p>]]></description>
      <pubDate>Sun, 03 Dec 2023 15:59:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/50833021dd39eece1d0c0cf268d7b36dd787c338</guid>
    </item>
    <item>
      <title>[inductor] Add dropout type check to match eager (#115040)</title>
      <link>https://github.com/pytorch/pytorch/commit/7979ba7b4327d1fabbeb06bafb2f1d2a2269f8b9</link>
      <description><![CDATA[<p>[inductor] Add dropout type check to match eager (#115040)</p>
<p>Fixes #98970</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115040<br />
Approved by: https://github.com/oulgen</p>]]></description>
      <pubDate>Sun, 03 Dec 2023 15:05:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7979ba7b4327d1fabbeb06bafb2f1d2a2269f8b9</guid>
    </item>
    <item>
      <title>[inductor] Fix shape mismatch in sdpa pattern matcher (#115038)</title>
      <link>https://github.com/pytorch/pytorch/commit/69a8f9b07ecce745c708b4c902e4e55a902869e5</link>
      <description><![CDATA[<p>[inductor] Fix shape mismatch in sdpa pattern matcher (#115038)</p>
<p>Fixes #100316</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/115038<br />
Approved by: https://github.com/oulgen</p>]]></description>
      <pubDate>Sun, 03 Dec 2023 14:32:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/69a8f9b07ecce745c708b4c902e4e55a902869e5</guid>
    </item>
    <item>
      <title>[aotinductor] support at::convolution for AOTInductor (#114961)</title>
      <link>https://github.com/pytorch/pytorch/commit/4d8b9964e18ad2507fa2bac0538b7b925a397a62</link>
      <description><![CDATA[<p>[aotinductor] support at::convolution for AOTInductor (#114961)</p>
<p>This PR adds support to at::convolution for AOTInductor</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114961<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Sat, 02 Dec 2023 23:52:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4d8b9964e18ad2507fa2bac0538b7b925a397a62</guid>
    </item>
    <item>
      <title>[inductor] Update triton pin (#114772)</title>
      <link>https://github.com/pytorch/pytorch/commit/8a90249bc2079f5ae7398ff87fc7c362852836e3</link>
      <description><![CDATA[<p>[inductor] Update triton pin (#114772)</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D51761353">D51761353</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/114772<br />
Approved by: https://github.com/shunting314, https://github.com/atalman</p>]]></description>
      <pubDate>Sat, 02 Dec 2023 11:13:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8a90249bc2079f5ae7398ff87fc7c362852836e3</guid>
    </item>
    <item>
      <title>Reland #113487 and #112527 (sdpa shim &amp; fp8 AOTInductor support) (#114974)</title>
      <link>https://github.com/pytorch/pytorch/commit/f1fd02503bd206fd810195294ead9dce3e2f5d2d</link>
      <description><![CDATA[<p>Reland #113487 and #112527 (sdpa shim &amp; fp8 AOTInductor support) (#114974)</p>
<p>This is a backout of #113747 which reverted the above two commits. Now that</p>
<h1>113997 has landed, this diff can be landed safely without breaking ABI compatibility.</h1>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114974<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Fri, 01 Dec 2023 19:25:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f1fd02503bd206fd810195294ead9dce3e2f5d2d</guid>
    </item>
    <item>
      <title>[AOTInductor] Generate Triton header even if scheduler is not invoked. (#114972)</title>
      <link>https://github.com/pytorch/pytorch/commit/a9aad4ea21f20e60b41e7724f9a0cc9bebdab6dc</link>
      <description><![CDATA[<p>[AOTInductor] Generate Triton header even if scheduler is not invoked. (#114972)</p>
<p>Summary:<br />
Generate Triton header for profiling.<br />
If Triton header isn't generated through Scheduler, generate it directly<br />
when in wrapper codegen.</p>
<p>Test Plan:<br />
Test included in commit.<br />
(test_aot_inductor.py:test_with_no_triton_profiler)</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114972<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire</p>]]></description>
      <pubDate>Fri, 01 Dec 2023 18:03:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a9aad4ea21f20e60b41e7724f9a0cc9bebdab6dc</guid>
    </item>
    <item>
      <title>[AOTInductor] Add method to get storage size in shim (#114976)</title>
      <link>https://github.com/pytorch/pytorch/commit/fb806f487f3679482429ff2e87c819d74dd4d4c7</link>
      <description><![CDATA[<p>[AOTInductor] Add method to get storage size in shim (#114976)</p>
<p>Summary:<br />
Add a method to get storage size.</p>
<p>Test Plan:<br />
N/A, for FC, test will come after packaged.</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/114976<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire</p>]]></description>
      <pubDate>Fri, 01 Dec 2023 17:54:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fb806f487f3679482429ff2e87c819d74dd4d4c7</guid>
    </item>
    <item>
      <title>[dynamo] handle setting .data on a tensor (#113080)</title>
      <link>https://github.com/pytorch/pytorch/commit/4cfe99749074ff8b9dd6576c100d805e57b0a836</link>
      <description><![CDATA[<p>[dynamo] handle setting .data on a tensor (#113080)</p>
<p><strong>Dynamo</strong></p>
<p>We don't want setattr in the graph. Setting data has interesting implications on both aliasing and on the autograd engine.</p>
<p>The safe recipe is:</p>
<p>1) Disable grad<br />
2) Call set_()<br />
3) Manually lower the version counter on the object to hide it from the autograd engine</p>
<p>This is effectively the same exact thing as setting .data, and it composes properly with aot_autograd and inductor.</p>
<p><strong>aot_autograd</strong></p>
<p>For aot_autograd, there's another snag.</p>
<p>Specifically, when we invoke aot_autograd, we call <code>fake_mode.from_tensor()</code>, relying on memo to get the right tensor out. For .data mutations, this doesn't work, because the memoized fake_tensor is in the state it will be in at the end of the trace, not at the beginning. This means that the .data call is already applied, and the tensor shape (as in the case of these tests) mismatches. aot_autograd produces an invalid graph, with illegal calls like <code>torch.ops.aten.view.default(primals_2, [0])</code> where primals is actually sized <code>([6])</code> on input.</p>
<p>The new plan here is to:<br />
1) Record tensor fakification policy in dynamo<br />
2) provide a fresh fake mode to all backends<br />
3) Invoke from_tensor with the stored policy to get fresh new fake tensors in aot_autograd</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113080<br />
Approved by: https://github.com/bdhirsh</p>]]></description>
      <pubDate>Fri, 01 Dec 2023 16:35:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4cfe99749074ff8b9dd6576c100d805e57b0a836</guid>
    </item>
    <item>
      <title>improve mkldnn_linear_pointwise performance for contiguous tensor with non default contiguous strides (#114939)</title>
      <link>https://github.com/pytorch/pytorch/commit/80d8a2a2376bca1b9c33527310f9f33e588da82d</link>
      <description><![CDATA[<p>improve mkldnn_linear_pointwise performance for contiguous tensor with non default contiguous strides (#114939)</p>
<p>This PR will convert the stride to the default contiguous stride in <code>mkldnn_linear_pointwise</code> before calling oneDNN to run into an optimization path similar to https://github.com/pytorch/pytorch/pull/99511. Also refactored the code to provide a common utility function.</p>
<p>https://github.com/pytorch/pytorch/pull/111976 will ignore Dims of value 1 in Require_Stride_order. For a tensor with <code>size = [1, 1280]</code>, <code>stride = [0, 1]</code>:<br />
<strong>Before the above PR</strong>, it is considered as non-contiguous, thus in the below call, it is converted to <code>size = [1, 1280]</code>, <code>stride = [1280,1]</code>:<br />
https://github.com/pytorch/pytorch/blob/25b83521be3174e91da8ce60123b4811c3d78a75/torch/_inductor/ir.py#L5263</p>
<p><strong>While after the above PR</strong>, dims of value 1 are ignored so this tensor is already contiguous and we'll feed a tensor with <code>stride = [0, 1]</code> to oneDNN, which results in poor performance.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114939<br />
Approved by: https://github.com/jgong5</p>]]></description>
      <pubDate>Fri, 01 Dec 2023 15:30:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/80d8a2a2376bca1b9c33527310f9f33e588da82d</guid>
    </item>
    <item>
      <title>[inductor] post_grad batched linear fusion (#112504)</title>
      <link>https://github.com/pytorch/pytorch/commit/1bcefaf5750f023e2adaa571e392191b156ecbac</link>
      <description><![CDATA[<p>[inductor] post_grad batched linear fusion (#112504)</p>
<p>Summary: Fusing independent nn.Linear() functions with aten.bmm and aten.cat.</p>
<p>Test Plan:<br />
Without the BMM fusion:<br />
<code>buck2 run @mode/opt //pytorch/benchmark:run -- test_module -d cuda --module test_linear_module --torchdynamo inductor --torchinductor_cudagraph 0 --torchinductor_batch_fusion 0</code><br />
https://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree/traces/test/torchbench_test_module_20231030_072536_6535183793.json.gz&amp;bucket=pyper_traces</p>
<p>100 aten::mm operators</p>
<p>With the BMM fusion:<br />
<code>buck2 run @mode/opt //pytorch/benchmark:run -- test_module -d cuda --module test_linear_module --torchdynamo inductor --torchinductor_cudagraph 0 --torchinductor_batch_fusion 1</code></p>
<p>20 aten::bmm operators</p>
<p>https://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree/traces/test/torchbench_test_module_20231030_072157_6535183793.json.gz&amp;bucket=pyper_traces</p>
<p>Passes accuracy test:<br />
<code>$ buck2 run @mode/opt //pytorch/benchmark:run -- test_module -d cuda --module test_linear_module --torchdynamo inductor --torchinductor_cudagraph 0 --torchinductor_batch_fusion 1 --accuracy
Running eval method from test_module on cuda in dynamo inductor mode with input batch size 4 and precision tf32.
Accuracy:                            pass</code><br />
Looks like the bmm and input cat has been fused successfully.</p>
<p>Checking the triton codegen:</p>
<p><code>TORCH_LOGS=+dynamo,+aot,+inductor buck2 run @mode/opt //pytorch/benchmark:run -- test_module -d cuda --module test_linear_module --torchdynamo inductor --torchinductor_cudagraph 0 --torchinductor_batch_fusion 1 --dump_triton 1</code></p>
<p>Triton code dump: https://www.internalfb.com/intern/everpaste/?handle=GHp1ABaqYuTjYCUBALiTWmteaI1PbsIXAAAB</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112504<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Fri, 01 Dec 2023 11:26:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1bcefaf5750f023e2adaa571e392191b156ecbac</guid>
    </item>
    <item>
      <title>[inductor][Observability] Add log for Optimus to enable easier debug (#110452)</title>
      <link>https://github.com/pytorch/pytorch/commit/93b1e47586d337e0c558226bd113e310971cb8d3</link>
      <description><![CDATA[<p>[inductor][Observability] Add log for Optimus to enable easier debug (#110452)</p>
<p>Summary: The log breaks one of ads-model export flows, and we change the log to debug</p>
<p>Test Plan: see details in D49710166</p>
<p>Differential Revision: D49844303</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110452<br />
Approved by: https://github.com/jackiexu1992</p>]]></description>
      <pubDate>Fri, 01 Dec 2023 10:25:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/93b1e47586d337e0c558226bd113e310971cb8d3</guid>
    </item>
    <item>
      <title>[inductor][easy] print out exception message upon failing to write to a file (#114836)</title>
      <link>https://github.com/pytorch/pytorch/commit/235eaabfedbecabd0c5e05ba5a6d92ba578453b1</link>
      <description><![CDATA[<p>[inductor][easy] print out exception message upon failing to write to a file (#114836)</p>
<p>To address Oleg's internal review feedback.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114836<br />
Approved by: https://github.com/khabinov</p>]]></description>
      <pubDate>Thu, 30 Nov 2023 18:40:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/235eaabfedbecabd0c5e05ba5a6d92ba578453b1</guid>
    </item>
    <item>
      <title>[inductor] Fix in CppPrinter._print_Pow (#114872)</title>
      <link>https://github.com/pytorch/pytorch/commit/c867fddab5daabc1e2cb976d6bacb03a00f7d8bd</link>
      <description><![CDATA[<p>[inductor] Fix in CppPrinter._print_Pow (#114872)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114872<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Thu, 30 Nov 2023 12:21:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c867fddab5daabc1e2cb976d6bacb03a00f7d8bd</guid>
    </item>
    <item>
      <title>Inductor cpp wrapper: fix buffer free in non-AOT mode (#114741)</title>
      <link>https://github.com/pytorch/pytorch/commit/e3c42d3fb360e632050a423797cdd763729c4f9c</link>
      <description><![CDATA[<p>Inductor cpp wrapper: fix buffer free in non-AOT mode (#114741)</p>
<p>We found performance regression when using cpp wrapper in non-AOT mode due to the change in https://github.com/pytorch/pytorch/pull/110892.<br />
https://github.com/pytorch/pytorch/pull/110892 only handles the buffer cache in AOT mode but removes the <code>reset</code> call without checking whether AOT mode is on or off. This PR updates the buffer free change to only happen when <code>V.graph.aot_mode is True</code>.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114741<br />
Approved by: https://github.com/jgong5, https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 30 Nov 2023 08:46:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e3c42d3fb360e632050a423797cdd763729c4f9c</guid>
    </item>
    <item>
      <title>[inductor] add a config to specify the shape attribute for the generated svg graphs (#114811)</title>
      <link>https://github.com/pytorch/pytorch/commit/5c3f03e2dd661dd1858fdcebb5d1448a8e23097b</link>
      <description><![CDATA[<p>[inductor] add a config to specify the shape attribute for the generated svg graphs (#114811)</p>
<p>We draw our fx graphs with the "record" shape attribute by default.<br />
Sometimes, when the graph is very complex, we may hit dot errors like below:<br />
  "flat edge between adjacent nodes one of which has a record shape -<br />
   replace records with HTML-like labels"<br />
and thus fail to generate a graph. So, let's give the user an option<br />
to specify the shape attribute for the dot graph. For example, passing<br />
INDUCTOR_DOT_GRAPH_SHAPE_SVG = "none" would let us generate HTML-like lables<br />
to workaround the above failure.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114811<br />
Approved by: https://github.com/weifengpy</p>]]></description>
      <pubDate>Wed, 29 Nov 2023 22:10:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5c3f03e2dd661dd1858fdcebb5d1448a8e23097b</guid>
    </item>
    <item>
      <title>[easy][aotinductor] fix typos &amp; add static typing (#114728)</title>
      <link>https://github.com/pytorch/pytorch/commit/5262484ecefb03da62ed36053c9a61c1ca938ede</link>
      <description><![CDATA[<p>[easy][aotinductor] fix typos &amp; add static typing (#114728)</p>
<p><code>// check all references
$ grep -rl 'cpp_kernel_overlad_name' *
ir.py</code></p>
<p><code>$ lintrunner --take MYPYINDUCTOR torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py
ok No lint issues.</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114728<br />
Approved by: https://github.com/Skylion007, https://github.com/chenyang78</p>]]></description>
      <pubDate>Wed, 29 Nov 2023 18:10:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5262484ecefb03da62ed36053c9a61c1ca938ede</guid>
    </item>
    <item>
      <title>[CI] Use linux.12xlarge for cpu_inductor integration tests (#114729)</title>
      <link>https://github.com/pytorch/pytorch/commit/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70</link>
      <description><![CDATA[<p>[CI] Use linux.12xlarge for cpu_inductor integration tests (#114729)</p>
<p>Summary: use linux.12xlarge for larger memory to avoid OOM</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114729<br />
Approved by: https://github.com/huydhn</p>]]></description>
      <pubDate>Wed, 29 Nov 2023 10:39:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70</guid>
    </item>
    <item>
      <title>[Inductor][fx pass] Refactor code to easily add pointwise op to do the batch fusion (#113381)</title>
      <link>https://github.com/pytorch/pytorch/commit/c1f7d4ad6ac7ee39991cb37e3b9d2d570135c5d5</link>
      <description><![CDATA[<p>[Inductor][fx pass] Refactor code to easily add pointwise op to do the batch fusion (#113381)</p>
<p>Summary:<br />
1. We refactor the code to have a unified API to add pointwise op</p>
<ol>
<li>Add one more op sigmoid since we observed it in MC models</li>
</ol>
<p>Test Plan:</p>
<h1>local reproduce for CMF</h1>
<p><code>buck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split_batch -c</code><br />
P876977403<br />
P876996776</p>
<p>diffing: https://www.internalfb.com/intern/diffing/?paste_number=876999623</p>
<p>Differential Revision: D51142990</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113381<br />
Approved by: https://github.com/xuzhao9</p>]]></description>
      <pubDate>Wed, 29 Nov 2023 10:29:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c1f7d4ad6ac7ee39991cb37e3b9d2d570135c5d5</guid>
    </item>
    <item>
      <title>[ROCm] Add ROCm AMDGPU support for inductor cpp codegen (#105141)</title>
      <link>https://github.com/pytorch/pytorch/commit/4a4c9fb0b8e2fe11316a1f998a71a2fba7acfc16</link>
      <description><![CDATA[<p>[ROCm] Add ROCm AMDGPU support for inductor cpp codegen (#105141)</p>
<p>Follows from previous enablement attempt: https://github.com/pytorch/pytorch/pull/101797</p>
<p>Adds support for hsaco binaries in inductor's cpp_wrapper codegen and enables the CUDA tests in test_cpp_wrapper.</p>
<p>This PR also brings in additional required hipify mappings for the wrapper codegen file.</p>
<p>NOTE: we can unskip some of these tests when we enabled MI210 runners.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/105141<br />
Approved by: https://github.com/jansel, https://github.com/malfet</p>]]></description>
      <pubDate>Wed, 29 Nov 2023 07:11:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4a4c9fb0b8e2fe11316a1f998a71a2fba7acfc16</guid>
    </item>
    <item>
      <title>[PyTorch] Remove hardcoded device=cuda in test_aot_inductor (#112797)</title>
      <link>https://github.com/pytorch/pytorch/commit/ce00c8fb456f057b4ad9916344a78bbd77e645a7</link>
      <description><![CDATA[<p>[PyTorch] Remove hardcoded device=cuda in test_aot_inductor (#112797)</p>
<p>All the other tests use self.device, so this seems like an oversight? Cost me a lot of time debugging the minimal arrayref interface, which is only intended for CPU.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50949928/">D50949928</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112797<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire, https://github.com/khabinov<br />
ghstack dependencies: #113997</p>]]></description>
      <pubDate>Tue, 28 Nov 2023 19:12:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ce00c8fb456f057b4ad9916344a78bbd77e645a7</guid>
    </item>
    <item>
      <title>[sparse][semi-structured][inductor] meta registrations for _cslt_sparse_mm + additional stride checking in test. (#114685)</title>
      <link>https://github.com/pytorch/pytorch/commit/ae593d039370e70869a01e84d4774fa58a3ecbf8</link>
      <description><![CDATA[<p>[sparse][semi-structured][inductor] meta registrations for _cslt_sparse_mm + additional stride checking in test. (#114685)</p>
<p>_cslt_sparse_mm + additional stride checking in test.</p>
<p>Summary:</p>
<p>This PR adds in meta registrations for _cslt_sparse_mm.</p>
<p>Based on the work @drisspg did<br />
in #114370.</p>
<p>Additionally, it updates the tests by checking that the strides of the<br />
spare result and the result returned by sparse+compile are the same, to<br />
avoid errors like those found in</p>
<p>https://github.com/pytorch/pytorch/pull/114477.</p>
<p>Test Plan:<br />
<code>python test/test_sparse_semi_structred -k compile_cusparselt
python test/test_sparse_semi_structred -k compile_cutlass</code></p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/114685<br />
Approved by: https://github.com/alexsamardzic, https://github.com/drisspg</p>]]></description>
      <pubDate>Tue, 28 Nov 2023 16:31:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ae593d039370e70869a01e84d4774fa58a3ecbf8</guid>
    </item>
    <item>
      <title>Add dtensor and fsdp/2d tests to inductor_distributed CI (#114642)</title>
      <link>https://github.com/pytorch/pytorch/commit/3fccc0446cc4f78f97e2b227a06e196ffee488d4</link>
      <description><![CDATA[<p>Add dtensor and fsdp/2d tests to inductor_distributed CI (#114642)</p>
<p>Smuggle important and not too slow tests to run on this trunk job,<br />
instead of just on the periodic job where they currently reside.<br />
 - test_dtensor_compile took 70sec, test_fsdp_2d_parallel took 198sec<br />
   locally</p>
<p>As a follow up, organize the distributed-mgpu tests better and maybe<br />
rename this job to reflect its more 'general dist mgpu'</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114642<br />
Approved by: https://github.com/wanchaol, https://github.com/malfet</p>]]></description>
      <pubDate>Tue, 28 Nov 2023 15:06:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3fccc0446cc4f78f97e2b227a06e196ffee488d4</guid>
    </item>
    <item>
      <title>Remove yet more type-ignores in dynamo/inductor (#114684)</title>
      <link>https://github.com/pytorch/pytorch/commit/47e6cc4d22c1721d6e537d1d397f766c3de3ba21</link>
      <description><![CDATA[<p>Remove yet more type-ignores in dynamo/inductor (#114684)</p>
<p>Probably the last big batch for a while</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114684<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Tue, 28 Nov 2023 14:09:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/47e6cc4d22c1721d6e537d1d397f766c3de3ba21</guid>
    </item>
    <item>
      <title>[inductor] Fix torch.split bug on unbacked symint (#113406)</title>
      <link>https://github.com/pytorch/pytorch/commit/74e10f0f60a26fe07a5aa39b699edb4a5ae29b14</link>
      <description><![CDATA[<p>[inductor] Fix torch.split bug on unbacked symint (#113406)</p>
<p>torch.split(x, l) fails when l's shape is the unbacked symint.</p>
<p>E.g. l =<br />
y.tolist() makes l the unbacked shape, because l depends on the<br />
data access of y. The downdtream call <code>SliceView.create()</code><br />
evaluates the shape even if the input shape is unbacked symint,<br />
which brings up the bug.</p>
<p>Test Plan:<br />
python test/inductor/test_unbacked_symints.py -k test_split_with_sizes</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113406<br />
Approved by: https://github.com/aakhundov, https://github.com/ezyang</p>]]></description>
      <pubDate>Tue, 28 Nov 2023 12:45:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/74e10f0f60a26fe07a5aa39b699edb4a5ae29b14</guid>
    </item>
    <item>
      <title>[inductor] `_sparse_semi_structured_linear` fallback - no meta registration; not on testing path (#114477)</title>
      <link>https://github.com/pytorch/pytorch/commit/cef79c0df4a340e812efd7fd8534a7fe3ba0109b</link>
      <description><![CDATA[<p>[inductor] <code>_sparse_semi_structured_linear</code> fallback - no meta registration; not on testing path (#114477)</p>
<p>Test was wrong in original PR and merged changes were never tested. Further, the sparse op was never actually compiled due to missing <code>fullgraph=True</code> and missing meta registration.</p>
<p>When meta is added as per this PR, it gives wrong answers when input needs to be padded and when input needs to be reshaped.</p>
<p>Is this something to do with the generated inductor code for:<br />
<code>constant_pad_nd: "f16[32, 128]" = torch.ops.aten.constant_pad_nd.default(primals_3, [0, 0, 0, 31], 0.0)
...
slice_1: "f16[1, 128]" = torch.ops.aten.slice.Tensor(_sparse_semi_structured_linear, 0, 0, 1);  _sparse_semi_structured_linear = None</code><br />
and</p>
<p><code>[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul: "Sym(s0*s1)" = primals_4 * primals_5
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         view: "f16[s0*s1, 128]" = torch.ops.aten.view.default(primals_6, [mul, 128]);  primals_6 = mul = None
...
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         view_1: "f16[s0, s1, 128]" = torch.ops.aten.view.default(slice_1, [primals_4, primals_5, 128]);  slice_1 = None</code></p>
<p>Failing graphs:<br />
Padded:<br />
```<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO] TRACED GRAPH<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]  ===== Forward graph 5 =====<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]  <eval_with_key>.66 class GraphModule(torch.nn.Module):<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]     def forward(self, primals_1: "f16[128, 64]", primals_2: "i16[128, 8]", primals_3: "f16[1, 128]"):<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /home/jonch/Desktop/Programming/mlsys/pytorch/test/test_sparse_semi_structured.py:145, code: x = self.linear(x)<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         constant_pad_nd: "f16[32, 128]" = torch.ops.aten.constant_pad_nd.default(primals_3, [0, 0, 0, 31], 0.0)<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         _sparse_semi_structured_linear: "f16[32, 128]" = torch.ops.aten._sparse_semi_structured_linear.default(constant_pad_nd, primals_1, primals_2);  constant_pad_nd = primals_1 = primals_2 = None<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         slice_1: "f16[1, 128]" = torch.ops.aten.slice.Tensor(_sparse_semi_structured_linear, 0, 0, 1);  _sparse_semi_structured_linear = None<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         slice_2: "f16[1, 128]" = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /home/jonch/Desktop/Programming/mlsys/pytorch/test/test_sparse_semi_structured.py:147, code: return torch.nn.functional.relu(x)<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         relu: "f16[1, 128]" = torch.ops.aten.relu.default(slice_2);  slice_2 = None<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         alias: "f16[1, 128]" = torch.ops.aten.alias.default(relu)<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         alias_1: "f16[1, 128]" = torch.ops.aten.alias.default(alias);  alias = None<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         le: "b8[1, 128]" = torch.ops.aten.le.Scalar(alias_1, 0);  alias_1 = None<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /home/jonch/Desktop/Programming/mlsys/pytorch/test/test_sparse_semi_structured.py:145, code: x = self.linear(x)<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         permute: "f16[128, 1]" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None<br />
[2023-11-23 13:59:51,102] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         return [relu, le, permute]</p>
<p>```</p>
<p>Reshape:</p>
<p>```<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]  <eval_with_key>.69 class GraphModule(torch.nn.Module):<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]     def forward(self, primals_1: "f16[128, 64]", primals_2: "i16[128, 8]", primals_3: "f16[128]", primals_4: "Sym(s0)", primals_5: "Sym(s1)", primals_6: "f16[s0, s1, 128]"):<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /home/jonch/Desktop/Programming/mlsys/pytorch/test/test_sparse_semi_structured.py:145, code: x = self.linear(x)<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         mul: "Sym(s0<em>s1)" = primals_4 * primals_5<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         view: "f16[s0</em>s1, 128]" = torch.ops.aten.view.default(primals_6, [mul, 128]);  primals_6 = mul = None<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         _sparse_semi_structured_linear: "f16[s0<em>s1, 128]" = torch.ops.aten._sparse_semi_structured_linear.default(view, primals_1, primals_2, bias = primals_3);  primals_1 = primals_2 = primals_3 = None<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         slice_1: "f16[s0</em>s1, 128]" = torch.ops.aten.slice.Tensor(_sparse_semi_structured_linear, 1, 0, 9223372036854775807);  _sparse_semi_structured_linear = None<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         view_1: "f16[s0, s1, 128]" = torch.ops.aten.view.default(slice_1, [primals_4, primals_5, 128]);  slice_1 = None<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         # File: /home/jonch/Desktop/Programming/mlsys/pytorch/test/test_sparse_semi_structured.py:147, code: return torch.nn.functional.relu(x)<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         relu: "f16[s0, s1, 128]" = torch.ops.aten.relu.default(view_1);  view_1 = None<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         alias: "f16[s0, s1, 128]" = torch.ops.aten.alias.default(relu)<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         alias_1: "f16[s0, s1, 128]" = torch.ops.aten.alias.default(alias);  alias = None<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         le: "b8[s0, s1, 128]" = torch.ops.aten.le.Scalar(alias_1, 0);  alias_1 = None<br />
[2023-11-23 14:01:03,463] [0/2] torch._functorch.aot_autograd.__aot_graphs: [INFO]         return [relu, view, le, primals_4, primals_5]</p>
<p>```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114477<br />
Approved by: https://github.com/jcaip</p>]]></description>
      <pubDate>Tue, 28 Nov 2023 11:35:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cef79c0df4a340e812efd7fd8534a7fe3ba0109b</guid>
    </item>
    <item>
      <title>[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op hardtanh (#114580)</title>
      <link>https://github.com/pytorch/pytorch/commit/95aec251aa9aec4502658dc7599c0a4d6a4eff73</link>
      <description><![CDATA[<p>[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op hardtanh (#114580)</p>
<p><strong>Summary</strong><br />
Enable the fusion pattern of <code>QConv2d -&gt; hardtanh</code> lowering to <code>hardtanh</code> as <code>QConv2d</code> post operator.</p>
<p><strong>Test Plan</strong><br />
```<br />
python -m pytest test_mkldnn_pattern_matcher.py -k test_qconv2d_relu6_cpu<br />
python -m pytest test_mkldnn_pattern_matcher.py -k test_qconv2d_hardtanh_cpu</p>
<p>python -m pytest test_mkldnn_pattern_matcher.py -k test_qat_qconv2d_relu6<br />
python -m pytest test_mkldnn_pattern_matcher.py -k test_qat_qconv2d_hardtanh<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114580<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168<br />
ghstack dependencies: #114578, #114579</p>]]></description>
      <pubDate>Mon, 27 Nov 2023 23:21:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/95aec251aa9aec4502658dc7599c0a4d6a4eff73</guid>
    </item>
    <item>
      <title>[Quant] [PT2] Add Hardtanh and ReLU6 into X86InductorQuantizer Conv2d Unary Annotation (#114579)</title>
      <link>https://github.com/pytorch/pytorch/commit/8c1f65dc2ba54bcb6766413c28ee558cd047ce83</link>
      <description><![CDATA[<p>[Quant] [PT2] Add Hardtanh and ReLU6 into X86InductorQuantizer Conv2d Unary Annotation (#114579)</p>
<p><strong>Summary</strong><br />
Add <code>Hardtanh</code> and <code>ReLU6</code> into X86InductorQuantizer Conv2d Unary Annotation</p>
<p><strong>TestPlan</strong><br />
<code>python -m pytest test_x86inductor_quantizer.py -k test_conv2d_unary
python -m pytest test_x86inductor_quantizer.py -k test_qat_conv2d_unary</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114579<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168<br />
ghstack dependencies: #114578</p>]]></description>
      <pubDate>Mon, 27 Nov 2023 23:18:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8c1f65dc2ba54bcb6766413c28ee558cd047ce83</guid>
    </item>
    <item>
      <title>[inductor] Remove more type: ignore comments (#114162)</title>
      <link>https://github.com/pytorch/pytorch/commit/71b742b42c12f9d90d1def70fc414735497492eb</link>
      <description><![CDATA[<p>[inductor] Remove more type: ignore comments (#114162)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114162<br />
Approved by: https://github.com/Skylion007, https://github.com/eellison</p>]]></description>
      <pubDate>Mon, 27 Nov 2023 22:45:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/71b742b42c12f9d90d1def70fc414735497492eb</guid>
    </item>
    <item>
      <title>[cpu] Modify inductor opt flag (#113347)</title>
      <link>https://github.com/pytorch/pytorch/commit/0de67e7949b899a90911f7249d5ccece34e46211</link>
      <description><![CDATA[<p>[cpu] Modify inductor opt flag (#113347)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/113014, https://github.com/pytorch/pytorch/issues/113012, https://github.com/pytorch/pytorch/issues/93598.</p>
<p>For CPU inductor path, remove <code>-funsafe-math-optimizations</code> from optimization flags to fix functional issues.</p>
<h3>Validation on 3 benchmark suites</h3>
<p><strong>FP32</strong><br />
<img width="582" alt="image" src="https://github.com/pytorch/pytorch/assets/23010269/5a648497-a8e2-4057-8dd4-b322e9334456"></p>
<ul>
<li>No accuracy problem</li>
<li>Slight geomean perf drop</li>
<li>3 outlier models (speed up &lt; 0.8). Could be solved by adding vectorizations later.</li>
</ul>
<p><strong>BF16</strong><br />
<img width="583" alt="image" src="https://github.com/pytorch/pytorch/assets/23010269/ca1cbd34-5712-4d79-9238-0cc11dd279b1"></p>
<ul>
<li>No accuracy problem</li>
<li>Slight geomean perf drop</li>
<li>4 outlier models (speed up &lt; 0.8). Could be solved by adding vectorizations later.</li>
</ul>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113347<br />
Approved by: https://github.com/jgong5, https://github.com/desertfire</p>]]></description>
      <pubDate>Mon, 27 Nov 2023 20:03:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0de67e7949b899a90911f7249d5ccece34e46211</guid>
    </item>
    <item>
      <title>[Quant] [Inductor] Fix an issue in QConv Binary Pattern Match (#114541)</title>
      <link>https://github.com/pytorch/pytorch/commit/11f11e95df9c205d427fe4dd7e63c9adb91ea03f</link>
      <description><![CDATA[<p>[Quant] [Inductor] Fix an issue in QConv Binary Pattern Match (#114541)</p>
<p><strong>Summary</strong><br />
Add the <code>extra_check</code> in <code>_register_quantized_conv_binary_lowering</code> to skip the pattern which matched unexpected. To match a Conv-Binary pattern, we should expect the extra input of binary node comes from a dequant pattern instead of a constant scalar.</p>
<p><strong>Test Plan</strong><br />
<code>python -m pytest test_mkldnn_pattern_matcher.py -k test_qconv2d_add_2</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114541<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168<br />
ghstack dependencies: #114540</p>]]></description>
      <pubDate>Mon, 27 Nov 2023 18:59:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/11f11e95df9c205d427fe4dd7e63c9adb91ea03f</guid>
    </item>
    <item>
      <title>[inductor] added a config to dump profiling results to a file (#114587)</title>
      <link>https://github.com/pytorch/pytorch/commit/ae40a3ebcfa233bd66f0e6a11656c0bea69904dc</link>
      <description><![CDATA[<p>[inductor] added a config to dump profiling results to a file (#114587)</p>
<p>Currently, we print out profile bandwidth result for each triton<br />
kernel to stdout after each profiling run finishes. Consequently,<br />
the profiling results are mixed with other debug outputs.</p>
<p>This PR adds a config, profile_bandwidth_output, to specify a file<br />
where we can dump the results in a sorted order. The new config can<br />
be set by setting "TORCHINDUCTOR_PROFILE_OUTPUT" environment variable.<br />
Hopefully it would offer a slightly better way to navigate the profiling<br />
results.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114587<br />
Approved by: https://github.com/Chillee</p>]]></description>
      <pubDate>Mon, 27 Nov 2023 18:21:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ae40a3ebcfa233bd66f0e6a11656c0bea69904dc</guid>
    </item>
    <item>
      <title>Add adaptive_avg_pool2d and flatten into x86 Inductor Quantizer recipe (#114442)</title>
      <link>https://github.com/pytorch/pytorch/commit/74370a8a9d97bc8738b22b1e49eabc8d424f93f4</link>
      <description><![CDATA[<p>Add adaptive_avg_pool2d and flatten into x86 Inductor Quantizer recipe (#114442)</p>
<p><strong>Summary</strong><br />
Add adaptive_avg_pool2d and flatten into x86 Inductor Quantizer recipe</p>
<p><strong>Test Plan</strong><br />
<code>python -m pytest test_x86inductor_quantizer.py -k test_adaptive_avg_pool2d_recipe
python -m pytest test_x86inductor_quantizer.py -k test_flatten_recipe</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114442<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168</p>]]></description>
      <pubDate>Mon, 27 Nov 2023 17:35:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/74370a8a9d97bc8738b22b1e49eabc8d424f93f4</guid>
    </item>
    <item>
      <title>[Quant] [PT2] Fix an issue in Conv Binary Quantization Annotation  (#114540)</title>
      <link>https://github.com/pytorch/pytorch/commit/e592b9a469e83defb92c8d8167940b13db2843de</link>
      <description><![CDATA[<p>[Quant] [PT2] Fix an issue in Conv Binary Quantization Annotation  (#114540)</p>
<p><strong>Summary</strong><br />
To annotate a conv-binary pattern, should skip the pattern if the conv node has more than one user.</p>
<p><strong>Test Plan</strong><br />
<code>python -m pytest test_x86inductor_quantizer.py -k test_conv2d_binary2
python -m pytest test_x86inductor_quantizer.py -k test_qat_conv2d_binary2</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114540<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168</p>]]></description>
      <pubDate>Mon, 27 Nov 2023 17:06:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e592b9a469e83defb92c8d8167940b13db2843de</guid>
    </item>
    <item>
      <title>[Inductor] fix wrong Inductor UTs (#114504)</title>
      <link>https://github.com/pytorch/pytorch/commit/bcfca41a2a8d80ca186e2550e4bca1a52e7873ff</link>
      <description><![CDATA[<p>[Inductor] fix wrong Inductor UTs (#114504)</p>
<h1>Motivation</h1>
<p>These UTs seem wrong. Fix them.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114504<br />
Approved by: https://github.com/aakhundov</p>]]></description>
      <pubDate>Mon, 27 Nov 2023 09:12:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bcfca41a2a8d80ca186e2550e4bca1a52e7873ff</guid>
    </item>
    <item>
      <title>Revert "[inductor] Fix torch.split bug on unbacked symint (#113406)"</title>
      <link>https://github.com/pytorch/pytorch/commit/ccb1de3595fad0d8dc1f9269130dede16547fb77</link>
      <description><![CDATA[<p>Revert "[inductor] Fix torch.split bug on unbacked symint (#113406)"</p>
<p>This reverts commit cd7d6938c18d90870356553d4631f1388d2bb699.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113406 on behalf of https://github.com/DanilBaibak due to Break internal build (<a href="https://github.com/pytorch/pytorch/pull/113406#issuecomment-1827727411">comment</a>)</p>]]></description>
      <pubDate>Mon, 27 Nov 2023 04:20:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ccb1de3595fad0d8dc1f9269130dede16547fb77</guid>
    </item>
    <item>
      <title>Revert "[BE]: Enable Ruff + Flake8 G201,G202 logging format rule. (#114474)"</title>
      <link>https://github.com/pytorch/pytorch/commit/8232d4d1c3a2f5468aa459ff823b041557dd1934</link>
      <description><![CDATA[<p>Revert "[BE]: Enable Ruff + Flake8 G201,G202 logging format rule. (#114474)"</p>
<p>This reverts commit d30497f6b62007c9d1e3c38179528e9d25ac1292.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/114474 on behalf of https://github.com/huydhn due to Sorry for reverting your change, but I see a bunch of inductor failure after the commit https://hud.pytorch.org/pytorch/pytorch/commit/d30497f6b62007c9d1e3c38179528e9d25ac1292, trying to revert to see if it helps fix the issues (<a href="https://github.com/pytorch/pytorch/pull/114474#issuecomment-1827271887">comment</a>)</p>]]></description>
      <pubDate>Sun, 26 Nov 2023 23:36:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8232d4d1c3a2f5468aa459ff823b041557dd1934</guid>
    </item>
    <item>
      <title>Update `torch.compiler_troubleshooting.rst` (#114530)</title>
      <link>https://github.com/pytorch/pytorch/commit/d37c4c69954ad7bdccca96854105c48e93d4587e</link>
      <description><![CDATA[<p>Update <code>torch.compiler_troubleshooting.rst</code> (#114530)</p>
<p>If you copy and paste the env var in the docs:<br />
<code>console
TORCHDYNAMO_REPRO_AFTER=“aot”</code><br />
it leads to this error:<br />
```python<br />
    @functools.wraps(unconfigured_compiler_fn)<br />
    def debug_wrapper(gm, example_inputs, <strong>kwargs):<br />
        compiler_fn = functools.partial(unconfigured_compiler_fn, </strong>kwargs)</p>
<blockquote>
<pre><code>  assert config.repro_after in ("dynamo", "aot", None)
</code></pre>
<p>E       torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
E       AssertionError:<br />
<code>``
because</code>config.repro_after<code>is being</code>'“aot”'<code>but not</code>'aot'`.</p>
</blockquote>
<hr />
<p>It would've saved a few minutes of my time 😄<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/114530<br />
Approved by: https://github.com/Chillee</p>]]></description>
      <pubDate>Sat, 25 Nov 2023 15:15:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d37c4c69954ad7bdccca96854105c48e93d4587e</guid>
    </item>
    <item>
      <title>[Inductor] Fix mutation tracking of ConvolutionBinaryInplace (#114501)</title>
      <link>https://github.com/pytorch/pytorch/commit/c6d88604d56a4e57b34a3c61982a57bdc0ccc0a1</link>
      <description><![CDATA[<p>[Inductor] Fix mutation tracking of ConvolutionBinaryInplace (#114501)</p>
<p>Init function reorders the arguments so the mutation actually happens on<br />
argument input[0]</p>
<p>I am not sure if there's a good way to test this unfortunately.. Added<br />
tests on https://github.com/pytorch/pytorch/pull/114436</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114501<br />
Approved by: https://github.com/leslie-fang-intel, https://github.com/aakhundov</p>]]></description>
      <pubDate>Fri, 24 Nov 2023 11:32:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c6d88604d56a4e57b34a3c61982a57bdc0ccc0a1</guid>
    </item>
    <item>
      <title>[inductor] Pass None and skip constexpr in custom Triton kernel calls from C++ (#114475)</title>
      <link>https://github.com/pytorch/pytorch/commit/0a063ad2c00d1a591bc3ecafa907031118e8c2a9</link>
      <description><![CDATA[<p>[inductor] Pass None and skip constexpr in custom Triton kernel calls from C++ (#114475)</p>
<p>Summary: <code>None</code> arguments are codegened as <code>*i8</code> in the <code>triton_meta</code> of the generated or user-defined Triton kernels:</p>
<p>https://github.com/pytorch/pytorch/blob/85aa3723749e0d06aa5fd34215b9b93529a60995/torch/_inductor/codegen/triton_utils.py#L33-L36</p>
<p>Due to this, in contrary to the conventional Triton, we actually should pass <code>nullptr</code> to the Triton kernels in C++ wrapper codegen instead of passing nothing (as normally <code>None</code> doesn't make it to the generated PTX parameters, just like <code>tl.constexpr</code> args).</p>
<p>This PR adds two things:</p>
<ol>
<li>
<p>Proper C++ wrapper codegening (ABI and non-ABI) of <code>nullptr</code> and <code>c10::nullopt</code>, as the prior way codegening <code>c10::nullopt</code> as tensor breaks (also <code>c10</code> breaks in the ABI mode).</p>
</li>
<li>
<p>Skipping <code>tl.constexpr</code> args when calling the loaded-from-cubin compiled Triton kernel in the C++ wrapper codegen. As a side effect, this also resolves an issue with string arguments: now they are simply omitted in the C++ wrapper codegen.</p>
</li>
</ol>
<p>Test Plan:</p>
<p>```<br />
$ python test/inductor/test_aot_inductor.py -k test_triton_kernel_with_none_input<br />
...</p>
<hr />
<p>Ran 4 tests in 40.364s</p>
<p>OK (skipped=2)<br />
```</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114475<br />
Approved by: https://github.com/oulgen</p>]]></description>
      <pubDate>Fri, 24 Nov 2023 04:51:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0a063ad2c00d1a591bc3ecafa907031118e8c2a9</guid>
    </item>
    <item>
      <title>[inductor] Fix torch.split bug on unbacked symint (#113406)</title>
      <link>https://github.com/pytorch/pytorch/commit/cd7d6938c18d90870356553d4631f1388d2bb699</link>
      <description><![CDATA[<p>[inductor] Fix torch.split bug on unbacked symint (#113406)</p>
<p>torch.split(x, l) fails when l's shape is the unbacked symint.</p>
<p>E.g. l =<br />
y.tolist() makes l the unbacked shape, because l depends on the<br />
data access of y. The downdtream call <code>SliceView.create()</code><br />
evaluates the shape even if the input shape is unbacked symint,<br />
which brings up the bug.</p>
<p>Test Plan:<br />
python test/inductor/test_unbacked_symints.py -k test_split_with_sizes</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113406<br />
Approved by: https://github.com/aakhundov, https://github.com/ezyang</p>]]></description>
      <pubDate>Thu, 23 Nov 2023 23:21:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cd7d6938c18d90870356553d4631f1388d2bb699</guid>
    </item>
    <item>
      <title>[inductor] Fixed conv issue with dynamic shapes (#114351)</title>
      <link>https://github.com/pytorch/pytorch/commit/85aa3723749e0d06aa5fd34215b9b93529a60995</link>
      <description><![CDATA[<p>[inductor] Fixed conv issue with dynamic shapes (#114351)</p>
<p>EDIT: fixes https://github.com/pytorch/pytorch/issues/114354</p>
<p>Description:<br />
The following code is failing:<br />
```python<br />
import torch</p>
<p>def func(x, w):<br />
    return torch.nn.functional.conv2d(x, w, groups=int(w.shape[0]))</p>
<p>x = torch.rand(1, 3, 64, 64)<br />
w = torch.rand(3, 1, 3, 3)<br />
y1 = func(x, w)<br />
cfunc = torch.compile(func, fullgraph=True, dynamic=True)<br />
y2 = cfunc(x, w)</p>
<p>torch.testing.assert_close(y1, y2)<br />
<code>with the error:</code><br />
  File "/pytorch/torch/_inductor/kernel/conv.py", line 315, in convolution<br />
    assert isinstance(groups, int)<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
LoweringException: AssertionError:<br />
  target: aten.convolution.default<br />
  args[0]: TensorBox(StorageBox(<br />
    InputBuffer(name='arg3_1', layout=FixedLayout('cpu', torch.float32, size=[1, s0, s1, s1], stride=[s0<em>s1</em><em>2, s1</em><em>2, s1, 1]))<br />
  ))<br />
  args[1]: TensorBox(StorageBox(<br />
    InputBuffer(name='arg1_1', layout=FixedLayout('cpu', torch.float32, size=[s0, 1, s0, s0], stride=[s0</em><em>2, s0</em>*2, s0, 1]))<br />
  ))<br />
  args[2]: None<br />
  args[3]: [1, 1]<br />
  args[4]: [0, 0]<br />
  args[5]: [1, 1]<br />
  args[6]: False<br />
  args[7]: [0, 0]<br />
  args[8]: s0<br />
<code>``
where</code>groups<code>argument is a symbol but expected to be</code>int`.</p>
<p>This PR specializes <code>group</code> to its int value and fixes the problem.</p>
<p>Context: Failing tests in torchvision with gaussian blur and adjust_sharpness ops<br />
- https://github.com/pytorch/vision/actions/runs/6955843968/job/18926393710?pr=8127</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114351<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Thu, 23 Nov 2023 05:13:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/85aa3723749e0d06aa5fd34215b9b93529a60995</guid>
    </item>
    <item>
      <title>Revert "Require less alignment for masking (#114173)"</title>
      <link>https://github.com/pytorch/pytorch/commit/88a8a0daa4447e19e8355fa93ca9b3d8b3347ce8</link>
      <description><![CDATA[<p>Revert "Require less alignment for masking (#114173)"</p>
<p>This reverts commit f882c175d8e9731238c3f29ca10821f2fe9f0797.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/114173 on behalf of https://github.com/huydhn due to Sorry for reverting you change, but it is failing some inductor tests https://hud.pytorch.org/pytorch/pytorch/commit/f882c175d8e9731238c3f29ca10821f2fe9f0797 (<a href="https://github.com/pytorch/pytorch/pull/114173#issuecomment-1823552362">comment</a>)</p>]]></description>
      <pubDate>Wed, 22 Nov 2023 13:49:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/88a8a0daa4447e19e8355fa93ca9b3d8b3347ce8</guid>
    </item>
    <item>
      <title>[inductor cpp] vectorize embedding lookup (#114062)</title>
      <link>https://github.com/pytorch/pytorch/commit/a0e3321f0c4bae8102961a50eafaef1dd304d8cf</link>
      <description><![CDATA[<p>[inductor cpp] vectorize embedding lookup (#114062)</p>
<p>For embedding lookup, there are indirect indexing with indices that are invariant to the vectorized itervar. To vectorize it, we need to keep the related indexing variables as scalars and allow vectorization when the related index_exprs are invariant to the vectorized itervar.</p>
<p>This PR adds the support by lazily broadcasting scalar values (index_expr and constant) to vectors so that vector operations are only generated if needed by <code>CppVecKernel</code> when any of the inputs are vectors, otherwise, scalar ops are generated. The cse variable in cpp is now represented with <code>CppCSEVariable</code> which bookkeeps the relevant itervars to the variable and has a flag to mark whether it is a scalar or a vector. <code>CppVecOverrides</code> is improved to propagate these states when the ops are executed.</p>
<p>For the added UT <code>test_embedding_vec</code>, the generated code before this PR is:<br />
<code>c++
extern "C" void kernel(const long* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0)
{
    #pragma omp parallel num_threads(64)
    {
        {
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(128L); x0+=static_cast&lt;long&gt;(1L))
            {
                #pragma GCC ivdep
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(128L); x1+=static_cast&lt;long&gt;(1L))
                {
                    auto tmp0 = in_ptr0[static_cast&lt;long&gt;(x0)];
                    auto tmp5 = in_ptr2[static_cast&lt;long&gt;(x1 + (128L*x0))];
                    auto tmp1 = decltype(tmp0)(tmp0 + 64);
                    auto tmp2 = tmp0 &lt; 0;
                    auto tmp3 = tmp2 ? tmp1 : tmp0;
                    TORCH_CHECK((0 &lt;= tmp3) &amp; (tmp3 &lt; 64L), "index out of bounds: 0 &lt;= tmp3 &lt; 64L")
                    auto tmp4 = in_ptr1[static_cast&lt;long&gt;(x1 + (128L*tmp3))];
                    auto tmp6 = decltype(tmp4)(tmp4 + tmp5);
                    out_ptr0[static_cast&lt;long&gt;(x1 + (128L*x0))] = tmp6;
                }
            }
        }
    }
}</code></p>
<p>After this PR, we have:<br />
<code>c++
extern "C" void kernel(const long* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0)
{
    #pragma omp parallel num_threads(64)
    {
        {
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(128L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(128L); x1+=static_cast&lt;long&gt;(16L))
                {
                    auto tmp0 = in_ptr0[static_cast&lt;long&gt;(x0)];
                    auto tmp5 = at::vec::Vectorized&lt;float&gt;::loadu(in_ptr2 + static_cast&lt;long&gt;(x1 + (128L*x0)));
                    auto tmp1 = decltype(tmp0)(tmp0 + 64);
                    auto tmp2 = tmp0 &lt; 0;
                    auto tmp3 = tmp2 ? tmp1 : tmp0;
                    TORCH_CHECK((0 &lt;= tmp3) &amp; (tmp3 &lt; 64L), "index out of bounds: 0 &lt;= tmp3 &lt; 64L")
                    auto tmp4 = at::vec::Vectorized&lt;float&gt;::loadu(in_ptr1 + static_cast&lt;long&gt;(x1 + (128L*tmp3)));
                    auto tmp6 = tmp4 + tmp5;
                    tmp6.store(out_ptr0 + static_cast&lt;long&gt;(x1 + (128L*x0)));
                }
            }
        }
    }
}</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114062<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 22 Nov 2023 03:19:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a0e3321f0c4bae8102961a50eafaef1dd304d8cf</guid>
    </item>
    <item>
      <title>[Inductor] Refactor group/batch fusion to support user defined execution order and configs (#113738)</title>
      <link>https://github.com/pytorch/pytorch/commit/9f0deb132b3f271561bea07610cd01a5e99fb3a6</link>
      <description><![CDATA[<p>[Inductor] Refactor group/batch fusion to support user defined execution order and configs (#113738)</p>
<p>Meta internal customers need more flexible configs on these group/batch fusion's execution order and parameters, I'd like to provide a new inductor config that users can fine and auto tune these group/batch fusions for different models.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113738<br />
Approved by: https://github.com/xuzhao9</p>]]></description>
      <pubDate>Tue, 21 Nov 2023 21:46:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9f0deb132b3f271561bea07610cd01a5e99fb3a6</guid>
    </item>
    <item>
      <title>Revert "[fx/DDP] add nested ctx_manager test for DDP Dynamo (#114056)"</title>
      <link>https://github.com/pytorch/pytorch/commit/2c4930a91d8e4b8f7938f712bf3b78c1fdc2882f</link>
      <description><![CDATA[<p>Revert "[fx/DDP] add nested ctx_manager test for DDP Dynamo (#114056)"</p>
<p>This reverts commit d5d62e85615fdf345e0556a9d8edbee2d3c64ae2.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/114056 on behalf of https://github.com/malfet due to Breaks inductor_distributed, see https://hud.pytorch.org/pytorch/pytorch/commit/d5d62e85615fdf345e0556a9d8edbee2d3c64ae2 (<a href="https://github.com/pytorch/pytorch/pull/114056#issuecomment-1822006423">comment</a>)</p>]]></description>
      <pubDate>Tue, 21 Nov 2023 18:52:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2c4930a91d8e4b8f7938f712bf3b78c1fdc2882f</guid>
    </item>
    <item>
      <title>[CI] Switch to check against expected result files for cpu inductor integration tests (#113668)</title>
      <link>https://github.com/pytorch/pytorch/commit/6ff72607000eb6019650fc44e352de528051e583</link>
      <description><![CDATA[<p>[CI] Switch to check against expected result files for cpu inductor integration tests (#113668)</p>
<p>Summary: With this, we can completely remove CI_SKIP from common.py.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113668<br />
Approved by: https://github.com/ezyang, https://github.com/jansel<br />
ghstack dependencies: #113574, #113575, #113446, #113559</p>]]></description>
      <pubDate>Tue, 21 Nov 2023 13:20:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6ff72607000eb6019650fc44e352de528051e583</guid>
    </item>
    <item>
      <title>[CI] Remove CI skip list for inductor integration tests (#113446)</title>
      <link>https://github.com/pytorch/pytorch/commit/212f668408f63f228f2d02c9be3ea62105552d67</link>
      <description><![CDATA[<p>[CI] Remove CI skip list for inductor integration tests (#113446)</p>
<p>Summary: Switch to completely rely on checking against expected result files.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113446<br />
Approved by: https://github.com/ezyang, https://github.com/malfet, https://github.com/jansel<br />
ghstack dependencies: #113574, #113575</p>]]></description>
      <pubDate>Tue, 21 Nov 2023 13:20:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/212f668408f63f228f2d02c9be3ea62105552d67</guid>
    </item>
    <item>
      <title>[CI] Rename the inductor test config names for dynamic shapes tests (#113574)</title>
      <link>https://github.com/pytorch/pytorch/commit/799d8c303558d1501eb7829cdf631fcbb71200de</link>
      <description><![CDATA[<p>[CI] Rename the inductor test config names for dynamic shapes tests (#113574)</p>
<p>Summary: To make the naming consistent with tests in inductor-periodic and simplify update_expected.py</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113574<br />
Approved by: https://github.com/eellison, https://github.com/malfet, https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 21 Nov 2023 13:20:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/799d8c303558d1501eb7829cdf631fcbb71200de</guid>
    </item>
    <item>
      <title>[aotinductor] don't generate python profiling code in the cpp world (#114182)</title>
      <link>https://github.com/pytorch/pytorch/commit/ebeaec71bf821d8ae34877e2e837eb70dd61c8c3</link>
      <description><![CDATA[<p>[aotinductor] don't generate python profiling code in the cpp world (#114182)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114182<br />
Approved by: https://github.com/aakhundov, https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 21 Nov 2023 13:11:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ebeaec71bf821d8ae34877e2e837eb70dd61c8c3</guid>
    </item>
    <item>
      <title>Correctly codegen math.inf in Inductor (#114159)</title>
      <link>https://github.com/pytorch/pytorch/commit/2abfb8ec7d7a3970097c12caabe1ccb7a05bb5d5</link>
      <description><![CDATA[<p>Correctly codegen math.inf in Inductor (#114159)</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114159<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Tue, 21 Nov 2023 12:16:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2abfb8ec7d7a3970097c12caabe1ccb7a05bb5d5</guid>
    </item>
    <item>
      <title>[inductor] Added decomposition for upsample_nearest_exact Nd (#113749)</title>
      <link>https://github.com/pytorch/pytorch/commit/1f8d00c5a312b490e97c31db5481cdc6544ebbcd</link>
      <description><![CDATA[<p>[inductor] Added decomposition for upsample_nearest_exact Nd (#113749)</p>
<p>Description:<br />
- Added decomposition for upsample_nearest_exact: 1d, 2d, 3d</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113749<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Tue, 21 Nov 2023 05:03:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1f8d00c5a312b490e97c31db5481cdc6544ebbcd</guid>
    </item>
    <item>
      <title>Revert "[inductor cpp] vectorize embedding lookup (#114062)"</title>
      <link>https://github.com/pytorch/pytorch/commit/dd6ef0877ebe3d6b0645870b0a6517905469097e</link>
      <description><![CDATA[<p>Revert "[inductor cpp] vectorize embedding lookup (#114062)"</p>
<p>This reverts commit 2c0474c02d3ac04a429504225d7f1a6536d3b9e6.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/114062 on behalf of https://github.com/huydhn due to Sorry for reverting your change, please help fix lint and reland it https://hud.pytorch.org/pytorch/pytorch/commit/2c0474c02d3ac04a429504225d7f1a6536d3b9e6 (<a href="https://github.com/pytorch/pytorch/pull/114062#issuecomment-1820526515">comment</a>)</p>]]></description>
      <pubDate>Tue, 21 Nov 2023 01:21:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dd6ef0877ebe3d6b0645870b0a6517905469097e</guid>
    </item>
    <item>
      <title>[inductor cpp] vectorize embedding lookup (#114062)</title>
      <link>https://github.com/pytorch/pytorch/commit/2c0474c02d3ac04a429504225d7f1a6536d3b9e6</link>
      <description><![CDATA[<p>[inductor cpp] vectorize embedding lookup (#114062)</p>
<p>For embedding lookup, there are indirect indexing with indices that are invariant to the vectorized itervar. To vectorize it, we need to keep the related indexing variables as scalars and allow vectorization when the related index_exprs are invariant to the vectorized itervar.</p>
<p>This PR adds the support by lazily broadcasting scalar values (index_expr and constant) to vectors so that vector operations are only generated if needed by <code>CppVecKernel</code> when any of the inputs are vectors, otherwise, scalar ops are generated. The cse variable in cpp is now represented with <code>CppCSEVariable</code> which bookkeeps the relevant itervars to the variable and has a flag to mark whether it is a scalar or a vector. <code>CppVecOverrides</code> is improved to propagate these states when the ops are executed.</p>
<p>For the added UT <code>test_embedding_vec</code>, the generated code before this PR is:<br />
<code>c++
extern "C" void kernel(const long* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0)
{
    #pragma omp parallel num_threads(64)
    {
        {
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(128L); x0+=static_cast&lt;long&gt;(1L))
            {
                #pragma GCC ivdep
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(128L); x1+=static_cast&lt;long&gt;(1L))
                {
                    auto tmp0 = in_ptr0[static_cast&lt;long&gt;(x0)];
                    auto tmp5 = in_ptr2[static_cast&lt;long&gt;(x1 + (128L*x0))];
                    auto tmp1 = decltype(tmp0)(tmp0 + 64);
                    auto tmp2 = tmp0 &lt; 0;
                    auto tmp3 = tmp2 ? tmp1 : tmp0;
                    TORCH_CHECK((0 &lt;= tmp3) &amp; (tmp3 &lt; 64L), "index out of bounds: 0 &lt;= tmp3 &lt; 64L")
                    auto tmp4 = in_ptr1[static_cast&lt;long&gt;(x1 + (128L*tmp3))];
                    auto tmp6 = decltype(tmp4)(tmp4 + tmp5);
                    out_ptr0[static_cast&lt;long&gt;(x1 + (128L*x0))] = tmp6;
                }
            }
        }
    }
}</code></p>
<p>After this PR, we have:<br />
<code>c++
extern "C" void kernel(const long* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0)
{
    #pragma omp parallel num_threads(64)
    {
        {
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(128L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(128L); x1+=static_cast&lt;long&gt;(16L))
                {
                    auto tmp0 = in_ptr0[static_cast&lt;long&gt;(x0)];
                    auto tmp5 = at::vec::Vectorized&lt;float&gt;::loadu(in_ptr2 + static_cast&lt;long&gt;(x1 + (128L*x0)));
                    auto tmp1 = decltype(tmp0)(tmp0 + 64);
                    auto tmp2 = tmp0 &lt; 0;
                    auto tmp3 = tmp2 ? tmp1 : tmp0;
                    TORCH_CHECK((0 &lt;= tmp3) &amp; (tmp3 &lt; 64L), "index out of bounds: 0 &lt;= tmp3 &lt; 64L")
                    auto tmp4 = at::vec::Vectorized&lt;float&gt;::loadu(in_ptr1 + static_cast&lt;long&gt;(x1 + (128L*tmp3)));
                    auto tmp6 = tmp4 + tmp5;
                    tmp6.store(out_ptr0 + static_cast&lt;long&gt;(x1 + (128L*x0)));
                }
            }
        }
    }
}</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114062<br />
Approved by: https://github.com/jansel<br />
ghstack dependencies: #113950</p>]]></description>
      <pubDate>Mon, 20 Nov 2023 23:37:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2c0474c02d3ac04a429504225d7f1a6536d3b9e6</guid>
    </item>
    <item>
      <title>Make V.graph properly typed (#114025)</title>
      <link>https://github.com/pytorch/pytorch/commit/87925789ae1509fd04dc9105d4fc8e00d8ad544a</link>
      <description><![CDATA[<p>Make V.graph properly typed (#114025)</p>
<p>Previously it lacked a type hint and so was treated as an Any type. This<br />
resulted in a lot of untyped code downstream as V.graph is referenced in<br />
many places in inductor code. I've typed it properly now as<br />
GraphLowering, and fixed the numerous type errors this surfaced.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114025<br />
Approved by: https://github.com/eellison<br />
ghstack dependencies: #114013</p>]]></description>
      <pubDate>Mon, 20 Nov 2023 18:14:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/87925789ae1509fd04dc9105d4fc8e00d8ad544a</guid>
    </item>
    <item>
      <title>[inductor] Delete more type-ignores in dependencies.py (#114013)</title>
      <link>https://github.com/pytorch/pytorch/commit/4812a62ca0ba66f7d72691666e550da27ecb7a3d</link>
      <description><![CDATA[<p>[inductor] Delete more type-ignores in dependencies.py (#114013)</p>
<p>A couple of type hints were wrong</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114013<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Mon, 20 Nov 2023 18:14:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4812a62ca0ba66f7d72691666e550da27ecb7a3d</guid>
    </item>
    <item>
      <title>[inductor] Add ABI shim function for torch.scatter (#114027)</title>
      <link>https://github.com/pytorch/pytorch/commit/ae00d9623e89168e4147a71f2e15378584630b9b</link>
      <description><![CDATA[<p>[inductor] Add ABI shim function for torch.scatter (#114027)</p>
<p>Summary: Scatter fallback calls <code>at::scatter</code> in the C++ wrapper codegen. This doesn't work in the ABI compatibility mode, as the latter requires a shim function. One is added in this PR.</p>
<p>Test Plan:</p>
<p>```<br />
$ python test/inductor/test_aot_inductor.py -k test_scatter_fallback<br />
s...</p>
<hr />
<p>Ran 4 tests in 52.713s</p>
<p>OK (skipped=1)<br />
```</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114027<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire<br />
ghstack dependencies: #114024</p>]]></description>
      <pubDate>Mon, 20 Nov 2023 14:51:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ae00d9623e89168e4147a71f2e15378584630b9b</guid>
    </item>
    <item>
      <title>[export] Allow shifted constraint ranges in dynamo._export (#114024)</title>
      <link>https://github.com/pytorch/pytorch/commit/4b07fca7d7f761dee3191c301024a290861e2587</link>
      <description><![CDATA[<p>[export] Allow shifted constraint ranges in dynamo._export (#114024)</p>
<p>Summary: Previously, when we had two dynamic shape symbols <code>s0</code> and <code>s1</code> bound by the relationship <code>s1 == s0 + 1</code>, even when the range constraints were set in accordance with the relationship (e.g., to <code>[2, 1024]</code> for <code>s0</code> and to <code>[3, 1025]</code> for <code>s1</code>), <code>torch._dynamo.export</code> raised an error saying that the constraint is violated. Here we add a range check between the expression and the constraint and, if the ranges match, don't declare the constraint violated.</p>
<p>We also add a flag to disable the dim constraint solver in <code>torch._dynamo.export</code> (not set by default for BC), passed down from the <code>torch._export.aot_compile</code>. This is because, even for simple constraints like <code>s1 == s0 + 1</code>, the solver claims that the constraint is too complex and the dimension <code>s0</code> must be specialized. The new flag is not exposed as a part of the public API (i.e., the one without <code>_</code>s in the module names).</p>
<p>Both changes are required to unblock PT2 compilation of an internal model with AOT Inductor.</p>
<p>Test Plan:</p>
<p>```<br />
$ python test/inductor/test_aot_inductor.py -k test_shifted_constraint_ranges<br />
s...</p>
<hr />
<p>Ran 4 tests in 53.247s</p>
<p>OK (skipped=1)<br />
```</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114024<br />
Approved by: https://github.com/zhxchen17</p>]]></description>
      <pubDate>Mon, 20 Nov 2023 14:49:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4b07fca7d7f761dee3191c301024a290861e2587</guid>
    </item>
    <item>
      <title>Insert deferred runtime asserts into Dynamo FX graph (#113958)</title>
      <link>https://github.com/pytorch/pytorch/commit/59ad51e10a72540da7b8197d731943423402f6cc</link>
      <description><![CDATA[<p>Insert deferred runtime asserts into Dynamo FX graph (#113958)</p>
<p>During the course of fake tensor propagation (and, potentially, also Dynamo execution, although I do not believe it is possible to exercise this right now), we may generate deferred runtime asserts, which represent "guards" on unbacked symbols which cannot be immediately checked on entry to a code block; instead, they have to be checked at runtime. However, we currently accumulate these deferred runtime asserts into the ShapeEnv, and don't do anything with them.</p>
<p>This PR modifies Dynamo to automatically insert these runtime asserts into the FX graph, before passing it on to the backend compiler. The assert format coincides with the export assert format as practiced in <code>torch/_export/passes/add_runtime_assertions_for_constraints_pass.py</code>, but actually these passes are completely disjoint right now as I only handle deferred runtime asserts, while export only handles ranges (which I should probably also handle, but don't in this PR.)</p>
<p>The assertions must be inserted by Dynamo, because you could potentially then pass the asserts onto another backend like "eager" which no longer looks at the ShapeEnv before. Thanks to previous work in export, these asserts are preserved in AOTAutograd, but they are dropped by Inductor, which needs to be fixed in future work. This piece will be a bit awkward, as Inductor would have preferred to work with the Sympy expressions directly, ah well.</p>
<p>Here is what the Dynamo traced FX graph looks like for the test in question:</p>
<p>```<br />
  <eval_with_key>.0 class GraphModule(torch.nn.Module):<br />
     def forward(self, L_x_ : torch.Tensor):<br />
         l_x_ = L_x_</p>
<pre><code>     # File: /data/users/ezyang/c/pytorch/wu.py:8, code: y = x.item()
     item = l_x_.item()

     # No stacktrace found for following nodes
     ge_1 = item &gt;= 0
     scalar_tensor_default = torch.ops.aten.scalar_tensor.default(ge_1);  ge_1 = None
     _assert_async_msg = torch.ops.aten._assert_async.msg(scalar_tensor_default, "Deferred runtime assert failed: i0 &gt;= 0, where i0 was defined by 'item' (for more information, run with TORCH_LOGS=+dynamo,dynamic)");  scalar_tensor_default = None

     # File: /data/users/ezyang/c/pytorch/wu.py:9, code: torch._check_is_size

     _check_is_size = torch._check_is_size(item)

     # File: /data/users/ezyang/c/pytorch/wu.py:10, code: if y &gt;= 0:
     ge = item &gt;= 0;  item = None

     # File: /data/users/ezyang/c/pytorch/wu.py:11, code: return x * 2
     mul = l_x_ * 2;  l_x_ = None
     return (mul,)
</code></pre>
<p>```</p>
<p>Note that we actually keep the <code>_check_is_size</code> in the graph redundantly. However, assert_async is retained in the graph, whereas _check_is_size ends up getting DCE'ed.</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113958<br />
Approved by: https://github.com/aakhundov, https://github.com/tugsbayasgalan<br />
ghstack dependencies: #113978</p>]]></description>
      <pubDate>Mon, 20 Nov 2023 13:25:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/59ad51e10a72540da7b8197d731943423402f6cc</guid>
    </item>
    <item>
      <title>Revert "Skip test_lazy_clone for Inductor (#114012)"</title>
      <link>https://github.com/pytorch/pytorch/commit/d40d72d664b8e8dace5f18265d631f3dadea74fb</link>
      <description><![CDATA[<p>Revert "Skip test_lazy_clone for Inductor (#114012)"</p>
<p>This reverts commit ecd8d388b9dec01c5abdf4978e632c9a3db34f95.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/114012 on behalf of https://github.com/DanilBaibak due to I revert the PR due to the original changes broke the internal build. Here is the original diff stack <a href="https://www.internalfb.com/diff/D51444337">D51444337</a> (<a href="https://github.com/pytorch/pytorch/pull/114012#issuecomment-1818745425">comment</a>)</p>]]></description>
      <pubDate>Mon, 20 Nov 2023 02:12:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d40d72d664b8e8dace5f18265d631f3dadea74fb</guid>
    </item>
    <item>
      <title>[AOTI] Improve the two-pass wrapper codegen (#114067)</title>
      <link>https://github.com/pytorch/pytorch/commit/5a96a42cea9cf9f24ba98f1c17e296a528a01ac2</link>
      <description><![CDATA[<p>[AOTI] Improve the two-pass wrapper codegen (#114067)</p>
<p>Summary: For the second-pass, we don't have to rerun the whole inductor flow again. This PR moves that second-pass to the codegen time. This change not only speeds up the compilation, but also removes kernel scheduling inconsistency between the two passes. Another future improvement is to make the second-pass reuse the scheduler and do the wrapper codegen only.</p>
<p>This is a copy of https://github.com/pytorch/pytorch/pull/113762 to land in github first.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114067<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Sun, 19 Nov 2023 15:30:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5a96a42cea9cf9f24ba98f1c17e296a528a01ac2</guid>
    </item>
    <item>
      <title>[reland][aotinductor] Add example_value metadata to nodes (#113986)</title>
      <link>https://github.com/pytorch/pytorch/commit/72a8329ec945aeda366353e171b7c110e30f7736</link>
      <description><![CDATA[<p>[reland][aotinductor] Add example_value metadata to nodes (#113986)</p>
<p>Test Plan:<br />
<code>TORCH_LOGS=dynamo,inductor,aot  CUDA_VISIBLE_DEVICES=7 TORCH_COMPILE_DEBUG=0 TORCHINDUCTOR_MAX_AUTOTUNE=1 buck2 run mode/opt-split-dwarf mode/inplace -c fbcode.enable_gpu_sections=true -c fbcode.platform=platform010  caffe2/torch/fb/model_transform/experimental/benchmark:mts_gpu_benchmark -- --local-model /tmp/409501788/66/gpu_lowering/input.predictor.disagg.gpu.merge --lower-backend="AOT_INDUCTOR"</code></p>
<p>Without passes:<br />
<code>BS: 2048, MFLOPS/BS: 40.51, TFLOP/s: 37.32, Time per iter: 2.22ms, Threads: 1, QPS: 921146.83, Accuracy: True (rtol=0.01), AOT_INDUCTOR lowering duration: 66.15s</code></p>
<p>With passes:<br />
<code>BS: 2048, MFLOPS/BS: 40.51, TFLOP/s: 37.49, Time per iter: 2.21ms, Threads: 1, QPS: 925450.82, Accuracy: True (rtol=0.01), AOT_INDUCTOR lowering duration: 261.11s</code></p>
<p>Differential Revision: D51436878</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113986<br />
Approved by: https://github.com/zhxchen17</p>]]></description>
      <pubDate>Sat, 18 Nov 2023 23:12:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/72a8329ec945aeda366353e171b7c110e30f7736</guid>
    </item>
    <item>
      <title>[Inductor] remove GPT2ForSequenceClassification from ci skip list (#112100)</title>
      <link>https://github.com/pytorch/pytorch/commit/fb3bc3949a64fd976f84806077bcc36167618505</link>
      <description><![CDATA[<p>[Inductor] remove GPT2ForSequenceClassification from ci skip list (#112100)</p>
<p><strong>Summary</strong><br />
As discussed in https://github.com/pytorch/pytorch/issues/109019, the accuracy issue of <code>GPT2ForSequenceClassification</code> has been fixed in https://github.com/pytorch/pytorch/pull/108690. Remove it from CI Skip list.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112100<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Sat, 18 Nov 2023 21:12:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fb3bc3949a64fd976f84806077bcc36167618505</guid>
    </item>
    <item>
      <title>[Inductor] Allow autotuned argument to be anywhere in the argument list (#114002)</title>
      <link>https://github.com/pytorch/pytorch/commit/11857e9a6462c2427b06c6e4c1f347b6a5811a0d</link>
      <description><![CDATA[<p>[Inductor] Allow autotuned argument to be anywhere in the argument list (#114002)</p>
<p>Prior to this PR, autotuned arguments could only be at the back of the argument list. This is an inductor limitation and not triton limitation. Fixing this allows more MRS kernels to use user defined triton kernels.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/114002<br />
Approved by: https://github.com/aakhundov<br />
ghstack dependencies: #113967</p>]]></description>
      <pubDate>Sat, 18 Nov 2023 10:19:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/11857e9a6462c2427b06c6e4c1f347b6a5811a0d</guid>
    </item>
    <item>
      <title>[Inductor] Support ReinterpretView in inductor codegen (#113967)</title>
      <link>https://github.com/pytorch/pytorch/commit/e0c3936843cedfb6fafe962d6ed62564d9f541bb</link>
      <description><![CDATA[<p>[Inductor] Support ReinterpretView in inductor codegen (#113967)</p>
<p>Adding support for ReinterpretView in inductor so that jagged MRS kernels can use native triton kernels</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113967<br />
Approved by: https://github.com/aakhundov</p>]]></description>
      <pubDate>Sat, 18 Nov 2023 10:19:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e0c3936843cedfb6fafe962d6ed62564d9f541bb</guid>
    </item>
    <item>
      <title>[inductor cpp] refactor: CppVecOverrides inherits CppOverrides (#113950)</title>
      <link>https://github.com/pytorch/pytorch/commit/b53d47a7196906891f22b4d154ce5053739fd1d8</link>
      <description><![CDATA[<p>[inductor cpp] refactor: CppVecOverrides inherits CppOverrides (#113950)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113950<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Sat, 18 Nov 2023 07:33:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b53d47a7196906891f22b4d154ce5053739fd1d8</guid>
    </item>
    <item>
      <title>Skip test_lazy_clone for Inductor (#114012)</title>
      <link>https://github.com/pytorch/pytorch/commit/ecd8d388b9dec01c5abdf4978e632c9a3db34f95</link>
      <description><![CDATA[<p>Skip test_lazy_clone for Inductor (#114012)</p>
<p>As half of those tests fail if run individually, but first failure masks all subsequent ones, i.e.<br />
```<br />
PYTORCH_TEST_WITH_INDUCTOR=1 python3 test/test_torch.py -v -k test_lazy_clone_cuda_float32<br />
test_lazy_clone_cuda_float32 (<strong>main</strong>.TestTorchDeviceTypeCUDA) ... FAIL<br />
...<br />
   self.assertTrue(torch._C._is_cow_tensor(t))<br />
AssertionError: False is not true</p>
<hr />
<p>Ran 1 test in 19.419s</p>
<p>FAILED (failures=1)<br />
<code>But</code><br />
$ PYTORCH_TEST_WITH_INDUCTOR=1 python3 test/test_torch.py -k test_lazy_clone_<br />
...<br />
......................</p>
<hr />
<p>Ran 24 tests in 24.969s</p>
<p>OK<br />
```<br />
This flaky behavior was already detected, for example see https://github.com/pytorch/pytorch/issues/113953<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/114012<br />
Approved by: https://github.com/huydhn, https://github.com/kit1980</p>]]></description>
      <pubDate>Fri, 17 Nov 2023 20:57:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ecd8d388b9dec01c5abdf4978e632c9a3db34f95</guid>
    </item>
    <item>
      <title>[ROCm] Enable several inductor UTs (#112777)</title>
      <link>https://github.com/pytorch/pytorch/commit/1567917e5aec511a245f7a4047989c9eca242484</link>
      <description><![CDATA[<p>[ROCm] Enable several inductor UTs (#112777)</p>
<ul>
<li>test_compiled_optimizers.py</li>
<li>test_foreach.py</li>
<li>test_profiler.py</li>
<li>Fix test_profiler.py:test_inductor_profiling_triton_launch - Look for hipModuleLaunchKernel in the events list for AMD GPUs instead of cuLaunchKernel</li>
</ul>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112777<br />
Approved by: https://github.com/jataylo, https://github.com/malfet</p>]]></description>
      <pubDate>Fri, 17 Nov 2023 18:05:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1567917e5aec511a245f7a4047989c9eca242484</guid>
    </item>
    <item>
      <title>[AOTInductor] Use ProxyExecutor for aten op if c-shim is missing (#113918)</title>
      <link>https://github.com/pytorch/pytorch/commit/8372983fe3e764fd15fcac6a27b52d210a4b4ab1</link>
      <description><![CDATA[<p>[AOTInductor] Use ProxyExecutor for aten op if c-shim is missing (#113918)</p>
<p>Summary:<br />
As discussed in the meeting, we are inverting the policy on the use of proxy executor for aten fallbacks.<br />
By default, aten fallback ops will use proxy executor, unless a c-shim is available.</p>
<p>Test Plan: CIs</p>
<p>Differential Revision: D51417683</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113918<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Fri, 17 Nov 2023 16:04:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8372983fe3e764fd15fcac6a27b52d210a4b4ab1</guid>
    </item>
    <item>
      <title>[inductor] Fix slice scatter shape calculation (#113838)</title>
      <link>https://github.com/pytorch/pytorch/commit/e736d27e386176fe24d16d77bd36d1c84a664e4a</link>
      <description><![CDATA[<p>[inductor] Fix slice scatter shape calculation (#113838)</p>
<p>Fixes #113641</p>
<p>As written, there is an off-by-one error whenever <code>end - start</code> doesn't evenly<br />
divide into <code>step</code>. e.g. if <code>end - start = 1</code> and <code>step = 2</code> we should get a<br />
single element but <code>1 // 2 == 0</code> so this wouldn't take anything from the slice.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113838<br />
Approved by: https://github.com/Chillee</p>]]></description>
      <pubDate>Fri, 17 Nov 2023 14:09:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e736d27e386176fe24d16d77bd36d1c84a664e4a</guid>
    </item>
    <item>
      <title>Fix failing test_mkldnn_pattern_matcher if built without MKL (#113949)</title>
      <link>https://github.com/pytorch/pytorch/commit/5d439b07cadce2cf2a89c6897f01aa6801cae4b0</link>
      <description><![CDATA[<p>Fix failing test_mkldnn_pattern_matcher if built without MKL (#113949)</p>
<p>The test checks for the <code>mkldnn_fusion.linear</code> pass which checks <code>_is_packable_linear</code> that depends on <code>torch._C.has_mkl</code>. So skip the test as it would fail due to no pattern matches counted.</p>
<p>See https://github.com/pytorch/pytorch/blob/main/torch/_inductor/fx_passes/mkldnn_fusion.py#L827</p>
<p>CC @XiaobingSuper as the author of the test.</p>
<p>Not sure how many other test are affected by similar issues but this is the one in pattern matcher I see failing.</p>
<p>Strangely the first part of the test succeeds where <code>bias = True</code> as it finds a match for <code>unfuse_bias_add_to_pointwise</code> (torch/_inductor/fx_passes/post_grad.py)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113949<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 17 Nov 2023 13:29:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5d439b07cadce2cf2a89c6897f01aa6801cae4b0</guid>
    </item>
    <item>
      <title>Revert "Fix checking symbolic shapes inside torch._check (#113811)"</title>
      <link>https://github.com/pytorch/pytorch/commit/76bf10e551743cc8448d711e3a1ee58b6f2d3015</link>
      <description><![CDATA[<p>Revert "Fix checking symbolic shapes inside torch._check (#113811)"</p>
<p>This reverts commit 4f8cb52ed94bcdce16c421d7a5e3e9d32acfa439.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113811 on behalf of https://github.com/huydhn due to This one still break inductor tests on main https://hud.pytorch.org/pytorch/pytorch/commit/4f8cb52ed94bcdce16c421d7a5e3e9d32acfa439 (<a href="https://github.com/pytorch/pytorch/pull/113811#issuecomment-1817001514">comment</a>)</p>]]></description>
      <pubDate>Fri, 17 Nov 2023 11:56:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/76bf10e551743cc8448d711e3a1ee58b6f2d3015</guid>
    </item>
    <item>
      <title>Enable import following in MYPYNOFOLLOW (now MYPYINDUCTOR) (#113830)</title>
      <link>https://github.com/pytorch/pytorch/commit/631fb33fd618c12053410574e0bad498e3e6e29e</link>
      <description><![CDATA[<p>Enable import following in MYPYNOFOLLOW (now MYPYINDUCTOR) (#113830)</p>
<p>Skipping importing some packages for now to make this change more<br />
tractable.</p>
<p>For some reason, lintrunner on CI raises errors in all imported <code>.pyi</code> files,<br />
even though it doesn't on my local machine. The errors are all from missing<br />
generic types, as the MYPYINDUCTOR config has <code>disallow_any_generics</code><br />
set. I have thus added <code>disable-error-code</code> comments to the relevant files,<br />
though I fixed a few that were easy enough.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113830<br />
Approved by: https://github.com/Skylion007<br />
ghstack dependencies: #113722, #113721</p>]]></description>
      <pubDate>Fri, 17 Nov 2023 10:24:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/631fb33fd618c12053410574e0bad498e3e6e29e</guid>
    </item>
    <item>
      <title>Revert "Fix checking symbolic shapes inside torch._check (#113811)"</title>
      <link>https://github.com/pytorch/pytorch/commit/7731c97e065cffe4efe82698980a2c8297816dbd</link>
      <description><![CDATA[<p>Revert "Fix checking symbolic shapes inside torch._check (#113811)"</p>
<p>This reverts commit 7f224f6714419f3d56e64a66079340b0e914a2ca.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113811 on behalf of https://github.com/jeanschmidt due to Breaking inductor tests on main (<a href="https://github.com/pytorch/pytorch/pull/113811#issuecomment-1816024288">comment</a>)</p>]]></description>
      <pubDate>Fri, 17 Nov 2023 01:29:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7731c97e065cffe4efe82698980a2c8297816dbd</guid>
    </item>
    <item>
      <title>Make offsets dynamic by default (#113734)</title>
      <link>https://github.com/pytorch/pytorch/commit/7c38b76efec65249e39ae2b8fd8280dfebd1d415</link>
      <description><![CDATA[<p>Make offsets dynamic by default (#113734)</p>
<p>Copied from @ezyang 's #113693.</p>
<p>The motivation for this change is that we'd like to guard on storage offset in inductor, to make assumptions about data alignment.</p>
<p>create_symbolic_sizes_strides_storage_offset() creates the sizes/strides/offset for fake tensors - they can either be integers or symints. This PR changes storage_offset to always be dynamic. In variables/builder.py, we remove a conditional so that all tensors get added to tracked_fakes. This is because the storage offset will be dynamic even if the other logic in builder.py suggests that it will be static; otherwise, we run into this issue:</p>
<p>https://github.com/pytorch/pytorch/blob/1e260c851b9e651794b8d1573c424dc2523a43b3/torch/fx/experimental/symbolic_shapes.py#L892-L895</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113734<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Thu, 16 Nov 2023 23:57:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7c38b76efec65249e39ae2b8fd8280dfebd1d415</guid>
    </item>
    <item>
      <title>[inductor] Relax symbolic guard for sizevars.evaluate_min (#113841)</title>
      <link>https://github.com/pytorch/pytorch/commit/2abb04d1dcf1e76161ea45dd3d8e6be4c0069d8e</link>
      <description><![CDATA[<p>[inductor] Relax symbolic guard for sizevars.evaluate_min (#113841)</p>
<p>We should shorten two conditional guards (guard_equals, guard_lt)<br />
into only one (guard_leq). Then we can save re-compilation for<br />
access-the-last-element-of-the-tensor op. <a href="https://github.com/pytorch/pytorch/blob/8efa6ad1fc0e6994a5644dd1f0e5a73b487ceedf/test/inductor/test_torchinductor.py#L6896C1-L6902">test_torchinductor.test_setitem_with_int_parameter</a><br />
will become <code>frame_count = 2 if torch._dynamo.config.assume_static_by_default else 1</code>.</p>
<p>Test plan:<br />
<code>python test/inductor/test_torchinductor.py -k test_setitem_with_int_parameter_cpu</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113841<br />
Approved by: https://github.com/peterbell10, https://github.com/aakhundov</p>]]></description>
      <pubDate>Thu, 16 Nov 2023 13:16:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2abb04d1dcf1e76161ea45dd3d8e6be4c0069d8e</guid>
    </item>
    <item>
      <title>[Inductor][fx pass] Fix a bug in the merge getitem cat pattern (#113822)</title>
      <link>https://github.com/pytorch/pytorch/commit/de4fd3843c8746c64cd8bb36b5dd159a940c07d5</link>
      <description><![CDATA[<p>[Inductor][fx pass] Fix a bug in the merge getitem cat pattern (#113822)</p>
<p>Summary: The split cat pattern in D50100667 may change the sliced node returned by split node if the getitem to be merged is not consecutive indices.</p>
<p>Test Plan:<br />
<code>buck2 test 'fbcode//mode/opt' fbcode//pytorch/benchmark/fb/test_gpu:run_test_gpu -- --exact 'pytorch/benchmark/fb/test_gpu:run_test_gpu - test_train_mimo_cmf_30x_inductor_accuracy (pytorch.benchmark.fb.test_gpu.test_gpu.TestBenchmarkFbGpu)' --run-disabled</code><br />
Buck UI: https://www.internalfb.com/buck2/1fd8fa6a-83d1-4cfd-bf33-c7ddb28de5b5<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/6473924659080211<br />
Network: Up: 1.3GiB  Down: 48MiB  (reSessionID-acaa2760-abff-442e-989f-3eefd1d1e034)<br />
Jobs completed: 75. Time elapsed: 18:37.5s.<br />
Cache hits: 0%. Commands: 68 (cached: 0, remote: 0, local: 68)<br />
Tests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<p><code>buck2 test 'fbcode//mode/opt' fbcode//pytorch/benchmark/fb/test_gpu:run_test_gpu -- --exact 'pytorch/benchmark/fb/test_gpu:run_test_gpu - test_train_mimo_cmf_30x_inductor_speedup (pytorch.benchmark.fb.test_gpu.test_gpu.TestBenchmarkFbGpu)'</code><br />
Buck UI: https://www.internalfb.com/buck2/7de122c6-23e0-4f13-b2b4-934cf780b60b<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/16888498613412388<br />
Network: Up: 90KiB  Down: 2.1MiB  (reSessionID-f75d6b7b-93ea-4d47-a52a-8d2429b30ad1)<br />
Jobs completed: 6. Time elapsed: 17:28.0s.<br />
Tests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<p>Differential Revision: D51378532</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113822<br />
Approved by: https://github.com/xuzhao9</p>]]></description>
      <pubDate>Thu, 16 Nov 2023 12:40:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/de4fd3843c8746c64cd8bb36b5dd159a940c07d5</guid>
    </item>
    <item>
      <title>Add torch.distributed.breakpoint (#113775)</title>
      <link>https://github.com/pytorch/pytorch/commit/3a3a979984560b9fc4b88aa6ee5199fdffb64238</link>
      <description><![CDATA[<p>Add torch.distributed.breakpoint (#113775)</p>
<p>I tested it works by patching</p>
<p><code>diff --git a/test/distributed/test_dynamo_distributed.py b/test/distributed/test_dynamo_distributed.py
index 96b3a82bdfa..dea9bac9302 100644
--- a/test/distributed/test_dynamo_distributed.py
+++ b/test/distributed/test_dynamo_distributed.py
@@ -18,6 +18,7 @@ from torch._dynamo import config
 from torch._dynamo.utils import same
 from torch._dynamo.testing import collect_results
 from torch.utils._triton import has_triton
+import torch.distributed as dist
 from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy, lambda_auto_wrap_policy
 from torch._higher_order_ops.wrap import tag_activation_checkpoint
 from torch.nn.parallel import DistributedDataParallel as DDP
@@ -398,6 +399,7 @@ class TestMultiProc(DynamoDistributedMultiProcTestCase):
     @unittest.skipIf(not has_triton(), "Inductor+gpu needs triton and recent GPU arch")
     def test_fsdp_activation_checkpointing(self):
         with _dynamo_dist_per_rank_init(self.rank, self.world_size):
+            dist.breakpoint()
             model, inputs = get_toy_model_for_activation_checkpointing(f"cuda:{self.rank}")
             is_inner = lambda module: isinstance(module, ToyInnerModel)  # noqa: E731
             wrap_policy = functools.partial(lambda_auto_wrap_policy, lambda_fn=is_inner)</code></p>
<p>and then running <code>python test/distributed/test_dynamo_distributed.py -k test_fsdp_activation_checkpointing</code></p>
<p>It prints:</p>
<p>```<br />
ATTENTION!!!</p>
<p>Type 'up' to get to the frame that called dist.breakpoint(rank=0)</p>
<blockquote>
<p>/data/users/ezyang/c/pytorch/torch/distributed/<strong>init</strong>.py(71)breakpoint()<br />
-&gt; barrier()<br />
(Pdb) up<br />
/data/users/ezyang/c/pytorch/test/distributed/test_dynamo_distributed.py(402)test_fsdp_activation_checkpointing()<br />
-&gt; dist.breakpoint()<br />
(Pdb) list<br />
397<br />
398         @skip_if_lt_x_gpu(1)<br />
399         @unittest.skipIf(not has_triton(), "Inductor+gpu needs triton and recent GPU arch")<br />
400         def test_fsdp_activation_checkpointing(self):<br />
401             with _dynamo_dist_per_rank_init(self.rank, self.world_size):<br />
402  -&gt;             dist.breakpoint()<br />
403                 model, inputs = get_toy_model_for_activation_checkpointing(f"cuda:{self.rank}")<br />
404                 is_inner = lambda module: isinstance(module, ToyInnerModel)  # noqa: E731<br />
405                 wrap_policy = functools.partial(lambda_auto_wrap_policy, lambda_fn=is_inner)<br />
406                 model = apply_fsdp_with_checkpointing(model, wrap_policy, is_inner)<br />
407                 correct_outputs = model(inputs)<br />
```</p>
</blockquote>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/113775<br />
Approved by: https://github.com/wconstab, https://github.com/wanchaol</p>]]></description>
      <pubDate>Thu, 16 Nov 2023 11:30:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3a3a979984560b9fc4b88aa6ee5199fdffb64238</guid>
    </item>
    <item>
      <title>[AOTInductor] Rename model_runner to model_container_runner (#111324)</title>
      <link>https://github.com/pytorch/pytorch/commit/eddce3c0544b16634e8dbae9adc69e2d15d60d17</link>
      <description><![CDATA[<p>[AOTInductor] Rename model_runner to model_container_runner (#111324)</p>
<p>Summary:<br />
We rename the model_runner to model_container_runner to prepare for<br />
adding tests of pure model without container.</p>
<p>Test Plan:<br />
commit itself is a test.</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111324<br />
Approved by: https://github.com/desertfire, https://github.com/chenyang78</p>]]></description>
      <pubDate>Thu, 16 Nov 2023 11:14:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/eddce3c0544b16634e8dbae9adc69e2d15d60d17</guid>
    </item>
    <item>
      <title>Make offsets dynamic by default (#113734)</title>
      <link>https://github.com/pytorch/pytorch/commit/9efbb4ea73009950a2d99e4d871351c898aae0dd</link>
      <description><![CDATA[<p>Make offsets dynamic by default (#113734)</p>
<p>Copied from @ezyang 's #113693.</p>
<p>The motivation for this change is that we'd like to guard on storage offset in inductor, to make assumptions about data alignment.</p>
<p>create_symbolic_sizes_strides_storage_offset() creates the sizes/strides/offset for fake tensors - they can either be integers or symints. This PR changes storage_offset to always be dynamic. In variables/builder.py, we remove a conditional so that all tensors get added to tracked_fakes. This is because the storage offset will be dynamic even if the other logic in builder.py suggests that it will be static; otherwise, we run into this issue:</p>
<p>https://github.com/pytorch/pytorch/blob/1e260c851b9e651794b8d1573c424dc2523a43b3/torch/fx/experimental/symbolic_shapes.py#L892-L895</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113734<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Wed, 15 Nov 2023 22:49:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9efbb4ea73009950a2d99e4d871351c898aae0dd</guid>
    </item>
    <item>
      <title>Make _inductor/fx_utils.py, _dynamo/utils.py pass follow_imports typechecking (#113722)</title>
      <link>https://github.com/pytorch/pytorch/commit/0a9dbbbaadb86039ba17c1a11990f8a5e518ac06</link>
      <description><![CDATA[<p>Make _inductor/fx_utils.py, _dynamo/utils.py pass follow_imports typechecking (#113722)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113722<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Wed, 15 Nov 2023 21:44:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0a9dbbbaadb86039ba17c1a11990f8a5e518ac06</guid>
    </item>
    <item>
      <title>[easy] encapsulate fb changes from OSS (#113677)</title>
      <link>https://github.com/pytorch/pytorch/commit/1364f84b42c44e3710631358693f7cc94ca7dff0</link>
      <description><![CDATA[<p>[easy] encapsulate fb changes from OSS (#113677)</p>
<p>Summary:<br />
encapsulate fb changes into <code>torch._inductor.fx_passes.fb</code>, so that adding new passes (<code>fb.xxx</code>) won't need to touch OSS code like so:</p>
<p>```</p>
<h1>in torch/_inductor/fx_passes/pre_grad.py</h1>
<p>if config.is_fbcode():<br />
 from .fb import xxx  # every new fb/xxx.py would have needed this change in OSS code base<br />
```</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D51315193</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113677<br />
Approved by: https://github.com/khabinov, https://github.com/chenyang78</p>]]></description>
      <pubDate>Wed, 15 Nov 2023 19:03:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1364f84b42c44e3710631358693f7cc94ca7dff0</guid>
    </item>
    <item>
      <title>[inductor] Make {freezing,ir}.py pass follow-imports typechecking (#113534)</title>
      <link>https://github.com/pytorch/pytorch/commit/df9acc61fb42eb98f098cfcdb40839c29ce74d89</link>
      <description><![CDATA[<p>[inductor] Make {freezing,ir}.py pass follow-imports typechecking (#113534)</p>
<p>I used a couple of type-ignore comments in ir.py because it constructs<br />
short-lived instances of FixedLayout and GraphModuleSerializer, just to<br />
call a single method on them that doesn't use all their members. Making<br />
those unused members optional would make the rest of the code a lot<br />
messier with sprinkled <code>assert</code> statements.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113534<br />
Approved by: https://github.com/albanD</p>]]></description>
      <pubDate>Wed, 15 Nov 2023 17:53:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/df9acc61fb42eb98f098cfcdb40839c29ce74d89</guid>
    </item>
    <item>
      <title>Back out "Support fp8 in AOTInductor + support optional&lt;&gt; in C ABI (#112527)" (#113747)</title>
      <link>https://github.com/pytorch/pytorch/commit/b19cf868e8ba5c068738b0ce940701027b67f84f</link>
      <description><![CDATA[<p>Back out "Support fp8 in AOTInductor + support optional&lt;&gt; in C ABI (#112527)" (#113747)</p>
<p>Test Plan: sandcastle</p>
<p>Differential Revision: D51330618</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113747<br />
Approved by: https://github.com/chenyang78, https://github.com/khabinov</p>]]></description>
      <pubDate>Wed, 15 Nov 2023 14:42:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b19cf868e8ba5c068738b0ce940701027b67f84f</guid>
    </item>
    <item>
      <title>Only check significant strides in test torchinductor (#113389)</title>
      <link>https://github.com/pytorch/pytorch/commit/c1315ae2b92a8b0c268a54f78cfdcad7e53fc72b</link>
      <description><![CDATA[<p>Only check significant strides in test torchinductor (#113389)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113389<br />
Approved by: https://github.com/int3</p>]]></description>
      <pubDate>Wed, 15 Nov 2023 11:47:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c1315ae2b92a8b0c268a54f78cfdcad7e53fc72b</guid>
    </item>
    <item>
      <title>[inductor][fx pass] handle numpy compatibility arg names (#113078)</title>
      <link>https://github.com/pytorch/pytorch/commit/b3423889fecdf510f22d714d4fa15db519e0a914</link>
      <description><![CDATA[<p>[inductor][fx pass] handle numpy compatibility arg names (#113078)</p>
<p>Fixes #113038</p>
<p>the "dim" kwarg can also be referred to with "axis" - handle this case.</p>
<p>https://github.com/pytorch/pytorch/blob/21b6030ac3e6461c914e18f9ed60f6f713b3133b/torch/csrc/utils/python_arg_parser.cpp#L72-L77</p>
<p>previously, if the "axis" kwarg was used, it would not be matched and "dim" would default to 0.</p>
<p>https://github.com/pytorch/pytorch/blob/75adb9f37150a670e88c82a2090fe9e40a5602c6/torch/_inductor/fx_passes/split_cat.py#L172-L176</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113078<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 15 Nov 2023 11:27:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b3423889fecdf510f22d714d4fa15db519e0a914</guid>
    </item>
    <item>
      <title>[inductor cpp] simplify test for uint8 add/sub (#113407)</title>
      <link>https://github.com/pytorch/pytorch/commit/1a8d076e0cc69e4ed9fe3fc6d28924a45a6ca413</link>
      <description><![CDATA[<p>[inductor cpp] simplify test for uint8 add/sub (#113407)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113407<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #113261</p>]]></description>
      <pubDate>Tue, 14 Nov 2023 22:17:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1a8d076e0cc69e4ed9fe3fc6d28924a45a6ca413</guid>
    </item>
    <item>
      <title>[inductor] Make {cudagraph_trees,decomposition,post_grad}.py pass follow_imports typechecking (#113609)</title>
      <link>https://github.com/pytorch/pytorch/commit/fda94124d7ba80d08ac01b36a097edb10cda3918</link>
      <description><![CDATA[<p>[inductor] Make {cudagraph_trees,decomposition,post_grad}.py pass follow_imports typechecking (#113609)</p>
<p>I added explicit imports to <code>kernel/__init__.py</code> as mypy doesn't seem to<br />
understand an empty <code>__init__.py</code>.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113609<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Tue, 14 Nov 2023 21:04:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fda94124d7ba80d08ac01b36a097edb10cda3918</guid>
    </item>
    <item>
      <title>[inductor cpp] fix non-contiguous reduction store (#113261)</title>
      <link>https://github.com/pytorch/pytorch/commit/fcdfcdeef93fa8243f63f819fe185f068b79177b</link>
      <description><![CDATA[<p>[inductor cpp] fix non-contiguous reduction store (#113261)</p>
<p>Fix https://github.com/pytorch/pytorch/issues/113018</p>
<p>The reduction store in this case works on non-contiguous buffer. Previously, we only do scalar fallback for normal stores but not reduction stores. This PR fixes this.</p>
<p>Before fix<br />
<code>c++
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(39L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(16L); x1+=static_cast&lt;long&gt;(16L))
                {
                    {
                        #pragma omp declare reduction(max:at::vec::Vectorized&lt;float&gt;:omp_out = at::vec::maximum(omp_out, omp_in)) initializer(omp_priv={at::vec::Vectorized&lt;float&gt;(-std::numeric_limits&lt;float&gt;::infinity())})
                        float tmp_acc0 = -std::numeric_limits&lt;float&gt;::infinity();
                        at::vec::Vectorized&lt;float&gt; tmp_acc0_vec = at::vec::Vectorized&lt;float&gt;(-std::numeric_limits&lt;float&gt;::infinity());
                        for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(18L); x2+=static_cast&lt;long&gt;(1L))
                        {
                            auto tmp0 = at::vec::Vectorized&lt;float&gt;::loadu(in_ptr1 + static_cast&lt;long&gt;(x1 + (17L*x2) + (306L*x0)));
                            tmp_acc0_vec = at::vec::maximum(tmp_acc0_vec, tmp0);
                        }
                        tmp_acc0_vec.store(out_ptr1 + static_cast&lt;long&gt;(x0 + (39L*x1))); // this is wrong since x0 is not vector dim
                    }
                }
                #pragma omp simd simdlen(8)
                for(long x1=static_cast&lt;long&gt;(16L); x1&lt;static_cast&lt;long&gt;(17L); x1+=static_cast&lt;long&gt;(1L))
                {
                    {
                        float tmp_acc0 = -std::numeric_limits&lt;float&gt;::infinity();
                        for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(18L); x2+=static_cast&lt;long&gt;(1L))
                        {
                            auto tmp0 = in_ptr1[static_cast&lt;long&gt;(x1 + (17L*x2) + (306L*x0))];
                            tmp_acc0 = max_propagate_nan(tmp_acc0, tmp0);
                        }
                        out_ptr1[static_cast&lt;long&gt;(x0 + (39L*x1))] = tmp_acc0;
                    }
                }
            }</code></p>
<p>After fix<br />
<code>c++
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(39L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(16L); x1+=static_cast&lt;long&gt;(16L))
                {
                    {
                        #pragma omp declare reduction(max:at::vec::Vectorized&lt;float&gt;:omp_out = at::vec::maximum(omp_out, omp_in)) initializer(omp_priv={at::vec::Vectorized&lt;float&gt;(-std::numeric_limits&lt;float&gt;::infinity())})
                        float tmp_acc0 = -std::numeric_limits&lt;float&gt;::infinity();
                        at::vec::Vectorized&lt;float&gt; tmp_acc0_vec = at::vec::Vectorized&lt;float&gt;(-std::numeric_limits&lt;float&gt;::infinity());
                        for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(18L); x2+=static_cast&lt;long&gt;(1L))
                        {
                            auto tmp0 = at::vec::Vectorized&lt;float&gt;::loadu(in_ptr1 + static_cast&lt;long&gt;(x1 + (17L*x2) + (306L*x0)));
                            tmp_acc0_vec = at::vec::maximum(tmp_acc0_vec, tmp0);
                        }
                        { __at_align__ float tmpbuf[16*sizeof(float)/sizeof(float)]; tmp_acc0_vec.store(tmpbuf); for (long x1_inner = 0; x1_inner &lt; 16; x1_inner++) out_ptr1[static_cast&lt;long&gt;(x0 + (39L*x1) + (39L*x1_inner))] = tmpbuf[x1_inner]; }
                    }
                }
                #pragma omp simd simdlen(8)
                for(long x1=static_cast&lt;long&gt;(16L); x1&lt;static_cast&lt;long&gt;(17L); x1+=static_cast&lt;long&gt;(1L))
                {
                    {
                        float tmp_acc0 = -std::numeric_limits&lt;float&gt;::infinity();
                        for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(18L); x2+=static_cast&lt;long&gt;(1L))
                        {
                            auto tmp0 = in_ptr1[static_cast&lt;long&gt;(x1 + (17L*x2) + (306L*x0))];
                            tmp_acc0 = max_propagate_nan(tmp_acc0, tmp0);
                        }
                        out_ptr1[static_cast&lt;long&gt;(x0 + (39L*x1))] = tmp_acc0;
                    }
                }
            }</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113261<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Tue, 14 Nov 2023 19:27:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fcdfcdeef93fa8243f63f819fe185f068b79177b</guid>
    </item>
    <item>
      <title>Revert "[inductor] Move things into torch/testing/_internal/inductor_utils.py (#113275)"</title>
      <link>https://github.com/pytorch/pytorch/commit/6bffde99b0b1c372d450f87fdbe8ec33ea460152</link>
      <description><![CDATA[<p>Revert "[inductor] Move things into torch/testing/_internal/inductor_utils.py (#113275)"</p>
<p>This reverts commit 66d09f82170c528698b5ec606ba7838268ae1f8a.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113275 on behalf of https://github.com/huydhn due to Sorry for reverting your stack, but it is failing to list test internally with buck2 (<a href="https://github.com/pytorch/pytorch/pull/113275#issuecomment-1811666004">comment</a>)</p>]]></description>
      <pubDate>Tue, 14 Nov 2023 17:44:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6bffde99b0b1c372d450f87fdbe8ec33ea460152</guid>
    </item>
    <item>
      <title>Revert "Only check significant strides in test torchinductor (#113389)"</title>
      <link>https://github.com/pytorch/pytorch/commit/45671be2a06dcfb01ea2c32828b353fb0b462f7e</link>
      <description><![CDATA[<p>Revert "Only check significant strides in test torchinductor (#113389)"</p>
<p>This reverts commit 28228e1517738f66f11ba278ed8e821c36dcff63.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113389 on behalf of https://github.com/huydhn due to Sorry for reverting your change, but it is conflicting with this revert https://github.com/pytorch/pytorch/pull/113275#issuecomment-1811651388, so I need to revert this to clean thing up (<a href="https://github.com/pytorch/pytorch/pull/113389#issuecomment-1811663791">comment</a>)</p>]]></description>
      <pubDate>Tue, 14 Nov 2023 17:41:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/45671be2a06dcfb01ea2c32828b353fb0b462f7e</guid>
    </item>
    <item>
      <title>[inductor] use fusion_log for verbose logs (#113701)</title>
      <link>https://github.com/pytorch/pytorch/commit/6a25bb8545d9a193adc4c04ed8f7cecb3754d59a</link>
      <description><![CDATA[<p>[inductor] use fusion_log for verbose logs (#113701)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/113696</p>
<p>Previous logs hygeine not respected.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113701<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Tue, 14 Nov 2023 17:39:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6a25bb8545d9a193adc4c04ed8f7cecb3754d59a</guid>
    </item>
    <item>
      <title>Revert "[dynamo] Add run_inductor_tests entrypoint (#113278)"</title>
      <link>https://github.com/pytorch/pytorch/commit/1e60174891c21e8de9a813eb2a454ac9819b4a50</link>
      <description><![CDATA[<p>Revert "[dynamo] Add run_inductor_tests entrypoint (#113278)"</p>
<p>This reverts commit b00311ce9e430cf1b98d2103e21ed2179450a424.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113278 on behalf of https://github.com/huydhn due to Sorry for reverting your stack, but it is failing to list test internally with buck2 (<a href="https://github.com/pytorch/pytorch/pull/113278#issuecomment-1811646325">comment</a>)</p>]]></description>
      <pubDate>Tue, 14 Nov 2023 17:19:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1e60174891c21e8de9a813eb2a454ac9819b4a50</guid>
    </item>
    <item>
      <title>Only check significant strides in test torchinductor (#113389)</title>
      <link>https://github.com/pytorch/pytorch/commit/28228e1517738f66f11ba278ed8e821c36dcff63</link>
      <description><![CDATA[<p>Only check significant strides in test torchinductor (#113389)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113389<br />
Approved by: https://github.com/int3</p>]]></description>
      <pubDate>Tue, 14 Nov 2023 14:45:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/28228e1517738f66f11ba278ed8e821c36dcff63</guid>
    </item>
    <item>
      <title>Add testing for foreach scalar Tensor overloads in inductor (#111600)</title>
      <link>https://github.com/pytorch/pytorch/commit/edd967fe78bca74dd35ef00830e03bd24153f796</link>
      <description><![CDATA[<p>Add testing for foreach scalar Tensor overloads in inductor (#111600)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111600<br />
Approved by: https://github.com/mlazos</p>]]></description>
      <pubDate>Mon, 13 Nov 2023 18:05:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/edd967fe78bca74dd35ef00830e03bd24153f796</guid>
    </item>
    <item>
      <title>Fix recompilation issue with content store (#113533)</title>
      <link>https://github.com/pytorch/pytorch/commit/4b09b08d2e482f8b604d3909ec98453237c019f4</link>
      <description><![CDATA[<p>Fix recompilation issue with content store (#113533)</p>
<p>While running the accuracy minifier, I was getting the error:<br />
<code>NotImplementedError("xor_sum only implemented with inductor")</code></p>
<p>The logs showed that the cache limit was exceeded, and it was falling back to<br />
eager mode which doesn't work for this function. The cache failures was due to<br />
the code guarding on the id of the function being compiled which in this case is<br />
a closure that gets re-created for each function call so the guard always fails.</p>
<p>This fixes the issue by making the storage hash kernel a global function and<br />
working around the dynamo dependency by the <code>lazy_compile</code> helper which defers<br />
the <code>torch.compile</code> call to the first invocation.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/113533<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Mon, 13 Nov 2023 15:58:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4b09b08d2e482f8b604d3909ec98453237c019f4</guid>
    </item>
    <item>
      <title>[aotinductor] add versions for the sdpa shim api (#113487)</title>
      <link>https://github.com/pytorch/pytorch/commit/a144eb502a8e2d0df695fecb5a36d2c1845c3957</link>
      <description><![CDATA[<p>[aotinductor] add versions for the sdpa shim api (#113487)</p>
<p>In our first implemenation of the sdpa shim api, we didn't consider<br />
the case where the optional scale argument could be None. It was<br />
unnoticed because we always got a default argument for the cuda backend.<br />
The issue was detected with the cpu backend.</p>
<p>This PR implements versioning for shim kernels. Currently, we only<br />
have different versions for the sdpa api. We expect we would only<br />
maintain a very small number of abi-compatible shim APIs that<br />
had different versions.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113487<br />
Approved by: https://github.com/int3, https://github.com/desertfire</p>]]></description>
      <pubDate>Mon, 13 Nov 2023 12:18:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a144eb502a8e2d0df695fecb5a36d2c1845c3957</guid>
    </item>
    <item>
      <title>[inductor] Handle variance corrections larger than number of data points (#113284)</title>
      <link>https://github.com/pytorch/pytorch/commit/44f1c6e41c3cf636ba8f691fafbe3b4be621e718</link>
      <description><![CDATA[<p>[inductor] Handle variance corrections larger than number of data points (#113284)</p>
<p>Fixes #113167</p>
<p>When correction is larger than the number of data points, we should return a nan<br />
by dividing by zero, as is done in the eager implementation.</p>
<p>https://github.com/pytorch/pytorch/blob/5ea76f17608d4e7b0dabac946c9164bce41dc8e2/aten/src/ATen/native/SharedReduceOps.h#L137</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113284<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Mon, 13 Nov 2023 03:16:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/44f1c6e41c3cf636ba8f691fafbe3b4be621e718</guid>
    </item>
    <item>
      <title>[inductor] Label align() with [[maybe_unused]] (#113502)</title>
      <link>https://github.com/pytorch/pytorch/commit/7afb503e3ce1225f8d3cc4395aba99581ccbd784</link>
      <description><![CDATA[<p>[inductor] Label align() with [[maybe_unused]] (#113502)</p>
<p>This squelches the "defined but not used" warning that occurs when<br />
memory planning is disabled.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113502<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Sun, 12 Nov 2023 08:33:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7afb503e3ce1225f8d3cc4395aba99581ccbd784</guid>
    </item>
    <item>
      <title>[inductor] Make graph.py pass follow_imports typechecking (#113518)</title>
      <link>https://github.com/pytorch/pytorch/commit/6805d1e1d604e3debb58c6ff0d67ea432f534f15</link>
      <description><![CDATA[<p>[inductor] Make graph.py pass follow_imports typechecking (#113518)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113518<br />
Approved by: https://github.com/Skylion007<br />
ghstack dependencies: #113413</p>]]></description>
      <pubDate>Sat, 11 Nov 2023 14:15:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6805d1e1d604e3debb58c6ff0d67ea432f534f15</guid>
    </item>
    <item>
      <title>[inductor] Make {output_graph,pad_mm}.py pass follow_imports typechecking (#113413)</title>
      <link>https://github.com/pytorch/pytorch/commit/a8cf04fd2a246ca7e21b189aacc899faae0c1ca8</link>
      <description><![CDATA[<p>[inductor] Make {output_graph,pad_mm}.py pass follow_imports typechecking (#113413)</p>
<p>I changed OutputGraph.nn_modules' type to <code>Dict[str, Any]</code> because it<br />
seems that <code>register_attr_or_module</code> can populate it with essentially<br />
any type.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113413<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Sat, 11 Nov 2023 14:15:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a8cf04fd2a246ca7e21b189aacc899faae0c1ca8</guid>
    </item>
    <item>
      <title>[indictor] Fix cat decomp when first tensor is empty (#113514)</title>
      <link>https://github.com/pytorch/pytorch/commit/8d41a5c6055407808938168870f15d38d98580c5</link>
      <description><![CDATA[<p>[indictor] Fix cat decomp when first tensor is empty (#113514)</p>
<p>Summary: Previously, when the first tensor argument to <code>aten.cat</code> was empty and there was only one non-empty tensor argument, the first (empty) tensor was erroneously returned by the <code>aten.cat</code> decomposition. Here we fix the bug.</p>
<p>Test Plan:</p>
<p>```<br />
$ python test/inductor/test_torchinductor.py -k test_cat_empty<br />
...</p>
<hr />
<p>Ran 2 tests in 5.760s</p>
<p>OK<br />
```</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113514<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Sat, 11 Nov 2023 12:34:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8d41a5c6055407808938168870f15d38d98580c5</guid>
    </item>
    <item>
      <title>[dynamo] Add run_inductor_tests entrypoint (#113278)</title>
      <link>https://github.com/pytorch/pytorch/commit/b00311ce9e430cf1b98d2103e21ed2179450a424</link>
      <description><![CDATA[<p>[dynamo] Add run_inductor_tests entrypoint (#113278)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113278<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Sat, 11 Nov 2023 00:54:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b00311ce9e430cf1b98d2103e21ed2179450a424</guid>
    </item>
    <item>
      <title>[inductor] Move things into torch/testing/_internal/inductor_utils.py (#113275)</title>
      <link>https://github.com/pytorch/pytorch/commit/66d09f82170c528698b5ec606ba7838268ae1f8a</link>
      <description><![CDATA[<p>[inductor] Move things into torch/testing/_internal/inductor_utils.py (#113275)</p>
<p>This PR is just moving things around, so code shared by multiple tests files is in torch/testing/_internal/inductor_utils.py.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113275<br />
Approved by: https://github.com/yanboliang<br />
ghstack dependencies: #113242</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 19:17:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/66d09f82170c528698b5ec606ba7838268ae1f8a</guid>
    </item>
    <item>
      <title>[inductor] Enable floor_div indexing to work under ABI-compat mode (#113276)</title>
      <link>https://github.com/pytorch/pytorch/commit/5e03af829568f5839e8fb67b8b196404020f0136</link>
      <description><![CDATA[<p>[inductor] Enable floor_div indexing to work under ABI-compat mode (#113276)</p>
<p>Previously, floor_div operations were defined in<br />
ATen/native/BinaryOps.h. Since this header was not included under<br />
ABI-compat mode, trying to use those indexing operations would result in<br />
compilation errors.</p>
<p>Technically, it is safe to use aten::native::floor_div_* functions in<br />
ABI-compat mode as they are header-only; we could simply include<br />
BinaryOps.h. However, there are other declarations in BinaryOps.h that<br />
are not binary-compatible, so this is not ideal. Thus, I have moved those<br />
functions into a separate file, and put them under c10/util, since they<br />
don't really have tensor-specific logic.</p>
<p>c10 functions are not all header-only, so this still isn't ideal, but<br />
this still seems like an improvement. Moreover, cpp_prefix.h -- used<br />
when compiling cpp kernels -- already includes c10 header files, so<br />
ABI-compatibility already depends on maintaining some c10 functions as<br />
header-only.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113276<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 18:51:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5e03af829568f5839e8fb67b8b196404020f0136</guid>
    </item>
    <item>
      <title>[BE] Add friendly error message if you compile_fx_inner but not return tuple/list (#113451)</title>
      <link>https://github.com/pytorch/pytorch/commit/48c2f89399347e65d7d6603d7c11d5838dfe6836</link>
      <description><![CDATA[<p>[BE] Add friendly error message if you compile_fx_inner but not return tuple/list (#113451)</p>
<p>Previously it would fail here:</p>
<p><code>File "/data/users/ezyang/a/pytorch/torch/_inductor/fx_passes/post_grad.py", line 597, in remove_noop_ops
    for out in tuple(graph.nodes)[-1].args[0]:</code></p>
<p>Now you'll trigger this assert instead.</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113451<br />
Approved by: https://github.com/albanD</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 13:43:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/48c2f89399347e65d7d6603d7c11d5838dfe6836</guid>
    </item>
    <item>
      <title>Allow inferring divisibility on unbacked SymInts and do replacement trick (#113165)</title>
      <link>https://github.com/pytorch/pytorch/commit/dfa9e7b5118f0b0a0b0d61c2aaaac10ffe6e6e27</link>
      <description><![CDATA[<p>Allow inferring divisibility on unbacked SymInts and do replacement trick (#113165)</p>
<p>We want something like torch.empty(i0, 12).view(4, -1, 12) to work.  Right now, it chokes on guards on data dependent accesses. It turns out we are very close to having it work based on experiments in https://github.com/pytorch/pytorch/issues/112347 if we do the replacement trick, setting i0 = i1 * 4 to explicitly encode in the divisibility; this is good enough for Sympy to be able to handle the rest.</p>
<p>There are two parts to this PR.</p>
<ul>
<li>First, we must discover that there is this divisibility constraint. The place where this happens on view is in <code>infer_size</code>; however, we are unable to discover the modulus test with <code>expect_true</code> because the condition is currently written with a Python boolean operator that forces guarding too early: <code>numel == newsize or (dim is not None and newsize &gt; 0 and numel % newsize == 0)</code>. We rewrite this into an equivalent version which tests on dim being None or not first, before performing individual tests. The main nontrivial reasoning here is that I must show that my set of tests in the <code>dim is None</code> branch are sufficient when <code>numel == newsize</code>. However, if <code>numel == newsize</code>, then the modulus must pass. Thus this is equivalent.</li>
<li>Given the modifications to <code>infer_size</code>, this suffices to produce a runtime assert <code>Eq(Mod(192*i0, 2304), 0)</code>. Now we must simply turn this into the replacement automatically. I wasn't really sure how to use Sympy to do this for me, so I just manually pattern matched for this particular expression form, and if it exists do the replacements.</li>
</ul>
<p>Note that this is kind of only useful for export, because inductor chokes on views involving unbacked SymInts. That will be follow up.</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/113165<br />
Approved by: https://github.com/lezcano, https://github.com/aakhundov</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 13:28:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dfa9e7b5118f0b0a0b0d61c2aaaac10ffe6e6e27</guid>
    </item>
    <item>
      <title>[inductor] Make codegen/{common,wrapper,cuda/cutlass_utils}.py pass follow_imports typechecking (#113411)</title>
      <link>https://github.com/pytorch/pytorch/commit/a2c32b8bd0d9eae72ca3d7b7518b2eff6046ca6d</link>
      <description><![CDATA[<p>[inductor] Make codegen/{common,wrapper,cuda/cutlass_utils}.py pass follow_imports typechecking (#113411)</p>
<p>SymIntType is referenced by wrapper.py, so I added its .pyi definition.<br />
I also added SymBoolType along the way for completeness.</p>
<p>The <code>insinstance</code> checks in wrapper.py reference torch.Type, which seems<br />
to cause mypy to choke. Not entirely sure why; I've just added<br />
type-ignore comments for now.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113411<br />
Approved by: https://github.com/Skylion007<br />
ghstack dependencies: #113409, #113410</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 11:58:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a2c32b8bd0d9eae72ca3d7b7518b2eff6046ca6d</guid>
    </item>
    <item>
      <title>[inductor] Make {joint_graph,inductor_prims,utils}.py pass follow_imports typechecking (#113410)</title>
      <link>https://github.com/pytorch/pytorch/commit/5a9f08feb5a43fbb1fe46cd4a9c80faddb0a2900</link>
      <description><![CDATA[<p>[inductor] Make {joint_graph,inductor_prims,utils}.py pass follow_imports typechecking (#113410)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113410<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #113409</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 11:58:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5a9f08feb5a43fbb1fe46cd4a9c80faddb0a2900</guid>
    </item>
    <item>
      <title>[inductor] Make pattern_matcher.py pass follow_imports typechecking (#113409)</title>
      <link>https://github.com/pytorch/pytorch/commit/b0ede09682ea6b1848c6e28b072c4838426f3601</link>
      <description><![CDATA[<p>[inductor] Make pattern_matcher.py pass follow_imports typechecking (#113409)</p>
<p>Import following reveals that a good number of hints were wrong...</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113409<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 11:58:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b0ede09682ea6b1848c6e28b072c4838426f3601</guid>
    </item>
    <item>
      <title>[inductor] Move `has_torchvision_roi_align` check inside test_roi_align (#113385)</title>
      <link>https://github.com/pytorch/pytorch/commit/6e243f475db7c0ad77abb00873680f8cfc24a2e4</link>
      <description><![CDATA[<p>[inductor] Move <code>has_torchvision_roi_align</code> check inside test_roi_align (#113385)</p>
<p>Currently <code>test_torchinductor.py</code> imports <code>torchvision</code> at import time, which<br />
is problematic when you have a broken <code>torchvision</code> install as test collection<br />
will fail. This could happen for example if <code>torchvision</code> was built against a<br />
different versions of PyTorch as may happen regularly in development.</p>
<p>This moves the check inside <code>test_roi_align</code> so a failure to import<br />
<code>torchvision</code> only causes a test failure and the other tests can run fine.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113385<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #113384</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 11:45:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6e243f475db7c0ad77abb00873680f8cfc24a2e4</guid>
    </item>
    <item>
      <title>[inductor] Fix test_dist on pre-sm80 and add skipCUDAIf decorator (#113384)</title>
      <link>https://github.com/pytorch/pytorch/commit/c4fe817a69e126b84ec580fb18397590b4feecda</link>
      <description><![CDATA[<p>[inductor] Fix test_dist on pre-sm80 and add skipCUDAIf decorator (#113384)</p>
<p><code>test_dist</code> uses bfloat16 which isn't well supported by triton on pre-sm80<br />
hardware, so split the test in two and add a skip. This also adds a<br />
<code>skipCUDAIf</code> decorator which only skips on CUDA devices so the test still runs<br />
on CPU.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113384<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 11:45:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c4fe817a69e126b84ec580fb18397590b4feecda</guid>
    </item>
    <item>
      <title>[PyTorch] AOTI: add AOTIInductorModelGetNumOutputs &amp; use for internal runner (#113299)</title>
      <link>https://github.com/pytorch/pytorch/commit/b794bec581fbd62d9e3b121f8de3ea0b3d6f9381</link>
      <description><![CDATA[<p>[PyTorch] AOTI: add AOTIInductorModelGetNumOutputs &amp; use for internal runner (#113299)</p>
<p>I don't see why you couldn't get the number of outputs for a model directly without going through a container. Now you can.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D51050435/">D51050435</a></p>
<p><strong>NOTE FOR REVIEWERS</strong>: This PR has internal Meta-specific changes or comments, please review them on <a href="https://our.internmc.facebook.com/intern/diff/D51050435/">Phabricator</a>!</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113299<br />
Approved by: https://github.com/desertfire, https://github.com/chenyang78</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 10:03:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b794bec581fbd62d9e3b121f8de3ea0b3d6f9381</guid>
    </item>
    <item>
      <title>Revert "[AOTI] Implement support for user defined kernels that use triton.autotune (#113229)"</title>
      <link>https://github.com/pytorch/pytorch/commit/2cd8c0565ce6683a0f5eac2ad8b8dbb924326744</link>
      <description><![CDATA[<p>Revert "[AOTI] Implement support for user defined kernels that use triton.autotune (#113229)"</p>
<p>This reverts commit 1488bafb274fcc82c8aac429bad61738bc3f950e.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113229 on behalf of https://github.com/PaliC due to breaking test_aot_inductor.py tests though a forward fix is coming (<a href="https://github.com/pytorch/pytorch/pull/113229#issuecomment-1806159396">comment</a>)</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 09:46:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2cd8c0565ce6683a0f5eac2ad8b8dbb924326744</guid>
    </item>
    <item>
      <title>[aotinductor] Add a demo tutorial (#112457)</title>
      <link>https://github.com/pytorch/pytorch/commit/c197c48cebd231632897e1eca16d4d00782be8fb</link>
      <description><![CDATA[<p>[aotinductor] Add a demo tutorial (#112457)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112457<br />
Approved by: https://github.com/msaroufim, https://github.com/albanD</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 09:01:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c197c48cebd231632897e1eca16d4d00782be8fb</guid>
    </item>
    <item>
      <title>Make FakeProcessGroup traceable (#113314)</title>
      <link>https://github.com/pytorch/pytorch/commit/08641a3232d7bfed3b24450c922693580f1a4c53</link>
      <description><![CDATA[<p>Make FakeProcessGroup traceable (#113314)</p>
<p>This PR mimics what we have done to trace ProcessGroup. This allows use to use FakeProcessGroup with torch.compile. FakeProcessGroup allows us to use world_size &gt; 1 without creating multiple processes thus enabling the usage of PDB to debug bucketing DDP allreduce in the Inductor. We can theoretically use GLOO with world_size==1 to achieve the same goal. However, the <code>wait()</code> seems to be optimized away when the world_size is 1.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D51136463/">D51136463</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113314<br />
Approved by: https://github.com/wanchaol</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 08:03:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/08641a3232d7bfed3b24450c922693580f1a4c53</guid>
    </item>
    <item>
      <title>[Inductor] [Quant] Enable QLinear int8-mixed-bf16 Lowering (#112486)</title>
      <link>https://github.com/pytorch/pytorch/commit/4f2b2883dc95302df435ca9e3d1826980b46bae9</link>
      <description><![CDATA[<p>[Inductor] [Quant] Enable QLinear int8-mixed-bf16 Lowering (#112486)</p>
<p><strong>Summary</strong><br />
- PR 7 for enabling Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor https://github.com/pytorch/pytorch/issues/111640.<br />
- Enable the QLinear int8-mixed-bf16 weight prepack and post grad lowering inside inductor.</p>
<p><strong>TestPlan</strong><br />
<code>python -m pytest test_mkldnn_pattern_matcher.py -k test_qlinear</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112486<br />
Approved by: https://github.com/jgong5, https://github.com/eellison, https://github.com/jerryzh168</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 04:35:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4f2b2883dc95302df435ca9e3d1826980b46bae9</guid>
    </item>
    <item>
      <title>Back out "[inductor] scale up num_warps for reductions to lower register pressure (#113039)" (#113400)</title>
      <link>https://github.com/pytorch/pytorch/commit/eb1534027f371f5b6942841efc77d56a3921d587</link>
      <description><![CDATA[<p>Back out "[inductor] scale up num_warps for reductions to lower register pressure (#113039)" (#113400)</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D51180501</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113400<br />
Approved by: https://github.com/htyu</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 01:22:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/eb1534027f371f5b6942841efc77d56a3921d587</guid>
    </item>
    <item>
      <title>[Inductor] [Quant] Enable QConv2d Binary int8-mixed-bf16 Lowering (#112551)</title>
      <link>https://github.com/pytorch/pytorch/commit/86d32bedc2d70c54f11264230c73b8b75c5b0aff</link>
      <description><![CDATA[<p>[Inductor] [Quant] Enable QConv2d Binary int8-mixed-bf16 Lowering (#112551)</p>
<p><strong>Summary</strong><br />
- PR 6 for enabling Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor https://github.com/pytorch/pytorch/issues/111640.<br />
- Enable the QConv2d Binary int8-mixed-bf16 post grad lowering inside inductor.</p>
<p><strong>TestPlan</strong><br />
<code>python -m pytest test_mkldnn_pattern_matcher.py -k test_qconv2d</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112551<br />
Approved by: https://github.com/jgong5, https://github.com/eellison, https://github.com/jerryzh168<br />
ghstack dependencies: #112550</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 01:11:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/86d32bedc2d70c54f11264230c73b8b75c5b0aff</guid>
    </item>
    <item>
      <title>[Inductor] [Quant] Enable QConv2d Unary int8-mixed-bf16 Lowering (#112550)</title>
      <link>https://github.com/pytorch/pytorch/commit/65e99357ae82a891c323123c72dde96d55ce5d69</link>
      <description><![CDATA[<p>[Inductor] [Quant] Enable QConv2d Unary int8-mixed-bf16 Lowering (#112550)</p>
<p><strong>Summary</strong><br />
- PR 5 for enabling Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor https://github.com/pytorch/pytorch/issues/111640.<br />
- Enable the QConv2d Unary int8-mixed-bf16 weight prepack and post grad lowering inside inductor.</p>
<p><strong>TestPlan</strong><br />
<code>python -m pytest test_mkldnn_pattern_matcher.py -k test_qconv2d</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112550<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168</p>]]></description>
      <pubDate>Fri, 10 Nov 2023 00:59:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/65e99357ae82a891c323123c72dde96d55ce5d69</guid>
    </item>
    <item>
      <title>Revert "[dynamo] Refactor test cross importing (#113242)"</title>
      <link>https://github.com/pytorch/pytorch/commit/59592389fc0cded415e5a36d5a677b11dae9be16</link>
      <description><![CDATA[<p>Revert "[dynamo] Refactor test cross importing (#113242)"</p>
<p>This reverts commit 8858edad656f505728c9810093f796f96e1285cb.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113242 on behalf of https://github.com/PaliC due to this diff appears to be causing inductor failures internally (<a href="https://github.com/pytorch/pytorch/pull/113242#issuecomment-1805132719">comment</a>)</p>]]></description>
      <pubDate>Thu, 09 Nov 2023 21:43:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/59592389fc0cded415e5a36d5a677b11dae9be16</guid>
    </item>
    <item>
      <title>Revert "[inductor] Move things into torch/testing/_internal/inductor_utils.py (#113275)"</title>
      <link>https://github.com/pytorch/pytorch/commit/68bf0f1e7d86fef72ca4d0200b3c3cc6d83991e9</link>
      <description><![CDATA[<p>Revert "[inductor] Move things into torch/testing/_internal/inductor_utils.py (#113275)"</p>
<p>This reverts commit c967dc526a40f4b15003f9c99383acabe66367a6.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113275 on behalf of https://github.com/PaliC due to the diff this is stacked on top of appears to be causing inductor failures internally (<a href="https://github.com/pytorch/pytorch/pull/113275#issuecomment-1805131017">comment</a>)</p>]]></description>
      <pubDate>Thu, 09 Nov 2023 21:40:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/68bf0f1e7d86fef72ca4d0200b3c3cc6d83991e9</guid>
    </item>
    <item>
      <title>[inductor cpu] fix uint8 add and sub (#113253)</title>
      <link>https://github.com/pytorch/pytorch/commit/cb48f7855adb9dcefb6855c2ef99aa5a26b16e86</link>
      <description><![CDATA[<p>[inductor cpu] fix uint8 add and sub (#113253)</p>
<p>Fix https://github.com/pytorch/pytorch/issues/113016 and https://github.com/pytorch/pytorch/issues/113020 and https://github.com/pytorch/pytorch/issues/113141 and https://github.com/pytorch/pytorch/issues/113143 and https://github.com/pytorch/pytorch/issues/113144<br />
Explicit typecast result of add/sub to uint8 (similar to how we fixed mul previously) to avoid implicit type promotion from C.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113253<br />
Approved by: https://github.com/lezcano, https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 09 Nov 2023 20:06:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cb48f7855adb9dcefb6855c2ef99aa5a26b16e86</guid>
    </item>
    <item>
      <title>[inductor] Move things into torch/testing/_internal/inductor_utils.py (#113275)</title>
      <link>https://github.com/pytorch/pytorch/commit/c967dc526a40f4b15003f9c99383acabe66367a6</link>
      <description><![CDATA[<p>[inductor] Move things into torch/testing/_internal/inductor_utils.py (#113275)</p>
<p>This PR is just moving things around, so code shared by multiple tests files is in torch/testing/_internal/inductor_utils.py.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113275<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Thu, 09 Nov 2023 16:11:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c967dc526a40f4b15003f9c99383acabe66367a6</guid>
    </item>
    <item>
      <title>[inductor] Make debug.py pass follow-imports typechecking (#113307)</title>
      <link>https://github.com/pytorch/pytorch/commit/e6f09607627f7f83c07f3a411b065b17a4519453</link>
      <description><![CDATA[<p>[inductor] Make debug.py pass follow-imports typechecking (#113307)</p>
<p>pydot accepts both a str and a list of str for its <code>prog</code> parameter.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/113307<br />
Approved by: https://github.com/Skylion007<br />
ghstack dependencies: #113304, #113305, #113306</p>]]></description>
      <pubDate>Thu, 09 Nov 2023 14:08:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e6f09607627f7f83c07f3a411b065b17a4519453</guid>
    </item>
    <item>
      <title>[inductor] Make codecache.py pass follow-imports typechecking (#113306)</title>
      <link>https://github.com/pytorch/pytorch/commit/a65969928cf918fa64ec32cffd37da164433b0e5</link>
      <description><![CDATA[<p>[inductor] Make codecache.py pass follow-imports typechecking (#113306)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113306<br />
Approved by: https://github.com/Skylion007<br />
ghstack dependencies: #113304, #113305</p>]]></description>
      <pubDate>Thu, 09 Nov 2023 14:08:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a65969928cf918fa64ec32cffd37da164433b0e5</guid>
    </item>
    <item>
      <title>Adds broadcast to functional collectives (#112668)</title>
      <link>https://github.com/pytorch/pytorch/commit/1d56e7b5af5bd27a1788f33536d23b60d966113e</link>
      <description><![CDATA[<p>Adds broadcast to functional collectives (#112668)</p>
<p>Adds <code>broadcast</code> to functional collectives, including inductor support.</p>
<p>Test with <code>python test_inductor_collectives.py -- TestCollectivesMultiProc.test_broadcast_inductor</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112668<br />
Approved by: https://github.com/wanchaol, https://github.com/wconstab</p>]]></description>
      <pubDate>Thu, 09 Nov 2023 07:47:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1d56e7b5af5bd27a1788f33536d23b60d966113e</guid>
    </item>
    <item>
      <title>[inductor test] enable dynamic loop for test_adaptive_avg_pool1d_argmax (#113339)</title>
      <link>https://github.com/pytorch/pytorch/commit/bf2c20be5560c83e843a5dfb1fe5e3db9b525e79</link>
      <description><![CDATA[<p>[inductor test] enable dynamic loop for test_adaptive_avg_pool1d_argmax (#113339)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113339<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #113168</p>]]></description>
      <pubDate>Thu, 09 Nov 2023 07:14:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bf2c20be5560c83e843a5dfb1fe5e3db9b525e79</guid>
    </item>
    <item>
      <title>[inductor cpp] fix argmax with &gt;1 reduction dims (#113168)</title>
      <link>https://github.com/pytorch/pytorch/commit/8c704f7a0e2794125be80cb372b2374f6a57218e</link>
      <description><![CDATA[<p>[inductor cpp] fix argmax with &gt;1 reduction dims (#113168)</p>
<p>Fix #113013.</p>
<p>The argmax (and argmin) implementation doesn't handle the index compute properly when the number of reduction dims is larger than 1. It wrongly assumed only one reduction dim.</p>
<p>With the given reproducer, the generated code before the change:<br />
```c++</p>
<h1>include "/tmp/torchinductor_jgong5/tb/ctbgktuhgnnlel6ipqkfk76lfztr5pledachdkcq3asdqtlxpzt6.h"</h1>
<p>extern "C" void kernel(const double<em> in_ptr0,<br />
                       long</em> out_ptr0)<br />
{<br />
    {<br />
        {<br />
            struct IndexValue_1 {size_t index; double value;};<br />
            IndexValue_1 tmp_acc0{0, -std::numeric_limits<double>::infinity()};<br />
            #if !defined(<strong>clang_major</strong>) || <strong>clang_major</strong> &gt; 9<br />
            #pragma omp declare reduction(argmax : IndexValue_1 :\<br />
                omp_out.value = omp_in.value &lt; omp_out.value ? omp_out.value : omp_in.value,\<br />
                omp_out.index = omp_in.value &lt; omp_out.value ? omp_out.index : omp_in.index)\<br />
                initializer(omp_priv = {0, -std::numeric_limits<double>::infinity()})<br />
            #endif<br />
            for(long x0=static_cast<long>(0L); x0&lt;static_cast<long>(9L); x0+=static_cast<long>(1L))<br />
            {<br />
                for(long x1=static_cast<long>(0L); x1&lt;static_cast<long>(2L); x1+=static_cast<long>(1L))<br />
                {<br />
                    auto tmp0 = c10::convert<long>(0);<br />
                    auto tmp1 = c10::convert<long>(1);<br />
                    auto tmp2 = tmp0 &lt; tmp1;<br />
                    auto tmp3 = c10::convert<long>(at::native::div_floor_integer((3L<em>x1), 2L));<br />
                    auto tmp4 = c10::convert<long>(2L + (at::native::div_floor_integer((3L</em>x1), 2L)));<br />
                    auto tmp5 = tmp3 &lt; tmp4;<br />
                    auto tmp6 = tmp2 &amp; tmp5;<br />
                    auto tmp7 = [&amp;]<br />
                    {<br />
                        auto tmp8 = in_ptr0[static_cast<long>((3L<em>x0) + (at::native::div_floor_integer((3L</em>x1), 2L)))];<br />
                        return tmp8;<br />
                    }<br />
                    ;<br />
                    auto tmp9 = tmp6 ? tmp7() : static_cast<decltype(tmp7())>(0.0);<br />
                    auto tmp10 = c10::convert<long>(1L + (at::native::div_floor_integer((3L<em>x1), 2L)));<br />
                    auto tmp11 = tmp10 &lt; tmp4;<br />
                    auto tmp12 = tmp2 &amp; tmp11;<br />
                    auto tmp13 = [&amp;]<br />
                    {<br />
                        auto tmp14 = in_ptr0[static_cast<long>(1L + (3L</em>x0) + (at::native::div_floor_integer((3L*x1), 2L)))];<br />
                        return tmp14;<br />
                    }<br />
                    ;<br />
                    auto tmp15 = tmp12 ? tmp13() : static_cast<decltype(tmp13())>(0.0);<br />
                    auto tmp16 = tmp15 + tmp9;<br />
                    auto tmp17 = [&amp;]<br />
                    {<br />
                        auto tmp18 = c10::convert<double>(1.0);<br />
                        return tmp18;<br />
                    }<br />
                    ;<br />
                    auto tmp19 = tmp6 ? tmp17() : static_cast<decltype(tmp17())>(0.0);<br />
                    auto tmp20 = [&amp;]<br />
                    {<br />
                        auto tmp21 = c10::convert<double>(1.0);<br />
                        return tmp21;<br />
                    }<br />
                    ;<br />
                    auto tmp22 = tmp12 ? tmp20() : static_cast<decltype(tmp20())>(0.0);<br />
                    auto tmp23 = tmp22 + tmp19;<br />
                    auto tmp24 = tmp16 / tmp23;<br />
                    if (tmp_acc0.value &lt; tmp24) {<br />
                        tmp_acc0.index = x1; tmp_acc0.value = tmp24; // both x0 and x1 are reduction vars while only x1 is assigned to tmp_acc0.index<br />
                    }<br />
                }<br />
            }<br />
            out_ptr0[static_cast<long>(0L)] = tmp_acc0.index;<br />
        }<br />
    }<br />
}<br />
<code>After fix:</code>c++</p>
<h1>include "/tmp/torchinductor_jgong5/tb/ctbgktuhgnnlel6ipqkfk76lfztr5pledachdkcq3asdqtlxpzt6.h"</h1>
<p>extern "C" void kernel(const double<em> in_ptr0,<br />
                       long</em> out_ptr0)<br />
{<br />
    {<br />
        {<br />
            struct IndexValue_1 {size_t index; double value;};<br />
            IndexValue_1 tmp_acc0{0, -std::numeric_limits<double>::infinity()};<br />
            #if !defined(<strong>clang_major</strong>) || <strong>clang_major</strong> &gt; 9<br />
            #pragma omp declare reduction(argmax : IndexValue_1 :\<br />
                omp_out.value = omp_in.value &lt; omp_out.value ? omp_out.value : omp_in.value,\<br />
                omp_out.index = omp_in.value &lt; omp_out.value ? omp_out.index : omp_in.index)\<br />
                initializer(omp_priv = {0, -std::numeric_limits<double>::infinity()})<br />
            #endif<br />
            for(long x0=static_cast<long>(0L); x0&lt;static_cast<long>(9L); x0+=static_cast<long>(1L))<br />
            {<br />
                for(long x1=static_cast<long>(0L); x1&lt;static_cast<long>(2L); x1+=static_cast<long>(1L))<br />
                {<br />
                    auto tmp0 = c10::convert<long>(0);<br />
                    auto tmp1 = c10::convert<long>(1);<br />
                    auto tmp2 = tmp0 &lt; tmp1;<br />
                    auto tmp3 = c10::convert<long>(at::native::div_floor_integer((3L<em>x1), 2L));<br />
                    auto tmp4 = c10::convert<long>(2L + (at::native::div_floor_integer((3L</em>x1), 2L)));<br />
                    auto tmp5 = tmp3 &lt; tmp4;<br />
                    auto tmp6 = tmp2 &amp; tmp5;<br />
                    auto tmp7 = [&amp;]<br />
                    {<br />
                        auto tmp8 = in_ptr0[static_cast<long>((3L<em>x0) + (at::native::div_floor_integer((3L</em>x1), 2L)))];<br />
                        return tmp8;<br />
                    }<br />
                    ;<br />
                    auto tmp9 = tmp6 ? tmp7() : static_cast<decltype(tmp7())>(0.0);<br />
                    auto tmp10 = c10::convert<long>(1L + (at::native::div_floor_integer((3L<em>x1), 2L)));<br />
                    auto tmp11 = tmp10 &lt; tmp4;<br />
                    auto tmp12 = tmp2 &amp; tmp11;<br />
                    auto tmp13 = [&amp;]<br />
                    {<br />
                        auto tmp14 = in_ptr0[static_cast<long>(1L + (3L</em>x0) + (at::native::div_floor_integer((3L<em>x1), 2L)))];<br />
                        return tmp14;<br />
                    }<br />
                    ;<br />
                    auto tmp15 = tmp12 ? tmp13() : static_cast<decltype(tmp13())>(0.0);<br />
                    auto tmp16 = tmp15 + tmp9;<br />
                    auto tmp17 = [&amp;]<br />
                    {<br />
                        auto tmp18 = c10::convert<double>(1.0);<br />
                        return tmp18;<br />
                    }<br />
                    ;<br />
                    auto tmp19 = tmp6 ? tmp17() : static_cast<decltype(tmp17())>(0.0);<br />
                    auto tmp20 = [&amp;]<br />
                    {<br />
                        auto tmp21 = c10::convert<double>(1.0);<br />
                        return tmp21;<br />
                    }<br />
                    ;<br />
                    auto tmp22 = tmp12 ? tmp20() : static_cast<decltype(tmp20())>(0.0);<br />
                    auto tmp23 = tmp22 + tmp19;<br />
                    auto tmp24 = tmp16 / tmp23;<br />
                    if (tmp_acc0.value &lt; tmp24) {<br />
                        tmp_acc0.index = static_cast<long>(x1 + (2L</em>x0)); tmp_acc0.value = tmp24;<br />
                    }<br />
                }<br />
            }<br />
            out_ptr0[static_cast<long>(0L)] = tmp_acc0.index;<br />
        }<br />
    }<br />
}<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113168<br />
Approved by: https://github.com/lezcano, https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 09 Nov 2023 03:47:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8c704f7a0e2794125be80cb372b2374f6a57218e</guid>
    </item>
    <item>
      <title>[Inductor] Fallback scatter when src dtype is bf16 (#113204)</title>
      <link>https://github.com/pytorch/pytorch/commit/fbf7866ac98258a8130e14de3021d3160bc28af3</link>
      <description><![CDATA[<p>[Inductor] Fallback scatter when src dtype is bf16 (#113204)</p>
<p>basic_gnn_gcn, basic_gnn_gin, basic_gnn_sage now pass</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113204<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 08 Nov 2023 19:43:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fbf7866ac98258a8130e14de3021d3160bc28af3</guid>
    </item>
    <item>
      <title>[inductor][easy] Fix fusion logging (#113308)</title>
      <link>https://github.com/pytorch/pytorch/commit/204ec11e6d1723eec6f606011fa8f9d594185368</link>
      <description><![CDATA[<p>[inductor][easy] Fix fusion logging (#113308)</p>
<p>We should use %s instead of %d as the numel may be sympy Exprs.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113308<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Wed, 08 Nov 2023 19:19:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/204ec11e6d1723eec6f606011fa8f9d594185368</guid>
    </item>
    <item>
      <title>Enable masked_scatter_backward for inductor (#109642)</title>
      <link>https://github.com/pytorch/pytorch/commit/325e0fdfdd4eddbb748f8f40101066de0711f710</link>
      <description><![CDATA[<p>Enable masked_scatter_backward for inductor (#109642)</p>
<p>masked_scatter_backward was previously implemented as a<br />
CompositeExplicitAutograd, which involved a decomp that calls<br />
masked_select, and masked_select in general produces data-dependent<br />
shapes that inductor doesn't support. But masked_scatter_backward<br />
reshapes the return value of masked_select such that the end result has<br />
a static shape again.</p>
<p>I have converted masked_scatter_backward into an aten op to avoid this<br />
issue.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109642<br />
Approved by: https://github.com/ezyang<br />
ghstack dependencies: #108170</p>]]></description>
      <pubDate>Wed, 08 Nov 2023 17:27:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/325e0fdfdd4eddbb748f8f40101066de0711f710</guid>
    </item>
    <item>
      <title>Persist copy_ in training graph for inputs that don't require grad (#111046)</title>
      <link>https://github.com/pytorch/pytorch/commit/84d64d72d66b9cccbc58066c731a18bf8493db05</link>
      <description><![CDATA[<p>Persist copy_ in training graph for inputs that don't require grad (#111046)</p>
<p>In this PR, we try to keep the input mutations in the forward graph IFF input mutation is data mutation and not metadata mutation and doesn't require grad. This is for optimizing inductor training graphs. (For more details: https://github.com/pytorch/pytorch/issues/109240)</p>
<p>We keep the input mutation in the graph by wrapping the original callable in a wrapper function where in the end we add input.copy_(updated_input) call which is then traced via make_fx. Previously, this was only enabled for forward-only path but unconditionally disabled for joint graph.</p>
<p>Another caveat is that when we are tracing through tensor subclasses, we won't allow any input mutations to be preserved in the graph. The reason is that it makes the code logic quite ugly for no obvious performance improvement.</p>
<p>Most of the changes in this PR are mechanical and I didn't have to make any change to the partitioner. Previously forward/backward heavily relied on metadata field <code>num_mutated_inps</code> to figure out whether something is returned as extra output or not. But now since we keep some mutations in the graph, we need to propogate something similar to <code>num_mutated_inps - num_graph_handled_inps</code>.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111046<br />
Approved by: https://github.com/ezyang, https://github.com/bdhirsh</p>]]></description>
      <pubDate>Wed, 08 Nov 2023 16:40:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/84d64d72d66b9cccbc58066c731a18bf8493db05</guid>
    </item>
    <item>
      <title>Inductor support for native c10d_functional (#112439)</title>
      <link>https://github.com/pytorch/pytorch/commit/625958d8bc7ae70188cdc0efc40fb629e18e3d0f</link>
      <description><![CDATA[<p>Inductor support for native c10d_functional (#112439)</p>
<p>This PR adds Inductor support for <a href="https://github.com/pytorch/pytorch/pull/110570">native c10d_functional ops</a>.</p>
<p>The Inductor IRs introduced in this PR will replace the existing <code>CollectiveKernel</code> IR hierarchy. Compared to the existing collective IRs, the new IRs:<br />
- Are target language agnostic and support AOTInductor.<br />
- Express the constraints solely with read/write deps. This maximizes the potential for buffer reuse.<br />
- Address an issue where out-of-place collective's input buffers could be mutated while being volatilely read.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112439<br />
Approved by: https://github.com/Chillee</p>]]></description>
      <pubDate>Wed, 08 Nov 2023 15:40:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/625958d8bc7ae70188cdc0efc40fb629e18e3d0f</guid>
    </item>
    <item>
      <title>Support fp8 in AOTInductor + support optional&lt;&gt; in C ABI (#112527)</title>
      <link>https://github.com/pytorch/pytorch/commit/297c26bb8e8c10be6ee4d889a3003e942dd86cfd</link>
      <description><![CDATA[<p>Support fp8 in AOTInductor + support optional&lt;&gt; in C ABI (#112527)</p>
<p>This was originally ipiszy's PR: https://github.com/pytorch/pytorch/pull/112358</p>
<p>It turns out that we need to add support for optional types in order to<br />
support fp8 gemm (i.e. scaled_mm). Since our ABI-stable C interface<br />
can't support optional&lt;&gt; directly, I am passing in optional types via<br />
pointer instead.</p>
<p><code>AtenTensorHandle</code>s are already pointers, so nothing needs to change<br />
there. Only value types need to change.</p>
<p>We decided on this approach instead of adding an extra <code>bool</code> param to<br />
the callee because this simplifies things. Having the same number of<br />
arguments regardless of whether we are emitting Python / C++ /<br />
ABI-compatible C++ makes codegen easier.</p>
<p>There are a number of existing ABI-compatible functions that have<br />
optional-typed value parameters. Previously, they just assumed they<br />
would never be passed a <code>nullopt</code> / <code>None</code> at runtime. Changing them to<br />
use pointer types now would break ABI stability, so I have created an<br />
exclude list for those functions.</p>
<p>Finally, I think the current implementation is kind of messy, and only<br />
works for FallbackKernels, even though technically ExternKernels could<br />
also have the same issue. It also doesn't support optional types nested<br />
in lists. I've left FIXME comments for both issues.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D51084289">D51084289</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/112527<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire</p>]]></description>
      <pubDate>Wed, 08 Nov 2023 14:56:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/297c26bb8e8c10be6ee4d889a3003e942dd86cfd</guid>
    </item>
    <item>
      <title>[aotinductor] Update the benchmarking script to clone an eager model (#113046)</title>
      <link>https://github.com/pytorch/pytorch/commit/f6c00b16c8b1549ce1c5563b84f0555583b94759</link>
      <description><![CDATA[<p>[aotinductor] Update the benchmarking script to clone an eager model (#113046)</p>
<p>Summary: fix https://github.com/pytorch/pytorch/issues/113029 where running a model in eager somehow can change a weight stride</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113046<br />
Approved by: https://github.com/angelayi</p>]]></description>
      <pubDate>Wed, 08 Nov 2023 14:05:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f6c00b16c8b1549ce1c5563b84f0555583b94759</guid>
    </item>
    <item>
      <title>[inductor] Add test for debug.trace mode (#113240)</title>
      <link>https://github.com/pytorch/pytorch/commit/24bb60d8a16ac72556f60b7828d047dfee348fb2</link>
      <description><![CDATA[<p>[inductor] Add test for debug.trace mode (#113240)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113240<br />
Approved by: https://github.com/oulgen</p>]]></description>
      <pubDate>Wed, 08 Nov 2023 13:50:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/24bb60d8a16ac72556f60b7828d047dfee348fb2</guid>
    </item>
    <item>
      <title>[Inductor][fx pass] Normalize nodes created by users (#113179)</title>
      <link>https://github.com/pytorch/pytorch/commit/81b0166ca25bfe9ce05804a2712080e1b1f54ff5</link>
      <description><![CDATA[<p>[Inductor][fx pass] Normalize nodes created by users (#113179)</p>
<p>Summary: We noticed that the nodes created by users are absent of example value, which could not be normalized in the normalization pass, thus we change the format to the normalization format for enable the split cat merge.</p>
<p>Test Plan: N/A</p>
<p>Differential Revision: D51058817</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113179<br />
Approved by: https://github.com/jackiexu1992</p>]]></description>
      <pubDate>Wed, 08 Nov 2023 11:08:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/81b0166ca25bfe9ce05804a2712080e1b1f54ff5</guid>
    </item>
    <item>
      <title>Reland "[aot inductor] Move constant loading logic from Container to Model" (#112197)</title>
      <link>https://github.com/pytorch/pytorch/commit/9bda1e874c4048596966ab7e7b8998c4379e09f8</link>
      <description><![CDATA[<p>Reland "[aot inductor] Move constant loading logic from Container to Model" (#112197)</p>
<p>Trying again, hopefully with 100% fewer merge conflicts</p>
<p>Original diff: D50582959<br />
Revert diff: D50657400</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50710815/">D50710815</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112197<br />
Approved by: https://github.com/desertfire, https://github.com/chenyang78</p>]]></description>
      <pubDate>Wed, 08 Nov 2023 07:08:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9bda1e874c4048596966ab7e7b8998c4379e09f8</guid>
    </item>
    <item>
      <title>[AOTInductor] Allow using ProxyExecutor for ATen fallbacks (#112976)</title>
      <link>https://github.com/pytorch/pytorch/commit/728ed37663487edbe612f55c58b1694b209f48fb</link>
      <description><![CDATA[<p>[AOTInductor] Allow using ProxyExecutor for ATen fallbacks (#112976)</p>
<p>Summary: Use ProxyExecutor for aten._scaled_dot_product_efficient_attention in ABI-mode</p>
<p>Test Plan: OSS CI</p>
<p>Differential Revision: D51005807</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112976<br />
Approved by: https://github.com/chenyang78, https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 08 Nov 2023 00:34:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/728ed37663487edbe612f55c58b1694b209f48fb</guid>
    </item>
    <item>
      <title>Fix visualize_overlap for Inductor comm reordering (#113066)</title>
      <link>https://github.com/pytorch/pytorch/commit/d01f8b291d437a37ec8809a18c1bb2ebfa825285</link>
      <description><![CDATA[<p>Fix visualize_overlap for Inductor comm reordering (#113066)</p>
<p>The following assumptions are not always valid and need checking:<br />
1. <code>snode.node</code> exists<br />
2. <code>snode.node.layout.size</code> exists<br />
3. <code>snode.node.layout.stride</code> exists<br />
4. <code>snode.node.name</code> exists</p>
<p>Also there is no guarantee that there won't be two collectives running at the same time. But it's hard to visualize the overlap in that case. So disable the visualization for that case for now.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113066<br />
Approved by: https://github.com/wanchaol</p>]]></description>
      <pubDate>Tue, 07 Nov 2023 21:27:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d01f8b291d437a37ec8809a18c1bb2ebfa825285</guid>
    </item>
    <item>
      <title>Handle unbacked SymInt sized outputs in AOTAutograd (#113159)</title>
      <link>https://github.com/pytorch/pytorch/commit/1f3fa13f0ace035f453651c6da3e96cb64413674</link>
      <description><![CDATA[<p>Handle unbacked SymInt sized outputs in AOTAutograd (#113159)</p>
<p>Thanks aakhundov for constructing the test case. This PR was constructed by running the failing test case, and then fixing problems until we got all the way to the end. There are a few distinct fixes:</p>
<ul>
<li>AOTAutograd performs equality tests on tensor metadata to determine if a metadata mutation had occurred. If we test i0 vs i1, we should report these are NOT equal, since obviously we have somehow resized the tensor from i0 to i1 (even if, on a particular run, it is possible i0 == i1).</li>
<li>There's a sketchy fix for <code>test_aot_autograd_exhaustive_matmul_cpu_float32</code> where we check if the output shape equals the tangent shape. Unfortunately, the same <code>definitely_true</code> treatment does not work here, it still fails on the example. I piled an extra sketchy fix on top of it, where I just try my best to avoid doing the view. Maybe we should have some sort of logging here.</li>
<li>Partitioner needs to get out a size for unbacked SymInt when partitioning. I just feed it a random heuristic value in this case, similar to how we've been dealing with this in Inductor.</li>
</ul>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113159<br />
Approved by: https://github.com/aakhundov, https://github.com/bdhirsh</p>]]></description>
      <pubDate>Tue, 07 Nov 2023 20:28:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1f3fa13f0ace035f453651c6da3e96cb64413674</guid>
    </item>
    <item>
      <title>TorchInductor Opinfo fixes for rng ops (#108170)</title>
      <link>https://github.com/pytorch/pytorch/commit/041b6b5c6be58db2cb05def50a2945eb7dd9a6e8</link>
      <description><![CDATA[<p>TorchInductor Opinfo fixes for rng ops (#108170)</p>
<p>Tests rng ops both with<br />
- fallback_random=True, assertEqual=True<br />
- fallback_random=False, assertEqual=False</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108170<br />
Approved by: https://github.com/davidberard98</p>]]></description>
      <pubDate>Tue, 07 Nov 2023 15:13:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/041b6b5c6be58db2cb05def50a2945eb7dd9a6e8</guid>
    </item>
    <item>
      <title>add test for consecutive aot inductor compiles (#111170)</title>
      <link>https://github.com/pytorch/pytorch/commit/0af8fb71ab8586da0d39313ae0cc60fe6fcbf4c3</link>
      <description><![CDATA[<p>add test for consecutive aot inductor compiles (#111170)</p>
<p>Differential Revision: D50246956</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111170<br />
Approved by: https://github.com/khabinov</p>]]></description>
      <pubDate>Tue, 07 Nov 2023 12:11:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0af8fb71ab8586da0d39313ae0cc60fe6fcbf4c3</guid>
    </item>
    <item>
      <title>[dynamo] run guard fail hooks for each cache entry for which there is a cache miss (#110325)</title>
      <link>https://github.com/pytorch/pytorch/commit/ad1c3467e2755ed203062074497d5b872f074e54</link>
      <description><![CDATA[<p>[dynamo] run guard fail hooks for each cache entry for which there is a cache miss (#110325)</p>
<p>Attempt number 2 at https://github.com/pytorch/pytorch/issues/108950.</p>
<p>Improves debugging for guard failures/recompilations by:<br />
- only running guard fail reason generation during recompilation, instead of when a guard fails during dynamo cache lookup (so generating guard failure reasons is not on the critical path)<br />
- ~~always reporting all guard failures~~ Reports the first-failing guard failure for each cache entry.</p>
<p>We don't expect a performance hit since the guard fail reasons are only generated at recompile time rather than runtime. Perf benchmark to check this (https://hud.pytorch.org/benchmark/torchbench/inductor_with_cudagraphs?startTime=Fri,%2027%20Oct%202023%2017:42:43%20GMT&amp;stopTime=Fri,%2003%20Nov%202023%2017:42:43%20GMT&amp;granularity=hour&amp;mode=training&amp;dtype=amp&amp;lBranch=gh/williamwen42/62/head&amp;lCommit=f4724f5ffc6d17ceae513a42fc18627be7b85482&amp;rBranch=main&amp;rCommit=29f3d392bf230072e3bffae37b078e770cae1956). We may also need to verify this on benchmarks where guard fails are common.</p>
<p>Sample script:<br />
```python<br />
import torch<br />
def generate_data(b):<br />
    return (<br />
        torch.randn(b, 3, 32, 32).to(torch.float32).cuda(),<br />
        torch.randint(1000, (b,)).cuda(),<br />
    )</p>
<p>from torchvision.models import resnet18<br />
def init_model():<br />
    return resnet18().to(torch.float32).cuda()</p>
<p>model = init_model()<br />
model_opt = torch.compile(model, dynamic=False)</p>
<p>for b in range(16, 32):<br />
    data = generate_data(b)<br />
    model_opt(data[0])<br />
```</p>
<p>Sample logs:<br />
<code>bash
(/data/users/williamwen/py310-env) [williamwen@devgpu020.odn1 /data/users/williamwen/pytorch (wwen/log-all-guards)]$ python playground5.py
/data/users/williamwen/pytorch/torch/_inductor/compile_fx.py:141: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
[2023-11-06 14:50:47,605] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (8)
[2023-11-06 14:50:47,605] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/data/users/williamwen/torchvision/torchvision/models/resnet.py:284)
[2023-11-06 14:50:47,605] torch._dynamo.convert_frame: [WARNING]    last reason: tensor 'L['x']' size mismatch at index 0. expected 16, actual 24
[2023-11-06 14:50:47,605] torch._dynamo.convert_frame: [WARNING] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[2023-11-06 14:50:47,605] torch._dynamo.convert_frame: [WARNING] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.
(/data/users/williamwen/py310-env) [williamwen@devgpu020.odn1 /data/users/williamwen/pytorch (wwen/log-all-guards)]$ TORCH_LOGS="recompiles" python playground5.py
/data/users/williamwen/pytorch/torch/_inductor/compile_fx.py:141: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
[2023-11-06 14:53:31,591] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function forward in /data/users/williamwen/torchvision/torchvision/models/resnet.py:284
[2023-11-06 14:53:31,591] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:53:31,591] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 16, actual 17
[2023-11-06 14:53:41,333] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function forward in /data/users/williamwen/torchvision/torchvision/models/resnet.py:284
[2023-11-06 14:53:41,333] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:53:41,333] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 17, actual 18
[2023-11-06 14:53:41,333] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 16, actual 18
[2023-11-06 14:53:50,463] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function forward in /data/users/williamwen/torchvision/torchvision/models/resnet.py:284
[2023-11-06 14:53:50,463] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:53:50,463] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 18, actual 19
[2023-11-06 14:53:50,463] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 17, actual 19
[2023-11-06 14:53:50,463] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 16, actual 19
[2023-11-06 14:53:59,848] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function forward in /data/users/williamwen/torchvision/torchvision/models/resnet.py:284
[2023-11-06 14:53:59,848] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:53:59,848] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 19, actual 20
[2023-11-06 14:53:59,848] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 18, actual 20
[2023-11-06 14:53:59,848] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 17, actual 20
[2023-11-06 14:53:59,848] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 16, actual 20
[2023-11-06 14:54:08,549] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function forward in /data/users/williamwen/torchvision/torchvision/models/resnet.py:284
[2023-11-06 14:54:08,549] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:54:08,549] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 20, actual 21
[2023-11-06 14:54:08,549] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 19, actual 21
[2023-11-06 14:54:08,549] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 18, actual 21
[2023-11-06 14:54:08,549] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 17, actual 21
[2023-11-06 14:54:08,549] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 16, actual 21
[2023-11-06 14:54:17,795] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function forward in /data/users/williamwen/torchvision/torchvision/models/resnet.py:284
[2023-11-06 14:54:17,795] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:54:17,795] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 21, actual 22
[2023-11-06 14:54:17,795] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 20, actual 22
[2023-11-06 14:54:17,795] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 19, actual 22
[2023-11-06 14:54:17,795] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 18, actual 22
[2023-11-06 14:54:17,795] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 17, actual 22
[2023-11-06 14:54:17,795] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 16, actual 22
[2023-11-06 14:54:27,430] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function forward in /data/users/williamwen/torchvision/torchvision/models/resnet.py:284
[2023-11-06 14:54:27,430] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:54:27,430] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 22, actual 23
[2023-11-06 14:54:27,430] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 21, actual 23
[2023-11-06 14:54:27,430] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 20, actual 23
[2023-11-06 14:54:27,430] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 19, actual 23
[2023-11-06 14:54:27,430] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 18, actual 23
[2023-11-06 14:54:27,430] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 17, actual 23
[2023-11-06 14:54:27,430] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 16, actual 23
[2023-11-06 14:54:36,744] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function forward in /data/users/williamwen/torchvision/torchvision/models/resnet.py:284
[2023-11-06 14:54:36,744] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:54:36,744] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 23, actual 24
[2023-11-06 14:54:36,744] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 22, actual 24
[2023-11-06 14:54:36,744] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 21, actual 24
[2023-11-06 14:54:36,744] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 20, actual 24
[2023-11-06 14:54:36,744] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 19, actual 24
[2023-11-06 14:54:36,744] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 18, actual 24
[2023-11-06 14:54:36,744] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 17, actual 24
[2023-11-06 14:54:36,744] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 16, actual 24
[2023-11-06 14:54:36,744] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (8)
[2023-11-06 14:54:36,744] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/data/users/williamwen/torchvision/torchvision/models/resnet.py:284)
[2023-11-06 14:54:36,744] torch._dynamo.convert_frame: [WARNING]    last reason: tensor 'L['x']' size mismatch at index 0. expected 16, actual 24
[2023-11-06 14:54:36,744] torch._dynamo.convert_frame: [WARNING] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[2023-11-06 14:54:36,744] torch._dynamo.convert_frame: [WARNING] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.
[2023-11-06 14:54:45,922] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function _forward_impl in /data/users/williamwen/torchvision/torchvision/models/resnet.py:266
[2023-11-06 14:54:45,922] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:54:45,922] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 24, actual 25
[2023-11-06 14:54:54,691] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function _forward_impl in /data/users/williamwen/torchvision/torchvision/models/resnet.py:266
[2023-11-06 14:54:54,691] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:54:54,691] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 25, actual 26
[2023-11-06 14:54:54,691] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 24, actual 26
[2023-11-06 14:55:03,591] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function _forward_impl in /data/users/williamwen/torchvision/torchvision/models/resnet.py:266
[2023-11-06 14:55:03,591] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:55:03,591] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 26, actual 27
[2023-11-06 14:55:03,591] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 25, actual 27
[2023-11-06 14:55:03,591] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 24, actual 27
[2023-11-06 14:55:12,384] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function _forward_impl in /data/users/williamwen/torchvision/torchvision/models/resnet.py:266
[2023-11-06 14:55:12,384] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:55:12,384] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 27, actual 28
[2023-11-06 14:55:12,384] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 26, actual 28
[2023-11-06 14:55:12,384] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 25, actual 28
[2023-11-06 14:55:12,384] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 24, actual 28
[2023-11-06 14:55:21,442] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function _forward_impl in /data/users/williamwen/torchvision/torchvision/models/resnet.py:266
[2023-11-06 14:55:21,442] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:55:21,442] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 28, actual 29
[2023-11-06 14:55:21,442] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 27, actual 29
[2023-11-06 14:55:21,442] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 26, actual 29
[2023-11-06 14:55:21,442] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 25, actual 29
[2023-11-06 14:55:21,442] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 24, actual 29
[2023-11-06 14:55:30,315] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function _forward_impl in /data/users/williamwen/torchvision/torchvision/models/resnet.py:266
[2023-11-06 14:55:30,315] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:55:30,315] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 29, actual 30
[2023-11-06 14:55:30,315] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 28, actual 30
[2023-11-06 14:55:30,315] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 27, actual 30
[2023-11-06 14:55:30,315] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 26, actual 30
[2023-11-06 14:55:30,315] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 25, actual 30
[2023-11-06 14:55:30,315] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 24, actual 30
[2023-11-06 14:55:39,839] torch._dynamo.guards.__recompiles: [DEBUG] Recompiling function _forward_impl in /data/users/williamwen/torchvision/torchvision/models/resnet.py:266
[2023-11-06 14:55:39,839] torch._dynamo.guards.__recompiles: [DEBUG]     triggered by the following guard failure(s):
[2023-11-06 14:55:39,839] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 30, actual 31
[2023-11-06 14:55:39,839] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 29, actual 31
[2023-11-06 14:55:39,839] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 28, actual 31
[2023-11-06 14:55:39,839] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 27, actual 31
[2023-11-06 14:55:39,839] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 26, actual 31
[2023-11-06 14:55:39,839] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 25, actual 31
[2023-11-06 14:55:39,839] torch._dynamo.guards.__recompiles: [DEBUG]     - tensor 'L['x']' size mismatch at index 0. expected 24, actual 31</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110325<br />
Approved by: https://github.com/ezyang, https://github.com/jon-chuang</p>]]></description>
      <pubDate>Tue, 07 Nov 2023 12:10:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ad1c3467e2755ed203062074497d5b872f074e54</guid>
    </item>
    <item>
      <title>[inductor][fx pass] Fix a bug for the merge_stack_tahn_unbind pattern (#113101)</title>
      <link>https://github.com/pytorch/pytorch/commit/82875e69fefd7d048b815c43609ac113ad5b0c72</link>
      <description><![CDATA[<p>[inductor][fx pass] Fix a bug for the merge_stack_tahn_unbind pattern (#113101)</p>
<p>Summary:<br />
Context:<br />
https://fb.workplace.com/groups/1075192433118967/permalink/1328366351134906/</p>
<p>Test Plan:<br />
local reproduce igctr:<br />
<code>buck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split_batch-group</code><br />
P874994427</p>
<p>Differential Revision: D51052304</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113101<br />
Approved by: https://github.com/jackiexu1992</p>]]></description>
      <pubDate>Tue, 07 Nov 2023 11:25:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/82875e69fefd7d048b815c43609ac113ad5b0c72</guid>
    </item>
    <item>
      <title>[Inductor] Kill MutationLayout from ir.py (#112925)</title>
      <link>https://github.com/pytorch/pytorch/commit/611a7457cad52d1aa81291f772a49dd280c17ba1</link>
      <description><![CDATA[<p>[Inductor] Kill MutationLayout from ir.py (#112925)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112925<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 07 Nov 2023 09:03:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/611a7457cad52d1aa81291f772a49dd280c17ba1</guid>
    </item>
    <item>
      <title>s390x: fix inductor constructing floats out of bytes (#112723)</title>
      <link>https://github.com/pytorch/pytorch/commit/65304d8fd0b3f4699c2cd496214c840f4027d294</link>
      <description><![CDATA[<p>s390x: fix inductor constructing floats out of bytes (#112723)</p>
<p>This change fixes test_embedding_bag_byte_unpack_cpu from test/inductor/test_torchinductor.py on s390x.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112723<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 22:51:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/65304d8fd0b3f4699c2cd496214c840f4027d294</guid>
    </item>
    <item>
      <title>[Inductor] Allow None values to be passed in as arguments to triton kernels (#113056)</title>
      <link>https://github.com/pytorch/pytorch/commit/68c4507bc297f54ad86a9aae8e4a9697a3171c5f</link>
      <description><![CDATA[<p>[Inductor] Allow None values to be passed in as arguments to triton kernels (#113056)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113056<br />
Approved by: https://github.com/jansel<br />
ghstack dependencies: #112752, #113008, #112801</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 21:29:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/68c4507bc297f54ad86a9aae8e4a9697a3171c5f</guid>
    </item>
    <item>
      <title>[Inductor] Improve reinplace_scatters pass (#112801)</title>
      <link>https://github.com/pytorch/pytorch/commit/bfa717c6a6636c8b70d768b4d4b159ad65748cc7</link>
      <description><![CDATA[<p>[Inductor] Improve reinplace_scatters pass (#112801)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112801<br />
Approved by: https://github.com/Chillee, https://github.com/jansel<br />
ghstack dependencies: #112752, #113008</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 21:29:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bfa717c6a6636c8b70d768b4d4b159ad65748cc7</guid>
    </item>
    <item>
      <title>[Inductor] Cache generated user defined triton kernels on tensor dtype and non tensor parameters (#112752)</title>
      <link>https://github.com/pytorch/pytorch/commit/dbf44dffc94d8cf9c6161cee1fd0b9394e0f84b9</link>
      <description><![CDATA[<p>[Inductor] Cache generated user defined triton kernels on tensor dtype and non tensor parameters (#112752)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112752<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 21:29:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dbf44dffc94d8cf9c6161cee1fd0b9394e0f84b9</guid>
    </item>
    <item>
      <title>[Inductor][fx pass] Remove split nodes with split section size one (#112922)</title>
      <link>https://github.com/pytorch/pytorch/commit/f99b5f1f233b37b6027f7cb1112b75ce8fe9b142</link>
      <description><![CDATA[<p>[Inductor][fx pass] Remove split nodes with split section size one (#112922)</p>
<p>Summary: We observe that DSNN has many split nodes with split section size one, which hinder the split cat merge in the later pass, thus we remove such nodes in the early stage.</p>
<p>Test Plan:</p>
<h1>local reproduce with DSNN model</h1>
<p><code>buck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split_batch-group -c</code><br />
P872705076<br />
diffing: https://www.internalfb.com/intern/diffing/?paste_number=872698775</p>
<h1>unit test</h1>
<p><code>buck2 test mode/dev-nosan //caffe2/test/inductor:split_cat_fx_passes</code><br />
Buck UI: https://www.internalfb.com/buck2/b248410e-a556-47a2-9293-7f113b49f0d6<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/10696049124469023<br />
Network: Up: 80KiB  Down: 47KiB  (reSessionID-a31dec17-d322-4757-ba84-4d262bd139cf)<br />
Jobs completed: 24. Time elapsed: 1:52.8s.<br />
Cache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)<br />
Tests finished: Pass 9. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<p>Differential Revision: D50990290</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112922<br />
Approved by: https://github.com/jackiexu1992</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 20:53:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f99b5f1f233b37b6027f7cb1112b75ce8fe9b142</guid>
    </item>
    <item>
      <title>Retarget sym_size/sym_stride lowerings to their .int overloads (#113054)</title>
      <link>https://github.com/pytorch/pytorch/commit/10a829b85d721a76ba436f2416d2d802aab6e9e0</link>
      <description><![CDATA[<p>Retarget sym_size/sym_stride lowerings to their .int overloads (#113054)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/112913</p>
<p>The new logging looks like this:</p>
<p><code>[2023-11-06 12:48:57,732] [0/0] torch._inductor.graph: [DEBUG] lowering %arg0_1 : [num_users=0] = placeholder[target=arg0_1]
[2023-11-06 12:48:57,732] [0/0] torch._inductor.graph: [DEBUG] lowering %arg1_1 : [num_users=2] = placeholder[target=arg1_1]
[2023-11-06 12:48:57,733] [0/0] torch._inductor.graph: [DEBUG] lowering %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg1_1, 1), kwargs = {})
[2023-11-06 12:48:57,733] [0/0] torch._inductor.graph: [DEBUG]   via &lt;function make_pointwise.&lt;locals&gt;.inner at 0x7f0abed28ee0&gt;
[2023-11-06 12:48:57,735] [0/0] torch._inductor.graph: [DEBUG] lowering %sym_stride_int : [num_users=1] = call_function[target=torch.ops.aten.sym_stride.int](args = (%add, 0), kwargs = {}) sym_stride
[2023-11-06 12:48:57,735] [0/0] torch._inductor.graph: [DEBUG] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg1_1, %sym_stride_int), kwargs = {})
[2023-11-06 12:48:57,735] [0/0] torch._inductor.graph: [DEBUG]   via &lt;function mul at 0x7f0abec8bd00&gt;
[2023-11-06 12:48:57,744] [0/0] torch._inductor.graph: [DEBUG] lowering return (mul,)</code></p>
<p>Notice that <code>sym_stride</code> no longer is hitting the lowering. This is what the behavior was before I broke it. A better refactor coming soon.</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113054<br />
Approved by: https://github.com/davidberard98</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 20:15:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/10a829b85d721a76ba436f2416d2d802aab6e9e0</guid>
    </item>
    <item>
      <title>Fixes a bug in inductor.triton.load (#113047)</title>
      <link>https://github.com/pytorch/pytorch/commit/74c24d23675d72461fcc7797b731d84c5d978134</link>
      <description><![CDATA[<p>Fixes a bug in inductor.triton.load (#113047)</p>
<p>Lettin CI/CD tell me if there is anything wrong with this</p>
<p>Original bug:<br />
<code>Shell
        r1 = rindex
        tmp37 = tl.load(out_ptr2 + (r1 + (8192*x0)), rmask, eviction_policy='evict_first', other=0)
                                                     ^
AssertionError('cannot cast int32[constexpr[1],constexpr[2048]] to &lt;[1, 2048], fp8e4nv&gt;')</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113047<br />
Approved by: https://github.com/Skylion007, https://github.com/ipiszy</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 20:06:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/74c24d23675d72461fcc7797b731d84c5d978134</guid>
    </item>
    <item>
      <title>Revert "Grandfather in c10d_functional ops to pt2_compliant (#113049)"</title>
      <link>https://github.com/pytorch/pytorch/commit/1fea599d9a3b4e15f5f3de57807e888f37c916a0</link>
      <description><![CDATA[<p>Revert "Grandfather in c10d_functional ops to pt2_compliant (#113049)"</p>
<p>This reverts commit fe8570a1fe5c6678a4be8deff561dbc48693410e.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113049 on behalf of https://github.com/clee2000 due to something in the stack broke distributed and inductor, pretty sure its this one (<a href="https://github.com/pytorch/pytorch/pull/113049#issuecomment-1797298969">comment</a>)</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 18:34:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1fea599d9a3b4e15f5f3de57807e888f37c916a0</guid>
    </item>
    <item>
      <title>Revert "Grandfather in some more pytorch ops to be pt2_compliant (#113050)"</title>
      <link>https://github.com/pytorch/pytorch/commit/19dbd8aca354119701d1a4f7cfa7b1a8c9fb9709</link>
      <description><![CDATA[<p>Revert "Grandfather in some more pytorch ops to be pt2_compliant (#113050)"</p>
<p>This reverts commit efae8449a83df2bcd2e5f3c0f531051b6860cf0c.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113050 on behalf of https://github.com/clee2000 due to something in the stack broke distributed and inductor, pretty sure its the c10 one (<a href="https://github.com/pytorch/pytorch/pull/113050#issuecomment-1797279756">comment</a>)</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 18:30:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/19dbd8aca354119701d1a4f7cfa7b1a8c9fb9709</guid>
    </item>
    <item>
      <title>Revert "Grandfather in built-in TorchScript ops to being pt2_compliant (#113061)"</title>
      <link>https://github.com/pytorch/pytorch/commit/d94d72b39767916ad86fc22f72bd0eb25905d812</link>
      <description><![CDATA[<p>Revert "Grandfather in built-in TorchScript ops to being pt2_compliant (#113061)"</p>
<p>This reverts commit 1d4d5e4319a5ddacdb4e0d1ac944bbb63921fdb1.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/113061 on behalf of https://github.com/clee2000 due to something in the stack broke distributed and inductor, pretty sure its the c10 one.  Not sure why so many things were flaky on this PR (<a href="https://github.com/pytorch/pytorch/pull/113061#issuecomment-1797251293">comment</a>)</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 18:28:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d94d72b39767916ad86fc22f72bd0eb25905d812</guid>
    </item>
    <item>
      <title>[inductor] fix out of shared memory issue (#112916)</title>
      <link>https://github.com/pytorch/pytorch/commit/ad844e7919e49958e0886e3885f86d16e86950b3</link>
      <description><![CDATA[<p>[inductor] fix out of shared memory issue (#112916)</p>
<p>Fix https://github.com/pytorch/pytorch/issues/112454 .</p>
<p>The current fix is quite simple. The kernel has multiple triton configs. Previously any triton config fail to compile, we skip everything else and fail. Now we just skip the bad configs and pick the best one from the remaining configs.</p>
<p>There are other ways to fix the issues more fundamentally but requires much more work:<br />
1. Horace mentioned an idea to make sure the largest one of size_hints is the right most dimension. This way, that largest dimension with be mapped to XBLOCK and we won't scale it up too much since the threshold for the max grid size for x dimension is quite large (2**31 - 1). But this may require us to change loop ordering heuristics which may have other perf impact.<br />
2. The issue happens because the kernel requires 2D tiling which uses shared memory. We can stop scaling up block size if: <code>XBLOCK * YBLOCK * element_size &gt;= max_shared_memory</code> . max_shared_memory is around 160K for A100. The tricky part here is we don't know dtype in <code>triton_config</code> method to decide the <code>element_size</code>.  From metadata, we can find the dtype for each tensor, but if the kernel uses tensors of mixed types, we won't know what dtype is actually used for the data loaded into the shared memory.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112916<br />
Approved by: https://github.com/Chillee, https://github.com/eellison, https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 17:47:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ad844e7919e49958e0886e3885f86d16e86950b3</guid>
    </item>
    <item>
      <title>[inductor] scale up num_warps for reductions to lower register pressure (#113039)</title>
      <link>https://github.com/pytorch/pytorch/commit/24b61a45c9e8eeb7b7bee23ad8173e41f1e59d54</link>
      <description><![CDATA[<p>[inductor] scale up num_warps for reductions to lower register pressure (#113039)</p>
<p>Recent work (https://github.com/pytorch/pytorch/pull/108193 and https://github.com/pytorch/pytorch/pull/109275) unveiled that bigger Triton kernel can regress performance due to increased register pressure which in turn lowers thread occupancy. By taking a look at the Triton internal, I see an opportunity to reduce the register pressure by decreasing the amount of work each thread does. I'm bumping up the <code>num_warps</code> to achieve this. The change should only affect reduction cases.</p>
<p>I'm seeing real compilation time reduction with this change which is likely due to smaller LLVM IR:<br />
https://hud.pytorch.org/benchmark/compilers?startTime=Mon%2C%2023%20Oct%202023%2017%3A57%3A40%20GMT&amp;stopTime=Mon%2C%2006%20Nov%202023%2018%3A57%3A40%20GMT&amp;granularity=hour&amp;suite=torchbench&amp;mode=training&amp;dtype=amp&amp;lBranch=hoy-reduction&amp;lCommit=f2d31b83aa170914018407d88a76d5951153b316&amp;rBranch=main&amp;rCommit=64f326097be8ac66ff057365f3bed2d64c697563</p>
<p>The slightly performance improvement can be noise, if not, the lower register pressure could explain.</p>
<p>Ideally, we should improve Triton to automatically reroll large kernel to an inner loop, without hurting vectorization. That's something I'm considering on the LLVM side.</p>
<p>I'm also seeing the fused kernel provided in https://github.com/pytorch/pytorch/pull/108193 gets a better performance by benefiting from a lower register pressure. PTXAS shows a usage of 32 registers compared to 55 previously.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/113039<br />
Approved by: https://github.com/shunting314</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 16:12:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/24b61a45c9e8eeb7b7bee23ad8173e41f1e59d54</guid>
    </item>
    <item>
      <title>Revert "Fix default timeouts for python entrypoints (e.g. init_process_group) (#112893)"</title>
      <link>https://github.com/pytorch/pytorch/commit/75adb9f37150a670e88c82a2090fe9e40a5602c6</link>
      <description><![CDATA[<p>Revert "Fix default timeouts for python entrypoints (e.g. init_process_group) (#112893)"</p>
<p>This reverts commit f9d47e13813bbefc9f19a6c0430b7122f9d09b91.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/112893 on behalf of https://github.com/clee2000 due to sorry this seems to have broken inductor https://hud.pytorch.org/pytorch/pytorch/commit/f9d47e13813bbefc9f19a6c0430b7122f9d09b91 https://github.com/pytorch/pytorch/actions/runs/6776367936/job/18418174752 (<a href="https://github.com/pytorch/pytorch/pull/112893#issuecomment-1796979811">comment</a>)</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 14:49:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/75adb9f37150a670e88c82a2090fe9e40a5602c6</guid>
    </item>
    <item>
      <title>[aotinductor] Solves a problem where a tensor is returned more than once (#112177)</title>
      <link>https://github.com/pytorch/pytorch/commit/67256d5c1c22e06255d4a8b4db36de22c5d8177f</link>
      <description><![CDATA[<p>[aotinductor] Solves a problem where a tensor is returned more than once (#112177)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112177<br />
Approved by: https://github.com/zhxchen17</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 12:12:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/67256d5c1c22e06255d4a8b4db36de22c5d8177f</guid>
    </item>
    <item>
      <title>[Inductor CUTLASS backend] Epilogue fusion codegen (Step 1) (#110890)</title>
      <link>https://github.com/pytorch/pytorch/commit/bdfde62e54de0351eeb1d55e7c0d16eed76a9a5f</link>
      <description><![CDATA[<p>[Inductor CUTLASS backend] Epilogue fusion codegen (Step 1) (#110890)</p>
<p>Summary:</p>
<p>This PR adds epilogue fusion code generation support for the new experimental<br />
<a href="[https://github.com/pytorch/pytorch/pull/108015]">Inductor Cutlass backend</a>.</p>
<p>Details:</p>
<p>A fusion happens on the GEMM template level by taking a Cutlass 3.x GEMM Universal Matmul Kernel template<br />
and adding a custom template functor based on Cutlass new “Epilogue Visitor Trees” (EVT) on top, which represents and<br />
performs the computation of the fused Pointwise / Elementwise computation nodes.</p>
<p>This is the approach dictated by <a href="https://github.com/NVIDIA/cutlass/blob/main/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu">NVIDIA/cutlass example 49</a>,<br />
which is currently the only documentation and example of Cutlass Epilogue Visitor Trees.</p>
<p>This EVT functor in turn is a hierarchical template expression which represents an abstract syntax tree of the fused computation to perform.<br />
A second codegen task is to create a hierarchical initializer expression, which provides potentially necessary arguments<br />
to each of the functor subexpressions.</p>
<p>Step 1 functionality:</p>
<ul>
<li>End to end code generation is possible using the above approach.</li>
<li>Supports simple elementwise expression fusion of chains of elementwise operations (with scalar constants )<br />
   after a matmul.</li>
<li>Elementwise operation support includes addition, subtraction, multiplication, division, minimum, maximum etc.</li>
<li>Examples / Unit tests include ReLU and ReLU6 fusion.</li>
<li>Support for fp16 and fp16 with fp32 accumulation data types.</li>
<li>Generates SM90 ( Hopper ) based CUDA Kernels ( as Cutlass up to 3.2.0 only supported EVT for SM90 )</li>
</ul>
<p>The following is not yet supported, and is left for future work:</p>
<ul>
<li>Full operation support ( e.g. full set of all ops usually handled via V.ops handlers )</li>
<li>Cutlass EVT with SM80 support ( possible in Cutlass 3.2.1 according to release notes, but not yet documented )</li>
<li>Add support for additional (auxiliary) inputs, which changes the Template Kernels' call signature</li>
<li>Add support for additional (auxiliary) outputs ( requires support for full computation graphs )</li>
<li>Add support for reduction operations and operations which use different output layouts than the input</li>
<li>Add support for additional dtypes ( as far as Cutlass allows )</li>
</ul>
<p>This PR updates third_party/cutlass to v3.2.2, which has some important improvements and features<br />
for the inductor backend.</p>
<p>See also Cutlass release notes:<br />
https://github.com/NVIDIA/cutlass/releases/tag/v3.2.1 and https://github.com/NVIDIA/cutlass/releases/tag/v3.2.2</p>
<p>Notable changes in Cutlass 3.2.1 include:<br />
 * Cutlass codegen python code has moved into a package with the "cutlass_library" namespace, which allows to<br />
   prevent namespace clashes without resolving to monkey-patching ( which was done earlier ).<br />
 * Support for SM80 epilogue visitor trees ( according to the Release Notes, not tried yet )<br />
 * Small API changes to the cutlass_library API ( requires adapting the inductor backend code )</p>
<p>Notable changes in Cutlass 3.2.2 include:<br />
 * Bugfix that led to CUDA Illegal memory access in some Pytorch unit tests involving flash attention</p>
<p>Test Plan:<br />
  * CI<br />
  * pytest test/inductor/test_max_autotune.py</p>
<p>Note: So far, the CUTLASS backend is still disabled by default. Benchmarks are planned once more advanced fusions are enabled.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50988161">D50988161</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110890<br />
Approved by: https://github.com/jansel<br />
ghstack dependencies: #112762</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 11:42:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bdfde62e54de0351eeb1d55e7c0d16eed76a9a5f</guid>
    </item>
    <item>
      <title>Fixed cat uint8 lowering (#112753)</title>
      <link>https://github.com/pytorch/pytorch/commit/59e003d15981ecff197b6be636028867cf36e0a0</link>
      <description><![CDATA[<p>Fixed cat uint8 lowering (#112753)</p>
<p>Description:<br />
- Fixed cat uint8 lowering</p>
<p>Otherwise, it gives the following issue on the repro code:<br />
```python<br />
def func(x):<br />
    batch_shape = x.shape[:1]<br />
    out = torch.cat([x.new_zeros(1).expand(batch_shape + (1,)), x], dim=-1)<br />
    return out</p>
<p>cfunc = torch.compile(func)</p>
<p>x = torch.randint(0, 256, size=(3, 255), dtype=torch.uint8)<br />
out = cfunc(x)<br />
<code>Error message:</code><br />
  File "/pytorch/torch/_inductor/lowering.py", line 1037, in <genexpr><br />
    if all(len(input.layout.size) == 4 for input in inputs):<br />
  File "/pytorch/torch/_inductor/ir.py", line 5795, in <strong>getattr</strong><br />
    fn = getattr(self.data, name)<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
LoweringException: AttributeError: 'ExpandView' object has no attribute 'layout'<br />
  target: aten.cat.default<br />
  args[0]: [TensorBox(<br />
    ExpandView(data=StorageBox(<br />
      ComputedBuffer(name='buf0', layout=FlexibleLayout('cpu', torch.uint8, size=[1], stride=[1]), data=Pointwise(<br />
        'cpu',<br />
        torch.uint8,<br />
        def inner_fn(index):<br />
            _ = index<br />
            tmp0 = ops.constant(0, torch.uint8)<br />
            return tmp0<br />
        ,<br />
        ranges=[1],<br />
        origin_node=full,<br />
        origins={full}<br />
      ))<br />
    ), size=[3, 1])<br />
  ), TensorBox(StorageBox(<br />
    InputBuffer(name='arg0_1', layout=FixedLayout('cpu', torch.uint8, size=[3, 255], stride=[255, 1]))<br />
  ))]<br />
  args[1]: 1</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information<br />
```</p>
<p>Context: compiling is not working for torchvision's <code>F.equalize</code> op: https://github.com/pytorch/vision/issues/8056</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112753<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 11:42:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/59e003d15981ecff197b6be636028867cf36e0a0</guid>
    </item>
    <item>
      <title>Revert "[aotinductor] Solves a problem where a tensor is returned more than once (#112177)"</title>
      <link>https://github.com/pytorch/pytorch/commit/2bc1378d7be563fa9b3050bb0e0fefd6e55a9e81</link>
      <description><![CDATA[<p>Revert "[aotinductor] Solves a problem where a tensor is returned more than once (#112177)"</p>
<p>This reverts commit a91baaf314999abaaf93260f87b1ee109bb36541.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/112177 on behalf of https://github.com/PaliC due to breaking internal tests (refer to internal diff) (<a href="https://github.com/pytorch/pytorch/pull/112177#issuecomment-1794153272">comment</a>)</p>]]></description>
      <pubDate>Sun, 05 Nov 2023 22:20:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2bc1378d7be563fa9b3050bb0e0fefd6e55a9e81</guid>
    </item>
    <item>
      <title>[aotinductor] Move cache_dir to utils.py (#112728)</title>
      <link>https://github.com/pytorch/pytorch/commit/bd9be877e4459d7889e5e3c8051caaffdfe21c85</link>
      <description><![CDATA[<p>[aotinductor] Move cache_dir to utils.py (#112728)</p>
<p>Summary: Some tests can utilize cache_dir()</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112728<br />
Approved by: https://github.com/jansel, https://github.com/chenyang78<br />
ghstack dependencies: #112651</p>]]></description>
      <pubDate>Sun, 05 Nov 2023 19:42:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bd9be877e4459d7889e5e3c8051caaffdfe21c85</guid>
    </item>
    <item>
      <title>Inductor cpp wrapper: fix QMaxPool (#112379)</title>
      <link>https://github.com/pytorch/pytorch/commit/46a34e8c755f1ce4479a1e192f8c29f35c42f5f8</link>
      <description><![CDATA[<p>Inductor cpp wrapper: fix QMaxPool (#112379)</p>
<p>Based on the <code>Argument types</code> section in this <a href="https://github.com/pytorch/pytorch/tree/cb942ef2b12134bfaa1727295380fe00ebb537c0/aten/src/ATen/native#func">file</a>, for non-inplace <code>Tensor</code> type in schema, it should be mapped to C++ argument of type <code>const Tensor&amp;</code>.</p>
<p>For <code>quantized_max_pool1d</code> and <code>quantized_max_pool2d</code>, the type of the <code>qx</code> input is <code>Tensor</code> type in the schema, thus modified the C++ type to be <code>const Tensor&amp;</code>:<br />
https://github.com/pytorch/pytorch/blob/cb942ef2b12134bfaa1727295380fe00ebb537c0/aten/src/ATen/native/quantized/library.cpp#L222-L223</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112379<br />
Approved by: https://github.com/jgong5, https://github.com/jansel<br />
ghstack dependencies: #112373, #112378</p>]]></description>
      <pubDate>Sun, 05 Nov 2023 18:07:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/46a34e8c755f1ce4479a1e192f8c29f35c42f5f8</guid>
    </item>
    <item>
      <title>Fix RecursionError in Inductor for large for loops (#112320)</title>
      <link>https://github.com/pytorch/pytorch/commit/674c104d122797644805b07151850e1dffb10fa2</link>
      <description><![CDATA[<p>Fix RecursionError in Inductor for large for loops (#112320)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/111686</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112320<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Sun, 05 Nov 2023 05:12:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/674c104d122797644805b07151850e1dffb10fa2</guid>
    </item>
    <item>
      <title>Back out "[aotinductor] Add example_value metadata to nodes (#112415)" (#112946)</title>
      <link>https://github.com/pytorch/pytorch/commit/ea4b63db6218c4e889c3aab3e3486afe4e311cd0</link>
      <description><![CDATA[<p>Back out "[aotinductor] Add example_value metadata to nodes (#112415)" (#112946)</p>
<p>Summary:<br />
Original commit changeset: 967c6272c8e2</p>
<p>Original Phabricator Diff: D50802786</p>
<p>D50802786 is introding perf regression for AOTInductor internal models.</p>
<p>Differential Revision: D51002032</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112946<br />
Approved by: https://github.com/houseroad</p>]]></description>
      <pubDate>Sat, 04 Nov 2023 17:27:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ea4b63db6218c4e889c3aab3e3486afe4e311cd0</guid>
    </item>
    <item>
      <title>BUG: gracefully fall back to numpy.random if asked in dynamo.config (#109205)</title>
      <link>https://github.com/pytorch/pytorch/commit/d5fff7338e1c16a817ea63f07a32cbc58ede5fec</link>
      <description><![CDATA[<p>BUG: gracefully fall back to numpy.random if asked in dynamo.config (#109205)</p>
<p>Graph break if <code>config.use_numpy_random_stream=True</code> instead of a hard failure in inductor.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109205<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Sat, 04 Nov 2023 06:54:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d5fff7338e1c16a817ea63f07a32cbc58ede5fec</guid>
    </item>
    <item>
      <title>[Inductor] [Quant] Re-structure Quantization testcase pattern matcher check (#112570)</title>
      <link>https://github.com/pytorch/pytorch/commit/b4ce5011379e62876218aff43646221106892196</link>
      <description><![CDATA[<p>[Inductor] [Quant] Re-structure Quantization testcase pattern matcher check (#112570)</p>
<p><strong>Summary</strong><br />
This Diff re-structures Quantization testcase pattern matcher check. Instead of checking all the pattern matched in the Inductor, we will only check the core pattern match count and node numbers such as: dequant promotion, QConv/Linear Unary and QConv Binary.</p>
<p><strong>TestPlan</strong><br />
<code>python -m pytest test_mkldnn_pattern_matcher.py -k test_q</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112570<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168</p>]]></description>
      <pubDate>Fri, 03 Nov 2023 17:11:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b4ce5011379e62876218aff43646221106892196</guid>
    </item>
    <item>
      <title>[Cutlass 3.2.2 submodule upgrade] Adapt Inductor cutlass backend to Cutlass 3.2.2 (#112762)</title>
      <link>https://github.com/pytorch/pytorch/commit/e36dba3a942b09822d8786c48482fb300e9cee70</link>
      <description><![CDATA[<p>[Cutlass 3.2.2 submodule upgrade] Adapt Inductor cutlass backend to Cutlass 3.2.2 (#112762)</p>
<p>The inductor cutlass backend was written against Cutlass version 3.1.x,<br />
there are some incompatible changes in Cutlass 3.2.2 which the<br />
Inductor cutlass backend needs to adapt to.</p>
<p>Test plan:</p>
<p>If third_party/cutlass is upgraded to Cutlass tag v3.2.2,<br />
several tests within test/inductor/test_max_autotune.py start to<br />
fail. With this diff applied, they pass again.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50986555">D50986555</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/112762<br />
Approved by: https://github.com/ipiszy, https://github.com/drisspg</p>]]></description>
      <pubDate>Fri, 03 Nov 2023 16:10:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e36dba3a942b09822d8786c48482fb300e9cee70</guid>
    </item>
    <item>
      <title>[Quant] [PT2] Enable Decomposed quant per tensor/channel to accept bfloat16 input (#112225)</title>
      <link>https://github.com/pytorch/pytorch/commit/6ba2748690f6c6079b3c3a5a778f790b6ebd6296</link>
      <description><![CDATA[<p>[Quant] [PT2] Enable Decomposed quant per tensor/channel to accept bfloat16 input (#112225)</p>
<p><strong>Summary</strong><br />
- PR 4 for enabling Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor https://github.com/pytorch/pytorch/issues/111640.<br />
- Enable <code>decomposed quant_per_tensor</code> and <code>quant_per_channel</code> accepts bfloat16 input.</p>
<p><strong>TestPlan</strong><br />
<code>python -m pytest test_quantized_tensor.py -k test_decomposed_quantize_per_tensor_bfloat16_input
python -m pytest test_quantized_tensor.py -k test_decomposed_quantize_per_channel_bfloat16_input</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112225<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168</p>]]></description>
      <pubDate>Fri, 03 Nov 2023 15:47:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6ba2748690f6c6079b3c3a5a778f790b6ebd6296</guid>
    </item>
    <item>
      <title>[Inductor] Kill has_aliasing (#112875)</title>
      <link>https://github.com/pytorch/pytorch/commit/67e8762e83e938c87165b85a919f938ec006c3db</link>
      <description><![CDATA[<p>[Inductor] Kill has_aliasing (#112875)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112875<br />
Approved by: https://github.com/Chillee</p>]]></description>
      <pubDate>Fri, 03 Nov 2023 15:22:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/67e8762e83e938c87165b85a919f938ec006c3db</guid>
    </item>
    <item>
      <title>[aotinductor] Solves a problem where a tensor is returned more than once (#112177)</title>
      <link>https://github.com/pytorch/pytorch/commit/a91baaf314999abaaf93260f87b1ee109bb36541</link>
      <description><![CDATA[<p>[aotinductor] Solves a problem where a tensor is returned more than once (#112177)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112177<br />
Approved by: https://github.com/zhxchen17</p>]]></description>
      <pubDate>Fri, 03 Nov 2023 10:26:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a91baaf314999abaaf93260f87b1ee109bb36541</guid>
    </item>
    <item>
      <title>[easy] skipIfTorchInductor - use condition variable (#112774)</title>
      <link>https://github.com/pytorch/pytorch/commit/d084a024aede3b2420c5977dd473988c63e9b9f6</link>
      <description><![CDATA[<p>[easy] skipIfTorchInductor - use condition variable (#112774)</p>
<p>Fixes #112465<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/112774<br />
Approved by: https://github.com/jon-chuang, https://github.com/aaronenyeshi</p>]]></description>
      <pubDate>Fri, 03 Nov 2023 09:55:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d084a024aede3b2420c5977dd473988c63e9b9f6</guid>
    </item>
    <item>
      <title>[Inductor] Support one node creating multiple mutations in scheduler (#112547)</title>
      <link>https://github.com/pytorch/pytorch/commit/001573b6874f8aa62eb123fcb5dfe018ed8d7b09</link>
      <description><![CDATA[<p>[Inductor] Support one node creating multiple mutations in scheduler (#112547)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112547<br />
Approved by: https://github.com/Chillee</p>]]></description>
      <pubDate>Fri, 03 Nov 2023 08:01:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/001573b6874f8aa62eb123fcb5dfe018ed8d7b09</guid>
    </item>
    <item>
      <title>[aotinductor] Allow specifying a .so name in the aot_inductor.output_path config (#112651)</title>
      <link>https://github.com/pytorch/pytorch/commit/7f143d7ef5691898b0cd98fe4133201f651fe484</link>
      <description><![CDATA[<p>[aotinductor] Allow specifying a .so name in the aot_inductor.output_path config (#112651)</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50902585">D50902585</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/112651<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Fri, 03 Nov 2023 04:56:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7f143d7ef5691898b0cd98fe4133201f651fe484</guid>
    </item>
    <item>
      <title>[Quant] [PT2] Remove the output Annotation of Conv/Linear in x86InductorQuantizer (#112140)</title>
      <link>https://github.com/pytorch/pytorch/commit/871e27a61c0f0bf49dab51f68534c49c95ee0ee3</link>
      <description><![CDATA[<p>[Quant] [PT2] Remove the output Annotation of Conv/Linear in x86InductorQuantizer (#112140)</p>
<p><strong>Summary</strong><br />
- PR 3 for enabling Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor https://github.com/pytorch/pytorch/issues/111640.<br />
- Remove the output annotation of QConv/QLinear in X86InductorQuantizer.</p>
<p><strong>Test Plan</strong><br />
<code>python -m pytest test_mkldnn_pattern_matcher.py -k test_qconv2d
python -m pytest test_mkldnn_pattern_matcher.py -k test_qlinear
python -m pytest test_x86inductor_quantizer.py -k Conv2d
python -m pytest test_x86inductor_quantizer.py -k Linear</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112140<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168<br />
ghstack dependencies: #112010, #112126</p>]]></description>
      <pubDate>Fri, 03 Nov 2023 00:24:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/871e27a61c0f0bf49dab51f68534c49c95ee0ee3</guid>
    </item>
    <item>
      <title>Enable oneDNN QLinear FP32/BF16 output (#112126)</title>
      <link>https://github.com/pytorch/pytorch/commit/a53d29cc1837f0e422d55fd5f10e2ed7e1c745dd</link>
      <description><![CDATA[<p>Enable oneDNN QLinear FP32/BF16 output (#112126)</p>
<p><strong>Summary</strong><br />
- PR 2 for enabling Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor https://github.com/pytorch/pytorch/issues/111640.<br />
- Enable QLinear (relu) with BFloat16 or Float32 output.</p>
<p><strong>TestPlan</strong><br />
<code>python -u -m pytest -s -v test_quantized_op.py -k test_qlinear_pt2e</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112126<br />
Approved by: https://github.com/jerryzh168, https://github.com/jgong5<br />
ghstack dependencies: #112010</p>]]></description>
      <pubDate>Fri, 03 Nov 2023 00:20:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a53d29cc1837f0e422d55fd5f10e2ed7e1c745dd</guid>
    </item>
    <item>
      <title>Enable oneDNN QConv FP32/BF16 output (#112010)</title>
      <link>https://github.com/pytorch/pytorch/commit/b6fc7af8a0fd158e4324aa9fd97dac4db2c038de</link>
      <description><![CDATA[<p>Enable oneDNN QConv FP32/BF16 output (#112010)</p>
<p><strong>Summary</strong></p>
<ul>
<li>PR 1 for enabling Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor https://github.com/pytorch/pytorch/issues/111640.</li>
<li>Enable QConv (relu, add, add_relu) with BFloat16 or Float32 output.</li>
</ul>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_quantized_op.py -k test_qconv1d_pt2e
python -u -m pytest -s -v test_quantized_op.py -k test_qconv2d_pt2e
python -u -m pytest -s -v test_quantized_op.py -k test_qconv3d_pt2e
python -u -m pytest test_quantized_op.py -k test_qconv2d_relu_pt2e
python -u -m pytest test_quantized_op.py -k test_qconv2d_add_pt2e
python -u -m pytest test_quantized_op.py -k test_qconv2d_add_relu_pt2e
python -u -m pytest test_quantized_op.py -k test_qconv2d_add_relu_float_output_pt2e</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112010<br />
Approved by: https://github.com/jerryzh168, https://github.com/jgong5</p>]]></description>
      <pubDate>Fri, 03 Nov 2023 00:16:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b6fc7af8a0fd158e4324aa9fd97dac4db2c038de</guid>
    </item>
    <item>
      <title>[Inductor] Add Dynamic shape support to user defined triton kernels (#112523)</title>
      <link>https://github.com/pytorch/pytorch/commit/13d62e28a3a88d8f3fce32c10d779e28495002bb</link>
      <description><![CDATA[<p>[Inductor] Add Dynamic shape support to user defined triton kernels (#112523)</p>
<p>1) This PR moves the grid function codegen to wrapper so that we can use<br />
   IndentBuffers as opposed to manually adding tabs for indentation.<br />
2) In inductor, emits the grid function in the body of the kernel call so<br />
   that it can use free symbols from dynamic shapes</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112523<br />
Approved by: https://github.com/Chillee</p>]]></description>
      <pubDate>Thu, 02 Nov 2023 15:58:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/13d62e28a3a88d8f3fce32c10d779e28495002bb</guid>
    </item>
    <item>
      <title>[pytree] Add back a default serialized name (#112748)</title>
      <link>https://github.com/pytorch/pytorch/commit/3904b8142066dcf19aef4c0fc268d40cb1429fd1</link>
      <description><![CDATA[<p>[pytree] Add back a default serialized name (#112748)</p>
<p>Previously we added a change which required users to pass in a serialized name if they want to serialize a pytree so that the serialized name does not depend on the python environment. However this is currently breaking AOTInductor benchmark tests as AOTInductor will serialize the pytree into the .so for flattening/unflattening the inputs. However, the registration for those pytree types in the AOTInductor benchmarks are in the huggingface repo, so I'm not sure what's a good fix for now.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/112748<br />
Approved by: https://github.com/zhxchen17, https://github.com/malfet</p>]]></description>
      <pubDate>Thu, 02 Nov 2023 14:34:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3904b8142066dcf19aef4c0fc268d40cb1429fd1</guid>
    </item>
    <item>
      <title>[inductor][fx pass] Fix a split cat bug in the pre grad (#112667)</title>
      <link>https://github.com/pytorch/pytorch/commit/543a618ae83e955ee22abf742e1899e8a659c9c2</link>
      <description><![CDATA[<p>[inductor][fx pass] Fix a split cat bug in the pre grad (#112667)</p>
<p>Summary: blue reels vdd v3 has unit test failure, we fix the bug</p>
<p>Test Plan:<br />
<code>buck2 test 'fbcode//mode/opt' fbcode//pytorch/benchmark/fb/test_gpu:run_test_gpu -- --exact 'pytorch/benchmark/fb/test_gpu:run_test_gpu - test_train_blue_reels_vdd_v3_inductor_accuracy (pytorch.benchmark.fb.test_gpu.test_gpu.TestBenchmarkFbGpu)'</code><br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/13229323914259182<br />
Network: Up: 2.5MiB  Down: 8.3MiB  (reSessionID-b3362362-c80a-4ac2-8332-bc1321aaf0bd)<br />
Jobs completed: 6. Time elapsed: 5:13.2s.<br />
Tests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<p><code>buck2 test 'fbcode//mode/opt' fbcode//pytorch/benchmark/fb/test_gpu:run_test_gpu -- --exact 'pytorch/benchmark/fb/test_gpu:run_test_gpu - test_train_blue_reels_vdd_v3_inductor_speedup (pytorch.benchmark.fb.test_gpu.test_gpu.TestBenchmarkFbGpu)'</code><br />
Buck UI: https://www.internalfb.com/buck2/aa3031a9-3f1b-4f42-a78c-decbf2beb14f<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/4785074810906355<br />
Network: Up: 1.3GiB  Down: 40MiB  (reSessionID-801ddf16-ff5d-4135-9758-ff286d1d59aa)<br />
Jobs completed: 69. Time elapsed: 10:12.4s.<br />
Cache hits: 10%. Commands: 61 (cached: 6, remote: 4, local: 51)<br />
Tests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<p>Differential Revision: D50901626</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112667<br />
Approved by: https://github.com/xuzhao9, https://github.com/Skylion007</p>]]></description>
      <pubDate>Thu, 02 Nov 2023 09:33:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/543a618ae83e955ee22abf742e1899e8a659c9c2</guid>
    </item>
    <item>
      <title>[quant][docs] Add x86 inductor quant docs (#112648)</title>
      <link>https://github.com/pytorch/pytorch/commit/6929ebf2b08d60decf8a90649247741a10ca144d</link>
      <description><![CDATA[<p>[quant][docs] Add x86 inductor quant docs (#112648)</p>
<p>Summary:<br />
att</p>
<p>Test Plan:<br />
.</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/112648<br />
Approved by: https://github.com/leslie-fang-intel, https://github.com/jgong5, https://github.com/andrewor14</p>]]></description>
      <pubDate>Thu, 02 Nov 2023 09:02:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6929ebf2b08d60decf8a90649247741a10ca144d</guid>
    </item>
    <item>
      <title>[pytree] Add back a default serialized name (#112748)</title>
      <link>https://github.com/pytorch/pytorch/commit/ca72d23613f7976b3ad70e54234b125c1b763dde</link>
      <description><![CDATA[<p>[pytree] Add back a default serialized name (#112748)</p>
<p>Previously we added a change which required users to pass in a serialized name if they want to serialize a pytree so that the serialized name does not depend on the python environment. However this is currently breaking AOTInductor benchmark tests as AOTInductor will serialize the pytree into the .so for flattening/unflattening the inputs. However, the registration for those pytree types in the AOTInductor benchmarks are in the huggingface repo, so I'm not sure what's a good fix for now.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/112748<br />
Approved by: https://github.com/zhxchen17, https://github.com/malfet</p>]]></description>
      <pubDate>Thu, 02 Nov 2023 08:18:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ca72d23613f7976b3ad70e54234b125c1b763dde</guid>
    </item>
    <item>
      <title>[inductor] Memory planning (#112178)</title>
      <link>https://github.com/pytorch/pytorch/commit/ae85ba820f398212534018376237e04b4e5f4a51</link>
      <description><![CDATA[<p>[inductor] Memory planning (#112178)</p>
<p>This was originally @jansel's PR:<br />
https://github.com/pytorch/pytorch/pull/102625, which I've built upon.</p>
<p>This diff implements static memory planning. It's disabled by default<br />
while we examine its performance.</p>
<p>We use a greedy-by-size approach. For dynamic shapes, the sizes of the<br />
example inputs are used as estimates when making planning decisions. We<br />
generate expressions to calculate the actual memory offsets and sizes at<br />
runtime when the values of the dynamic shapes are known. In order to<br />
simplify these calculations, we have organized the allocations into a<br />
tree that branches on space (address offsets) and time (live ranges).<br />
Finally, we need to align these offsets, so we have added an <code>align</code><br />
sympy Expr to express these calculations.</p>
<p>Some limitations:</p>
<ol>
<li>It is only enabled during inference for now. Enabling it for training<br />
   increases peak memory usage as we allocate all the memory needed for<br />
   training upfront, before freeing the memory allocated during<br />
   inference. We can probably address this by doing planning for both<br />
   the inference and training passes together.</li>
<li>It doesn't work with PyTorch Distributed, because kernels like<br />
   AllGatherIntoTensor codegen strings which do memory operations. We<br />
   can fix this down the line by having them emit MemoryPlanningLines<br />
   instead.</li>
</ol>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112178<br />
Approved by: https://github.com/desertfire, https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 01 Nov 2023 23:39:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ae85ba820f398212534018376237e04b4e5f4a51</guid>
    </item>
    <item>
      <title>[inductor] nan-checker (#112091)</title>
      <link>https://github.com/pytorch/pytorch/commit/493ae7820100ab6afdfc19883dd9ad8c5198f1e2</link>
      <description><![CDATA[<p>[inductor] nan-checker (#112091)</p>
<p>This PR is spilt out of https://github.com/pytorch/pytorch/pull/108193 . It adds the ability to add assertion after each triton kernel calls to make sure all tensor arguments are not nan/inf. It helps me find a few bugs when working on benchmark fusion (due to messing up some kernel/graph level states when generating kernel code).</p>
<p>Right now we have to disable cudagraphs to enable the nan/inf checks. Otherwise we will see errors like: https://gist.github.com/shunting314/053db66c4f121e5f4c5de159bf0032ed . My best guess is it's due to GPU-&gt;CPU copy during capturing for cudagraphs. cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @eellison  if there is easy way to make it work with cudagraphs.  But even if the nan-checker is not compatible with cudagraphs, it's probably still fine since it's just for debugging purpose.</p>
<p>Test command:<br />
<code>TORCHINDUCTOR_BENCHMARK_KERNEL=1 TORCHINDUCTOR_NAN_ASSERTS=1 python benchmarks/dynamo/huggingface.py --backend inductor --amp --performance --only BertForMaskedLM --training --disable-cudagraphs</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112091<br />
Approved by: https://github.com/eellison, https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 01 Nov 2023 18:32:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/493ae7820100ab6afdfc19883dd9ad8c5198f1e2</guid>
    </item>
    <item>
      <title>[Quant] [PT2] Add ConvBNAdd(ReLU) Annotation into X86InductorQuantizer (#111281)</title>
      <link>https://github.com/pytorch/pytorch/commit/6c19de07cdfb7cdab4cafe7c6c2caf0d385fea46</link>
      <description><![CDATA[<p>[Quant] [PT2] Add ConvBNAdd(ReLU) Annotation into X86InductorQuantizer (#111281)</p>
<p><strong>Summary</strong><br />
This PR adds ConvBNAdd(ReLU) QAT Annotation into <code>X86InductorQuantizer</code>.</p>
<p><strong>Test Plan</strong><br />
<code>python -m pytest test_x86inductor_quantizer.py -k test_qat_conv2d_binary_with_quantizer_api
python -m pytest test_x86inductor_quantizer.py -k test_qat_conv2d_binary_unary_with_quantizer_api
python -m pytest test_mkldnn_pattern_matcher.py -k test_qat_qconv2d_add
python -m pytest test_mkldnn_pattern_matcher.py -k test_qat_qconv2d_add_relu</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111281<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168<br />
ghstack dependencies: #111280</p>]]></description>
      <pubDate>Wed, 01 Nov 2023 18:05:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6c19de07cdfb7cdab4cafe7c6c2caf0d385fea46</guid>
    </item>
    <item>
      <title>[Quant] [PT2] Enable QAT Quantization flow in X86InductorQuantizer (#111280)</title>
      <link>https://github.com/pytorch/pytorch/commit/56ca0043f6906be5ee3936ee0af9c3135b96ee11</link>
      <description><![CDATA[<p>[Quant] [PT2] Enable QAT Quantization flow in X86InductorQuantizer (#111280)</p>
<p><strong>Summary</strong><br />
This PR enables PT2 QAT Quantization flow in <code>X86InductorQuantizer</code>.</p>
<p><strong>Test Plan</strong><br />
<code>python -m pytest test_x86inductor_quantizer.py -k test_qat_conv2d_with_quantizer_api
python -m pytest test_x86inductor_quantizer.py -k test_qat_conv2d_unary_with_quantizer_api
python -m pytest test_mkldnn_pattern_matcher.py -k test_qat_qconv2d
python -m pytest test_mkldnn_pattern_matcher.py -k test_qat_qconv2d_relu</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111280<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168</p>]]></description>
      <pubDate>Wed, 01 Nov 2023 18:03:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/56ca0043f6906be5ee3936ee0af9c3135b96ee11</guid>
    </item>
    <item>
      <title>[Reland2] [inductor][BE] split triton_meta and inductor_meta (#112351)</title>
      <link>https://github.com/pytorch/pytorch/commit/8191fb3e060f600090c839d6b86c28dbb687b679</link>
      <description><![CDATA[<p>[Reland2] [inductor][BE] split triton_meta and inductor_meta (#112351)</p>
<p>triton_meta is intended to be passed directly to triton. Previous we were also putting other metadata into triton_meta; but we should split out the other metadata into a separate dict to avoid possible conficts in the future.</p>
<p>This PR splits out triton_meta and inductor_meta so we have a place to put additional metadata that isn't intended to be passed to triton.</p>
<p>Tests - wait for CI</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50864493">D50864493</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/112351<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 01 Nov 2023 16:40:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8191fb3e060f600090c839d6b86c28dbb687b679</guid>
    </item>
    <item>
      <title>[aotinductor] Add example_value metadata to nodes (#112415)</title>
      <link>https://github.com/pytorch/pytorch/commit/00d6d2f66bb23e9ab2b810a98c179ec2f217ec5c</link>
      <description><![CDATA[<p>[aotinductor] Add example_value metadata to nodes (#112415)</p>
<p>split_cat fx passes expect the <code>example_value</code> metadata on every node. However, the graph module from _export_torch_ir does not contain this metadata, causing the split_cat fx passes to not run. So, I added a pass to add this metadata to every node in the graph.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112415<br />
Approved by: https://github.com/frank-wei</p>]]></description>
      <pubDate>Wed, 01 Nov 2023 14:44:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/00d6d2f66bb23e9ab2b810a98c179ec2f217ec5c</guid>
    </item>
    <item>
      <title>[Inductor] Clarify mutation related comments (#112466)</title>
      <link>https://github.com/pytorch/pytorch/commit/7f77ec37be8077079e118d29e209dc202e240d6e</link>
      <description><![CDATA[<p>[Inductor] Clarify mutation related comments (#112466)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112466<br />
Approved by: https://github.com/Chillee</p>]]></description>
      <pubDate>Wed, 01 Nov 2023 10:39:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7f77ec37be8077079e118d29e209dc202e240d6e</guid>
    </item>
    <item>
      <title>[AOTInductor] Include AOTI debug folder in package (#112514)</title>
      <link>https://github.com/pytorch/pytorch/commit/a126bbfea3e3b5c09db32c5582eb9bbd341a13e5</link>
      <description><![CDATA[<p>[AOTInductor] Include AOTI debug folder in package (#112514)</p>
<p>Summary:<br />
Allow user to set debug dir for Inductor</p>
<p>Include AOTInductor debug folder in the package.</p>
<p><code>zipinfo package.zip
Archive:  package.zip
Zip file size: 1325264 bytes, number of entries: 46
-rw----     0.0 fat      212 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/aotinductor_pickle_data.json
-rw----     0.0 fat     6024 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/debug/torchinductor/model___9.0/fx_graph_runnable.py
-rw----     0.0 fat     9031 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/debug/torchinductor/model___9.0/fx_graph_readable.py
-rw----     0.0 fat     9202 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/debug/torchinductor/model___9.0/fx_graph_transformed.py
-rw----     0.0 fat    10865 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/debug/torchinductor/model___9.0/ir_pre_fusion.txt
-rw----     0.0 fat    10865 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/debug/torchinductor/model___9.0/ir_post_fusion.txt
-rw----     0.0 fat    13553 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/debug/torchinductor/model___9.0/output_code.py
-rw----     0.0 fat     5822 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/debug/torchinductor/model___9.1/fx_graph_runnable.py
-rw----     0.0 fat     8817 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/debug/torchinductor/model___9.1/fx_graph_readable.py
-rw----     0.0 fat     8988 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/debug/torchinductor/model___9.1/fx_graph_transformed.py
-rw----     0.0 fat    10858 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/debug/torchinductor/model___9.1/ir_pre_fusion.txt
-rw----     0.0 fat    10858 bl stor 80-000-00 00:00 package/data/aotinductor/merge-a100/debug/torchinductor/model___9.1/ir_post_fusion.txt</code></p>
<p>Test Plan: CIs</p>
<p>Reviewed By: chenyang78</p>
<p>Differential Revision: D50815320</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112514<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire</p>]]></description>
      <pubDate>Wed, 01 Nov 2023 00:25:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a126bbfea3e3b5c09db32c5582eb9bbd341a13e5</guid>
    </item>
    <item>
      <title>Inductor cpp wrapper: support QLinear (#112378)</title>
      <link>https://github.com/pytorch/pytorch/commit/29f3d392bf230072e3bffae37b078e770cae1956</link>
      <description><![CDATA[<p>Inductor cpp wrapper: support QLinear (#112378)</p>
<p>Align the type of <code>post_op_args</code> in the schema of <code>onednn::qlinear_pointwise</code> to be the same as other fusion OPs like qconv, conv, conv_transpose, linear by changing from <code>float[]</code> to <code>Scalar?[]</code>:<br />
https://github.com/pytorch/pytorch/blob/cb942ef2b12134bfaa1727295380fe00ebb537c0/aten/src/ATen/native/quantized/library.cpp#L260-L266</p>
<p>https://github.com/pytorch/pytorch/blob/cb942ef2b12134bfaa1727295380fe00ebb537c0/aten/src/ATen/native/mkldnn/RegisterMkldnnOpContextClass.cpp#L48-L59</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112378<br />
Approved by: https://github.com/jgong5, https://github.com/desertfire<br />
ghstack dependencies: #112373</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 22:22:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/29f3d392bf230072e3bffae37b078e770cae1956</guid>
    </item>
    <item>
      <title>Inductor cpp wrapper: support QConv (#112373)</title>
      <link>https://github.com/pytorch/pytorch/commit/337d69e40a57cdb442f548c5b06bf2e823d69dc3</link>
      <description><![CDATA[<p>Inductor cpp wrapper: support QConv (#112373)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112373<br />
Approved by: https://github.com/jgong5, https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 22:15:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/337d69e40a57cdb442f548c5b06bf2e823d69dc3</guid>
    </item>
    <item>
      <title>[inductor] replace ops.div with ops.truediv (#112243)</title>
      <link>https://github.com/pytorch/pytorch/commit/e061144aaf0bea11693d47739d6e34d49f1f1d9d</link>
      <description><![CDATA[<p>[inductor] replace ops.div with ops.truediv (#112243)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112243<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #112234</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 21:50:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e061144aaf0bea11693d47739d6e34d49f1f1d9d</guid>
    </item>
    <item>
      <title>[inductor][fx pass] Add split-stack-tahn-unbind pattern detection (#111854)</title>
      <link>https://github.com/pytorch/pytorch/commit/dcd94814a341532d662fd793020ccf581abf9458</link>
      <description><![CDATA[<p>[inductor][fx pass] Add split-stack-tahn-unbind pattern detection (#111854)</p>
<p>Summary: We add a new patten to further close the gap between fxt and pt2</p>
<p>Test Plan:</p>
<h1>unit test</h1>
<p><code>buck2 test mode/dev-nosan //caffe2/test/inductor:split_cat_fx_passes</code><br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/1407375224343119</p>
<h1>icvr local test</h1>
<p><a href="https://www.internalfb.com/intern/paste/P865759493/">P865759493</a></p>
<p>before vs after transformation after "merge_getitem_cat_pass":<br />
https://www.internalfb.com/intern/diffing/?paste_number=854132317</p>
<h1>e2e test</h1>
<p>The proposal is bundled D50207610, D50397173 and D50100667</p>
<h3>ICVR</h3>
<p>baseline:<br />
f489286934<br />
baseline + optimus:<br />
f489287369<br />
proposal:<br />
f492987960</p>
<h3>CMF</h3>
<p>baseline:<br />
f489195078<br />
baseline + optimus:<br />
f489215258<br />
proposal:<br />
f492970293</p>
<h3>IG_CTR</h3>
<p>baseline:<br />
f489237630<br />
baseline + optimus:<br />
f489238767<br />
proposal:<br />
f492977663</p>
<p>Differential Revision: D50397173</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111854<br />
Approved by: https://github.com/jackiexu1992</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 19:04:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dcd94814a341532d662fd793020ccf581abf9458</guid>
    </item>
    <item>
      <title>metric table (#109245)</title>
      <link>https://github.com/pytorch/pytorch/commit/a1e222ef02f07d8c2626d635d6c6b87401816d92</link>
      <description><![CDATA[<p>metric table (#109245)</p>
<p>In dynamo/inductor, sometimes it helps to gather metrics/statistics for each model in different levels like model level, graph level, kernel level or pair of fusion nodes level. This kind of thing will be very easy to do with Scuba, but we only have scuba in fbcode. This PR build metric tables to solve part of the problem.</p>
<p>Q: why not log to stdout/err direclty<br />
A: sometimes we need more structured data. E.g., it would be helpful to gather all the stats in a CSV and then do post-processing (like calculating a geomean etc.). Also metric table will tag each row with the model name which is helpful.</p>
<p>Q: what's the difference with speedup_indcutor.csv<br />
A: speedup_indcutor.csv is a special case that gather statistics on model level: i.e., we have one row for each model. But recording statistics on finer grain level like graph etc. is also helpful.</p>
<p>Example use cases:<br />
- As a followup on the bechmark fusion PR, I want to gather all the 'slow' fusion and analyze them. With the metric table, I can easily log slow fusion for each model into a csv file. Here is the log gathered for huggingface:<br />
 https://gist.github.com/shunting314/964e73cc98368b301414ec7b7ad4c702 .<br />
- To help understand the effect of 'loop ordering after fusion' PR, it would be helpful to gather stats like how many fusions happens for each graph. Previously we log the metric to stderr directly. But logging these metrics in a structural way is useful.<br />
- gather number of registers, register spills, shared memory usage for each kernel in each model with runnable kernel code logged.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109245<br />
Approved by: https://github.com/jansel, https://github.com/mlazos</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 18:33:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a1e222ef02f07d8c2626d635d6c6b87401816d92</guid>
    </item>
    <item>
      <title>Revert "[inductor] Memory planning (#112178)"</title>
      <link>https://github.com/pytorch/pytorch/commit/74e6c877e9bc9c99466b1953f8a73b292d8e8ae7</link>
      <description><![CDATA[<p>Revert "[inductor] Memory planning (#112178)"</p>
<p>This reverts commit f64a97c6f88873363c5b3c4c33f231b5578085b2.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/112178 on behalf of https://github.com/huydhn due to Sorry for reverting your change, but it seems that ROCm will need to be fixed for the new test too https://hud.pytorch.org/pytorch/pytorch/commit/f64a97c6f88873363c5b3c4c33f231b5578085b2 (<a href="https://github.com/pytorch/pytorch/pull/112178#issuecomment-1788195311">comment</a>)</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 16:03:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/74e6c877e9bc9c99466b1953f8a73b292d8e8ae7</guid>
    </item>
    <item>
      <title>[inductor] Memory planning (#112178)</title>
      <link>https://github.com/pytorch/pytorch/commit/f64a97c6f88873363c5b3c4c33f231b5578085b2</link>
      <description><![CDATA[<p>[inductor] Memory planning (#112178)</p>
<p>This was originally @jansel's PR:<br />
https://github.com/pytorch/pytorch/pull/102625, which I've built upon.</p>
<p>This diff implements static memory planning. It's disabled by default<br />
while we examine its performance.</p>
<p>We use a greedy-by-size approach. For dynamic shapes, the sizes of the<br />
example inputs are used as estimates when making planning decisions. We<br />
generate expressions to calculate the actual memory offsets and sizes at<br />
runtime when the values of the dynamic shapes are known. In order to<br />
simplify these calculations, we have organized the allocations into a<br />
tree that branches on space (address offsets) and time (live ranges).<br />
Finally, we need to align these offsets, so we have added an <code>align</code><br />
sympy Expr to express these calculations.</p>
<p>Some limitations:</p>
<ol>
<li>It is only enabled during inference for now. Enabling it for training<br />
   increases peak memory usage as we allocate all the memory needed for<br />
   training upfront, before freeing the memory allocated during<br />
   inference. We can probably address this by doing planning for both<br />
   the inference and training passes together.</li>
<li>It doesn't work with PyTorch Distributed, because kernels like<br />
   AllGatherIntoTensor codegen strings which do memory operations. We<br />
   can fix this down the line by having them emit MemoryPlanningLines<br />
   instead.</li>
</ol>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112178<br />
Approved by: https://github.com/desertfire, https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 12:02:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f64a97c6f88873363c5b3c4c33f231b5578085b2</guid>
    </item>
    <item>
      <title>[inductor] FX graph cache: Add support for symbolic shapes (#111421)</title>
      <link>https://github.com/pytorch/pytorch/commit/6ae21e73d3b64afbe503e974d822ac9edb4ccbea</link>
      <description><![CDATA[<p>[inductor] FX graph cache: Add support for symbolic shapes (#111421)</p>
<p>Summary: Add support for caching graphs that have tensor args with symbolic shapes. The high-level appraoch is to serialize guards with the on-disk cached object and validating those guards pass before serving a cached object.</p>
<p>Test Plan: New unit tests</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111421<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 11:31:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6ae21e73d3b64afbe503e974d822ac9edb4ccbea</guid>
    </item>
    <item>
      <title>fix(inductor): `ForeachKernelSchedulerNode` group shape should be opaque for graph debug (#110336)</title>
      <link>https://github.com/pytorch/pytorch/commit/a21851c69dd50ebd1489b293ccb7e3e2db6fb0b3</link>
      <description><![CDATA[<p>fix(inductor): <code>ForeachKernelSchedulerNode</code> group shape should be opaque for graph debug (#110336)</p>
<p>~~Shape is assumed by <code>TensorMetadata</code> to be torch.Shape/tuple, however, some of the scheduler node groups utilize <code>int</code>, so convert to tuple.~~</p>
<p>Root cause is actually <code>foreach</code> scheduler node having silent-error group of int, when in fact it ought to be opaque <code>foreach</code>.</p>
<p><strong>Previously:</strong> silent error / confusing shape of (0,)<br />
<img alt="image" src="https://github.com/pytorch/pytorch/assets/9093549/5bc2a3c7-151f-4433-bbf8-044c7b03e989" /></p>
<p><strong>Now:</strong> clear that it is foreach which does not have well-defined shape:<br />
<img alt="image" src="https://github.com/pytorch/pytorch/assets/9093549/8373080d-4519-4e74-8a3b-da463e9968da" /></p>
<p>~~Alternate might be to create list of shapes for each of its subnodes. Actually, for debuggability sake, I may prefer this. We can ensure that the recursive generation of this string is only done dynamically in a debug code path. Else, incrementally computing it on initialization of ForeachKernel may also be feasible.~~ This is quite infeasible for 100s of params.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110336<br />
Approved by: https://github.com/mlazos</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 10:44:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a21851c69dd50ebd1489b293ccb7e3e2db6fb0b3</guid>
    </item>
    <item>
      <title>[aotinductor] reland: return a copy of any constant (#112370)</title>
      <link>https://github.com/pytorch/pytorch/commit/94f3df27e4ec05b7d95fff60543a153055ec2338</link>
      <description><![CDATA[<p>[aotinductor] reland: return a copy of any constant (#112370)</p>
<p>When the model returns a constant, we cannot "release" its handle,<br />
because the constant doesn't have any handle at all. Instead,<br />
we should allocate a new tensor and then return a copy of the constant.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112370<br />
Approved by: https://github.com/hl475, https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 10:36:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/94f3df27e4ec05b7d95fff60543a153055ec2338</guid>
    </item>
    <item>
      <title>[reland][inductor] benchmark fusion (#112450)</title>
      <link>https://github.com/pytorch/pytorch/commit/fbafff3668a8d7d401ff697a9e7688030c9310b6</link>
      <description><![CDATA[<p>[reland][inductor] benchmark fusion (#112450)</p>
<p>reland https://github.com/pytorch/pytorch/pull/108193</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112450<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 10:17:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fbafff3668a8d7d401ff697a9e7688030c9310b6</guid>
    </item>
    <item>
      <title>[inductor cpp] vectorize support for truediv (#112234)</title>
      <link>https://github.com/pytorch/pytorch/commit/a1c56df1f0c569e842bfcd365b417a907237330b</link>
      <description><![CDATA[<p>[inductor cpp] vectorize support for truediv (#112234)</p>
<p>Ops like group_norm has <code>ops.truediv</code> that doesn't have vectorization support yet. This PR adds the support.</p>
<p><code>test_group_norm_vec</code><br />
Before:<br />
<code>c++
extern "C" void kernel(const float* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       float* out_ptr2)
{
    #pragma omp parallel num_threads(64)
    {
        {
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(64L); x0+=static_cast&lt;long&gt;(1L))
            {
                {
                    #pragma omp declare reduction(welford:Welford&lt;float&gt;:omp_out = welford_combine(omp_out, omp_in)) initializer(omp_priv={Welford&lt;float&gt;()})
                    #pragma omp declare reduction(welford:Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;:omp_out = welford_combine(omp_out, omp_in)) initializer(omp_priv={Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;()})
                    Welford&lt;float&gt; tmp_acc0 = Welford&lt;float&gt;();
                    Welford&lt;at::vec::Vectorized&lt;float&gt;&gt; tmp_acc0_vec = Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;();
                    for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(1024L); x1+=static_cast&lt;long&gt;(16L))
                    {
                        auto tmp0 = at::vec::Vectorized&lt;float&gt;::loadu(in_ptr0 + static_cast&lt;long&gt;(x1 + (1024L*x0)));
                        tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp0);
                    }
                    tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                    out_ptr0[static_cast&lt;long&gt;(x0)] = static_cast&lt;float&gt;(tmp_acc0.mean);
                    out_ptr1[static_cast&lt;long&gt;(x0)] = static_cast&lt;float&gt;(tmp_acc0.m2);
                }
            }
        }
        {
            #pragma omp for  collapse(2)
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(2L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(32L); x1+=static_cast&lt;long&gt;(1L))
                {
                    #pragma GCC ivdep
                    for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(1024L); x2+=static_cast&lt;long&gt;(1L))
                    {
                        auto tmp0 = in_ptr0[static_cast&lt;long&gt;(x2 + (1024L*x1) + (32768L*x0))];
                        auto tmp1 = out_ptr0[static_cast&lt;long&gt;(x1 + (32L*x0))];
                        auto tmp3 = out_ptr1[static_cast&lt;long&gt;(x1 + (32L*x0))];
                        auto tmp10 = in_ptr1[static_cast&lt;long&gt;(x1)];
                        auto tmp12 = in_ptr2[static_cast&lt;long&gt;(x1)];
                        auto tmp2 = tmp0 - tmp1;
                        auto tmp4 = c10::convert&lt;float&gt;(1024.0);
                        auto tmp5 = tmp3 / tmp4;
                        auto tmp6 = c10::convert&lt;float&gt;(1e-05);
                        auto tmp7 = tmp5 + tmp6;
                        auto tmp8 = 1 / std::sqrt(tmp7);
                        auto tmp9 = decltype(tmp2)(tmp2 * tmp8);
                        auto tmp11 = decltype(tmp9)(tmp9 * tmp10);
                        auto tmp13 = tmp11 + tmp12;
                        out_ptr2[static_cast&lt;long&gt;(x2 + (1024L*x1) + (32768L*x0))] = tmp13;
                    }
                }
            }
        }
    }
}</code></p>
<p>After:<br />
<code>c++
extern "C" void kernel(const float* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       float* out_ptr2)
{
    #pragma omp parallel num_threads(64)
    {
        {
            #pragma omp for
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(64L); x0+=static_cast&lt;long&gt;(1L))
            {
                {
                    #pragma omp declare reduction(welford:Welford&lt;float&gt;:omp_out = welford_combine(omp_out, omp_in)) initializer(omp_priv={Welford&lt;float&gt;()})
                    #pragma omp declare reduction(welford:Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;:omp_out = welford_combine(omp_out, omp_in)) initializer(omp_priv={Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;()})
                    Welford&lt;float&gt; tmp_acc0 = Welford&lt;float&gt;();
                    Welford&lt;at::vec::Vectorized&lt;float&gt;&gt; tmp_acc0_vec = Welford&lt;at::vec::Vectorized&lt;float&gt;&gt;();
                    for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(1024L); x1+=static_cast&lt;long&gt;(16L))
                    {
                        auto tmp0 = at::vec::Vectorized&lt;float&gt;::loadu(in_ptr0 + static_cast&lt;long&gt;(x1 + (1024L*x0)));
                        tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp0);
                    }
                    tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                    out_ptr0[static_cast&lt;long&gt;(x0)] = static_cast&lt;float&gt;(tmp_acc0.mean);
                    out_ptr1[static_cast&lt;long&gt;(x0)] = static_cast&lt;float&gt;(tmp_acc0.m2);
                }
            }
        }
        {
            #pragma omp for  collapse(2)
            for(long x0=static_cast&lt;long&gt;(0L); x0&lt;static_cast&lt;long&gt;(2L); x0+=static_cast&lt;long&gt;(1L))
            {
                for(long x1=static_cast&lt;long&gt;(0L); x1&lt;static_cast&lt;long&gt;(32L); x1+=static_cast&lt;long&gt;(1L))
                {
                    for(long x2=static_cast&lt;long&gt;(0L); x2&lt;static_cast&lt;long&gt;(1024L); x2+=static_cast&lt;long&gt;(16L))
                    {
                        auto tmp0 = at::vec::Vectorized&lt;float&gt;::loadu(in_ptr0 + static_cast&lt;long&gt;(x2 + (1024L*x1) + (32768L*x0)));
                        auto tmp1 = at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(out_ptr0[static_cast&lt;long&gt;(x1 + (32L*x0))]));
                        auto tmp3 = at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(out_ptr1[static_cast&lt;long&gt;(x1 + (32L*x0))]));
                        auto tmp10 = at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(in_ptr1[static_cast&lt;long&gt;(x1)]));
                        auto tmp12 = at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(in_ptr2[static_cast&lt;long&gt;(x1)]));
                        auto tmp2 = tmp0 - tmp1;
                        auto tmp4 = at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(1024.0));
                        auto tmp5 = tmp3 / tmp4;
                        auto tmp6 = at::vec::Vectorized&lt;float&gt;(static_cast&lt;float&gt;(1e-05));
                        auto tmp7 = tmp5 + tmp6;
                        auto tmp8 = tmp7.rsqrt();
                        auto tmp9 = tmp2 * tmp8;
                        auto tmp11 = tmp9 * tmp10;
                        auto tmp13 = tmp11 + tmp12;
                        tmp13.store(out_ptr2 + static_cast&lt;long&gt;(x2 + (1024L*x1) + (32768L*x0)));
                    }
                }
            }
        }
    }
}</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112234<br />
Approved by: https://github.com/lezcano, https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 31 Oct 2023 09:15:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a1c56df1f0c569e842bfcd365b417a907237330b</guid>
    </item>
    <item>
      <title>[Inductor] Extend Pattern Matcher to Match Equivalent Function Invocation (#107832)</title>
      <link>https://github.com/pytorch/pytorch/commit/710337244d1fa1b5e92c4c4c4e06f1607ac1b8ac</link>
      <description><![CDATA[<p>[Inductor] Extend Pattern Matcher to Match Equivalent Function Invocation (#107832)</p>
<p>Fixes #104391</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107832<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 30 Oct 2023 19:32:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/710337244d1fa1b5e92c4c4c4e06f1607ac1b8ac</guid>
    </item>
    <item>
      <title>inductor cpp wrapper: add GIL release and acquire (#111888)</title>
      <link>https://github.com/pytorch/pytorch/commit/f50ec341bc4921cebac331510dde33d9f1ee18fc</link>
      <description><![CDATA[<p>inductor cpp wrapper: add GIL release and acquire (#111888)</p>
<p>Support multiple instances inference (in different threads of the same process) as in https://github.com/pytorch/pytorch/issues/93524#issuecomment-1421816158.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111888<br />
Approved by: https://github.com/jgong5, https://github.com/jansel, https://github.com/desertfire</p>]]></description>
      <pubDate>Mon, 30 Oct 2023 19:23:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f50ec341bc4921cebac331510dde33d9f1ee18fc</guid>
    </item>
    <item>
      <title>Fix regression from pointwise + multi-level reduction fusion (#112297)</title>
      <link>https://github.com/pytorch/pytorch/commit/c3113514e91635e2cdb3fe26171a023897eb390d</link>
      <description><![CDATA[<p>Fix regression from pointwise + multi-level reduction fusion (#112297)</p>
<p>In https://github.com/pytorch/pytorch/pull/111122, an optimization is introduced for reduction + pointwise + multi-level reduction fusion. The main idea of this optimization is to have the first-level reduction of the multi-level reduction reuses the reduction sizes of the first reduction kernel so that there are better chances that the first reduction kernel and the first-level reduction of the multi-level reduction kernel can be fused. However, it introduces a bug for pattern pointwise + multi-level reduction, where the first-level reduction kernel wrongly reuses the reduction ranges (which is []) from the previous pointwise kernel. This PR fixes this issue.</p>
<p>Test plan:<br />
<code>python timm_models.py --training --amp --performance --only=dm_nfnet_f0 --inductor</code><br />
Results before this PR: 0.869x<br />
Results after this PR: 1.232x</p>
<p>Benchmark results:<br />
<img alt="Screenshot 2023-10-30 at 2 30 10 PM" src="https://github.com/pytorch/pytorch/assets/10527447/c7b241c0-92a4-49ff-96fb-2805c8fcc45a" /></p>
<p><img width="1491" alt="Screenshot 2023-10-30 at 3 10 06 PM" src="https://github.com/pytorch/pytorch/assets/10527447/608d26ea-dcc5-4f2a-8700-4a928701392b"></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112297<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 30 Oct 2023 17:47:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c3113514e91635e2cdb3fe26171a023897eb390d</guid>
    </item>
    <item>
      <title>[inductor] Fix bug handling output_strides in fx graph cache (#112041)</title>
      <link>https://github.com/pytorch/pytorch/commit/12a9e09200151adc772e0fa03f081f24b13528c7</link>
      <description><![CDATA[<p>[inductor] Fix bug handling output_strides in fx graph cache (#112041)</p>
<p>Summary: The current implementation is not properly attaching output strides to the tracing context when an fx graph is loaded from the cache. That bugs leads to assertion failures like <code>AssertionError: expected size 3==3, stride 1==9 at dim=1</code>. This change saves the output strides in the serialized object cached on disk and inserts them into the tracing context whether the graph is loaded from cache or compiled.</p>
<p>Test Plan:<br />
* New unit test using resnet18 (which repros the problem)<br />
* Ran the timm benchmark suite with <code>--training</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112041<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Mon, 30 Oct 2023 15:49:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/12a9e09200151adc772e0fa03f081f24b13528c7</guid>
    </item>
    <item>
      <title>torch.compile: fix bug of fallback_randn when 'generator' is None (#112240)</title>
      <link>https://github.com/pytorch/pytorch/commit/67638d4dadc120ed48a3f7d70cce05ca0483a0fc</link>
      <description><![CDATA[<p>torch.compile: fix bug of fallback_randn when 'generator' is None (#112240)</p>
<p>When I run Stable Diffusion in <a href="https://github.com/huggingface/diffusers">Huggingface/Diffusers</a>，an error occured:<br />
<code>LoweringException: AssertionError: should have been handled in replace_random.py.
   target:  aten.randn.generator
   args[0]:  [1, 4, 64, 64]
   kwargs: {'generator': None, 'dtype': torch.float16, 'layout': torch.strided, 'device': device(type='cuda', index=0), 'pin_memory': False}</code><br />
It looks like some bug of dynamo, and you can reproduce this bug like this:<br />
<code>python
import torch
def model(shape, generator):
      return torch.randn(shape, generator=generator, device="cuda:0")
model = torch.compile(model)
x = model((1, 3, 64, 64), None)
print(x)</code><br />
Error occurs because 'None' is passed into ‘generator' ,  and dynamo has to process <code>torch.randn</code> into fx node <code>torch.ops.aten.randn.generator</code>.<br />
aten.randn.generator is not processed by decomposition and  it is processed by lowering in <a href="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/lowering.py#L1815">torch/_inductor/lowering.py</a>, randn.generator is processed like this:<br />
<code>python
@register_lowering(aten.randn)
def randn(*args, **kwargs):
    if kwargs.get("generator", None) is not None:
        return fallback_randn_generator(*args, **kwargs)
    elif config.fallback_random:
        return fallback_randn_default(*args, **kwargs)
    raise AssertionError("should have been handled in replace_random.py")</code><br />
As you can see, because 'generator' is None, it will not step into <code>fallback_randn_generator</code>, and of course, if you don't open <code>config.fallback_random</code>, it will not step into <code>fallback_randn_default</code>, too. Actually, if 'generator' is None, it could also be processed as<code>aten.randn.default</code>.  And then, AssertionError will be throw, but in here, I will not disscuss too much about how to process this bug and will open an issue.</p>
<p>Actually, <code>config.fallback_random</code> offers a way to debug randn in <a href="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/config.py#L190">config.py</a>, so I try to open <code>config.fallback_random</code> to debug my model. But when I open it by:<br />
```python</p>
<h1>fallback to eager for random/dropout, this is slow but useful for debugging</h1>
<p>fallback_random = True<br />
<code>Another error occurs!</code>python<br />
LoweringException: RuntimeError: Unknown keyword argument 'generator' for operator 'aten::randn'. Schema: aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layouit? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor<br />
<code>``
Obviously,</code>aten::randn<code>does not support</code>kwargs:{generator: None}<code>, so it should be popped before kwargs is feeded into</code>fallback_randn_default`.</p>
<p>That's all I'm going to say. Thanks for reading carefully.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112240<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 30 Oct 2023 13:10:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/67638d4dadc120ed48a3f7d70cce05ca0483a0fc</guid>
    </item>
    <item>
      <title>Revert "[inductor] Fix bug handling output_strides in fx graph cache (#112041)"</title>
      <link>https://github.com/pytorch/pytorch/commit/d713b8dd5d20e00a21a192f0fc1d0e4e61edcdda</link>
      <description><![CDATA[<p>Revert "[inductor] Fix bug handling output_strides in fx graph cache (#112041)"</p>
<p>This reverts commit 3d2041b34210bef3902f6ba86881b38ac0fbc57e.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/112041 on behalf of https://github.com/ZainRizvi due to fbcode failures (<a href="https://github.com/pytorch/pytorch/pull/112041#issuecomment-1785929233">comment</a>)</p>]]></description>
      <pubDate>Mon, 30 Oct 2023 11:50:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d713b8dd5d20e00a21a192f0fc1d0e4e61edcdda</guid>
    </item>
    <item>
      <title>[Inductor] Add triton.autotune support for user defined triton kernels with complex grids (#112290)</title>
      <link>https://github.com/pytorch/pytorch/commit/1250032c2eb0cb55bcd376b499982bcc9ab231f8</link>
      <description><![CDATA[<p>[Inductor] Add triton.autotune support for user defined triton kernels with complex grids (#112290)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112290<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 30 Oct 2023 09:48:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1250032c2eb0cb55bcd376b499982bcc9ab231f8</guid>
    </item>
    <item>
      <title>[inductor][fx pass] Add new split cat pattern detection (#110923)</title>
      <link>https://github.com/pytorch/pytorch/commit/5a1a9dc354e4098a96cb03adce3b3460809ca23a</link>
      <description><![CDATA[<p>[inductor][fx pass] Add new split cat pattern detection (#110923)</p>
<p>Summary: We add a new pattern to merge getitem_cat to enable further split merges</p>
<p>Test Plan:</p>
<h3>test mcf model</h3>
<p>Patch D49972740<br />
<code>buck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split-only -c</code></p>
<p>P850153017</p>
<h3>unit test</h3>
<p><code>buck2 test mode/dev-nosan //caffe2/test/inductor:split_cat_fx_passes -- test_getitem_cat_merge</code><br />
Buck UI: https://www.internalfb.com/buck2/eb7411a5-a6bd-46bc-bf66-756341e3ce10<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/13792273864439068<br />
Network: Up: 48KiB  Down: 15KiB  (reSessionID-39ca57cc-5743-423e-b94f-9d0f642010f8)<br />
Jobs completed: 8. Time elapsed: 1:44.7s.<br />
Cache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)<br />
Tests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<h3>before vs after transformation</h3>
<p>https://www.internalfb.com/intern/diffing/?paste_number=847958889</p>
<p>Differential Revision: D50100667</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110923<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Mon, 30 Oct 2023 09:46:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5a1a9dc354e4098a96cb03adce3b3460809ca23a</guid>
    </item>
    <item>
      <title>[Inductor] Add triton.autotune support for user defined triton kernels with constant/simple grids (#112228)</title>
      <link>https://github.com/pytorch/pytorch/commit/c14c4efc0eb97ccc471ac6cbdcc659417227ae97</link>
      <description><![CDATA[<p>[Inductor] Add triton.autotune support for user defined triton kernels with constant/simple grids (#112228)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112228<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Sat, 28 Oct 2023 09:30:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c14c4efc0eb97ccc471ac6cbdcc659417227ae97</guid>
    </item>
    <item>
      <title>Revert "[Inductor] Add triton.autotune support for user defined triton kernels with constant/simple grids (#112228)"</title>
      <link>https://github.com/pytorch/pytorch/commit/8d44999183e564e849519215f423a8ea5c4918ea</link>
      <description><![CDATA[<p>Revert "[Inductor] Add triton.autotune support for user defined triton kernels with constant/simple grids (#112228)"</p>
<p>This reverts commit dbb31a2984fa616b4bb6fac7abb2a06ec0533eb1.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/112228 on behalf of https://github.com/huydhn due to Sorry for reverting your change but it is failing ROCm test in trunk https://hud.pytorch.org/pytorch/pytorch/commit/dbb31a2984fa616b4bb6fac7abb2a06ec0533eb1 (<a href="https://github.com/pytorch/pytorch/pull/112228#issuecomment-1783660326">comment</a>)</p>]]></description>
      <pubDate>Fri, 27 Oct 2023 17:51:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8d44999183e564e849519215f423a8ea5c4918ea</guid>
    </item>
    <item>
      <title>[inductor] Fix bug handling output_strides in fx graph cache (#112041)</title>
      <link>https://github.com/pytorch/pytorch/commit/3d2041b34210bef3902f6ba86881b38ac0fbc57e</link>
      <description><![CDATA[<p>[inductor] Fix bug handling output_strides in fx graph cache (#112041)</p>
<p>Summary: The current implementation is not properly attaching output strides to the tracing context when an fx graph is loaded from the cache. That bugs leads to assertion failures like <code>AssertionError: expected size 3==3, stride 1==9 at dim=1</code>. This change saves the output strides in the serialized object cached on disk and inserts them into the tracing context whether the graph is loaded from cache or compiled.</p>
<p>Test Plan:<br />
* New unit test using resnet18 (which repros the problem)<br />
* Ran the timm benchmark suite with <code>--training</code></p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50756653">D50756653</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/112041<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Fri, 27 Oct 2023 14:30:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3d2041b34210bef3902f6ba86881b38ac0fbc57e</guid>
    </item>
    <item>
      <title>[Inductor] Add triton.autotune support for user defined triton kernels with constant/simple grids (#112228)</title>
      <link>https://github.com/pytorch/pytorch/commit/dbb31a2984fa616b4bb6fac7abb2a06ec0533eb1</link>
      <description><![CDATA[<p>[Inductor] Add triton.autotune support for user defined triton kernels with constant/simple grids (#112228)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112228<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 27 Oct 2023 13:40:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dbb31a2984fa616b4bb6fac7abb2a06ec0533eb1</guid>
    </item>
    <item>
      <title>[AOTInductor] Enforce no_grad for Run entries (#111613)</title>
      <link>https://github.com/pytorch/pytorch/commit/7265c22a5d6a0d0649d06407c2bd1e86ef5c7a9e</link>
      <description><![CDATA[<p>[AOTInductor] Enforce no_grad for Run entries (#111613)</p>
<p>Summary:<br />
Always enter no_grad mode in AOTInductor run entries.</p>
<p><code>// AOTInductor uses at::addmm_out, which doesn't supports
// arguments that requires gradient. For this reason, we
// enforce no_grad context for run APIs.</code></p>
<p>Test Plan:<br />
buck2 test mode/dev-nosan caffe2/test/inductor:test_aot_inductor</p>
<p>and OSS CI</p>
<p>Differential Revision: D50432042</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111613<br />
Approved by: https://github.com/chenyang78, https://github.com/khabinov</p>]]></description>
      <pubDate>Fri, 27 Oct 2023 01:14:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7265c22a5d6a0d0649d06407c2bd1e86ef5c7a9e</guid>
    </item>
    <item>
      <title>Revert "[inductor] benchmark fusion (#108193)"</title>
      <link>https://github.com/pytorch/pytorch/commit/64fd027f2e0ee743235f9339f97b3a9224527cae</link>
      <description><![CDATA[<p>Revert "[inductor] benchmark fusion (#108193)"</p>
<p>This reverts commit 73cc5d1cdda118007ccdb0be8d775ba76726596e.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/108193 on behalf of https://github.com/izaitsevfb due to Trying to unblock the revert of #108690, please rebase and reland. (<a href="https://github.com/pytorch/pytorch/pull/108193#issuecomment-1782157638">comment</a>)</p>]]></description>
      <pubDate>Thu, 26 Oct 2023 17:40:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/64fd027f2e0ee743235f9339f97b3a9224527cae</guid>
    </item>
    <item>
      <title>Avoid c++ exception and stack trace (#111438)</title>
      <link>https://github.com/pytorch/pytorch/commit/5b7183478508f03bccc0a662614d77ebaf5fedf7</link>
      <description><![CDATA[<p>Avoid c++ exception and stack trace (#111438)</p>
<p>Summary:<br />
When raising an exception here this causes pybind11's dispatcher to kick in, which causes aiplatform's logic to kick in (aiplatform::error_reporting::util::printAddressesWithBestEffortLocationInfo), which ultimately uses <code>folly::symbolizer::Symbolizer::symbolize</code> for building up the stack trace.  In 3.8 this uses about 3.62% of the CPU time per pyperf (https://fburl.com/scuba/pyperf_experimental/on_demand/oi554uvy).  In Cinder 3.8 for some reason this is worse - using 5.94% of the CPU.</p>
<p>This exception is happening when doing a hasattr() on <code>prims</code> for things like <code>bitwise_left_shift</code> which don't exist: https://www.internalfb.com/code/fbsource/[2d695f650d00]/fbcode/caffe2/torch/_inductor/lowering.py?lines=590</p>
<p>That exception is ultimately going to be swallowed anyway, and the stack trace has no meaningful value.  Furthermore because this is kind of an expected outcome in the code versus some random C++ exception the stack trace is less valuable as well.</p>
<p>This changes this to return a (None, None) on the failure case instead of returning a valid op/overload list, avoiding the exception, and reclaiming the 3.62%-5.94% of time.</p>
<p>Test Plan: Existing CI and perf run: https://fburl.com/scuba/pyperf_experimental/on_demand/oi554uvy</p>
<p>Differential Revision: D50018789</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111438<br />
Approved by: https://github.com/davidberard98</p>]]></description>
      <pubDate>Thu, 26 Oct 2023 15:55:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5b7183478508f03bccc0a662614d77ebaf5fedf7</guid>
    </item>
    <item>
      <title>[inductor] benchmark fusion (#108193)</title>
      <link>https://github.com/pytorch/pytorch/commit/73cc5d1cdda118007ccdb0be8d775ba76726596e</link>
      <description><![CDATA[<p>[inductor] benchmark fusion (#108193)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108193<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 26 Oct 2023 14:18:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/73cc5d1cdda118007ccdb0be8d775ba76726596e</guid>
    </item>
    <item>
      <title>[aotinductor] Add a debug compile flag (#112021)</title>
      <link>https://github.com/pytorch/pytorch/commit/73f36e44fbb2298cfb537362646d72788f71f510</link>
      <description><![CDATA[<p>[aotinductor] Add a debug compile flag (#112021)</p>
<p>Summary: When the debug compile flag is specified, model.so is compiled with "-O0 -g".</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112021<br />
Approved by: https://github.com/chenyang78<br />
ghstack dependencies: #111823</p>]]></description>
      <pubDate>Thu, 26 Oct 2023 13:11:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/73f36e44fbb2298cfb537362646d72788f71f510</guid>
    </item>
    <item>
      <title>[aotinductor] Fix duplicated unbacked symbol declarations (#111823)</title>
      <link>https://github.com/pytorch/pytorch/commit/f66cc675629fead56304205c24b712adba1401a3</link>
      <description><![CDATA[<p>[aotinductor] Fix duplicated unbacked symbol declarations (#111823)</p>
<p>Summary: For https://github.com/pytorch/pytorch/issues/111711</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111823<br />
Approved by: https://github.com/ezyang, https://github.com/aakhundov</p>]]></description>
      <pubDate>Thu, 26 Oct 2023 13:11:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f66cc675629fead56304205c24b712adba1401a3</guid>
    </item>
    <item>
      <title>[aotinductor] only include -lc10 for non-fbcode case (#112125)</title>
      <link>https://github.com/pytorch/pytorch/commit/5e5329155e0aea98418923cf8046fa3cdbde81cc</link>
      <description><![CDATA[<p>[aotinductor] only include -lc10 for non-fbcode case (#112125)</p>
<p>Summary: otherwise, we would break internal uses</p>
<p>Differential Revision: D50681467</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112125<br />
Approved by: https://github.com/swolchok, https://github.com/desertfire, https://github.com/SherlockNoMad</p>]]></description>
      <pubDate>Thu, 26 Oct 2023 11:47:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5e5329155e0aea98418923cf8046fa3cdbde81cc</guid>
    </item>
    <item>
      <title>Revert "[inductor] benchmark fusion (#108193)"</title>
      <link>https://github.com/pytorch/pytorch/commit/485cc0faaef207736b988045df06e197852e4933</link>
      <description><![CDATA[<p>Revert "[inductor] benchmark fusion (#108193)"</p>
<p>This reverts commit ec0cdcdf6a816eadb4d868284eea86732f50da2e.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/108193 on behalf of https://github.com/ZainRizvi due to This test is breaking trunk. In the future please make sure to add the ciflow/trunk label before force merging any PR to ensure your code doesn't break those tests (<a href="https://github.com/pytorch/pytorch/pull/108193#issuecomment-1781473282">comment</a>)</p>]]></description>
      <pubDate>Thu, 26 Oct 2023 08:41:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/485cc0faaef207736b988045df06e197852e4933</guid>
    </item>
    <item>
      <title>[aotinductor] Pass TorchIR to AOTInductor (#110020)</title>
      <link>https://github.com/pytorch/pytorch/commit/b126adcdeedb92e1b4d6edb25f982b616f1ac9ce</link>
      <description><![CDATA[<p>[aotinductor] Pass TorchIR to AOTInductor (#110020)</p>
<p>Updates <code>_export.aot_compile</code> to pass a torch IR graph to inductor, allowing inductor to now run the pre_grad_passes, and reuse more of inductor's code.<br />
Also updates the API to only return the <code>so_path</code>, and not returning the exported program. The pytree call spec is now serialized and placed inside of the generated model code. When calling the model, because there is no c++ pytree implementation linked yet, we can access the call specs through <code>get_call_spec()</code>, and call pytree flatten/unflattenin python.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110020<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 26 Oct 2023 07:54:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b126adcdeedb92e1b4d6edb25f982b616f1ac9ce</guid>
    </item>
    <item>
      <title>[aotinductor] allow generating default args in fbcode (#112085)</title>
      <link>https://github.com/pytorch/pytorch/commit/7671be8108b63d0b6225692fd805651440548e96</link>
      <description><![CDATA[<p>[aotinductor] allow generating default args in fbcode (#112085)</p>
<p>Summary:<br />
Previously, we want to maintain forward-compatibility by skipping<br />
default args in the serialized artifacts in fbcode. However, some of our shim<br />
interfaces require default values being set. Discussed with Sherlock offline<br />
and we decided to allow serializing default args into the C++ wrapper code<br />
for now. We will refine this part if we see real FC requirement.</p>
<p>Test Plan: ci</p>
<p>Differential Revision: D50638663</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112085<br />
Approved by: https://github.com/SherlockNoMad</p>]]></description>
      <pubDate>Thu, 26 Oct 2023 06:17:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7671be8108b63d0b6225692fd805651440548e96</guid>
    </item>
    <item>
      <title>Add regex matching to Inductor all2all collective unit tests (#112077)</title>
      <link>https://github.com/pytorch/pytorch/commit/fbff99ffea908c4e6b4c3da8f92962febaaaf2a7</link>
      <description><![CDATA[<p>Add regex matching to Inductor all2all collective unit tests (#112077)</p>
<p>Fixes #111776</p>
<p>Support check_regex in FileCheck() by adding <code>find_regex</code> in <code>struct TORCH_API StringCordView</code>.<br />
Callsite accepts RE syntax for std::regex.</p>
<p>However, I haven't figured out submatch ID yet.<br />
For example, "buf5[0], buf6_inputs[0]" is still considered a match.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/112077<br />
Approved by: https://github.com/yf225</p>]]></description>
      <pubDate>Thu, 26 Oct 2023 00:29:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fbff99ffea908c4e6b4c3da8f92962febaaaf2a7</guid>
    </item>
    <item>
      <title>[dynamo] Enable typechecking for allowed_functions.py (#111894)</title>
      <link>https://github.com/pytorch/pytorch/commit/e67d2c9825672e316b755a31f5a7d212c5c2b860</link>
      <description><![CDATA[<p>[dynamo] Enable typechecking for allowed_functions.py (#111894)</p>
<p>Motivation: MYPYNOFOLLOW currently typechecks almost all inductor files<br />
and some dynamo files as well. However, it has <code>follow_imports=skip</code><br />
enabled which greatly nerfs its effectiveness. I would like to enable<br />
import following for all the files currently checked by MYPYNOFOLLOW.<br />
But that leads to a lot of new errors in other files.</p>
<p>I can exclude errors from files in other directories, but it is somewhat<br />
difficult to do that for dynamo and inductor files themselves. Thus I am<br />
making sure all the dynamo files typecheck first.</p>
<p>Note on changes: I could not type the return value of<br />
<code>make_function_id_set</code> since it was returning a class defined in the<br />
function body. Thus I deleted <code>make_function_id_set</code> and replaced it<br />
with a direct construction of the <code>FunctionIdSet</code> instead.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111894<br />
Approved by: https://github.com/Skylion007, https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 25 Oct 2023 20:54:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e67d2c9825672e316b755a31f5a7d212c5c2b860</guid>
    </item>
    <item>
      <title>[inductor] benchmark fusion (#108193)</title>
      <link>https://github.com/pytorch/pytorch/commit/ec0cdcdf6a816eadb4d868284eea86732f50da2e</link>
      <description><![CDATA[<p>[inductor] benchmark fusion (#108193)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108193<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 25 Oct 2023 20:14:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ec0cdcdf6a816eadb4d868284eea86732f50da2e</guid>
    </item>
    <item>
      <title>[inductor] Remove redundant views (#111773)</title>
      <link>https://github.com/pytorch/pytorch/commit/7a3a00bb0b4fb2d068dc258cc1277f7992d71000</link>
      <description><![CDATA[<p>[inductor] Remove redundant views (#111773)</p>
<p>As a follow-up to https://github.com/pytorch/pytorch/pull/110740, this patches enables removing redundant complex views to allow more operation fusing.</p>
<p>E.g,  given</p>
<p><code>@torch.compile
def foo(X, Y):
    Z = X + Y
    A = X + Y
    return A + Z</code></p>
<p>the generated code is:</p>
<p>```<br />
@triton.jit<br />
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):<br />
    xnumel = 6<br />
    xoffset = tl.program_id(0) * XBLOCK<br />
    xindex = xoffset + tl.arange(0, XBLOCK)[:]<br />
    xmask = xindex &lt; xnumel<br />
    x0 = xindex<br />
    tmp0 = tl.load(in_ptr0 + (x0), xmask)<br />
    tmp1 = tl.load(in_ptr1 + (x0), xmask)<br />
    tmp2 = tmp0 + tmp1<br />
    tmp3 = tmp2 + tmp2<br />
    tl.store(out_ptr0 + (x0), tmp3, xmask)<br />
''')</p>
<p>def call(args):<br />
    arg0_1, arg1_1 = args<br />
    args.clear()<br />
    assert_size_stride(arg0_1, (3, ), (1, ))<br />
    assert_size_stride(arg1_1, (3, ), (1, ))<br />
    with torch.cuda._DeviceGuard(0):<br />
        torch.cuda.set_device(0) # no-op to ensure context<br />
        # Source Nodes: [A], Original ATen: [aten.add]<br />
        buf0 = aten.view.dtype(arg0_1, torch.float32)<br />
        del arg0_1<br />
        buf1 = buf0<br />
        del buf0<br />
        # Source Nodes: [A], Original ATen: [aten.add]<br />
        buf2 = aten.view.dtype(arg1_1, torch.float32)<br />
        del arg1_1<br />
        buf3 = buf2<br />
        del buf2<br />
        buf4 = empty_strided((6, ), (1, ), device='cuda', dtype=torch.float32)<br />
        # Source Nodes: [add_2], Original ATen: [aten.add]<br />
        stream0 = get_cuda_stream(0)<br />
        triton_poi_fused_add_0.run(buf1, buf3, buf4, 6, grid=grid(6), stream=stream0)<br />
        del buf1<br />
        del buf3<br />
        # Source Nodes: [add_2], Original ATen: [aten.add]<br />
        buf5 = aten.view.dtype(buf4, torch.complex64)<br />
        del buf4<br />
        buf6 = buf5<br />
        del buf5<br />
        return (buf6, )<br />
```</p>
<p>whereas previously the generated code was:</p>
<p>```<br />
@triton.jit<br />
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):<br />
    xnumel = 6<br />
    xoffset = tl.program_id(0) * XBLOCK<br />
    xindex = xoffset + tl.arange(0, XBLOCK)[:]<br />
    xmask = xindex &lt; xnumel<br />
    x0 = xindex<br />
    tmp0 = tl.load(in_ptr0 + (x0), xmask)<br />
    tmp1 = tl.load(in_ptr1 + (x0), xmask)<br />
    tmp2 = tmp0 + tmp1<br />
    tl.store(out_ptr0 + (x0), tmp2, xmask)</p>
<p>def call(args):<br />
    arg0_1, arg1_1 = args<br />
    args.clear()<br />
    assert_size_stride(arg0_1, (3, ), (1, ))<br />
    assert_size_stride(arg1_1, (3, ), (1, ))<br />
    with torch.cuda._DeviceGuard(0):<br />
        torch.cuda.set_device(0) # no-op to ensure context<br />
        # Source Nodes: [A], Original ATen: [aten.add]<br />
        buf0 = aten.view.dtype(arg0_1, torch.float32)<br />
        buf1 = buf0<br />
        del buf0<br />
        # Source Nodes: [A], Original ATen: [aten.add]<br />
        buf2 = aten.view.dtype(arg1_1, torch.float32)<br />
        buf3 = buf2<br />
        del buf2<br />
        buf4 = empty_strided((6, ), (1, ), device='cuda', dtype=torch.float32)<br />
        # Source Nodes: [A], Original ATen: [aten.add]<br />
        stream0 = get_cuda_stream(0)<br />
        triton_poi_fused_add_0.run(buf1, buf3, buf4, 6, grid=grid(6), stream=stream0)<br />
        del buf1<br />
        del buf3<br />
        # Source Nodes: [A], Original ATen: [aten.add]<br />
        buf5 = aten.view.dtype(buf4, torch.complex64)<br />
        buf6 = buf5<br />
        del buf5<br />
        # Source Nodes: [add_2], Original ATen: [aten.add]<br />
        buf7 = aten.view.dtype(buf6, torch.float32)<br />
        del buf6<br />
        buf8 = buf7<br />
        del buf7<br />
        # Source Nodes: [Z], Original ATen: [aten.add]<br />
        buf9 = aten.view.dtype(arg0_1, torch.float32)<br />
        del arg0_1<br />
        buf10 = buf9<br />
        del buf9<br />
        # Source Nodes: [Z], Original ATen: [aten.add]<br />
        buf11 = aten.view.dtype(arg1_1, torch.float32)<br />
        del arg1_1<br />
        buf12 = buf11<br />
        del buf11<br />
        buf13 = buf4; del buf4  # reuse<br />
        # Source Nodes: [Z], Original ATen: [aten.add]<br />
        triton_poi_fused_add_0.run(buf10, buf12, buf13, 6, grid=grid(6), stream=stream0)<br />
        del buf10<br />
        del buf12<br />
        # Source Nodes: [Z], Original ATen: [aten.add]<br />
        buf14 = aten.view.dtype(buf13, torch.complex64)<br />
        buf15 = buf14<br />
        del buf14<br />
        # Source Nodes: [add_2], Original ATen: [aten.add]<br />
        buf16 = aten.view.dtype(buf15, torch.float32)<br />
        del buf15<br />
        buf17 = buf16<br />
        del buf16<br />
        buf18 = buf13; del buf13  # reuse<br />
        # Source Nodes: [add_2], Original ATen: [aten.add]<br />
        triton_poi_fused_add_0.run(buf8, buf17, buf18, 6, grid=grid(6), stream=stream0)<br />
        del buf17<br />
        del buf8<br />
        # Source Nodes: [add_2], Original ATen: [aten.add]<br />
        buf19 = aten.view.dtype(buf18, torch.complex64)<br />
        del buf18<br />
        buf20 = buf19<br />
        del buf19<br />
        return (buf20, )<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111773<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 25 Oct 2023 18:37:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7a3a00bb0b4fb2d068dc258cc1277f7992d71000</guid>
    </item>
    <item>
      <title>Revert "[cpu][inductor] improve cpu vec implementations of log (#111898)"</title>
      <link>https://github.com/pytorch/pytorch/commit/d641450180f483e772d51dce57b1f050c19b10df</link>
      <description><![CDATA[<p>Revert "[cpu][inductor] improve cpu vec implementations of log (#111898)"</p>
<p>This reverts commit b5703203647644176220676af0e8e5f23de8d45a.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/111898 on behalf of https://github.com/facebook-github-bot due to Diff reverted internally (<a href="https://github.com/pytorch/pytorch/pull/111898#issuecomment-1780263780">comment</a>)</p>]]></description>
      <pubDate>Wed, 25 Oct 2023 17:12:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d641450180f483e772d51dce57b1f050c19b10df</guid>
    </item>
    <item>
      <title>Native c10d_functional ops (#110570)</title>
      <link>https://github.com/pytorch/pytorch/commit/ec18ef62f44e68e154b939ed4f860a2bda69716d</link>
      <description><![CDATA[<p>Native c10d_functional ops (#110570)</p>
<p>This PR introduces a native version of c10d_functional ops. The main goal is to add collective support in AOTInductor and allow collective ops to work in multi-threaded native runtimes.</p>
<p>The native version also incorporated API improvements we wished to implement in Python c10d_functional:</p>
<ul>
<li>Removed <code>ranks</code> and <code>group_size</code> from collective op signatures which were proven to be redundant.</li>
<li>Use tensor storage as opposed to <code>void*</code> to resolve in-flight work.</li>
</ul>
<p>The native process group registration/resolution mechansim is only used for native c10d_functional in the PR. It will become the single source of truth in upcoming PRs.</p>
<p>The upcoming PRs will implement Inductor/AOTInductor support for c10d_functional, after which native c10d_functional will replace Python c10d_functional.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110570<br />
Approved by: https://github.com/wanchaol</p>]]></description>
      <pubDate>Wed, 25 Oct 2023 14:56:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ec18ef62f44e68e154b939ed4f860a2bda69716d</guid>
    </item>
    <item>
      <title>[aotinductor] Turn clang warning ignored-optimization-argument into error (#112008)</title>
      <link>https://github.com/pytorch/pytorch/commit/6b7b90462ffb1378d26779d5277e8c284e1e0474</link>
      <description><![CDATA[<p>[aotinductor] Turn clang warning ignored-optimization-argument into error (#112008)</p>
<p>Now we compile the generated wrapper C++ code with clang in fbcode.<br />
When the Model's run_impl function is too large, clang will issue<br />
a warning like:</p>
<p>Function foo is too big to optimize [-Wignored-optimization-argument]</p>
<p>and compile the code without any optimization.</p>
<p>I think we may want to be more proactive in such cases. If the<br />
generated C++ code is too complex or too large to be optimized,<br />
we would like to be notified loudly with errors, so that we<br />
would figure out ways to address the issue.</p>
<p>Later if we feel that turning this warning into an error is too<br />
aggressive, we would add a config to disable it.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112008<br />
Approved by: https://github.com/desertfire, https://github.com/htyu</p>]]></description>
      <pubDate>Wed, 25 Oct 2023 11:14:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6b7b90462ffb1378d26779d5277e8c284e1e0474</guid>
    </item>
    <item>
      <title>Fix unit tests and add logging for Inductor intra-graph reordering (#111981)</title>
      <link>https://github.com/pytorch/pytorch/commit/e9804aaaccd57e546aa9de9a2a9483a6be7b7cde</link>
      <description><![CDATA[<p>Fix unit tests and add logging for Inductor intra-graph reordering (#111981)</p>
<ol>
<li>Fix code to make unit tests pass (incl. collect_env issue called out by @int3  in https://github.com/pytorch/pytorch/pull/108091#discussion_r1362901686).</li>
<li>Add logging for Inductor intra-graph reordering passes (<code>TORCH_LOGS="overlap"</code>), for easier debugging. Example log:<br />
```<br />
[rank0]:[2023-10-24 16:28:26,446] [0/0] torch._inductor.comms.__overlap: [DEBUG] ==== Visualize overlap before reordering pass <function reorder_compute_for_overlap at 0x7fa68c5568e0> ====<br />
[rank0]:[2023-10-24 16:28:26,446] [0/0] torch._inductor.comms.__overlap: [DEBUG] ComputedBuffer (size=[4, 4], stride=[4, 1]) (buf0)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] ExternKernelOut (extern_kernels.mm) (size=[4, 4], stride=[4, 1]) (buf1)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] InPlaceHint (size=[4, 4], stride=[4, 1]) (buf2)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] AllReduce (size=[4, 4], stride=[4, 1]) (buf3)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] Wait (size=[4, 4], stride=[4, 1]) (buf4)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] ComputedBuffer (size=[4, 4], stride=[4, 1]) (buf5)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] InPlaceHint (size=[4, 4], stride=[4, 1]) (buf6)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] AllReduce (size=[4, 4], stride=[4, 1]) (buf7)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] Wait (size=[4, 4], stride=[4, 1]) (buf8)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] ExternKernelOut (extern_kernels.mm) (size=[4, 4], stride=[4, 1]) (buf9)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] ComputedBuffer (size=[4, 4], stride=[4, 1]) (buf10)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] ExternKernelOut (extern_kernels.mm) (size=[4, 4], stride=[4, 1]) (buf11)<br />
[rank0]:[2023-10-24 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] Est. runtime (ms): 0.000228</li>
</ol>
<p>[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] ==== Visualize overlap after reordering pass <function reorder_compute_for_overlap at 0x7fa68c5568e0> ====<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] InPlaceHint (size=[4, 4], stride=[4, 1]) (buf2)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] AllReduce (size=[4, 4], stride=[4, 1]) (buf3)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] | ComputedBuffer (size=[4, 4], stride=[4, 1]) (buf0)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] | ExternKernelOut (extern_kernels.mm) (size=[4, 4], stride=[4, 1]) (buf1)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] | ExternKernelOut (extern_kernels.mm) (size=[4, 4], stride=[4, 1]) (buf9)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] Wait (size=[4, 4], stride=[4, 1]) (buf4)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] ComputedBuffer (size=[4, 4], stride=[4, 1]) (buf5)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] InPlaceHint (size=[4, 4], stride=[4, 1]) (buf6)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] AllReduce (size=[4, 4], stride=[4, 1]) (buf7)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] Wait (size=[4, 4], stride=[4, 1]) (buf8)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] ComputedBuffer (size=[4, 4], stride=[4, 1]) (buf10)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] ExternKernelOut (extern_kernels.mm) (size=[4, 4], stride=[4, 1]) (buf11)<br />
[rank0]:[2023-10-24 16:28:26,448] [0/0] torch._inductor.comms.__overlap: [DEBUG] Est. runtime (ms): 0.000217<br />
<code>``
The</code>| SomeComputeOp` means the compute op is overlapped with the comm op above.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111981<br />
Approved by: https://github.com/wanchaol</p>]]></description>
      <pubDate>Wed, 25 Oct 2023 10:19:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e9804aaaccd57e546aa9de9a2a9483a6be7b7cde</guid>
    </item>
    <item>
      <title>[pytorch][PR] [Inductor][FX passes] Pre grad batch relu fusion (#111146)</title>
      <link>https://github.com/pytorch/pytorch/commit/2b952834c743049f41fca336306986d1aadb4b97</link>
      <description><![CDATA[<p>[pytorch][PR] [Inductor][FX passes] Pre grad batch relu fusion (#111146)</p>
<p>Summary: We detect independent relu operators and do the fusion in the pre grad.</p>
<p>Test Plan:</p>
<h3>unit test</h3>
<p><code>buck2 test mode/dev-nosan //caffe2/test/inductor:group_batch_fusion</code><br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/16888498608558485</p>
<h3>Inlinve cvr</h3>
<p>f479655232<br />
<code>buck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split_batch_group</code><br />
before vs after transformation<br />
https://www.internalfb.com/intern/diffing/?paste_number=851907099</p>
<p><code>buck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split_batch_group -c</code></p>
<p>P852036786</p>
<p>Differential Revision: D50207610</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111146<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Wed, 25 Oct 2023 09:37:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2b952834c743049f41fca336306986d1aadb4b97</guid>
    </item>
    <item>
      <title>Make require_stride_order peek into AliasedLayout (#111681)</title>
      <link>https://github.com/pytorch/pytorch/commit/6fd36593910d9d8cdf05dd182a78993e76f12a80</link>
      <description><![CDATA[<p>Make require_stride_order peek into AliasedLayout (#111681)</p>
<p>Summary:</p>
<p><code>require_stride_order</code> doesn't know how to handle storage with <code>AliasedLayout</code>. It always resorts to a copy even when the view refers to a storage with <code>FixedLayout</code>. This causes an unneccessary allocation + copy for collective outputs. Peeking into <code>AliasedLayout</code> in <code>require_stride_order</code> seems to be the proper way to address the issue.</p>
<p>Original program:<br />
```python<br />
import tempfile</p>
<p>import torch<br />
import torch.distributed as dist<br />
from torch.distributed._functional_collectives import *  # noqa<br />
from torch._inductor.utils import run_and_get_triton_code</p>
<p>def func(arg: torch.Tensor) -&gt; torch.Tensor:<br />
    buf0 = arg + 42<br />
    out0 = torch.ops.c10d_functional.all_reduce(buf0, "avg", "default", [0], 1)<br />
    out0 = torch.ops.c10d_functional.wait_tensor(out0)<br />
    return out0</p>
<p>if <strong>name</strong> == "<strong>main</strong>":<br />
    with tempfile.NamedTemporaryFile(delete=False) as tmpf:<br />
        dist.init_process_group(<br />
            backend="nccl", init_method=f"file://{tmpf.name}", rank=0, world_size=1<br />
        )<br />
        device = torch.device("cuda:0")</p>
<pre><code>    compiled = torch.compile(func)
    print(run_and_get_triton_code(compiled, torch.rand(4, 4, device=device)))

    torch.cuda.synchronize()
    dist.destroy_process_group()
</code></pre>
<p>```</p>
<p>Before:<br />
<code>python
def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (4, 4), (4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0) # no-op to ensure context
        buf0 = empty_strided((4, 4), (4, 1), device='cuda', dtype=torch.float32)
        # Source Nodes: [buf0], Original ATen: [aten.add]
        stream0 = get_cuda_stream(0)
        triton_poi_fused_add_0.run(arg0_1, buf0, 16, grid=grid(16), stream=stream0)
        del arg0_1
        buf1 = buf0; del buf0  # reuse
        buf2_pg = c10d._find_or_create_pg_by_ranks_and_tag('default', [0], 1)
        buf2 = buf1
        buf2_work = dist.all_reduce(buf2, async_op=True, group=buf2_pg, op=fun_col_impl._str_to_reduce_op('avg'))
        fun_col_impl._register_tensor_work(buf2, buf2_work)
        buf1 = _wait_tensor(buf1)
        buf3 = buf1
        buf4 = empty_strided((4, 4), (4, 1), device='cuda', dtype=torch.float32)
        # Source Nodes: [out0_1], Original ATen: [c10d_functional.wait_tensor]
        triton_poi_fused_wait_tensor_1.run(buf3, buf4, 16, grid=grid(16), stream=stream0)
        del buf1
        del buf3
        return (buf4, )</code></p>
<p>After:<br />
<code>python
def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (4, 4), (4, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0) # no-op to ensure context
        buf0 = empty_strided((4, 4), (4, 1), device='cuda', dtype=torch.float32)
        # Source Nodes: [buf0], Original ATen: [aten.add]
        stream0 = get_cuda_stream(0)
        triton_poi_fused_add_0.run(arg0_1, buf0, 16, grid=grid(16), stream=stream0)
        del arg0_1
        buf1 = buf0; del buf0  # reuse
        buf2_pg = c10d._find_or_create_pg_by_ranks_and_tag('default', [0], 1)
        buf2 = buf1
        buf2_work = dist.all_reduce(buf2, async_op=True, group=buf2_pg, op=fun_col_impl._str_to_reduce_op('avg'))
        fun_col_impl._register_tensor_work(buf2, buf2_work)
        buf1 = _wait_tensor(buf1)
        buf3 = buf1
        del buf3
        return (buf1, )</code></p>
<p>Test Plan:</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111681<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 25 Oct 2023 07:44:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6fd36593910d9d8cdf05dd182a78993e76f12a80</guid>
    </item>
    <item>
      <title>Add "device not supported" assert to inductor (#112001)</title>
      <link>https://github.com/pytorch/pytorch/commit/8253e0524cf12d0e5e114618644f21d7f5c9b2e2</link>
      <description><![CDATA[<p>Add "device not supported" assert to inductor (#112001)</p>
<p>Fixes #111999</p>
<p>Adds an assert that provides a more informative error message</p>
<p>For example, when running a compiled function with mps (currently unsupported):<br />
<code>...
  File "/Users/andrew.hu/Desktop/pytorch/torch/_inductor/graph.py", line 927, in init_wrapper_code
    assert wrapper_code_gen_cls is not None, f"Device {device_type} not supported"
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
AssertionError: Device mps not supported</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/112001<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Wed, 25 Oct 2023 06:19:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8253e0524cf12d0e5e114618644f21d7f5c9b2e2</guid>
    </item>
    <item>
      <title>yolov3: reduce batch size due to OOM (#111959)</title>
      <link>https://github.com/pytorch/pytorch/commit/28ebe5df7a86f78332b39a48d192ebb6855ad41c</link>
      <description><![CDATA[<p>yolov3: reduce batch size due to OOM (#111959)</p>
<p>yolov3 w/ cudagraphs (known to use more memory) is failing perf test due to OOM (https://hud.pytorch.org/benchmark/torchbench/inductor_with_cudagraphs?startTime=Mon,%2016%20Oct%202023%2020:19:47%20GMT&amp;stopTime=Mon,%2023%20Oct%202023%2020:19:47%20GMT&amp;granularity=hour&amp;mode=training&amp;dtype=amp&amp;lBranch=main&amp;lCommit=0b424ee0b7bfe09e0a438a63e8336e95eea85901&amp;rBranch=main&amp;rCommit=29048be41ca3aa8974795d93b9ea9fd6dee415fc)</p>
<p>I'm reducing the batch size from 16 to 8 to keep the same batch size for all yolov3 HUD benchmarks</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111959<br />
Approved by: https://github.com/xuzhao9</p>]]></description>
      <pubDate>Tue, 24 Oct 2023 22:18:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/28ebe5df7a86f78332b39a48d192ebb6855ad41c</guid>
    </item>
    <item>
      <title>Dynamo runner: add FSDP handcrafted module wrapping policy (#111505)</title>
      <link>https://github.com/pytorch/pytorch/commit/9e6c97890b0cb6822057fc3557e73d3bf56c68b1</link>
      <description><![CDATA[<p>Dynamo runner: add FSDP handcrafted module wrapping policy (#111505)</p>
<p>The default size based auto wrap policy may not be representative of actual usage of the models. We add support for a few handpicked models, and fallback to the size based policy.</p>
<p>sample command:<br />
<code>PYTHONPATH=~/benchmark/ python benchmarks/dynamo/torchbench.py -dcuda --training --backend=inductor --multiprocess --performance --only nanogpt --fsdp</code></p>
<p>1.257x<br />
1.256x<br />
1.257x<br />
1.252x<br />
1.257x<br />
1.262x<br />
1.258x<br />
1.272x</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111505<br />
Approved by: https://github.com/H-Huang, https://github.com/xuzhao9</p>]]></description>
      <pubDate>Tue, 24 Oct 2023 19:05:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9e6c97890b0cb6822057fc3557e73d3bf56c68b1</guid>
    </item>
    <item>
      <title>[Inductor] Support top level constants in user defined triton kernels (#111970)</title>
      <link>https://github.com/pytorch/pytorch/commit/a29a84493814ec2fbe8f58e6919a52141201c90f</link>
      <description><![CDATA[<p>[Inductor] Support top level constants in user defined triton kernels (#111970)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111970<br />
Approved by: https://github.com/jansel<br />
ghstack dependencies: #111956</p>]]></description>
      <pubDate>Tue, 24 Oct 2023 18:43:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a29a84493814ec2fbe8f58e6919a52141201c90f</guid>
    </item>
    <item>
      <title>[Inductor] Support user defined triton kernels calling other triton kernels and activation functions (#111956)</title>
      <link>https://github.com/pytorch/pytorch/commit/bb550b25c93a1541461bfff508551450f6f636d7</link>
      <description><![CDATA[<p>[Inductor] Support user defined triton kernels calling other triton kernels and activation functions (#111956)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111956<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 24 Oct 2023 18:39:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bb550b25c93a1541461bfff508551450f6f636d7</guid>
    </item>
    <item>
      <title>[cpu][inductor] improve cpu vec implementations of log (#111898)</title>
      <link>https://github.com/pytorch/pytorch/commit/b5703203647644176220676af0e8e5f23de8d45a</link>
      <description><![CDATA[<p>[cpu][inductor] improve cpu vec implementations of log (#111898)</p>
<p>Fixes #110611.</p>
<p>The current Torchinductor's <code>log</code> implementations will call <code>sleef</code> functions in <code>aten::Vec</code> which show worse performance than Aten's <code>log</code> implementations that invoke <code>MKL</code> functions. The reason is that the <code>sleef</code> algorithms sacrifice performance in order to have a higher precision. This PR changes Torchinductor's <code>log</code> implementations from the <code>sleef</code> functions with <code>1.0</code> ULP error bound to the ones with <code>3.5</code> ULP error bound.</p>
<p><strong>Performance</strong><br />
Machine: ICX</p>
<p>The original perf number, perf with <code>Sleef_logf16_u10</code>:<br />
<code>bash
numactl -C0 python test.py
log
eager:    368.8463559374213
compiled: 616.8672097846866
logit
eager:    565.499295014888
compiled: 1010.4096410796046</code></p>
<p>Perf with <code>Sleef_logf16_u35</code>:<br />
<code>bash
numactl -C0 python test.py
log
eager:    364.8629770614207
compiled: 360.2141812443733
logit
eager:    562.3160391114652
compiled: 545.2622110024095</code></p>
<p><strong>Accuracy</strong><br />
error_bound | tol=1e-6 | tol=1e-7<br />
-- | -- | --<br />
1.0 ULP | PASS | FAIL<br />
3.5 ULP | PASS | FAIL</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111898<br />
Approved by: https://github.com/jgong5, https://github.com/desertfire, https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 24 Oct 2023 17:26:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b5703203647644176220676af0e8e5f23de8d45a</guid>
    </item>
    <item>
      <title>[inductor] Decompose boolean min/max into all/any (#110311)</title>
      <link>https://github.com/pytorch/pytorch/commit/1dd57082a44c4d9c247996d6e498bb5475d05fbc</link>
      <description><![CDATA[<p>[inductor] Decompose boolean min/max into all/any (#110311)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110311<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #110310</p>]]></description>
      <pubDate>Tue, 24 Oct 2023 13:33:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1dd57082a44c4d9c247996d6e498bb5475d05fbc</guid>
    </item>
    <item>
      <title>[aotinductor] Fix a problem when the generated graph is empty (#111822)</title>
      <link>https://github.com/pytorch/pytorch/commit/e72fcd382b908857c5d2d0b80f8c0df3c476d7bd</link>
      <description><![CDATA[<p>[aotinductor] Fix a problem when the generated graph is empty (#111822)</p>
<p>Summary: For https://github.com/pytorch/pytorch/issues/111691</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111822<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Tue, 24 Oct 2023 12:00:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e72fcd382b908857c5d2d0b80f8c0df3c476d7bd</guid>
    </item>
    <item>
      <title>[Inductor] Support calling user defined kernels with different type of arguments (#111939)</title>
      <link>https://github.com/pytorch/pytorch/commit/ddcf9c050b015a6a68099022b9885bcc7f7c1c52</link>
      <description><![CDATA[<p>[Inductor] Support calling user defined kernels with different type of arguments (#111939)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111939<br />
Approved by: https://github.com/jansel, https://github.com/zou3519<br />
ghstack dependencies: #111770, #111808</p>]]></description>
      <pubDate>Tue, 24 Oct 2023 11:49:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ddcf9c050b015a6a68099022b9885bcc7f7c1c52</guid>
    </item>
    <item>
      <title>Fix reduction + () + multi-level reduction optimization (#111781)</title>
      <link>https://github.com/pytorch/pytorch/commit/099efd8346d5889d57743ea2b03a692aef2c440a</link>
      <description><![CDATA[<p>Fix reduction + () + multi-level reduction optimization (#111781)</p>
<p>In https://github.com/pytorch/pytorch/pull/111122, an optimization is introduced for reduction() + () + multi-level reduction. In this case, we make a multi-level reduction first-level reduction ranges the same as the previous reduction ranges so that the Inductor has better chances to fuse the first reduction and the first-level reduction of the multi-level reduction kernel together.</p>
<p>There is a corner case that the multi-level reduction kernel has <code>keepdim=True</code>. In this case, ranges of the multi-level reduction kernel is not empty, and the dim info needs to be used to create the inner loader of the first-level reduction kernel. To keep the logic simple, for now we simply disable optimization when <code>keepdim=True</code>.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50544876">D50544876</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111781<br />
Approved by: https://github.com/malfet, https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 24 Oct 2023 07:42:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/099efd8346d5889d57743ea2b03a692aef2c440a</guid>
    </item>
    <item>
      <title>[inductor] Defer memory operation lowering to wrapper (#111402)</title>
      <link>https://github.com/pytorch/pytorch/commit/cbc6213f5d2a43bde7cf49cfec5047782824bd6b</link>
      <description><![CDATA[<p>[inductor] Defer memory operation lowering to wrapper (#111402)</p>
<p>Right now, memory ops are being lowered to strings partly in<br />
scheduler.codegen() and partly in wrapper.codegen(). But that makes<br />
static memory planning (which is done entirely in <code>wrapper.codegen()</code>)<br />
difficult to implement as information is "lost" by that point.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111402<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 23 Oct 2023 19:47:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cbc6213f5d2a43bde7cf49cfec5047782824bd6b</guid>
    </item>
    <item>
      <title>[inductor] decomposition for complex addition (#110740)</title>
      <link>https://github.com/pytorch/pytorch/commit/6977ba6e3c70def3532a44e9ebcb1c6b3d257a8d</link>
      <description><![CDATA[<p>[inductor] decomposition for complex addition (#110740)</p>
<p>Tracks https://github.com/pytorch/pytorch/issues/98161</p>
<p>Complex number support in Pytorch isn't ideal today as complex operations will mostly end up taken care of by the aten runtime, except for <code>torch.angle</code> which is handled in <a href="https://github.com/pytorch/pytorch/pull/105609">105609</a>. In general a better way to handle that could be to decompose complex operations first so that more opportunities for fusion could be unveiled, and then to have Triton take care of non-continuous (strided) tensor operations more efficiently. This change adds support to decompose complex addtions.</p>
<p><code>@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tl.load(in_ptr1 + (x0), xmask)
    tmp2 = tmp0 + tmp1
    tl.store(out_ptr0 + (x0), tmp2, xmask)</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110740<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 23 Oct 2023 19:41:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6977ba6e3c70def3532a44e9ebcb1c6b3d257a8d</guid>
    </item>
    <item>
      <title>Revert "[inductor][BE] split triton_meta and inductor_meta (#111397)"</title>
      <link>https://github.com/pytorch/pytorch/commit/e62c887babea75b4966c4ec3cef292229df81076</link>
      <description><![CDATA[<p>Revert "[inductor][BE] split triton_meta and inductor_meta (#111397)"</p>
<p>This reverts commit 070b94dc08c73e133c5231ec6acbe407ae1580f3.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/111397 on behalf of https://github.com/facebook-github-bot due to Diff reverted internally (<a href="https://github.com/pytorch/pytorch/pull/111397#issuecomment-1776282039">comment</a>)</p>]]></description>
      <pubDate>Mon, 23 Oct 2023 16:52:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e62c887babea75b4966c4ec3cef292229df81076</guid>
    </item>
    <item>
      <title>[inductor][easy] skip test_extension_backend.py in fbcode (#111591)</title>
      <link>https://github.com/pytorch/pytorch/commit/4ed4753ac391b7a551b59455ca21f201bc147631</link>
      <description><![CDATA[<p>[inductor][easy] skip test_extension_backend.py in fbcode (#111591)</p>
<p>Summary: It's currently failing. We should skip it in fbcode because cpp extensions don't work right now.</p>
<p>Differential Revision: D48852412</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111591<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Mon, 23 Oct 2023 14:37:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4ed4753ac391b7a551b59455ca21f201bc147631</guid>
    </item>
    <item>
      <title>[inductor][BE] split triton_meta and inductor_meta (#111397)</title>
      <link>https://github.com/pytorch/pytorch/commit/070b94dc08c73e133c5231ec6acbe407ae1580f3</link>
      <description><![CDATA[<p>[inductor][BE] split triton_meta and inductor_meta (#111397)</p>
<p>triton_meta is intended to be passed directly to triton. Previous we were also putting other metadata into triton_meta; but we should split out the other metadata into a separate dict to avoid possible conficts in the future.</p>
<p>This PR splits out triton_meta and inductor_meta so we have a place to put additional metadata that isn't intended to be passed to triton.</p>
<p>Tests - wait for CI</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50442547">D50442547</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/111397<br />
Approved by: https://github.com/shunting314, https://github.com/eellison</p>]]></description>
      <pubDate>Mon, 23 Oct 2023 13:38:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/070b94dc08c73e133c5231ec6acbe407ae1580f3</guid>
    </item>
    <item>
      <title>Don't DCE unbacked SymInt if it is returned as shape constant buffer (#111803)</title>
      <link>https://github.com/pytorch/pytorch/commit/6c384cf4a6fcc675a6b999752db86f6e4151dc4c</link>
      <description><![CDATA[<p>Don't DCE unbacked SymInt if it is returned as shape constant buffer (#111803)</p>
<p>Also adds some logging for the inductor scheduler</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111803<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 23 Oct 2023 11:57:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6c384cf4a6fcc675a6b999752db86f6e4151dc4c</guid>
    </item>
    <item>
      <title>[inductor] Adding a way to force fusion of int_mm with mul (#111413)</title>
      <link>https://github.com/pytorch/pytorch/commit/335582584ff758b77eb339a813e4113d0fe44e38</link>
      <description><![CDATA[<p>[inductor] Adding a way to force fusion of int_mm with mul (#111413)</p>
<p>Summary: When doing quantization int_mm -&gt; mul or int_mm -&gt; mul -&gt;<br />
to(dtype) is an extremely common op pattern which is currently not<br />
handled well by inductor. Ideally, since the output of<br />
int_mm has dtype int32 we'd prefer to only realize a smaller dtype like<br />
bf16 or float16. Currently inductor doesn't have a way to force this, in<br />
many cases the mul gets fused with a bunch of subsequent pointwise and reduction<br />
ops from the dequant  and following ops creating an increase in memory overhead and a general<br />
slowdown compared to the fused version.</p>
<p>as an external benchmark, for SAM this seems to improve our e2e image encoder times by 3-5% depending on<br />
batchsize and reduces memory usage by 20%</p>
<p>Test Plan: python test/inductor/test_pattern_matcher.py -k<br />
"int_mm_mul"</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111413<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 23 Oct 2023 11:18:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/335582584ff758b77eb339a813e4113d0fe44e38</guid>
    </item>
    <item>
      <title>[re-land][inductor] Refactor and optimize allocation calls (#111117) (#111511)</title>
      <link>https://github.com/pytorch/pytorch/commit/e264b42a2e0783ffe062dbf2d6eb3a2b61d4a9ff</link>
      <description><![CDATA[<p>[re-land][inductor] Refactor and optimize allocation calls (#111117) (#111511)</p>
<p>Summary:<br />
This is a re-land of https://github.com/pytorch/pytorch/pull/111117 with<br />
updates to our internal tests included.</p>
<p>This splits out changes from<br />
https://github.com/pytorch/pytorch/pull/102625 to make things easier to<br />
review.</p>
<p>This diff creates a <code>make_allocation()</code> method that extracts the logic<br />
from <code>make_buffer_allocation()</code> while allowing us to allocate non-buffer<br />
objects. In particular, we will use this to allocate memory pools during<br />
memory planning.</p>
<p>This diff also includes a small optimization -- if the desired<br />
allocation is contiguous, then we emit a call to <code>empty()</code> instead of<br />
<code>empty_strided()</code> with its superfluous stride argument.</p>
<p>Test Plan: contbuild &amp; OSS CI, see https://hud.pytorch.org/commit/pytorch/pytorch/9ce0ae836d6801a39776897b9e891cd978b28aea</p>
<p>Differential Revision: D50429424</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111511<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 23 Oct 2023 11:18:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e264b42a2e0783ffe062dbf2d6eb3a2b61d4a9ff</guid>
    </item>
    <item>
      <title>[aotinductor] Update test utility to use AOTIModelRunner (#111657)</title>
      <link>https://github.com/pytorch/pytorch/commit/ce48d36324f6da1060243e8b101d3b12f27792a7</link>
      <description><![CDATA[<p>[aotinductor] Update test utility to use AOTIModelRunner (#111657)</p>
<p>Summary: Use AOTIModelRunner provided by libtorch instead of the custom written RAIIModelContainer for testing. This change also makes running AOTInductor benchmarks on CPU possbile.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50560764">D50560764</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/111657<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Mon, 23 Oct 2023 10:21:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ce48d36324f6da1060243e8b101d3b12f27792a7</guid>
    </item>
    <item>
      <title>[inductor] Implement clone removal for user defined triton kernel via reinplace_scatters (#111627)</title>
      <link>https://github.com/pytorch/pytorch/commit/2b2b6caf8f4da5599841678967202fb9443c56e5</link>
      <description><![CDATA[<p>[inductor] Implement clone removal for user defined triton kernel via reinplace_scatters (#111627)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111627<br />
Approved by: https://github.com/jansel<br />
ghstack dependencies: #111434</p>]]></description>
      <pubDate>Sun, 22 Oct 2023 14:28:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2b2b6caf8f4da5599841678967202fb9443c56e5</guid>
    </item>
    <item>
      <title>[Inductor] Support user defined triton kernels in inductor (#111434)</title>
      <link>https://github.com/pytorch/pytorch/commit/977d3bcc46b81306093ee77b8a7fac2526c7f307</link>
      <description><![CDATA[<p>[Inductor] Support user defined triton kernels in inductor (#111434)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111434<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Sun, 22 Oct 2023 09:04:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/977d3bcc46b81306093ee77b8a7fac2526c7f307</guid>
    </item>
    <item>
      <title>[inductor] Move inductor ops to CompositeExplicitAutograd (#111702)</title>
      <link>https://github.com/pytorch/pytorch/commit/7bd004297a8280af138bbd92d370fcae9aa1b4a9</link>
      <description><![CDATA[<p>[inductor] Move inductor ops to CompositeExplicitAutograd (#111702)</p>
<p>Relands #111274<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/111702<br />
Approved by: https://github.com/voznesenskym<br />
ghstack dependencies: #111700, #111701</p>]]></description>
      <pubDate>Sat, 21 Oct 2023 09:31:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7bd004297a8280af138bbd92d370fcae9aa1b4a9</guid>
    </item>
    <item>
      <title>Fix test listing error (#111630)</title>
      <link>https://github.com/pytorch/pytorch/commit/e9422b1fb0692dcddb28517b8ced690743a3b085</link>
      <description><![CDATA[<p>Fix test listing error (#111630)</p>
<p>Summary: Fix fbcode internal test listing error</p>
<p>Test Plan: buck2 test 'fbcode//mode/opt' fbcode//caffe2/test/inductor:max_autotune -- --run-disabled</p>
<p>Differential Revision: D50485766</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111630<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Fri, 20 Oct 2023 15:00:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e9422b1fb0692dcddb28517b8ced690743a3b085</guid>
    </item>
    <item>
      <title>S390x inductor support (#111367)</title>
      <link>https://github.com/pytorch/pytorch/commit/ba04d84089d49d959c05e32d10148b0c81277194</link>
      <description><![CDATA[<p>S390x inductor support (#111367)</p>
<p>Use arch compile flags. They are needed for vectorization support on s390x.<br />
Implement new helper functions for inductor.</p>
<p>This change fixes multiple tests in test_cpu_repro.py</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111367<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Fri, 20 Oct 2023 11:38:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ba04d84089d49d959c05e32d10148b0c81277194</guid>
    </item>
    <item>
      <title>Disable inductor layout_opt on ROCm (#111474)</title>
      <link>https://github.com/pytorch/pytorch/commit/619ae87a1d1ae086f59a64d3b71dbfe4af8b804a</link>
      <description><![CDATA[<p>Disable inductor layout_opt on ROCm (#111474)</p>
<p>Previously we disabled this option on none MI200 GPUs (https://github.com/pytorch/pytorch/pull/107812 due to worse NHWC conv performance on some cards. This PR will disable this feature for all GPUs to make this uniform for ROCm and due to perf regressions noted here https://github.com/pytorch/pytorch/pull/110319</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111474<br />
Approved by: https://github.com/jithunnair-amd, https://github.com/eellison</p>]]></description>
      <pubDate>Fri, 20 Oct 2023 01:31:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/619ae87a1d1ae086f59a64d3b71dbfe4af8b804a</guid>
    </item>
    <item>
      <title>[AOTInductor] Disable NonABI tests in fbcode (#111616)</title>
      <link>https://github.com/pytorch/pytorch/commit/ff835fb464b02ef92ac3d8e4adfd5953f6b64f77</link>
      <description><![CDATA[<p>[AOTInductor] Disable NonABI tests in fbcode (#111616)</p>
<p>Summary: NonABI mode is not intended to be used in fbcode.</p>
<p>Test Plan: buck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:test_aot_inductor</p>
<p>Differential Revision: D50478575</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111616<br />
Approved by: https://github.com/desertfire, https://github.com/khabinov</p>]]></description>
      <pubDate>Thu, 19 Oct 2023 20:37:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ff835fb464b02ef92ac3d8e4adfd5953f6b64f77</guid>
    </item>
    <item>
      <title>Optimize reduction + amax fusion (#111122)</title>
      <link>https://github.com/pytorch/pytorch/commit/dc31dbbcab841a1b1dc066aa8d19df62b3e86ac1</link>
      <description><![CDATA[<p>Optimize reduction + amax fusion (#111122)</p>
<p>This PR optimizes cases like layer_norm + fp8 quant (which includes amax and fp8 quant) fusion when amax is split into multiple reduction kernels.</p>
<p>Benchmark:<br />
```<br />
python test/inductor/test_fp8.py -k test_layernorm_fp8_quant_benchmark</p>
<p>Before this PR:<br />
Config: float8_dtype=torch.float8_e5m2, shape=(4, 2048, 4096).<br />
Benchmark results: Inductor: 0.13262102689486555ms, Eager: 0.8211962616822429ms, LN only Inductor: 0.09606276150627614ms.</p>
<p>After this PR:<br />
Config: float8_dtype=torch.float8_e5m2, shape=(4, 2048, 4096).<br />
Benchmark results: Inductor: 0.08281274131274131ms, Eager: 0.8217452830188678ms, LN only Inductor: 0.09586902286902287ms.<br />
```</p>
<p>LN + fp8 quant is even faster than LN itself. The reason could be that LN + fp8 outputs fp8 while LN outputs fp16.</p>
<p>From Inductor nightly benchmark test:<br />
There are perf differences in cuda_graph / cuda_graph_dynamic / default runs, but no difference in inductor_max_autotune. So it seems to me that the perf differences are mostly like fluctuations.</p>
<p><img alt="Screenshot 2023-10-18 at 4 58 55 PM" src="https://github.com/pytorch/pytorch/assets/10527447/6640474a-1e1d-4d33-97e9-0a60d0bc9f1f" /></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111122<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 19 Oct 2023 12:53:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dc31dbbcab841a1b1dc066aa8d19df62b3e86ac1</guid>
    </item>
    <item>
      <title>[cuda] Preserve operations order between vectorized and non-vectorized in ln grad input (#111488)</title>
      <link>https://github.com/pytorch/pytorch/commit/5ce2ab84661778d60b9e270705ff84540ed98837</link>
      <description><![CDATA[<p>[cuda] Preserve operations order between vectorized and non-vectorized in ln grad input (#111488)</p>
<p>The vectorized implementation in https://github.com/pytorch/pytorch/pull/111021 changed the order of arithmetic instructions in <code>layer_norm_grad_input</code>, causing non bitwise identical results when compared to the non-vectorized implementation. At merging, all accuracy checks passed, including internal inductor ones.</p>
<p>There are CI periodic inductor dynamo tests (e.g. <code>pit_b_224</code>) that run eager mode models several times and compare results. If the input buffers are aligned to the vector length, the vectorized implementation will be used. If not, the default one will be used. If the 2 eager runs end up having different buffer alignments, 2 implementations will be called and then the results would be very close but not bitwise identical. The tests check for bitwise identical results and in some cases they may fail.</p>
<p>This fix makes sure that the operation order between non-vectorized and vectorized is the same and the 2 implementations <strong>should</strong> produce bitwise identical results.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/111488<br />
Approved by: https://github.com/malfet</p>]]></description>
      <pubDate>Wed, 18 Oct 2023 22:00:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5ce2ab84661778d60b9e270705ff84540ed98837</guid>
    </item>
    <item>
      <title>[AOTInductor] ProxyExecutor skips serializing missing args with default value (#111425)</title>
      <link>https://github.com/pytorch/pytorch/commit/b72a1402f5616e44264dddc25f0be23f20bee171</link>
      <description><![CDATA[<p>[AOTInductor] ProxyExecutor skips serializing missing args with default value (#111425)</p>
<p>Summary: In AOTInductor ABI Compatible-mode, we don't serialize missing args with default value.</p>
<p>Test Plan: buck2 run mode/dev-nosan deeplearning/aot_inductor/test:test_custom_ops</p>
<p>Differential Revision: D50345729</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111425<br />
Approved by: https://github.com/angelayi</p>]]></description>
      <pubDate>Wed, 18 Oct 2023 09:10:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b72a1402f5616e44264dddc25f0be23f20bee171</guid>
    </item>
    <item>
      <title>Revert "[inductor] Move inductor ops to CompositeExplicitAutograd (#111274)"</title>
      <link>https://github.com/pytorch/pytorch/commit/a389e2c7c7881a5fd29d3097671429c299eedda5</link>
      <description><![CDATA[<p>Revert "[inductor] Move inductor ops to CompositeExplicitAutograd (#111274)"</p>
<p>This reverts commit 8b46a106f254fd860a4b7b99c8bb640ba58cb176.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/111274 on behalf of https://github.com/jeanschmidt due to Breaking internal CI (<a href="https://github.com/pytorch/pytorch/pull/111274#issuecomment-1768517555">comment</a>)</p>]]></description>
      <pubDate>Wed, 18 Oct 2023 05:57:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a389e2c7c7881a5fd29d3097671429c299eedda5</guid>
    </item>
    <item>
      <title>Revert "[aot_inductor] return a copy of any constant (#111356)"</title>
      <link>https://github.com/pytorch/pytorch/commit/ed7739d690af78f0cedcfdf2139801dcd2d7078e</link>
      <description><![CDATA[<p>Revert "[aot_inductor] return a copy of any constant (#111356)"</p>
<p>This reverts commit 71e1f34923af186dff46a8641c977a1cf507e06c.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/111356 on behalf of https://github.com/jeanschmidt due to Breaking internal ci (<a href="https://github.com/pytorch/pytorch/pull/111356#issuecomment-1768503640">comment</a>)</p>]]></description>
      <pubDate>Wed, 18 Oct 2023 05:51:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ed7739d690af78f0cedcfdf2139801dcd2d7078e</guid>
    </item>
    <item>
      <title>Revert "[inductor] Refactor and optimize allocation calls (#111117)"</title>
      <link>https://github.com/pytorch/pytorch/commit/08f580d49822abd46d004e6965f74861fe7a750c</link>
      <description><![CDATA[<p>Revert "[inductor] Refactor and optimize allocation calls (#111117)"</p>
<p>This reverts commit 9ce0ae836d6801a39776897b9e891cd978b28aea.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/111117 on behalf of https://github.com/jeanschmidt due to Braking internal CI (<a href="https://github.com/pytorch/pytorch/pull/111117#issuecomment-1768489865">comment</a>)</p>]]></description>
      <pubDate>Wed, 18 Oct 2023 05:45:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/08f580d49822abd46d004e6965f74861fe7a750c</guid>
    </item>
    <item>
      <title>[aotinductor] Make writing of the weight files to be conditional (#111379)</title>
      <link>https://github.com/pytorch/pytorch/commit/1ac36dbd2a4e890859ae7e8997e7e6bc90e0d4fa</link>
      <description><![CDATA[<p>[aotinductor] Make writing of the weight files to be conditional (#111379)</p>
<p>Summary: Since we cache the AOTInductor generated library file, we should not need to write the weights as binary file if the library file already exists.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111379<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Tue, 17 Oct 2023 20:52:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1ac36dbd2a4e890859ae7e8997e7e6bc90e0d4fa</guid>
    </item>
    <item>
      <title>[aotinductor] Refactor the generated result (#111080)</title>
      <link>https://github.com/pytorch/pytorch/commit/a9b3afd3d8bbe5cfa7bb922043704e7d196d3750</link>
      <description><![CDATA[<p>[aotinductor] Refactor the generated result (#111080)</p>
<p>Summary: Return the compiled library path as a string instead of wrap it as a callable.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50246941">D50246941</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/111080<br />
Approved by: https://github.com/jansel, https://github.com/chenyang78</p>]]></description>
      <pubDate>Tue, 17 Oct 2023 20:35:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a9b3afd3d8bbe5cfa7bb922043704e7d196d3750</guid>
    </item>
    <item>
      <title>Compile NestedTensor with AOTAutograd (#110529)</title>
      <link>https://github.com/pytorch/pytorch/commit/2dc1726ab79d64de56f4778e0a609675f215e400</link>
      <description><![CDATA[<p>Compile NestedTensor with AOTAutograd (#110529)</p>
<p>This PR has a number of changes that improve subclass support for AOTAutograd/Inductor in general:<br />
-  previously if a subclass does extra aliasing between graph outputs/inputs in a way, the partitioner would complain because grad_outputs are the outputs reused as-is. Now we do a view_as(self) to workaround this.<br />
- Use dense -&gt; dense metadata when working with fwd_output_strides during backward. This is important since the stride information comes from inductor which sees the dense to dense graph.<br />
- Inductor requires that the inputs to the compiled backward to match some expected strides computed during compilation. We make sure to make the inner tensors of the subclass contiguous (previously, we only made the subclass itself contiguous)</p>
<p>Changes specific to NestedTensor relevant to compilation:<br />
- Properly handle the case where <code>__tensor_unflatten__</code> is passed non-symbolic dense tensors and with meta extracted from fake subclasses.<br />
- Skip var_to_range logic for singleton int<br />
- Skip size hint logic in inductor for singleton int</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110529<br />
Approved by: https://github.com/bdhirsh</p>]]></description>
      <pubDate>Tue, 17 Oct 2023 13:17:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2dc1726ab79d64de56f4778e0a609675f215e400</guid>
    </item>
    <item>
      <title>[reland] [inductor] fix a max-autotune rng state related bug (#111381)</title>
      <link>https://github.com/pytorch/pytorch/commit/cc9b7bb85c4cccb7913383eb959935fd4f56f293</link>
      <description><![CDATA[<p>[reland] [inductor] fix a max-autotune rng state related bug (#111381)</p>
<p>reland https://github.com/pytorch/pytorch/pull/109828</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111381<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Tue, 17 Oct 2023 11:16:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cc9b7bb85c4cccb7913383eb959935fd4f56f293</guid>
    </item>
    <item>
      <title>[Reland][Inductor] Disallow OpOverloadPacket in ir.FallbackKernel (#110567) (#111396)</title>
      <link>https://github.com/pytorch/pytorch/commit/1aad6d803a46693fe66ed716800b92381502998c</link>
      <description><![CDATA[<p>[Reland][Inductor] Disallow OpOverloadPacket in ir.FallbackKernel (#110567) (#111396)</p>
<p>This is a reland of #110567 with additional fbcode fixed.</p>
<p>Summary:<br />
In ABI compatible mode, We always need op_overload.schema for FallbackKernel.</p>
<p>Approved by: https://github.com/jansel</p>
<p>Test Plan: contbuild &amp; OSS CI, see https://hud.pytorch.org/commit/pytorch/pytorch/37a02659921490d85b2b0712ad52b924e0c431cd</p>
<p>Differential Revision: D50339346</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111396<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Tue, 17 Oct 2023 10:53:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1aad6d803a46693fe66ed716800b92381502998c</guid>
    </item>
    <item>
      <title>[aot_inductor] return a copy of any constant (#111356)</title>
      <link>https://github.com/pytorch/pytorch/commit/71e1f34923af186dff46a8641c977a1cf507e06c</link>
      <description><![CDATA[<p>[aot_inductor] return a copy of any constant (#111356)</p>
<p>When the model returns a constant, we cannot "release" its handle,<br />
because the constant doesn't have any handle at all. Instead,<br />
we should allocate a new tensor and then return a copy of the constant.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111356<br />
Approved by: https://github.com/hl475</p>]]></description>
      <pubDate>Tue, 17 Oct 2023 00:44:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/71e1f34923af186dff46a8641c977a1cf507e06c</guid>
    </item>
    <item>
      <title>[inductor] Refactor and optimize allocation calls (#111117)</title>
      <link>https://github.com/pytorch/pytorch/commit/9ce0ae836d6801a39776897b9e891cd978b28aea</link>
      <description><![CDATA[<p>[inductor] Refactor and optimize allocation calls (#111117)</p>
<p>This splits out changes from<br />
https://github.com/pytorch/pytorch/pull/102625 to make things easier to<br />
review.</p>
<p>This diff creates a <code>make_allocation()</code> method that extracts the logic<br />
from <code>make_buffer_allocation()</code> while allowing us to allocate non-buffer<br />
objects. In particular, we will use this to allocate memory pools during<br />
memory planning.</p>
<p>This diff also includes a small optimization -- if the desired<br />
allocation is contiguous, then we emit a call to <code>empty()</code> instead of<br />
<code>empty_strided()</code> with its superfluous stride argument.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111117<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 16 Oct 2023 19:06:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9ce0ae836d6801a39776897b9e891cd978b28aea</guid>
    </item>
    <item>
      <title>Reland "AOTAutograd: Go down inference path if no outputs require grad (#111011)" (#111347)</title>
      <link>https://github.com/pytorch/pytorch/commit/7fb09b804b8e5ce3209ba9f7707ec5f3c1c11681</link>
      <description><![CDATA[<p>Reland "AOTAutograd: Go down inference path if no outputs require grad (#111011)" (#111347)</p>
<p>Re-land of https://github.com/pytorch/pytorch/pull/111011.</p>
<p>The original PR ended up having a bad interaction with code that tried to run <code>torch.compile</code> under <code>with torch.inference_mode</code>, which caused some internal tests to fail.</p>
<p>The issue was that:</p>
<p>(1) AOTInductor invokes the pattern matcher passes in inductor</p>
<p>(2) The pattern matcher registers some code with <a href="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/fx_passes/pad_mm.py#L461">training_graph</a></p>
<p>(3) The <code>training_graph</code> function expects to be able to set the global autograd state to <code>requires_grad</code>, and always get out a join graph (assertion <a href="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/pattern_matcher.py#L1196">here</a>).</p>
<p>(4) However, when inference_mode is activated, and you try to run AOTAutograd, AOTAutograd will witness that all outputs to the traced function will not require grad, and (now correctly) think that we are tracing an inference graph, which fails the above assert.</p>
<p>After talking to Bin, it sounds like these training-only patterns aren't necessary when we know we are compiling an inference graph (which should always be the case if you're running torch.compile with inference_mode). So I updated the pattern matcher to ignore any pattern matches using <code>training_graph</code>, when inference_mode is enabled.</p>
<p>This reverts commit cf6b1cdf6ac74d375b0787bd8f9463cb3a53b0e5.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111347<br />
Approved by: https://github.com/Chillee</p>]]></description>
      <pubDate>Mon, 16 Oct 2023 16:11:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7fb09b804b8e5ce3209ba9f7707ec5f3c1c11681</guid>
    </item>
    <item>
      <title>[inductor] Move inductor ops to CompositeExplicitAutograd (#111274)</title>
      <link>https://github.com/pytorch/pytorch/commit/8b46a106f254fd860a4b7b99c8bb640ba58cb176</link>
      <description><![CDATA[<p>[inductor] Move inductor ops to CompositeExplicitAutograd (#111274)</p>
<p>I suspect in practice this won't matter, but if we do end up tracing this it causes them not to get decomposed.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111274<br />
Approved by: https://github.com/voznesenskym, https://github.com/desertfire<br />
ghstack dependencies: #111271, #111273</p>]]></description>
      <pubDate>Mon, 16 Oct 2023 13:16:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8b46a106f254fd860a4b7b99c8bb640ba58cb176</guid>
    </item>
    <item>
      <title>[Compiled Autograd] Turn accumulate_grad into an op (#111271)</title>
      <link>https://github.com/pytorch/pytorch/commit/04b04c068659127a53d659c44b0dd75fa9fd5887</link>
      <description><![CDATA[<p>[Compiled Autograd] Turn accumulate_grad into an op (#111271)</p>
<p>Rather than baking the behavior of <code>AccumulateGrad</code> nodes into the generated graph (either as <code>+=</code>, or as a return value of the graph).  This creates a new <code>accumulate_grad_</code> dispatcher op that is included in the generated graph like:<br />
```<br />
def forward(self, inputs, sizes, hooks):<br />
    getitem = inputs[0]<br />
    getitem_1 = inputs[1]<br />
    getitem_2 = inputs[2]<br />
    getitem_3 = inputs[3]<br />
    getitem_4 = inputs[4]<br />
    getitem_5 = inputs[5]<br />
    getitem_6 = inputs[6]<br />
    getitem_7 = inputs[7]<br />
    getitem_8 = inputs[8]<br />
    getitem_9 = inputs[9];  inputs = None<br />
    expand = torch.ops.aten.expand.default(getitem, [2, 4]);  getitem = None<br />
    threshold_backward = torch.ops.aten.threshold_backward.default(expand, getitem_1, 0);  expand = getitem_1 = None<br />
    t = torch.ops.aten.t.default(getitem_3);  getitem_3 = None<br />
    mm = torch.ops.aten.mm.default(threshold_backward, t);  t = None<br />
    t_1 = torch.ops.aten.t.default(threshold_backward)<br />
    mm_1 = torch.ops.aten.mm.default(t_1, getitem_2);  t_1 = getitem_2 = None<br />
    t_2 = torch.ops.aten.t.default(mm_1);  mm_1 = None<br />
    sum_1 = torch.ops.aten.sum.dim_IntList(threshold_backward, [0], True);  threshold_backward = None<br />
    view = torch.ops.aten.view.default(sum_1, [4]);  sum_1 = None<br />
    t_3 = torch.ops.aten.t.default(t_2);  t_2 = None<br />
    accumulate_grad_ = torch.ops.inductor.accumulate_grad_.default(getitem_4, t_3);  getitem_4 = t_3 = None<br />
    threshold_backward_1 = torch.ops.aten.threshold_backward.default(mm, getitem_5, 0);  mm = getitem_5 = None<br />
    t_4 = torch.ops.aten.t.default(threshold_backward_1)<br />
    mm_2 = torch.ops.aten.mm.default(t_4, getitem_6);  t_4 = getitem_6 = None<br />
    t_5 = torch.ops.aten.t.default(mm_2);  mm_2 = None<br />
    sum_2 = torch.ops.aten.sum.dim_IntList(threshold_backward_1, [0], True);  threshold_backward_1 = None<br />
    view_1 = torch.ops.aten.view.default(sum_2, [4]);  sum_2 = None<br />
    t_6 = torch.ops.aten.t.default(t_5);  t_5 = None<br />
    accumulate_grad__1 = torch.ops.inductor.accumulate_grad_.default(getitem_7, t_6);  getitem_7 = t_6 = None<br />
    accumulate_grad__2 = torch.ops.inductor.accumulate_grad_.default(getitem_8, view_1);  getitem_8 = view_1 = None<br />
    accumulate_grad__3 = torch.ops.inductor.accumulate_grad_.default(getitem_9, view);  getitem_9 = view = None<br />
    return []</p>
<p>```</p>
<p>The motivation here is <code>AccumulateGrad</code> nodes are causing trouble in FSDP tracing, since FSDP is in-place resizing parameters and parameter storage in hooks.  We will model this mutation in dynamo, but not during the initial compiled autograd capture.  This allows us to bypass failing shape checks in the initial capture.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111271<br />
Approved by: https://github.com/voznesenskym</p>]]></description>
      <pubDate>Mon, 16 Oct 2023 13:16:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/04b04c068659127a53d659c44b0dd75fa9fd5887</guid>
    </item>
    <item>
      <title>Revert "[inductor] Adding a way to force fusion of int_mm with mul (#111125)"</title>
      <link>https://github.com/pytorch/pytorch/commit/89f11c69a8eb53fef7d28581abf0e9ee626aac61</link>
      <description><![CDATA[<p>Revert "[inductor] Adding a way to force fusion of int_mm with mul (#111125)"</p>
<p>This reverts commit f4297576e63e4110f6bdf2522ae6a5fb4c7f3816.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/111125 on behalf of https://github.com/huydhn due to Sorry for reverting your change, but it fails on ROCm https://hud.pytorch.org/pytorch/pytorch/commit/f4297576e63e4110f6bdf2522ae6a5fb4c7f3816 (<a href="https://github.com/pytorch/pytorch/pull/111125#issuecomment-1764956174">comment</a>)</p>]]></description>
      <pubDate>Mon, 16 Oct 2023 09:37:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/89f11c69a8eb53fef7d28581abf0e9ee626aac61</guid>
    </item>
    <item>
      <title>[aotinductor] Relax ExternKernel kwargs checking (#111167)</title>
      <link>https://github.com/pytorch/pytorch/commit/73d288fdf9d0beb76229cabc8566ee116f8a21a2</link>
      <description><![CDATA[<p>[aotinductor] Relax ExternKernel kwargs checking (#111167)</p>
<p>Summary: When a fallback kernel is called without specifying any kwargs, we still need to fill in default values for those kwargs when generating cpp call.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111167<br />
Approved by: https://github.com/chenyang78, https://github.com/jgong5</p>]]></description>
      <pubDate>Sat, 14 Oct 2023 13:41:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/73d288fdf9d0beb76229cabc8566ee116f8a21a2</guid>
    </item>
    <item>
      <title>Intra-graph reordering pass on Inductor scheduler IR (based on #100762) (#108091)</title>
      <link>https://github.com/pytorch/pytorch/commit/b28cb43f5c00b10ab0dfd3f4c6d629e1f998497e</link>
      <description><![CDATA[<p>Intra-graph reordering pass on Inductor scheduler IR (based on #100762) (#108091)</p>
<p>This PR implements intra-graph communication reordering pass on Inductor scheduler IR, based on Horace's previous PR #100762.</p>
<p>Main algorithm:<br />
1. Greedily moves waits as late as possible (i.e. until we reach a use)<br />
2. Greedily moves comms as early as possible (i.e. until we reach an input)<br />
3. Move computes following simple heuristics to improve overlap.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108091<br />
Approved by: https://github.com/Chillee, https://github.com/wanchaol</p>]]></description>
      <pubDate>Sat, 14 Oct 2023 06:51:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b28cb43f5c00b10ab0dfd3f4c6d629e1f998497e</guid>
    </item>
    <item>
      <title>[inductor] Allow backend compiler to skip (#111153)</title>
      <link>https://github.com/pytorch/pytorch/commit/0013611c8129c4481852e25a7b0813d631399d9f</link>
      <description><![CDATA[<p>[inductor] Allow backend compiler to skip (#111153)</p>
<p>Summary:<br />
Sometimes the backend compiler can encounter a transient failure (in<br />
our case, a remote build service infrequently hits a hiccup).  We'd rather run<br />
eager than fail the training job.</p>
<p>Test Plan:<br />
Inject an exception in the RE path and run:<br />
<code>buck2 run @//mode/{opt,inplace} //caffe2/test/inductor:smoke</code></p>
<p>Differential Revision: D50234516</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111153<br />
Approved by: https://github.com/ezyang, https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 13 Oct 2023 18:44:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0013611c8129c4481852e25a7b0813d631399d9f</guid>
    </item>
    <item>
      <title>[inductor] Adding a way to force fusion of int_mm with mul (#111125)</title>
      <link>https://github.com/pytorch/pytorch/commit/f4297576e63e4110f6bdf2522ae6a5fb4c7f3816</link>
      <description><![CDATA[<p>[inductor] Adding a way to force fusion of int_mm with mul (#111125)</p>
<p>Summary: When doing quantization int_mm -&gt; mul or int_mm -&gt; mul -&gt;<br />
to(dtype) is an extremely common op pattern which is currently not<br />
handled well by inductor. Ideally, since the output of<br />
int_mm has dtype int32 we'd prefer to only realize a smaller dtype like<br />
bf16 or float16. Currently inductor doesn't have a way to force this, in<br />
many cases the mul gets fused with a bunch of subsequent pointwise<br />
ops from the dequant creating an increase in memory overhead and a general<br />
slowdown compared to the fused version.</p>
<p>Theoretically with better control of/smarter inductor fusion, this could be something we get for free, at which point these changes can be removed.</p>
<p>Test Plan: python test/inductor/test_pattern_matcher.py -k<br />
"int_mm_mul"</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111125<br />
Approved by: https://github.com/jansel, https://github.com/cpuhrsch</p>]]></description>
      <pubDate>Fri, 13 Oct 2023 15:37:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f4297576e63e4110f6bdf2522ae6a5fb4c7f3816</guid>
    </item>
    <item>
      <title>[CI] Add auto label rule for torch/_export (#111181)</title>
      <link>https://github.com/pytorch/pytorch/commit/058cb70ad9f23c6d6f718844966bdb3a83728df7</link>
      <description><![CDATA[<p>[CI] Add auto label rule for torch/_export (#111181)</p>
<p>Summary: Auto label all torch/_export changes with ciflow/inductor to trigger AOTInductor tests.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/111181<br />
Approved by: https://github.com/angelayi</p>]]></description>
      <pubDate>Fri, 13 Oct 2023 12:07:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/058cb70ad9f23c6d6f718844966bdb3a83728df7</guid>
    </item>
    <item>
      <title>Revert "AOTAutograd: Go down inference path if no outputs require grad (#111011)"</title>
      <link>https://github.com/pytorch/pytorch/commit/cf6b1cdf6ac74d375b0787bd8f9463cb3a53b0e5</link>
      <description><![CDATA[<p>Revert "AOTAutograd: Go down inference path if no outputs require grad (#111011)"</p>
<p>This reverts commit ded5ee75ac51af1614cc79cd9c6f76524f10c3d8.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/111011 on behalf of https://github.com/kit1980 due to broke internal aotinductor tests with inference_mode (<a href="https://github.com/pytorch/pytorch/pull/111011#issuecomment-1762056233">comment</a>)</p>]]></description>
      <pubDate>Fri, 13 Oct 2023 11:11:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cf6b1cdf6ac74d375b0787bd8f9463cb3a53b0e5</guid>
    </item>
    <item>
      <title>[inductor] Implement Fx graph caching to improve warm compilation time. (#103453)</title>
      <link>https://github.com/pytorch/pytorch/commit/0dfa35457001f380468dba7912ed5acac109ac64</link>
      <description><![CDATA[<p>[inductor] Implement Fx graph caching to improve warm compilation time. (#103453)</p>
<p>Summary: Implement an on-disk cache to save and reuse compiled FX Graphs. This implementation does not handle tensors with symbolic shapes. This needs to be done in a follow-up PR.</p>
<p>Test Plan:<br />
* New unit tests exercising saving and load from the cache.<br />
* New unit tests to exercise the cache key calculations.<br />
* Ran several benchmarks to see cache hit and resulting compilation times.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50255289">D50255289</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/103453<br />
Approved by: https://github.com/eellison, https://github.com/Chillee</p>]]></description>
      <pubDate>Fri, 13 Oct 2023 05:33:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0dfa35457001f380468dba7912ed5acac109ac64</guid>
    </item>
    <item>
      <title>[AOTInductor] Improve validation for C++ wrapper codegen (#111102)</title>
      <link>https://github.com/pytorch/pytorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f</link>
      <description><![CDATA[<p>[AOTInductor] Improve validation for C++ wrapper codegen (#111102)</p>
<p>It's a reimplementation of #111089</p>
<ol>
<li>When using fake inputs make sure they are on the same device as the original inputs.</li>
<li>Don't change the value of self.cpp_wrapper from True to False if can't generate a C++ wrapper, instead have a check and fail early to avoid producing Python code for C++ compiler.</li>
</ol>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111102<br />
Approved by: https://github.com/desertfire, https://github.com/jgong5, https://github.com/chunyuan-w</p>]]></description>
      <pubDate>Fri, 13 Oct 2023 00:46:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f</guid>
    </item>
    <item>
      <title>[logging] log exceptions when provided (#111164)</title>
      <link>https://github.com/pytorch/pytorch/commit/898482f1bfd93f94d0be32d9201cbb6a093ec272</link>
      <description><![CDATA[<p>[logging] log exceptions when provided (#111164)</p>
<p>This PR will cause logging.exception() to also dump the exception and stacktrace. Copied from https://github.com/python/cpython/blob/74723e11109a320e628898817ab449b3dad9ee96/Lib/logging/<strong>init</strong>.py#L707-L711</p>
<p>repro:</p>
<details>

```python
import torch
import torch._inductor.config

torch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = "runtime_error"

def fn(x, y):
    return (x @ y).relu()

x, y = [torch.rand((16, 16), device='cuda') for _ in range (2)]
torch.compile(fn)(x, y)
```
run with TORCHDYNAMO_REPRO_AFTER=aot TORCHDYNAMO_REPRO_LEVEL=4

</details>

<p>before:<br />
<code>...
[2023-10-12 14:18:52,902] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph.</code></p>
<p>now:<br />
<code>...
[2023-10-12 14:18:52,902] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph.
Traceback (most recent call last):
  File "/data/users/dberard/scripts/relu_accuracy_issue.py", line 10, in &lt;module&gt;
    torch.compile(fn)(x, y)
...</code><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/111164<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Thu, 12 Oct 2023 19:52:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/898482f1bfd93f94d0be32d9201cbb6a093ec272</guid>
    </item>
    <item>
      <title>[PyTorch] -DNDEBUG in inductor codecache builds (#110876)</title>
      <link>https://github.com/pytorch/pytorch/commit/b85f8482331e82176b6656991559189b4587b1cb</link>
      <description><![CDATA[<p>[PyTorch] -DNDEBUG in inductor codecache builds (#110876)</p>
<p>Things like TORCH_INTERNAL_ASSERT_DEBUG_ONLY care about this!</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49972742/">D49972742</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110876<br />
Approved by: https://github.com/chenyang78, https://github.com/jansel, https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 12 Oct 2023 18:16:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b85f8482331e82176b6656991559189b4587b1cb</guid>
    </item>
    <item>
      <title>[inductor]: Better debugging of `can_fuse` decisions with `TORCH_LOGS=fusion` (#110415)</title>
      <link>https://github.com/pytorch/pytorch/commit/9c7f464eefebe90e26f7007b6df7a96aa35b096a</link>
      <description><![CDATA[<p>[inductor]: Better debugging of <code>can_fuse</code> decisions with <code>TORCH_LOGS=fusion</code> (#110415)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/110393</p>
<p>Example logs (for adagrad on main). In this case, it clearly identifies device mismatch as a potential red flag, which is indeed the obstacle to adagrad's successful fusion. (see: https://github.com/pytorch/pytorch/pull/110339)<br />
```<br />
[2023-10-03 21:50:24,084] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] ===== attempting fusion (1/10): 18 nodes =====<br />
[2023-10-03 21:50:24,084] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (foreach:3): candidate consumer has no dep in any foreach producer<br />
[2023-10-03 21:50:24,084] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (foreach:3): candidate consumer has no dep in any foreach producer<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (foreach:3): candidate consumer has no dep in any foreach producer<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (foreach:3): candidate consumer has no dep in any foreach producer<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (foreach:3): candidate consumer has no dep in any foreach producer<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (foreach:3): candidate consumer has no dep in any foreach producer<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (foreach:3): candidate consumer has no dep in any foreach producer<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (foreach:3): candidate consumer has no dep in any foreach producer<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] 13 possible fusions:<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (ForeachKernelSchedulerNode(nodes=buf0_buf1_buf2_buf3), ForeachKernelSchedulerNode(nodes=buf4_buf5_buf6_buf7))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (ForeachKernelSchedulerNode(nodes=buf4_buf5_buf6_buf7), SchedulerNode(name='buf8'))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (ForeachKernelSchedulerNode(nodes=buf4_buf5_buf6_buf7), SchedulerNode(name='buf10'))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (ForeachKernelSchedulerNode(nodes=buf0_buf1_buf2_buf3), SchedulerNode(name='buf12'))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (ForeachKernelSchedulerNode(nodes=buf0_buf1_buf2_buf3), SchedulerNode(name='buf14'))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (ForeachKernelSchedulerNode(nodes=buf4_buf5_buf6_buf7), SchedulerNode(name='buf9'))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (ForeachKernelSchedulerNode(nodes=buf4_buf5_buf6_buf7), SchedulerNode(name='buf11'))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (ForeachKernelSchedulerNode(nodes=buf0_buf1_buf2_buf3), SchedulerNode(name='buf13'))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (ForeachKernelSchedulerNode(nodes=buf0_buf1_buf2_buf3), SchedulerNode(name='buf15'))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (SchedulerNode(name='buf25'), SchedulerNode(name='buf33'))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (SchedulerNode(name='buf43'), SchedulerNode(name='buf51'))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (SchedulerNode(name='buf34'), SchedulerNode(name='buf42'))<br />
[2023-10-03 21:50:24,085] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] (SchedulerNode(name='buf16'), SchedulerNode(name='buf24'))<br />
[2023-10-03 21:50:24,087] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] completed fusion round (1/10): fused 18 nodes into 5 nodes<br />
[2023-10-03 21:50:24,087] [0/0] torch._inductor.scheduler.__schedule: [DEBUG]<br />
[2023-10-03 21:50:24,087] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] ===== attempting fusion (2/10): 5 nodes =====<br />
[2023-10-03 21:50:24,087] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,087] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,087] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,087] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] cannot fuse (7): device mismatch (node1: cuda:0, node2: cpu)<br />
[2023-10-03 21:50:24,087] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] 0 possible fusions:<br />
[2023-10-03 21:50:24,087] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] completed fusion round (2/10): fused 5 nodes into 5 nodes<br />
[2023-10-03 21:50:24,087] [0/0] torch._inductor.scheduler.__schedule: [DEBUG]<br />
[2023-10-03 21:50:24,087] [0/0] torch._inductor.scheduler.__schedule: [DEBUG] ===== fusion complete (2 iterations) =====</p>
<p>```</p>
<p>CC @jansel @ngimel @mlazos @shunting314 @peterbell10  as code owners</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110415<br />
Approved by: https://github.com/mlazos</p>]]></description>
      <pubDate>Thu, 12 Oct 2023 16:36:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9c7f464eefebe90e26f7007b6df7a96aa35b096a</guid>
    </item>
    <item>
      <title>[aot_inductor] add a test with AOTInductor + TorchScript (#111124)</title>
      <link>https://github.com/pytorch/pytorch/commit/6748a14a71f235501cd050e15d1810f7a0a75dbd</link>
      <description><![CDATA[<p>[aot_inductor] add a test with AOTInductor + TorchScript (#111124)</p>
<p>This test may be of a reference for using AOTInductor with<br />
TorchScript.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/111124<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 12 Oct 2023 11:29:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6748a14a71f235501cd050e15d1810f7a0a75dbd</guid>
    </item>
    <item>
      <title>Revert "[inductor] Implement Fx graph caching to improve warm compilation time. (#103453)"</title>
      <link>https://github.com/pytorch/pytorch/commit/7fbfa4e0203b43c42390feb1743829098ffb6ed1</link>
      <description><![CDATA[<p>Revert "[inductor] Implement Fx graph caching to improve warm compilation time. (#103453)"</p>
<p>This reverts commit fc1105b2827ee2febc85a3c353470edfd70a66ed.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/103453 on behalf of https://github.com/kit1980 due to Same issue unfortunately, the newly added test fails on internal builds (<a href="https://github.com/pytorch/pytorch/pull/103453#issuecomment-1760202365">comment</a>)</p>]]></description>
      <pubDate>Thu, 12 Oct 2023 10:54:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7fbfa4e0203b43c42390feb1743829098ffb6ed1</guid>
    </item>
    <item>
      <title>Added decorator `skipRocmIfTorchInductor` and skipped failing tests (#107760)</title>
      <link>https://github.com/pytorch/pytorch/commit/53a9ac534c777047abc2b6b293f238865592291e</link>
      <description><![CDATA[<p>Added decorator <code>skipRocmIfTorchInductor</code> and skipped failing tests (#107760)</p>
<p>This PR adds a skip decorator which will disable tests in CI for ROCm inductor workflow. This new workflow will be coming in via https://github.com/pytorch/pytorch/pull/110544</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107760<br />
Approved by: https://github.com/jataylo, https://github.com/pruthvistony, https://github.com/atalman</p>]]></description>
      <pubDate>Thu, 12 Oct 2023 08:00:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/53a9ac534c777047abc2b6b293f238865592291e</guid>
    </item>
    <item>
      <title>[Inductor] support channel last for xpu conv in inductor layout opt path (#111018)</title>
      <link>https://github.com/pytorch/pytorch/commit/918054f42239c6ddd3367a3ada54f9cc0b37868c</link>
      <description><![CDATA[<p>[Inductor] support channel last for xpu conv in inductor layout opt path (#111018)</p>
<h1>Motivation</h1>
<p>support xpu channel last for inductor layout optimization path.<br />
Currently, <code>_conv_determine_backend_memory_format</code> always returns torch.contiguous_format for XPU conv.</p>
<h1>Solution</h1>
<p>Add xpu channel last detection stragey in <code>determine_backend_memory_format</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111018<br />
Approved by: https://github.com/jansel, https://github.com/eellison, https://github.com/EikanWang</p>]]></description>
      <pubDate>Thu, 12 Oct 2023 07:13:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/918054f42239c6ddd3367a3ada54f9cc0b37868c</guid>
    </item>
    <item>
      <title>[aotinductor] Fail models temporarily (#111100)</title>
      <link>https://github.com/pytorch/pytorch/commit/577e3dff884b489eb6db9512ed83640bad52cfa5</link>
      <description><![CDATA[<p>[aotinductor] Fail models temporarily (#111100)</p>
<p>Temporarily mark these models as fail. Failures are due to https://github.com/pytorch/pytorch/pull/111030 which is needed for ExecuTorch's release so it can't be reverted. Will forward fix the failures.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111100<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Wed, 11 Oct 2023 16:48:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/577e3dff884b489eb6db9512ed83640bad52cfa5</guid>
    </item>
    <item>
      <title>torch.compile DTensor E2E (#105236)</title>
      <link>https://github.com/pytorch/pytorch/commit/4d29b40299e9af1e4287b48024b6ef28b7cbf738</link>
      <description><![CDATA[<p>torch.compile DTensor E2E (#105236)</p>
<p>This PR updates DTensor to support torch.compile</p>
<p>Cool stuff: there are some new tests in <code>test_dtensor.py</code> that show both the forward and backward graphs that we can send to inductor, when running a matmul with DTensor's. In particular, for this user code:<br />
<code>def fn(x, y):
            dt = DTensor.from_local(x.reshape(2, 4), mesh, [Shard(0)], run_check=False)
            dt2 = DTensor.from_local(y.reshape(4, 2), mesh, [Shard(1)], run_check=False)
            dt_out = torch.matmul(dt, dt2)
            dt_out_redistribute = dt_out.redistribute(mesh, [Replicate()])
            return dt_out.to_local()</code></p>
<p>We generate the following fw and backward graphs.</p>
<p>Forward graph:<br />
<code>def forward(self, primals_1, primals_2):
    view = torch.ops.aten.view.default(primals_1, [2, 4]);  primals_1 = None
    _to_copy = torch.ops.aten._to_copy.default(view, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  view = None
    detach = torch.ops.aten.detach.default(_to_copy);  _to_copy = None
    detach_1 = torch.ops.aten.detach.default(detach);  detach = None
    view_1 = torch.ops.aten.view.default(primals_2, [4, 2]);  primals_2 = None
    _to_copy_1 = torch.ops.aten._to_copy.default(view_1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  view_1 = None
    detach_2 = torch.ops.aten.detach.default(_to_copy_1);  _to_copy_1 = None
    detach_3 = torch.ops.aten.detach.default(detach_2);  detach_2 = None
    detach_4 = torch.ops.aten.detach.default(detach_1)
    all_gather_into_tensor = torch.ops.c10d_functional.all_gather_into_tensor.default(detach_3, 'ptd:0', [0, 1], 2)
    wait_tensor = torch.ops.c10d_functional.wait_tensor.default(all_gather_into_tensor);  all_gather_into_tensor = None
    split = torch.ops.aten.split.Tensor(wait_tensor, 4);  wait_tensor = None
    getitem = split[0]
    getitem_1 = split[1];  split = None
    cat = torch.ops.aten.cat.default([getitem, getitem_1], 1);  getitem = getitem_1 = None
    detach_5 = torch.ops.aten.detach.default(cat);  cat = None
    mm = torch.ops.aten.mm.default(detach_4, detach_5);  detach_4 = detach_5 = None
    detach_6 = torch.ops.aten.detach.default(mm);  mm = None
    detach_9 = torch.ops.aten.detach.default(detach_6);  detach_6 = None
    detach_10 = torch.ops.aten.detach.default(detach_9);  detach_9 = None
    t = torch.ops.aten.t.default(detach_1);  detach_1 = None
    detach_13 = torch.ops.aten.detach.default(t);  t = None
    t_1 = torch.ops.aten.t.default(detach_3);  detach_3 = None
    detach_15 = torch.ops.aten.detach.default(t_1);  t_1 = None
    clone = torch.ops.aten.clone.default(detach_15, memory_format = torch.contiguous_format);  detach_15 = None
    return [detach_10, detach_13, clone]</code></p>
<p>Backward graph:<br />
<code>def forward(self, detach_13, clone, tangents_1):
    detach_11 = torch.ops.aten.detach.default(tangents_1);  tangents_1 = None
    detach_12 = torch.ops.aten.detach.default(detach_11);  detach_11 = None
    mm_1 = torch.ops.aten.mm.default(detach_13, detach_12);  detach_13 = None
    detach_14 = torch.ops.aten.detach.default(mm_1);  mm_1 = None
    detach_16 = torch.ops.aten.detach.default(detach_12);  detach_12 = None
    all_gather_into_tensor_2 = torch.ops.c10d_functional.all_gather_into_tensor.default(clone, 'ptd:0', [0, 1], 2);  clone = None
    wait_tensor_2 = torch.ops.c10d_functional.wait_tensor.default(all_gather_into_tensor_2);
    detach_17 = torch.ops.aten.detach.default(wait_tensor_2);  wait_tensor_2 = None
    mm_2 = torch.ops.aten.mm.default(detach_16, detach_17);  detach_16 = detach_17 = None
    detach_18 = torch.ops.aten.detach.default(mm_2);  mm_2 = None
    split_1 = torch.ops.aten.split.Tensor(detach_14, 2, 1);  detach_14 = None
    getitem_2 = split_1[0]
    getitem_3 = split_1[1];  split_1 = None
    cat_1 = torch.ops.aten.cat.default([getitem_2, getitem_3]);  getitem_2 = getitem_3 = None
    reduce_scatter_tensor = torch.ops.c10d_functional.reduce_scatter_tensor.default(cat_1, 'SUM', 'ptd:0', [0, 1], 2);  cat_1 = None
    wait_tensor_3 = torch.ops.c10d_functional.wait_tensor.default(reduce_scatter_tensor);  reduce_scatter_tensor = None
    detach_19 = torch.ops.aten.detach.default(wait_tensor_3);  wait_tensor_3 = None
    detach_20 = torch.ops.aten.detach.default(detach_19);  detach_19 = None
    detach_21 = torch.ops.aten.detach.default(detach_20);  detach_20 = None
    detach_22 = torch.ops.aten.detach.default(detach_21);  detach_21 = None
    _to_copy_2 = torch.ops.aten._to_copy.default(detach_22, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  detach_22 = None
    view_2 = torch.ops.aten.view.default(_to_copy_2, [8]);  _to_copy_2 = None
    detach_23 = torch.ops.aten.detach.default(detach_18);  detach_18 = None
    detach_24 = torch.ops.aten.detach.default(detach_23);  detach_23 = None
    _to_copy_3 = torch.ops.aten._to_copy.default(detach_24, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  detach_24 = None
    view_3 = torch.ops.aten.view.default(_to_copy_3, [8]);  _to_copy_3 = None
    return [view_3, view_2]</code></p>
<p>Some of the stuff in this graph looks kinda of silly though (e.g. an unnecessary split() + cat(), and all the extra detach() calls).</p>
<p>Stuff that's broken:<br />
- functionalization is pretty horribly broken. In particular, the original strategy I used in this stack was to have functionalization run <strong>above</strong> subclass desugaring. But that doesn't play well with with the way we want to compile DTensor. DTensor has a few API's like <code>.redistribute()</code>, <code>.to_local()</code>, and the <code>DTensor()</code> constructor, that we want to put directly into the graph so that we can compile them (e.g. redistribute() will desugar into collective ops). Doing this requires functionalization to run <strong>underneath</strong> the subclass though. I hacked around this for now, by forcing these functions to run functionalization first if they need to.<br />
- the backward test that I have is... wrong. The backward graph that we trace out looks kind of reasonable, but it gives incorrect gradients on one of the two inputs. This needs further debugging (presumably we should be able to stare at the graph and identify which part of it is wrong?).</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/105236<br />
Approved by: https://github.com/wanchaol</p>]]></description>
      <pubDate>Wed, 11 Oct 2023 13:55:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4d29b40299e9af1e4287b48024b6ef28b7cbf738</guid>
    </item>
    <item>
      <title>Implement tensor slice in inductor to stop falling back for aten.index (#111015)</title>
      <link>https://github.com/pytorch/pytorch/commit/fd4ba806f6cdf516c3e8eda8a95de1df3ea8dcbe</link>
      <description><![CDATA[<p>Implement tensor slice in inductor to stop falling back for aten.index (#111015)</p>
<p>Fixes #110711</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/111015<br />
Approved by: https://github.com/Chillee</p>]]></description>
      <pubDate>Wed, 11 Oct 2023 09:53:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fd4ba806f6cdf516c3e8eda8a95de1df3ea8dcbe</guid>
    </item>
    <item>
      <title>[aotinductor] Add both cpu and cuda tests for the AOTInductor cpp test (#110920)</title>
      <link>https://github.com/pytorch/pytorch/commit/86619c9c9d0b0fda14f832449c4b7556d10909ea</link>
      <description><![CDATA[<p>[aotinductor] Add both cpu and cuda tests for the AOTInductor cpp test (#110920)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110920<br />
Approved by: https://github.com/chenyang78<br />
ghstack dependencies: #110652, #110891</p>]]></description>
      <pubDate>Wed, 11 Oct 2023 07:58:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/86619c9c9d0b0fda14f832449c4b7556d10909ea</guid>
    </item>
    <item>
      <title>[aotinductor] Add AOTIModelRunner as a utility class (#110891)</title>
      <link>https://github.com/pytorch/pytorch/commit/3058700f7f91170a7f34ea2dd1fa0ae32cc901b4</link>
      <description><![CDATA[<p>[aotinductor] Add AOTIModelRunner as a utility class (#110891)</p>
<p>Summary: Introduce a utility class AOTIModelRunner to take care of running an AOTInductor compiled model. It does things like dlopen a model, initialize the model container, setup inputs and outputs, and destroy the model container.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110891<br />
Approved by: https://github.com/chenyang78<br />
ghstack dependencies: #110652</p>]]></description>
      <pubDate>Wed, 11 Oct 2023 07:58:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3058700f7f91170a7f34ea2dd1fa0ae32cc901b4</guid>
    </item>
    <item>
      <title>[inductor] Implement Fx graph caching to improve warm compilation time. (#103453)</title>
      <link>https://github.com/pytorch/pytorch/commit/fc1105b2827ee2febc85a3c353470edfd70a66ed</link>
      <description><![CDATA[<p>[inductor] Implement Fx graph caching to improve warm compilation time. (#103453)</p>
<p>Summary: Implement an on-disk cache to save and reuse compiled FX Graphs. This implementation does not handle tensors with symbolic shapes. This needs to be done in a follow-up PR.</p>
<p>Test Plan:<br />
* New unit tests exercising saving and load from the cache.<br />
* New unit tests to exercise the cache key calculations.<br />
* Ran several benchmarks to see cache hit and resulting compilation times.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/103453<br />
Approved by: https://github.com/eellison, https://github.com/Chillee</p>]]></description>
      <pubDate>Wed, 11 Oct 2023 06:39:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fc1105b2827ee2febc85a3c353470edfd70a66ed</guid>
    </item>
    <item>
      <title>[aotinductor] Add a perf smoke test for AOTInductor (#110972)</title>
      <link>https://github.com/pytorch/pytorch/commit/4abfa228123f0d0822150d35a80acb908de4b1b0</link>
      <description><![CDATA[<p>[aotinductor] Add a perf smoke test for AOTInductor (#110972)</p>
<p>Summary: To prevent perf regression like the one caused by #110510</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110972<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Wed, 11 Oct 2023 05:30:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4abfa228123f0d0822150d35a80acb908de4b1b0</guid>
    </item>
    <item>
      <title>feat(inductor): fx graph debug should display device (#110346)</title>
      <link>https://github.com/pytorch/pytorch/commit/79212430df958d594feaeb8a45b91ea6419cf6b7</link>
      <description><![CDATA[<p>feat(inductor): fx graph debug should display device (#110346)</p>
<p>Device mismatch issues are root cause of: https://github.com/pytorch/pytorch/issues/107006, hence make device-related scheduling issues easier to diagnose.<br />
Also format single-kwarg graphs to be more concise</p>
<p>Example rendering:<br />
<img alt="image" src="https://github.com/pytorch/pytorch/assets/9093549/1b59a994-f2df-45c9-8cb7-37eb3ba12654" /></p>
<p>CC code owners: @ngimel @jansel @shunting314 @mlazos @peterbell10</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110346<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Tue, 10 Oct 2023 16:34:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/79212430df958d594feaeb8a45b91ea6419cf6b7</guid>
    </item>
    <item>
      <title>[Reland] [Inductor] Break the loop fusion when node2 depends on node1 mutations (#110677)</title>
      <link>https://github.com/pytorch/pytorch/commit/a11d4a8378323a1f611fbb21f7f3ad94be783fec</link>
      <description><![CDATA[<p>[Reland] [Inductor] Break the loop fusion when node2 depends on node1 mutations (#110677)</p>
<p>Reland PR https://github.com/pytorch/pytorch/pull/109172 which has been reverted in https://github.com/pytorch/pytorch/pull/110622</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D50097373">D50097373</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110677<br />
Approved by: https://github.com/jgong5, https://github.com/ezyang</p>]]></description>
      <pubDate>Tue, 10 Oct 2023 16:26:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a11d4a8378323a1f611fbb21f7f3ad94be783fec</guid>
    </item>
    <item>
      <title>[core ATen IR] Add decompositions for max, min, var_mean (#110906)</title>
      <link>https://github.com/pytorch/pytorch/commit/9606cda64e97210cfcca07110ef4872cedc5a1d9</link>
      <description><![CDATA[<p>[core ATen IR] Add decompositions for max, min, var_mean (#110906)</p>
<h2>Context</h2>
<p>Add decompositions for <code>aten.max</code>, <code>aten.min</code>, and <code>aten.var_mean</code>. These operators follow a pattern of returning a tuple of outputs from two component operators:</p>
<p><code>aten.max(x) -&gt; return aten.amax(x), aten.argmax(x)
aten.min(x) -&gt; return aten.amin(x), aten.argmin(x)
aten.var_mean(x) -&gt; return aten.var(x), aten.mean(x)</code></p>
<p>For <code>var_mean</code>, the <code>refs</code> implementation was doing something similar, so I changed it to call <code>torch.</code> ops instead like was done for other <code>refs</code> implementations previously. cc: @peterbell10 @lezcano</p>
<p>Note that Inductor lowers all these directly, so they are excluded from the Inductor decomp table.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110906<br />
Approved by: https://github.com/manuelcandales</p>]]></description>
      <pubDate>Tue, 10 Oct 2023 16:06:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9606cda64e97210cfcca07110ef4872cedc5a1d9</guid>
    </item>
    <item>
      <title>Revert "[inductor] Implement Fx graph caching to improve warm compilation time. (#103453)"</title>
      <link>https://github.com/pytorch/pytorch/commit/3100d3e661f5e21ae1936ddfa420adfbd23e887e</link>
      <description><![CDATA[<p>Revert "[inductor] Implement Fx graph caching to improve warm compilation time. (#103453)"</p>
<p>This reverts commit 8a8668e1aeac8d1726ac746372f5a93262994f62.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/103453 on behalf of https://github.com/kit1980 due to The newly added test fails on internal builds (<a href="https://github.com/pytorch/pytorch/pull/103453#issuecomment-1756449919">comment</a>)</p>]]></description>
      <pubDate>Tue, 10 Oct 2023 15:21:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3100d3e661f5e21ae1936ddfa420adfbd23e887e</guid>
    </item>
    <item>
      <title>[inductor cpp] use c10::bit_cast to avoid violating strict-aliasing (#110809)</title>
      <link>https://github.com/pytorch/pytorch/commit/8bc04f46fe8e69188fa46f1611b46788a7d4824d</link>
      <description><![CDATA[<p>[inductor cpp] use c10::bit_cast to avoid violating strict-aliasing (#110809)</p>
<p>Fix https://github.com/pytorch/pytorch/issues/110807</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110809<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 10 Oct 2023 03:16:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8bc04f46fe8e69188fa46f1611b46788a7d4824d</guid>
    </item>
    <item>
      <title>Revise def of contiguity in bmm (#110811)</title>
      <link>https://github.com/pytorch/pytorch/commit/8820dda943ac471933d0d8cc4b640c78181c2a86</link>
      <description><![CDATA[<p>Revise def of contiguity in bmm (#110811)</p>
<p>Fixes #108754.</p>
<p><code>hf_T5_generate</code> would encounter a regression when calling <code>extern_kernels.bmm</code>, if one input is <code>reinterpret_tensor(buf2, (8, 1, 64), (64, 0, 1))</code> rather than <code>reinterpret_tensor(buf2, (8, 1, 64), (64, 512, 1), 0)</code>. As @jgong5 mentioned in comment, in fact the two tensors are equivalent: The stride doesn't matter when the corresponding size is 1.</p>
<p>We revise the definition of contiguity in <code>bmm</code> to add the above situation as a contiguous case. Thus, when stride equals to 0, <code>extern_kernels.bmm</code> could still use <code>gemm</code> of MKL to gain the performance.</p>
<p>Speedup of <code>hf_T5_generate</code> is <strong>1.343x</strong> now and <strong>1.138x</strong> before, with script <code>bash inductor_single_test.sh multiple inference performance torchbench hf_T5_generate float32 first dynamic default 0</code>.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110811<br />
Approved by: https://github.com/jgong5, https://github.com/lezcano, https://github.com/Chillee</p>]]></description>
      <pubDate>Mon, 09 Oct 2023 22:48:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8820dda943ac471933d0d8cc4b640c78181c2a86</guid>
    </item>
    <item>
      <title>Revert "[Inductor] Disallow OpOverloadPacket in ir.FallbackKernel (#110567)"</title>
      <link>https://github.com/pytorch/pytorch/commit/19ecb5d0d52eb055ce18f8935fa3718736368c93</link>
      <description><![CDATA[<p>Revert "[Inductor] Disallow OpOverloadPacket in ir.FallbackKernel (#110567)"</p>
<p>This reverts commit 37a02659921490d85b2b0712ad52b924e0c431cd.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/110567 on behalf of https://github.com/kit1980 due to breaking internal builds, see D50091340 (<a href="https://github.com/pytorch/pytorch/pull/110567#issuecomment-1754308982">comment</a>)</p>]]></description>
      <pubDate>Mon, 09 Oct 2023 19:49:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/19ecb5d0d52eb055ce18f8935fa3718736368c93</guid>
    </item>
    <item>
      <title>[inductor] Re-enable more fixed tests (#110798)</title>
      <link>https://github.com/pytorch/pytorch/commit/ddb0c265110cfd053009dee34cbc5151bed8cddc</link>
      <description><![CDATA[<p>[inductor] Re-enable more fixed tests (#110798)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110798<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Sun, 08 Oct 2023 20:36:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ddb0c265110cfd053009dee34cbc5151bed8cddc</guid>
    </item>
    <item>
      <title>[inductor] Implement Fx graph caching to improve warm compilation time. (#103453)</title>
      <link>https://github.com/pytorch/pytorch/commit/8a8668e1aeac8d1726ac746372f5a93262994f62</link>
      <description><![CDATA[<p>[inductor] Implement Fx graph caching to improve warm compilation time. (#103453)</p>
<p>Summary: Implement an on-disk cache to save and reuse compiled FX Graphs. This implementation does not handle tensors with symbolic shapes. This needs to be done in a follow-up PR.</p>
<p>Test Plan:<br />
* New unit tests exercising saving and load from the cache.<br />
* New unit tests to exercise the cache key calculations.<br />
* Ran several benchmarks to see cache hit and resulting compilation times.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/103453<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Sun, 08 Oct 2023 12:32:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8a8668e1aeac8d1726ac746372f5a93262994f62</guid>
    </item>
    <item>
      <title>Update AOTInductor compile logic for CPU backend for Meta internal env (#110729)</title>
      <link>https://github.com/pytorch/pytorch/commit/5ef490f7369e3bc70e15098017c82f6d4342ef2d</link>
      <description><![CDATA[<p>Update AOTInductor compile logic for CPU backend for Meta internal env (#110729)</p>
<p>Reviewed By: muchulee8, chenyang78</p>
<p>Differential Revision: D49944410</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110729<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Sun, 08 Oct 2023 11:48:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5ef490f7369e3bc70e15098017c82f6d4342ef2d</guid>
    </item>
    <item>
      <title>[AOTInductor][ez] Fix FallbackKernel.codegen() (#110777)</title>
      <link>https://github.com/pytorch/pytorch/commit/a119efe9c75e34f2a4b819e453befba8f8403936</link>
      <description><![CDATA[<p>[AOTInductor][ez] Fix FallbackKernel.codegen() (#110777)</p>
<p>Summary: ProxyExecutor should only used in fbcode for cpp codegen.</p>
<p>Test Plan: Existing CIs</p>
<p>Differential Revision: D50048488</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110777<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Sat, 07 Oct 2023 07:29:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a119efe9c75e34f2a4b819e453befba8f8403936</guid>
    </item>
    <item>
      <title>[inductor] Add AOTI ABI shim function for torch.nonzero (#110766)</title>
      <link>https://github.com/pytorch/pytorch/commit/98b79e9488c115916bd30915078039f3714d3022</link>
      <description><![CDATA[<p>[inductor] Add AOTI ABI shim function for torch.nonzero (#110766)</p>
<p>Summary: <code>torch.nonzero</code> doesn't have inductor lowering (yet). To invoke the operator in AOT Inductor's ABI compatibility mode we need a dedicated shim function.</p>
<p>Test Plan:</p>
<p>```<br />
$ python test/inductor/test_aot_inductor.py -k test_zero_grid_with_unbacked_symbols<br />
...</p>
<hr />
<p>Ran 4 tests in 78.650s</p>
<p>OK<br />
```</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110766<br />
Approved by: https://github.com/chenyang78<br />
ghstack dependencies: #110713, #110745, #110764</p>]]></description>
      <pubDate>Sat, 07 Oct 2023 00:32:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/98b79e9488c115916bd30915078039f3714d3022</guid>
    </item>
    <item>
      <title>[inductor] Add size, stride, storage_offset to RAIIAtenTensorHandle (#110764)</title>
      <link>https://github.com/pytorch/pytorch/commit/13a2f42635eb502d76b28a338b757d123807a61f</link>
      <description><![CDATA[<p>[inductor] Add size, stride, storage_offset to RAIIAtenTensorHandle (#110764)</p>
<p>Summary: For unbacked SymInts, the C++ wrapper codegen can generate expressions like <code>buf123.size()</code> or <code>.stride()</code> or <code>.storage_offset()</code>:</p>
<p>https://github.com/pytorch/pytorch/blob/7cc0020a80527207a1725e6d21ce7c326668fe0d/torch/_inductor/ir.py#L2504-L2520</p>
<p>Here we add corresponding methods to the <code>RAIIAtenTensorHandle</code> class so that the above codegen works in the ABI compatibility mode.</p>
<p>Test Plan: CI + the following PR</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110764<br />
Approved by: https://github.com/chenyang78<br />
ghstack dependencies: #110713, #110745</p>]]></description>
      <pubDate>Sat, 07 Oct 2023 00:26:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/13a2f42635eb502d76b28a338b757d123807a61f</guid>
    </item>
    <item>
      <title>[inductor] Add AOTI ABI shim function for repeat_interleave.Tensor (#110745)</title>
      <link>https://github.com/pytorch/pytorch/commit/abb00f66d8075db9af5adffa4e60154e43075b22</link>
      <description><![CDATA[<p>[inductor] Add AOTI ABI shim function for repeat_interleave.Tensor (#110745)</p>
<p>Summary: <code>repeat_interleave.Tensor</code> doesn't have inductor lowering. To invoke the operator in AOT Inductor's ABI compatibility mode we need a dedicated shim function.</p>
<p>Test Plan:</p>
<p>```<br />
$ python test/inductor/test_aot_inductor.py -k test_repeat_interleave<br />
...</p>
<hr />
<p>Ran 4 tests in 70.526s</p>
<p>OK<br />
```</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110745<br />
Approved by: https://github.com/chenyang78<br />
ghstack dependencies: #110713</p>]]></description>
      <pubDate>Sat, 07 Oct 2023 00:18:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/abb00f66d8075db9af5adffa4e60154e43075b22</guid>
    </item>
    <item>
      <title>[inductor] added a config to always add tensor constants (#110491)</title>
      <link>https://github.com/pytorch/pytorch/commit/432df71820e61b7534c798fcc30337672f70649e</link>
      <description><![CDATA[<p>[inductor] added a config to always add tensor constants (#110491)</p>
<p>Summary:<br />
In some scenarios, we want to update constants at runtime.<br />
In such cases, we have to keep the original constants in<br />
the generated code without applying any constant-inlining<br />
optimizations.</p>
<p>This PR adds a config to force us to add tensor constants.</p>
<p>Differential Revision: D49895154</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110491<br />
Approved by: https://github.com/mikekgfb</p>]]></description>
      <pubDate>Fri, 06 Oct 2023 23:51:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/432df71820e61b7534c798fcc30337672f70649e</guid>
    </item>
    <item>
      <title>[AOTInductor] Change UpdateConstants to UpdateConstantsMap (#110576)</title>
      <link>https://github.com/pytorch/pytorch/commit/840e68301c0c5093d9361c211866a8cfead2b22b</link>
      <description><![CDATA[<p>[AOTInductor] Change UpdateConstants to UpdateConstantsMap (#110576)</p>
<p>Summary: Change name of UpdateConstants to UpdateConstantsMap</p>
<p>Test Plan:</p>
<p>Differential Revision: D49937744</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110576<br />
Approved by: https://github.com/chenyang78, https://github.com/khabinov</p>]]></description>
      <pubDate>Fri, 06 Oct 2023 23:36:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/840e68301c0c5093d9361c211866a8cfead2b22b</guid>
    </item>
    <item>
      <title>Enable typechecking in _inductor/ir.py (#110112)</title>
      <link>https://github.com/pytorch/pytorch/commit/c77dd684c99874d12d0106898ce7e046f9c7fc9b</link>
      <description><![CDATA[<p>Enable typechecking in _inductor/ir.py (#110112)</p>
<p>I used a bunch of ignore-type comments, mostly due to<br />
https://github.com/pytorch/pytorch/issues/109963.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110112<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Fri, 06 Oct 2023 20:19:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c77dd684c99874d12d0106898ce7e046f9c7fc9b</guid>
    </item>
    <item>
      <title>[Inductor] Allow matmul to have flexiable layout when we are not autotuning (#110726)</title>
      <link>https://github.com/pytorch/pytorch/commit/e8ef8bfdce6230b5f2a739c205a2abffac77e894</link>
      <description><![CDATA[<p>[Inductor] Allow matmul to have flexiable layout when we are not autotuning (#110726)</p>
<p>Fixes #102804</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110726<br />
Approved by: https://github.com/Chillee</p>]]></description>
      <pubDate>Fri, 06 Oct 2023 20:08:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e8ef8bfdce6230b5f2a739c205a2abffac77e894</guid>
    </item>
    <item>
      <title>[inductor] Add aoti_torch_dtype_bool to AOTI ABI shim (#110713)</title>
      <link>https://github.com/pytorch/pytorch/commit/2aa30643642df8068837a79afae7176be8f8bcab</link>
      <description><![CDATA[<p>[inductor] Add aoti_torch_dtype_bool to AOTI ABI shim (#110713)</p>
<p>Summary: ATT</p>
<p>Test Plan: CI</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110713<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire</p>]]></description>
      <pubDate>Fri, 06 Oct 2023 14:16:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2aa30643642df8068837a79afae7176be8f8bcab</guid>
    </item>
    <item>
      <title>Remove runtime assertions between export and AOT compilation (#110710)</title>
      <link>https://github.com/pytorch/pytorch/commit/f74937741eaa4a69cac1cdd705d3a89c68b2127c</link>
      <description><![CDATA[<p>Remove runtime assertions between export and AOT compilation (#110710)</p>
<p>Summary: The runtime assertions inserted in the <code>torch._export.export</code> by the <code>_AddRuntimeAssertionsForInlineConstraintsPass</code> lead to errors in AOT Inductor like #109884. In <code>torch._export.aot_compile</code> export and AOT compilation are run consecutively which would lead to the above issue if any assertions are inserted.</p>
<p>In this PR, we're adding a new parameter / flag to <code>torch._export.aot_compile</code>, <code>remove_runtime_assertions</code>, to remove the assertions inserted during export before AOT compilation. The flag is set to <code>False</code> for BC.</p>
<p>Additionally, we remove the flag <code>add_runtime_assertions_for_inline_constraints</code> recently added to <code>torch._dynamo.config</code>, as it can lead to undesirable <code>torch._export</code> behavior and is 's no longer required for the AOT Inductor testing purposes.</p>
<p>Test Plan: CI</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110710<br />
Approved by: https://github.com/zhxchen17, https://github.com/chenyang78</p>]]></description>
      <pubDate>Fri, 06 Oct 2023 13:09:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f74937741eaa4a69cac1cdd705d3a89c68b2127c</guid>
    </item>
    <item>
      <title>[Inductor] Disallow OpOverloadPacket in ir.FallbackKernel (#110567)</title>
      <link>https://github.com/pytorch/pytorch/commit/37a02659921490d85b2b0712ad52b924e0c431cd</link>
      <description><![CDATA[<p>[Inductor] Disallow OpOverloadPacket in ir.FallbackKernel (#110567)</p>
<p>In ABI compatible mode, We always need op_overload.schema for FallbackKernel.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110567<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 06 Oct 2023 11:20:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/37a02659921490d85b2b0712ad52b924e0c431cd</guid>
    </item>
    <item>
      <title>Revert tl.int1 casting change for ROCm to avoid hangs (#110531)</title>
      <link>https://github.com/pytorch/pytorch/commit/96f616a0547809f3846b8ba4264476576dc9c2ad</link>
      <description><![CDATA[<p>Revert tl.int1 casting change for ROCm to avoid hangs (#110531)</p>
<p>Seeing hangs on ROCm seemingly after this PR https://github.com/pytorch/pytorch/pull/110388<br />
https://ossci-raw-job-status.s3.amazonaws.com/log/17381916785<br />
<code>inductor/test_torchinductor_opinfo.py::TestInductorOpInfoCUDA::test_comprehensive_exp2_cuda_bool Command took &gt;30min, returning 124</code></p>
<p>Conditionalising out of this while we investigate.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110531<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Fri, 06 Oct 2023 00:53:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/96f616a0547809f3846b8ba4264476576dc9c2ad</guid>
    </item>
    <item>
      <title>perf(inductor): improve `Adam` compile times by shortcutting for loops (via `has_complex`) (#110607)</title>
      <link>https://github.com/pytorch/pytorch/commit/d279979102d8676c5ea09f76c32997ab9b9ca38d</link>
      <description><![CDATA[<p>perf(inductor): improve <code>Adam</code> compile times by shortcutting for loops (via <code>has_complex</code>) (#110607)</p>
<p>Adam part of: https://github.com/pytorch/pytorch/issues/110506</p>
<p>TODO:<br />
- If this approach is validated as a good one, it an also be applied to all other optimizers which convert <code>complex</code> via list comprehensions</p>
<h3>Results:</h3>
<p><code>NUM_PARAMS=200, foreach=True</code><br />
- main: dynamo: 43s, inductor: 31s, total: 74s<br />
- this PR: dynamo: 3.5s, inductor: 30s, total: 34s (dynamo speedup: 12.3x, overall speedup: 34s, 2.1x)</p>
<p><code>NUM_PARAMS=1000, foreach=True, has_complex shortcut</code>:</p>
<p>```<br />
<class 'torch.optim.adam.Adam'> {'lr': 0.01, 'foreach': True} torch.float32 TorchDynamo compilation metrics:<br />
Function                              Runtimes (s)</p>
<hr />
<p>_compile.<locals>.compile_inner       0.0329, 50.0806, 0.0041<br />
OutputGraph.call_user_compiler        44.9924<br />
```</p>
<p><code>NUM_PARAMS=1000, foreach=True</code>:<br />
```<br />
<class 'torch.optim.adam.Adam'> {'lr': 0.01, 'foreach': True} torch.float32 TorchDynamo compilation metrics:<br />
Function                              Runtimes (s)</p>
<hr />
<p>_compile.<locals>.compile_inner       0.0389, 58.6069, 0.0043<br />
OutputGraph.call_user_compiler        44.1425<br />
```</p>
<h3>Discussion</h3>
<ul>
<li><code>has_complex</code> shortcut provides additional 2x dynamo speedup. It is not necessary to achieve a significant overall speedup.</li>
</ul>
<p>CC: @janeyx99 @mlazos</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110607<br />
Approved by: https://github.com/janeyx99, https://github.com/lezcano</p>]]></description>
      <pubDate>Thu, 05 Oct 2023 21:08:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d279979102d8676c5ea09f76c32997ab9b9ca38d</guid>
    </item>
    <item>
      <title>[aotinductor] Fix benchmarks with self.autocast (#110490)</title>
      <link>https://github.com/pytorch/pytorch/commit/83061ee177268b014dbca5c75b8d7886c33aae09</link>
      <description><![CDATA[<p>[aotinductor] Fix benchmarks with self.autocast (#110490)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/108173</p>
<p>The original error was that there was a type mismatch between the output of eager mode (float16) and from aot_compile (float32). This is because when we run the model eagerly in the benchmarks, we call <a href="https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/common.py#L2072-L2076">self.model_iter_fn</a> to run the model, rather than directly calling the model. In the case of timm models, it calls the model with <a href="https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/timm_models.py#L321-L323">self.autocast()</a>, causing the eager model to return a float16 value. However, the model we export with aot_compile does not have the self.autocast context, so it returns float32.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110490<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 05 Oct 2023 18:13:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/83061ee177268b014dbca5c75b8d7886c33aae09</guid>
    </item>
    <item>
      <title>Add functional collective all_to_all_single and support it in Inductor (#110195)</title>
      <link>https://github.com/pytorch/pytorch/commit/f274c7b32c982fbae15fe9f49a81c3cc8397e29a</link>
      <description><![CDATA[<p>Add functional collective all_to_all_single and support it in Inductor (#110195)</p>
<p>Copy of https://github.com/pytorch/pytorch/pull/106655 from yf225<br />
rebased on top of item() support changes</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110195<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Thu, 05 Oct 2023 15:11:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f274c7b32c982fbae15fe9f49a81c3cc8397e29a</guid>
    </item>
    <item>
      <title>perf(inductor): use for loop with shortcut in `Optimizer`s to speedup against list comprehensions (e.g. complex conversion) (#110613)</title>
      <link>https://github.com/pytorch/pytorch/commit/df7d01aed5173966cf232e75ecb86852f8475a45</link>
      <description><![CDATA[<p>perf(inductor): use for loop with shortcut in <code>Optimizer</code>s to speedup against list comprehensions (e.g. complex conversion) (#110613)</p>
<p>Fully fixes: https://github.com/pytorch/pytorch/issues/110506</p>
<p>Depends: https://github.com/pytorch/pytorch/pull/110607<br />
Potential merge conflicts:<br />
- https://github.com/pytorch/pytorch/pull/110339<br />
- https://github.com/pytorch/pytorch/pull/110345<br />
- https://github.com/pytorch/pytorch/pull/110454</p>
<p>Related:<br />
- https://github.com/pytorch/pytorch/issues/110606 (we can apply the improvements here orthogonally to the complex support)</p>
<h3>Results</h3>
<p>Benchmark: 100 params.</p>
<p>Breakdowns (float32, dynamo):<br />
<code>Adagrad: this PR: 4.4s, main: 8.8s
Adam: this PR: 2.1s, main: 9.8s
AdamW: this PR: 2.5s, main: 8.2s
ASGD: this PR: 3.1s, main: 8.5s
RMSProp: this PR: 1.3s, main: 4.2s
RProp: this PR: 6.7s, main: 14.9s</code></p>
<p>Notes:<br />
1. Adagrad is still slow due to <code>_get_value</code> list comprehension. Can be fixed in https://github.com/pytorch/pytorch/pull/110339/files by utilizing capturable path<br />
2. Adamax is not actually compiled (it is currently disabled).<br />
3. Inductor compile time is quite variable. We calculate dynamo by subtracting <code>call_user_compiler</code> from <code>compile_inner</code> timing.</p>
<details>

This PR:
```
Adagrad (torch.float32): 28.47496461868286s
Adagrad (torch.complex64): 29.379547357559204s
Adam (torch.float32): 17.334211587905884s
Adam (torch.complex64): 29.637500524520874s
Adamax (torch.float32): 2.4749321937561035s
Adamax (torch.complex64): 3.1997995376586914s
AdamW (torch.float32): 18.06532859802246s
AdamW (torch.complex64): 28.25661015510559s
ASGD (torch.float32): 23.70255398750305s
ASGD (torch.complex64): 25.33756995201111s
RMSprop (torch.float32): 7.964028596878052s
RMSprop (torch.complex64): 12.909599781036377s
Rprop (torch.float32): 30.512362003326416s
Rprop (torch.complex64): 44.74405765533447s
```

Main
```
Adagrad (torch.float32): 26.919506072998047s
Adagrad (torch.complex64): 35.190622091293335s
Adam (torch.float32): 25.715000867843628s
Adam (torch.complex64): 24.17716670036316s
Adamax (torch.float32): 2.4404726028442383s
Adamax (torch.complex64): 3.3538928031921387s
AdamW (torch.float32): 25.2022807598114s
AdamW (torch.complex64): 28.915700912475586s
ASGD (torch.float32): 24.108731985092163s
ASGD (torch.complex64): 26.589075088500977s
RMSprop (torch.float32): 10.781344175338745s
RMSprop (torch.complex64): 15.136352777481079s
Rprop (torch.float32): 42.46482181549072s
Rprop (torch.complex64): 48.28277635574341s
```

Seems that it doesn't help the complex case by much (but that's not the majority case). torch.float32 is generally positive, when it does not show drastic improvement / regresses, it is due to inductor variance (by manually inspecting the logs).

</details>

<h3>Benchmark Script</h3>
<p>```python<br />
import torch<br />
import time<br />
from torch.optim import Adagrad, Adam, Adamax, AdamW, ASGD, RMSprop, Rprop</p>
<p>OPTIMS = [Adagrad, Adam, Adamax, AdamW, ASGD, RMSprop, Rprop]<br />
DTYPES = [torch.float, torch.cfloat]</p>
<p>NUM_PARAMS = 100<br />
kwargs = { "lr": 0.01, "foreach": True }<br />
summary = []</p>
<p>for optim_cls in OPTIMS:<br />
    for dtype in DTYPES:<br />
        torch._dynamo.reset()<br />
        # torch._inductor.metrics.reset()<br />
        input = torch.ones([10, 10], dtype=dtype, device="cuda:0")<br />
        model = torch.nn.Sequential(<br />
            *[torch.nn.Linear(10, 10, dtype=dtype, device="cuda:0") for _ in range(NUM_PARAMS)]<br />
        )</p>
<pre><code>    model(input).sum().abs().backward()
    opt_compiled = optim_cls(model.parameters(), **kwargs)
    compiled_step = torch.compile(opt_compiled.step)

    with torch.set_grad_enabled(False):
        start_time = time.time()
        compiled_step()
        summary.append(f"{optim_cls.__name__} ({dtype}): {time.time() - start_time}s")

    print(optim_cls, kwargs, dtype, torch._dynamo.utils.compile_times())
</code></pre>
<p>for s in summary:<br />
    print(s)<br />
```</p>
<p>CC: @janeyx99 @mlazos<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110613<br />
Approved by: https://github.com/janeyx99</p>]]></description>
      <pubDate>Thu, 05 Oct 2023 15:10:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/df7d01aed5173966cf232e75ecb86852f8475a45</guid>
    </item>
    <item>
      <title>Back out "[Inductor] Break the loop fusion when node2 depends on node1 mutations (#109172)" (#110622)</title>
      <link>https://github.com/pytorch/pytorch/commit/ecdd1bcf0399878ed862d56be7f5a30c875ebf57</link>
      <description><![CDATA[<p>Back out "[Inductor] Break the loop fusion when node2 depends on node1 mutations (#109172)" (#110622)</p>
<p>Summary:<br />
Original commit changeset: 03980fb054d5</p>
<p>Original Phabricator Diff: D49519512</p>
<p>Bisecting shows that this diff is the cause of S369683. Since this affects Ads production, need to back out this diff immediately.</p>
<p>Test Plan: See S369683</p>
<p>Reviewed By: ezyang</p>
<p>Differential Revision: D49958638</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110622<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Thu, 05 Oct 2023 12:09:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ecdd1bcf0399878ed862d56be7f5a30c875ebf57</guid>
    </item>
    <item>
      <title>[aotinductor] Avoid generating redundant kernel loading code (#110510)</title>
      <link>https://github.com/pytorch/pytorch/commit/298f01d9a2d872051016a43e8a3c96bdad791896</link>
      <description><![CDATA[<p>[aotinductor] Avoid generating redundant kernel loading code (#110510)</p>
<p>Summary: 1) Stop forcing triton.unique_kernel_names to True for AOTInductor, because the unique kernel name can be read from metadata; 2) Only generate load_kernel once for each kernel since we don't have control flow in our generated code.  This solves https://github.com/pytorch/pytorch/issues/105553.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110510<br />
Approved by: https://github.com/chenyang78, https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 05 Oct 2023 11:59:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/298f01d9a2d872051016a43e8a3c96bdad791896</guid>
    </item>
    <item>
      <title>[AOTInductor]  ProxyExecutor support Dynamic Shape (#110526)</title>
      <link>https://github.com/pytorch/pytorch/commit/f1b94461aabec129164685f230d914e789ac42ba</link>
      <description><![CDATA[<p>[AOTInductor]  ProxyExecutor support Dynamic Shape (#110526)</p>
<p>Summary:<br />
Extend ProxyExecutor to support dynamic shape.</p>
<p>Example of ProxyExecutor invocation with symints.<br />
<code>int64_t* arg0_1_size;
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_sizes(arg0_1, &amp;arg0_1_size));
    auto s0 = arg0_1_size[0];
    auto s1 = arg0_1_size[1];
    int64_t* arg1_1_size;
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_sizes(arg1_1, &amp;arg1_1_size));
    auto s2 = arg1_1_size[0];
    auto s3 = arg1_1_size[1];
    ...
    aoti_torch_proxy_executor_call_function(proxy_executor, 0, 15, std::vector&lt;int64_t&gt;{42, 16, 17, s0 + s1, s0 + s1, s2*s3, 45, 67, 16, 17, s2*s3, s2*s3, s0 + s1, 89, 910}.data(), 7, std::vector&lt;AtenTensorHandle&gt;{arg0_1, arg0_1, arg1_1, buf2, arg0_1, arg1_1, buf4}.data());</code></p>
<p>Example of serialized SymInt(s) arguments:<br />
<code>{
            "name": "symint",
            "arg": {
              "asSymInt": {
                "asName": "s0 + s1"
              }
            }
          },
          {
            "name": "symints",
            "arg": {
              "asSymInts": [
                {
                  "asName": "s0 + s1"
                },
                {
                  "asName": "s2*s3"
                }
              ]
            }
          },
          ...
          {
            "name": "o_symint",
            "arg": {
              "asSymInt": {
                "asName": "s2*s3"
              }
            }
          },
          {
            "name": "o_symints",
            "arg": {
              "asSymInts": [
                {
                  "asName": "s2*s3"
                },
                {
                  "asName": "s0 + s1"
                }
              ]
            }
          },</code></p>
<p>Test Plan: buck2 run mode/dev-nosan deeplearning/aot_inductor/test:test_custom_ops</p>
<p>Differential Revision: D49887555</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110526<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Thu, 05 Oct 2023 11:05:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f1b94461aabec129164685f230d914e789ac42ba</guid>
    </item>
    <item>
      <title>[AOTInductor] Store loaded kernels in the model (#110554)</title>
      <link>https://github.com/pytorch/pytorch/commit/cf1b494afd0d0368c22e70e93d91da3d9fe1ddce</link>
      <description><![CDATA[<p>[AOTInductor] Store loaded kernels in the model (#110554)</p>
<p>Defining kernels as static vars is problematic for subsequent model loading on non-default CUDA devices.</p>
<p>Assuming those kernels were loaded in context of the device #0, so, they are not nullptr anymore, therefore kernels won't work on devices other than the device #0.</p>
<p>This change makes devices remembered at model level in AOT mode.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110554<br />
Approved by: https://github.com/chenyang78, https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 05 Oct 2023 02:17:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cf1b494afd0d0368c22e70e93d91da3d9fe1ddce</guid>
    </item>
    <item>
      <title>Fix typo under torch/_inductor directory (#110530)</title>
      <link>https://github.com/pytorch/pytorch/commit/434a996c42506ebd1829de71edeac2afd9f73558</link>
      <description><![CDATA[<p>Fix typo under torch/_inductor directory (#110530)</p>
<p>This PR fixes typo of comments and messages in files under <code>torch/_dynamo</code> directory.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110530<br />
Approved by: https://github.com/kit1980</p>]]></description>
      <pubDate>Wed, 04 Oct 2023 18:17:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/434a996c42506ebd1829de71edeac2afd9f73558</guid>
    </item>
    <item>
      <title>[aotinductor] Enable test_non_default_cuda_device on CI (#110509)</title>
      <link>https://github.com/pytorch/pytorch/commit/c121f957c2ece13b00a518cd47218e8f01f300a5</link>
      <description><![CDATA[<p>[aotinductor] Enable test_non_default_cuda_device on CI (#110509)</p>
<p>Summary: test_non_default_cuda_device needs to run on a multi-gpu CI instance</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49937115">D49937115</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110509<br />
Approved by: https://github.com/angelayi, https://github.com/khabinov, https://github.com/chenyang78</p>]]></description>
      <pubDate>Wed, 04 Oct 2023 17:25:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c121f957c2ece13b00a518cd47218e8f01f300a5</guid>
    </item>
    <item>
      <title>fix(inductor): Increase coverage of Inductor ATen lowering (#110473)</title>
      <link>https://github.com/pytorch/pytorch/commit/37afa0c349e3441438f3f7884ebc72014f77dd9f</link>
      <description><![CDATA[<p>fix(inductor): Increase coverage of Inductor ATen lowering (#110473)</p>
<p>Add sqrt to decomp testing path and fix missing <code>minimum</code>, <code>clamp_min</code>,<code>clamp_max</code> lowerings and/or registrations.</p>
<p>Follow up to: https://github.com/pytorch/pytorch/pull/110468#issuecomment-1745718602 (requires upstream to merge to avoid merge conflict)</p>
<p>CC: @janeyx99</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110473<br />
Approved by: https://github.com/janeyx99</p>]]></description>
      <pubDate>Wed, 04 Oct 2023 15:40:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/37afa0c349e3441438f3f7884ebc72014f77dd9f</guid>
    </item>
    <item>
      <title> Cleanup the code in the `dynamo` userbenchmark (#110519)</title>
      <link>https://github.com/pytorch/pytorch/commit/2e31fae5c5bd4c059d512f5ab393d113bd37c28f</link>
      <description><![CDATA[<p>Cleanup the code in the <code>dynamo</code> userbenchmark (#110519)</p>
<p>Summary:<br />
Skip importing the modules that are only available in the pytorch source code, not pytorch nightly release.</p>
<p>Make dynamo benchmark work on both OSS and internal.</p>
<p>X-link: https://github.com/pytorch/benchmark/pull/1960</p>
<p>Test Plan:<br />
<code>$ python run_benchmark.py dynamo --only alexnet --training --performance --inductor
loading model: 0it [00:05, ?it/s]
cuda train alexnet
running benchmark: 100%|█████████████████| 30/30 [00:00&lt;00:00, 41.46it/s]
1.129x</code></p>
<p><code>$ buck2 run mode/opt //pytorch/benchmark:run_benchmark -- dynamo --only alexnet --training --inductor --performance --output-directory $HOME
loading model: 0it [00:16, ?it/s]
running benchmark: 100%|█████████████████| 30/30 [00:00&lt;00:00, 37.94it/s]
cuda train alexnet
1.120x</code></p>
<p>Differential Revision: D49912006</p>
<p>Pulled By: xuzhao9</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110519<br />
Approved by: https://github.com/desertfire, https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 04 Oct 2023 15:26:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2e31fae5c5bd4c059d512f5ab393d113bd37c28f</guid>
    </item>
    <item>
      <title>[inductor] get_system shouldn't error if CUDA is not installed (#110282)</title>
      <link>https://github.com/pytorch/pytorch/commit/a9df9e518765104f2b521d5f564a18c2596dd8cc</link>
      <description><![CDATA[<p>[inductor] get_system shouldn't error if CUDA is not installed (#110282)</p>
<p>Using inductor on a CPU-only device should be OK.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49749912/">D49749912</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110282<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Wed, 04 Oct 2023 13:28:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a9df9e518765104f2b521d5f564a18c2596dd8cc</guid>
    </item>
    <item>
      <title>[AOTInductor] Simplified AOTInductor interface and model class (#110411)</title>
      <link>https://github.com/pytorch/pytorch/commit/46a5558cd57a2101ad7e48b331f7f370b6f06328</link>
      <description><![CDATA[<p>[AOTInductor] Simplified AOTInductor interface and model class (#110411)</p>
<p>Summary:<br />
This PR removed several APIs from the AOTInductor interface,<br />
which are not used by the client.</p>
<p>It also simplified AOTInductor's model class by removing<br />
the dim info for input/output tensors. We included dim info<br />
before to return max output shapes, which was used by the client<br />
to allocate memory for output tensors. Now, we allocate output<br />
tensor memory from the .so so that we don't need to maintain<br />
such information any more. The deletion of dim info from<br />
the model class also simplified the codegen quite a bit.</p>
<p>Test Plan: ci</p>
<p>Reviewed By: khabinov</p>
<p>Differential Revision: D49835430</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110411<br />
Approved by: https://github.com/khabinov, https://github.com/desertfire, https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 04 Oct 2023 10:35:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/46a5558cd57a2101ad7e48b331f7f370b6f06328</guid>
    </item>
    <item>
      <title>[AOTInductor] Implement autograd eager backend for native triton kernels (#110403)</title>
      <link>https://github.com/pytorch/pytorch/commit/f04b1a0d278d606e21e001e373eb9184356d9b20</link>
      <description><![CDATA[<p>[AOTInductor] Implement autograd eager backend for native triton kernels (#110403)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110403<br />
Approved by: https://github.com/zou3519, https://github.com/bdhirsh</p>]]></description>
      <pubDate>Wed, 04 Oct 2023 09:56:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f04b1a0d278d606e21e001e373eb9184356d9b20</guid>
    </item>
    <item>
      <title>[aotinductor] Clean up fallback kernel cpp name generation (#110267)</title>
      <link>https://github.com/pytorch/pytorch/commit/c0c2e052a4dcd36e59885753244539ea8421a307</link>
      <description><![CDATA[<p>[aotinductor] Clean up fallback kernel cpp name generation (#110267)</p>
<p>Summary: Unify the way to generate cpp kernel name when the kernel is from OpOverload</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110267<br />
Approved by: https://github.com/zou3519<br />
ghstack dependencies: #110233</p>]]></description>
      <pubDate>Wed, 04 Oct 2023 09:18:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c0c2e052a4dcd36e59885753244539ea8421a307</guid>
    </item>
    <item>
      <title>[inductor] Lower small gemvs on CPU (#110456)</title>
      <link>https://github.com/pytorch/pytorch/commit/4c3d3b717619d9d09e95878c1f01bfb5d4fee0d0</link>
      <description><![CDATA[<p>[inductor] Lower small gemvs on CPU (#110456)</p>
<p>If the gemv fits in registers, like [1,16]*[16,16], MKL isn't going to<br />
do much better than compiling a simple for-loop, and we end up paying<br />
allocation overhead and ATen overhead.</p>
<p>A very small internal inference model drops from 7-&gt;5 us with this change.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49875991/">D49875991</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110456<br />
Approved by: https://github.com/chenyang78, https://github.com/jgong5</p>]]></description>
      <pubDate>Wed, 04 Oct 2023 07:16:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4c3d3b717619d9d09e95878c1f01bfb5d4fee0d0</guid>
    </item>
    <item>
      <title>add `foreach_abs` meta registration and inductor decomp (#110468)</title>
      <link>https://github.com/pytorch/pytorch/commit/3fd938369fbf7f5bbc01d9edbde0a13fae4d6f28</link>
      <description><![CDATA[<p>add <code>foreach_abs</code> meta registration and inductor decomp (#110468)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/110458</p>
<p>Somehow it is on allowlist but not on testing path.</p>
<p>CC @janeyx99</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110468<br />
Approved by: https://github.com/janeyx99</p>]]></description>
      <pubDate>Tue, 03 Oct 2023 22:09:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3fd938369fbf7f5bbc01d9edbde0a13fae4d6f28</guid>
    </item>
    <item>
      <title>[AOTInductor] ProxyExecutor support ReinterpretView inputs (#110451)</title>
      <link>https://github.com/pytorch/pytorch/commit/50054b1a62e79b9c083c37b81f3ebb66b085b7e1</link>
      <description><![CDATA[<p>[AOTInductor] ProxyExecutor support ReinterpretView inputs (#110451)</p>
<p>Summary:<br />
See wrapper.codegen_reinterpret_view(), it return a temporary handle for tensor, which has following problem.<br />
<code># NB, the return handle here represents a temporary tensor, which will be automatically
            # released.
            # Here's a sample usage in the cpp wrapper code:
            #</code><br />
            # aoti_torch_addmm_out(<br />
            #     buf1,<br />
            #     arg1_1,<br />
            #     RAIIAtenTensorHandle(tmp_tensor_handle_0),<br />
            #     buf0,<br />
            #     1L,<br />
            #     1L));<br />
            # <code># RAIIAtenTensorHandle(tmp_tensor_handle_0) will be released after the call to addmm_out.
            # This could be problematic when it's used in a different pattern, for example:
            # ````
            # AtenTensorHandle tensor_args[] = {RAIIAtenTensorHandle(tmp_tensor_handle_2), buf5, buf6};
            # aoti_torch_proxy_executor_call_function(..., tensor_args);
            # ````
            # RAIIAtenTensorHandle(tmp_tensor_handle_2) will be invalid when it's used in the latter
            # kernel call.
            return f"RAIIAtenTensorHandle({tmp_name})"</code></p>
<p>As a result, ProxyExecutor would generate following code, which cause invalid memory access.</p>
<p>Before:</p>
<p><code>// Source Nodes: [fn_with_tuple_output], Original ATen: [fb.fn_with_tuple_output]
    AtenTensorHandle tmp_tensor_handle_2;
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch__reinterpret_tensor(buf3, 2, int_array_0, int_array_1, 0L, &amp;tmp_tensor_handle_2));
    ...
    AtenTensorHandle tensor_args[] = {RAIIAtenTensorHandle(tmp_tensor_handle_2), buf5, buf6};
    int64_t int_args[] = {1};
    aoti_torch_proxy_executor_call_function(proxy_executor, 1, 1, int_args, 3, tensor_args);
    buf3.reset();</code></p>
<p>With fix in this diff, ProxyExecutor generates following code</p>
<p>After:</p>
<p><code>// Source Nodes: [fn_with_tuple_output], Original ATen: [fb.fn_with_tuple_output]
    AtenTensorHandle tmp_tensor_handle_2;
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch__reinterpret_tensor(buf3, 2, int_array_0, int_array_1, 0L, &amp;tmp_tensor_handle_2));
    ...
    aoti_torch_proxy_executor_call_function(proxy_executor, 1, 1, std::vector&lt;int64_t&gt;{1}.data(), 3, std::vector&lt;AtenTensorHandle&gt;{RAIIAtenTensorHandle(tmp_tensor_handle_2), buf5, buf6}.data());
    buf3.reset();</code></p>
<p>I am not exactly a big fan of such <code>std::vector{...}.data()</code> for creating a temp array, but I can't think of another fix.</p>
<p>Test Plan: buck2 run mode/dev-nosan deeplearning/aot_inductor/test:test_custom_ops</p>
<p>Reviewed By: desertfire</p>
<p>Differential Revision: D49758764</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110451<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 03 Oct 2023 18:20:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/50054b1a62e79b9c083c37b81f3ebb66b085b7e1</guid>
    </item>
    <item>
      <title>[AOTInductor] Initial functionality for Inf and NaN checker (#109526)</title>
      <link>https://github.com/pytorch/pytorch/commit/836ba6430a85358e7739b3f1655999cf293d4bcd</link>
      <description><![CDATA[<p>[AOTInductor] Initial functionality for Inf and NaN checker (#109526)</p>
<p>Summary:<br />
Add initial functionality for Inf and NaN checker for AOTInductor.</p>
<p>Test Plan:<br />
Included in commit. Skipped for CI as SIGABRT can't be captured by pytest.</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49379751">D49379751</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/109526<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Tue, 03 Oct 2023 14:39:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/836ba6430a85358e7739b3f1655999cf293d4bcd</guid>
    </item>
    <item>
      <title>[aotinductor] Remove output_spec from AOTInductorModelCache (#110462)</title>
      <link>https://github.com/pytorch/pytorch/commit/06e88d2cfc5783215dd501b2e117376170d89082</link>
      <description><![CDATA[<p>[aotinductor] Remove output_spec from AOTInductorModelCache (#110462)</p>
<p>Summary: No need to store output_spec as the returned exported.call_spec already contains that.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110462<br />
Approved by: https://github.com/angelayi</p>]]></description>
      <pubDate>Tue, 03 Oct 2023 14:29:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/06e88d2cfc5783215dd501b2e117376170d89082</guid>
    </item>
    <item>
      <title>[AOTInductor] remove CUDA dependency for cpp backend (#110409)</title>
      <link>https://github.com/pytorch/pytorch/commit/da63c7f2c3b53674fe4bd00105df7c45a1d18a71</link>
      <description><![CDATA[<p>[AOTInductor] remove CUDA dependency for cpp backend (#110409)</p>
<p>Summary:<br />
Previously, we link against cuda libs even for pure cpp backend.<br />
This caused issues for cases where the inference platform does not<br />
have GPUs. This diff removed cuda dependency for cpp backend.</p>
<p>Reviewed By: bertmaher, muchulee8, mikekgfb</p>
<p>Differential Revision: D49800712</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110409<br />
Approved by: https://github.com/bertmaher, https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 03 Oct 2023 10:36:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/da63c7f2c3b53674fe4bd00105df7c45a1d18a71</guid>
    </item>
    <item>
      <title>[inductor][easy] Free functions in headers should be declared inline (#110445)</title>
      <link>https://github.com/pytorch/pytorch/commit/174e46b853d4b99e6e4d60ecd46fd81891a93240</link>
      <description><![CDATA[<p>[inductor][easy] Free functions in headers should be declared inline (#110445)</p>
<p>If multiple files include model.h, you end up with duplicate symbols errors.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49842167/">D49842167</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110445<br />
Approved by: https://github.com/desertfire, https://github.com/Skylion007</p>]]></description>
      <pubDate>Tue, 03 Oct 2023 09:44:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/174e46b853d4b99e6e4d60ecd46fd81891a93240</guid>
    </item>
    <item>
      <title>[AOTInductor] Fix ProxyExecutor's handling on multiple outputs (#110374)</title>
      <link>https://github.com/pytorch/pytorch/commit/15219f53d11a839ffccf3728ef8218e6797af507</link>
      <description><![CDATA[<p>[AOTInductor] Fix ProxyExecutor's handling on multiple outputs (#110374)</p>
<p>Summary: Fix ProxyExecutor after D49780781</p>
<p>Test Plan: buck2 run mode/dev-nosan deeplearning/aot_inductor/test:test_custom_ops</p>
<p>Differential Revision:<br />
D49816044</p>
<p>Privacy Context Container: 368960445142440</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110374<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Mon, 02 Oct 2023 22:42:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/15219f53d11a839ffccf3728ef8218e6797af507</guid>
    </item>
    <item>
      <title>minor tf32 fixes for unit tests on H100 and L40 (#110201)</title>
      <link>https://github.com/pytorch/pytorch/commit/e55d6f923cb7c34a3a29cf70edb74cc23fa93885</link>
      <description><![CDATA[<p>minor tf32 fixes for unit tests on H100 and L40 (#110201)</p>
<p>fixes the following tests which were failing in the NVIDIA internal CI on H100 and L40:</p>
<p>test/test_nn.py:<br />
* test_TransformerEncoderLayer_gelu_activation_cuda_tf32<br />
* test_Transformer_multilayer_coder_cuda_tf32</p>
<p>test/inductor/test_torchinductor.py:<br />
* test_batch_norm_2d_2_cuda</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110201<br />
Approved by: https://github.com/mikaylagawarecki, https://github.com/jansel, https://github.com/Skylion007</p>]]></description>
      <pubDate>Mon, 02 Oct 2023 16:10:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e55d6f923cb7c34a3a29cf70edb74cc23fa93885</guid>
    </item>
    <item>
      <title>Preserve layout on like constructors (#110242)</title>
      <link>https://github.com/pytorch/pytorch/commit/3812f2e40cbf2f2aadb36cbd9bc6210612fe0ed7</link>
      <description><![CDATA[<p>Preserve layout on like constructors (#110242)</p>
<p>Partially fixes <code>test_memory_format_factory_like_functions_preserve</code> with PYTORCH_TEST_WITH_INDUCTOR. Inductor preserves memory layouts for user-visible outputs as annotated on the fx graph that it is passed in. That graph is generated from running aot_autograd with decompositions. If the decompositions give incorrect strides, so will inductor.</p>
<p>This preserves the layout of <code>_like</code> operators when it corresponds to a <code>torch.memory_format</code>. It doesnt fix a) arbitrary permutations, b) striding of non-dense outputs. Both of these are lower-pri compared to preserving channels last. We would need either https://github.com/pytorch/pytorch/issues/92920 or a <code>to</code> variant that takes in a physical layout arbitrary permutations. I converted the output of rand to the correct layout instead of passing the layout in so that this would compose with the <code>replace_random</code> pass, and because the two pointwise ops will get fused anyway.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110242<br />
Approved by: https://github.com/int3</p>]]></description>
      <pubDate>Mon, 02 Oct 2023 15:53:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3812f2e40cbf2f2aadb36cbd9bc6210612fe0ed7</guid>
    </item>
    <item>
      <title>[inductor] Cast loads from boolean tensors to `tl.int1` (#110388)</title>
      <link>https://github.com/pytorch/pytorch/commit/01b2f25ebda85d307b27847ad67efe2b5bb54265</link>
      <description><![CDATA[<p>[inductor] Cast loads from boolean tensors to <code>tl.int1</code> (#110388)</p>
<p>Triton currently loads pointer to <code>tl.int1</code> as <code>tl.int8</code>.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110388<br />
Approved by: https://github.com/lezcano, https://github.com/Skylion007</p>]]></description>
      <pubDate>Mon, 02 Oct 2023 14:52:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/01b2f25ebda85d307b27847ad67efe2b5bb54265</guid>
    </item>
    <item>
      <title>Actually enable typechecking for _inductor/index_propagation.py (#110110)</title>
      <link>https://github.com/pytorch/pytorch/commit/15dfe7b8e3035463186b14c9fcf38d79e8d3b893</link>
      <description><![CDATA[<p>Actually enable typechecking for _inductor/index_propagation.py (#110110)</p>
<p>It was supposed to be enabled in #105622 but that PR neglected to update<br />
.lintrunner.toml.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110110<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Mon, 02 Oct 2023 12:57:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/15dfe7b8e3035463186b14c9fcf38d79e8d3b893</guid>
    </item>
    <item>
      <title>Add function to port FX minified graph to HLO via StableHLO (#109084)</title>
      <link>https://github.com/pytorch/pytorch/commit/16e3f158b947c2c14a98178670f22c047a40807c</link>
      <description><![CDATA[<p>Add function to port FX minified graph to HLO via StableHLO (#109084)</p>
<p>If <code>XLA_HLO_DEBUG</code> flag is enabled, generated a minified HLO graph when using the minifier. This function enables HLO minification support by porting the minified FX graph to StableHLO via the <code>save_torch_model_as_stablehlo</code> function.</p>
<p>This allows users to port the minified graph to compilers that are not compatible with TorchDynamo/Inductor workflow and use XLA instead. The purpose of this PR is to help XLA users debug accuracy and compilation errors. It will also be helpful for existing TorchDynamo/XLA workflow on <code>torchxla_trace_once</code> backend as well.</p>
<p>Fixes <a href="https://github.com/pytorch/xla/issues/5461">#5461</a> in Torch XLA repo. CC @GleasonK @qihqi<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/109084<br />
Approved by: https://github.com/anijain2305</p>]]></description>
      <pubDate>Mon, 02 Oct 2023 11:36:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/16e3f158b947c2c14a98178670f22c047a40807c</guid>
    </item>
    <item>
      <title>[aotinductor] Use dynamic_shape instead of constraints (#110360)</title>
      <link>https://github.com/pytorch/pytorch/commit/e47e946bbf488890858fe1491df3bffa441d9011</link>
      <description><![CDATA[<p>[aotinductor] Use dynamic_shape instead of constraints (#110360)</p>
<p>Summary:<br />
Previously we used export's constraints to specify all batch-size dimensions being dynamic. This is done by creating 1 constraint <code>dynamic_dim(inp[0][0], lower, upper)</code>, followed by <code>dynamic_dim(inp[0][0]) == dynamic_dim(inp[i][0])</code> for every input <code>i</code>.</p>
<p>Through the new <code>dynamic_shapes</code> API, we can use <code>Dims("batch_size")</code> on every dimension to specify which dimensions are dynamic and equal to each other, and <code>None</code> otherwise: <code>{i: [Dims("batch_size", lower, upper), None] for every input i}</code></p>
<p>Note: <code>dynamic_shapes</code> and <code>constraints</code> utilize the same "constraints" backend so this diff should be idempotent.</p>
<p>Test Plan: <code>buck2 run @//mode/dev-nosan //caffe2/torch/fb/model_transform/experimental/benchmark/test/aotinductor:test_aot_inductor_benchmark</code></p>
<p>Reviewed By: chenyang78, aakhundov</p>
<p>Differential Revision: D49784351</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110360<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Mon, 02 Oct 2023 08:09:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e47e946bbf488890858fe1491df3bffa441d9011</guid>
    </item>
    <item>
      <title>Avoid undefined behavior in JIT-generated conversion code (#110212)</title>
      <link>https://github.com/pytorch/pytorch/commit/e0348ceceb9a094c2817161f40b6888d42f7323c</link>
      <description><![CDATA[<p>Avoid undefined behavior in JIT-generated conversion code (#110212)</p>
<p>The inductor/dynamo JIT generator creates C++ code using <code>static_cast</code> for type conversions.<br />
This is can be undefined behavior for e.g. <code>static_cast&lt;uint8_t&gt;(floatVal)</code> where <code>floatVal</code> is a negative value.</p>
<p>To avoid this in the "regular" C++ code <code>c10::convert</code> is used. So use it in the JIT generated code too.</p>
<p>Fixes #110077</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110212<br />
Approved by: https://github.com/ezyang, https://github.com/jgong5, https://github.com/desertfire</p>]]></description>
      <pubDate>Mon, 02 Oct 2023 04:56:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e0348ceceb9a094c2817161f40b6888d42f7323c</guid>
    </item>
    <item>
      <title>[inductor][Optimus]Improve logging for Optimus (#110186)</title>
      <link>https://github.com/pytorch/pytorch/commit/f7812cdbd9b259e702085270f14876a09f573ea4</link>
      <description><![CDATA[<p>[inductor][Optimus]Improve logging for Optimus (#110186)</p>
<p>Summary: It is based on the diff D49340843. We add more logs for better debug and logging purposes.</p>
<p>Test Plan:<br />
<code>[2023-09-27 20:35:53,844] [0/0] torch._inductor.fx_passes.group_batch_fusion: [INFO] Before group_batch fusion in pre grads pass. Print graph: https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GEoA8xb22jibUNEEAPYecF9_RVM1br0LAAAz
[2023-09-27 20:35:55,001] [0/0] torch._inductor.fx_passes.group_batch_fusion: [INFO] Apply fusion BatchLinearFusion. Print graph: https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GPMR9BYffjwToEQCAFS7rgixMi0pbr0LAAAz
[2023-09-27 20:35:57,419] [0/0] torch._inductor.fx_passes.group_batch_fusion: [INFO] Apply fusion BatchLinearLHSFusion. Print graph: https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GKiA8hNycGpBdAIDAOn0c1Hpef4sbr0LAAAz
[2023-09-27 20:35:57,585] [0/0] torch._inductor.fx_passes.group_batch_fusion: [INFO] BatchLayernormFusion: key = ('batch_layernorm', 'torch.Size([2048, 128])', 'torch.Size([128])', 'torch.Size([128])', '(128,)', '1e-05'); subset size = 7
[2023-09-27 20:35:58,493] [0/0] torch._inductor.fx_passes.group_batch_fusion: [INFO] Apply fusion BatchLayernormFusion. Print graph: https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GKpftRa9Glxm-MYDAOZb_D80JHsYbr0LAAAz
[2023-09-27 20:35:59,754] [0/0] torch._inductor.fx_passes.group_batch_fusion: [INFO] Apply fusion BatchTanhFusion. Print graph: https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GPgh9BZQl4EKGckAAES094iV3Atrbr0LAAAz
I0927 20:36:00.532000 3750607 pre_grad.py:71] After group_batch_fusion_pre_grad_passes: https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GBPb8xYxfrbXuCMDAI5d_a4YyhFBbr0LAAAz</code></p>
<p>Differential Revision: D49710166</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110186<br />
Approved by: https://github.com/jackiexu1992, https://github.com/yanboliang</p>]]></description>
      <pubDate>Sun, 01 Oct 2023 23:29:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f7812cdbd9b259e702085270f14876a09f573ea4</guid>
    </item>
    <item>
      <title>Add heuristic for when `evict_first` should be set (and some other minor things) (#108841)</title>
      <link>https://github.com/pytorch/pytorch/commit/13681382d5e362c4a2de07f846385111bc61d32f</link>
      <description><![CDATA[<p>Add heuristic for when <code>evict_first</code> should be set (and some other minor things) (#108841)</p>
<p>Example of when the <code>evict_first</code> heuristic helps.<br />
```<br />
@torch.compile<br />
def f(a, b):<br />
    return (a * b).sum(dim=-1)</p>
<p>N = 512<br />
inps = (torch.randn(N, N, N).permute(2, 1, 0), torch.randn(N, N, N).permute(1, 2, 0))<br />
from torch._inductor.utils import do_bench<br />
print(do_bench(lambda: f(*inps)))<br />
```</p>
<p>This generates code like this: http://ix.io/4HFs</p>
<p><code>Original: 3.8 ms
This PR: 3.54 ms
Always `evict_first: 5.4ms</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108841<br />
Approved by: https://github.com/lezcano, https://github.com/jansel</p>]]></description>
      <pubDate>Sun, 01 Oct 2023 09:06:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/13681382d5e362c4a2de07f846385111bc61d32f</guid>
    </item>
    <item>
      <title>[AOTInductor] Add non-default device test (#110024)</title>
      <link>https://github.com/pytorch/pytorch/commit/669faab0add54252eb6bb88c2802fc7f906bc9cd</link>
      <description><![CDATA[<p>[AOTInductor] Add non-default device test (#110024)</p>
<p>Differential Revision: D49604597</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110024<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Sat, 30 Sep 2023 21:08:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/669faab0add54252eb6bb88c2802fc7f906bc9cd</guid>
    </item>
    <item>
      <title>[AOTInductor] Add model runner to avoid using torch_extension (#110263)</title>
      <link>https://github.com/pytorch/pytorch/commit/e8c0364f36831712ce6498e538bf4cf282d37877</link>
      <description><![CDATA[<p>[AOTInductor] Add model runner to avoid using torch_extension (#110263)</p>
<p>Differential Revision: D49609669</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110263<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Sat, 30 Sep 2023 16:52:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e8c0364f36831712ce6498e538bf4cf282d37877</guid>
    </item>
    <item>
      <title>[AOTInductor] ProxyExecutor supports Tuple of Tensor and List[Tensor] in returns (#110187)</title>
      <link>https://github.com/pytorch/pytorch/commit/898656e9d166851e244749a715be8f74613e4d83</link>
      <description><![CDATA[<p>[AOTInductor] ProxyExecutor supports Tuple of Tensor and List[Tensor] in returns (#110187)</p>
<p>Summary:<br />
ProxyExecutor supports custom ops that return a tuple mixed of Tensor and List[Tensor]<br />
e.g. <code>"fn_with_mix_outputs(Tensor t, Tensor[] tensors) -&gt; (Tensor, Tensor[])"</code></p>
<p>Example:<br />
<code>out7, [out8, out9] = torch.ops.fb.fn_with_mix_outputs(out5, [out6, out4])</code><br />
got compiled into<br />
<code>AtenTensorHandle buf11_handle;  // output buffer
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_new_uninitialized_tensor(&amp;buf11_handle));
    RAIIAtenTensorHandle buf11(buf11_handle);
    AtenTensorHandle buf12_handle;  // output buffer
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_new_uninitialized_tensor(&amp;buf12_handle));
    RAIIAtenTensorHandle buf12(buf12_handle);
    AtenTensorHandle buf13_handle;  // output buffer
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_new_uninitialized_tensor(&amp;buf13_handle));
    RAIIAtenTensorHandle buf13(buf13_handle);
    AtenTensorHandle tensor_args_var_7[] = {buf8.get(), buf9.get(), buf6.get(), buf11.get(), buf12.get(), buf13.get()};
    int64_t int_args_var_8[] = {};
    aoti_torch_proxy_executor_call_function(proxy_executor, 3, 0, int_args_var_8, 6, tensor_args_var_7);</code></p>
<p>Serialized extern node<br />
<code>{
      "name": "buf10",
      "node": {
        "target": "fb::fn_with_mix_outputs",
        "inputs": [
          {
            "name": "t",
            "arg": {
              "asTensor": {
                "name": "buf8"
              }
            }
          },
          {
            "name": "tensors",
            "arg": {
              "asTensors": [
                {
                  "name": "buf9"
                },
                {
                  "name": "buf6"
                }
              ]
            }
          }
        ],
        "outputs": [
          {
            "asTensor": {
              "name": "buf11"
            }
          },
          {
            "asTensors": [
              {
                "name": "buf12"
              },
              {
                "name": "buf13"
              }
            ]
          }
        ],
        "metadata": {}
      }
    }</code></p>
<p>Test Plan: Test</p>
<p>Differential Revision: D49710320</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110187<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Sat, 30 Sep 2023 11:47:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/898656e9d166851e244749a715be8f74613e4d83</guid>
    </item>
    <item>
      <title>[inductor][fbcode] Add -D C10_DISABLE_TENSORIMPL_EXTENSIBILITY to cpp_compile_command (#110122)</title>
      <link>https://github.com/pytorch/pytorch/commit/6bb448a2d3b37aaa1013b265f62643185f74dd88</link>
      <description><![CDATA[<p>[inductor][fbcode] Add -D C10_DISABLE_TENSORIMPL_EXTENSIBILITY to cpp_compile_command (#110122)</p>
<p>Summary:</p>
<h2>Why?</h2>
<p>The .so and .h files are compiled seperately with different flags. The .so is compiled by AOTInductor and .h files (eg. c10/core/TensorImpl.h) are compiled by buck2.</p>
<p>Let's make sure the .so is also compiled with this macro in fbcode.</p>
<p>Differential Revision: D49664078</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110122<br />
Approved by: https://github.com/chenyang78, https://github.com/khabinov</p>]]></description>
      <pubDate>Sat, 30 Sep 2023 08:34:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6bb448a2d3b37aaa1013b265f62643185f74dd88</guid>
    </item>
    <item>
      <title>Skip launching kernels with zero grid in AOT Inductor (#110312)</title>
      <link>https://github.com/pytorch/pytorch/commit/2ead6c2f6eaa76eb897d8dd87061bbbdf0824314</link>
      <description><![CDATA[<p>Skip launching kernels with zero grid in AOT Inductor (#110312)</p>
<p>Summary: with the grid computed in terms of unbacked <code>SymInt</code>s, it can happen that the grid is zero size. This causes CUDA error on <code>cuLaunchKernel</code> in the AOT Inductor codegen.</p>
<p>In this PR, when the grid contains unbacked <code>SymInt</code>s, a check is added around the <code>launchKernel</code> in the AOT Inductor's C++ wrapper codegen to make sure that the grid is not zero-size.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110312<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Sat, 30 Sep 2023 01:12:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2ead6c2f6eaa76eb897d8dd87061bbbdf0824314</guid>
    </item>
    <item>
      <title>[Inductor] Enable the item() and nonzero() codegen test on CPU (#110262)</title>
      <link>https://github.com/pytorch/pytorch/commit/7eeb392eb3dbc3c5a00b07a8b9db7a4d85cfa18f</link>
      <description><![CDATA[<p>[Inductor] Enable the item() and nonzero() codegen test on CPU (#110262)</p>
<p><strong>Summary</strong><br />
Follow up https://github.com/pytorch/pytorch/pull/109893 which has issue in support of CPU as reported in https://github.com/pytorch/pytorch/issues/109897. This fix mainly includes 2 changes:</p>
<ul>
<li>Current implementation of <code>rename_indexing</code><br />
https://github.com/pytorch/pytorch/blob/10c646295d3512237cfb3ab44aa21dfcc9832441/torch/_inductor/codegen/common.py#L1023 only add symbol name start with <code>s</code> or <code>ps</code> into <code>kernel.args.sizevars</code>. However, <code>Unbacked symint</code> will start as <code>i</code>, so we extend the implementation of <code>rename_indexing</code> to support symbol start with <code>i</code>.</li>
<li>Currently, the internal loop index also name start as <code>i</code>. Since <code>i</code> has has been used as <code>Unbacked symint</code>, change the name to start with <code>x</code> which should align with trition.</li>
</ul>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_torchinductor_dynamic_shapes.py -k test_bool_mask_nobreak
python -u -m pytest -s -v test_torchinductor_dynamic_shapes.py -k test_nonzero_size_factory_nobreak
python -u -m pytest -s -v test_torchinductor_dynamic_shapes.py -k test_item_zeros_nobreak</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110262<br />
Approved by: https://github.com/ezyang, https://github.com/jgong5</p>]]></description>
      <pubDate>Fri, 29 Sep 2023 16:13:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7eeb392eb3dbc3c5a00b07a8b9db7a4d85cfa18f</guid>
    </item>
    <item>
      <title>[aotinductor] Fix a missing schema issue for repeat_interleave (#110105)</title>
      <link>https://github.com/pytorch/pytorch/commit/993eea0edd3159d37059ffd5980dc4e9384ae19e</link>
      <description><![CDATA[<p>[aotinductor] Fix a missing schema issue for repeat_interleave (#110105)</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49686812">D49686812</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110105<br />
Approved by: https://github.com/zou3519, https://github.com/jansel, https://github.com/aakhundov</p>]]></description>
      <pubDate>Fri, 29 Sep 2023 15:01:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/993eea0edd3159d37059ffd5980dc4e9384ae19e</guid>
    </item>
    <item>
      <title>[inductor] Make sure unfuse_addmm and addmm patterns don't overlap (#110235)</title>
      <link>https://github.com/pytorch/pytorch/commit/bc047ec906d8e1730e2ccd8192cef3c3467d75d1</link>
      <description><![CDATA[<p>[inductor] Make sure unfuse_addmm and addmm patterns don't overlap (#110235)</p>
<p>Inductor has two opposing patterns,<br />
<code>addmm -&gt; add + mm
add + mm -&gt; addmm</code></p>
<p>This uses the <code>extra_check</code> to disable the addmm fusion pattern when the<br />
heuristic to unfuse add is met, for consistency.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110235<br />
Approved by: https://github.com/lezcano, https://github.com/eellison<br />
ghstack dependencies: #110232</p>]]></description>
      <pubDate>Fri, 29 Sep 2023 11:35:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bc047ec906d8e1730e2ccd8192cef3c3467d75d1</guid>
    </item>
    <item>
      <title>[inductor] Fix bug in input mutation (#107614)</title>
      <link>https://github.com/pytorch/pytorch/commit/d04b35e7e31547c3cdb9684e1dd0b556bf593a93</link>
      <description><![CDATA[<p>[inductor] Fix bug in input mutation (#107614)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107614<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 29 Sep 2023 10:27:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d04b35e7e31547c3cdb9684e1dd0b556bf593a93</guid>
    </item>
    <item>
      <title>[AOTInductor] ProxyExecutor supports List[Tensor] return type (#110182)</title>
      <link>https://github.com/pytorch/pytorch/commit/d7de26804eaf66546d025dcea83365dfc894c3cf</link>
      <description><![CDATA[<p>[AOTInductor] ProxyExecutor supports List[Tensor] return type (#110182)</p>
<p>Summary:<br />
Support custom ops returns List[Tensor] type, like <code>"fn_with_list_output(Tensor[] tensors, int i) -&gt; Tensor[]"</code></p>
<p>As an example<br />
<code>out5, out6 = torch.ops.fb.fn_with_list_output([out3, out4], 1)</code></p>
<p>got compiled into</p>
<p><code>AtenTensorHandle buf8_handle;  // output buffer
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_new_uninitialized_tensor(&amp;buf8_handle));
    RAIIAtenTensorHandle buf8(buf8_handle);
    AtenTensorHandle buf9_handle;  // output buffer
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_new_uninitialized_tensor(&amp;buf9_handle));
    RAIIAtenTensorHandle buf9(buf9_handle);
    AtenTensorHandle tensor_args_var_5[] = {buf5.get(), buf6.get(), buf8.get(), buf9.get()};
    int64_t int_args_var_6[] = {1};
    aoti_torch_proxy_executor_call_function(proxy_executor, 2, 1, int_args_var_6, 4, tensor_args_var_5);</code></p>
<p>Test Plan: Test</p>
<p>Differential Revision: D49694691</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110182<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Fri, 29 Sep 2023 10:21:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d7de26804eaf66546d025dcea83365dfc894c3cf</guid>
    </item>
    <item>
      <title>Add weight update for DSOModel. (#110273)</title>
      <link>https://github.com/pytorch/pytorch/commit/d6d3f6cfe5bde945f39a3e2128d0971e24bc4284</link>
      <description><![CDATA[<p>Add weight update for DSOModel. (#110273)</p>
<p>Summary: Add weight update for DSOModel and AOTInductorModel</p>
<p>Test Plan: buck2 test accelerators/workloads/models/slimdsnn:slimdsnn_dso_test - SlimDSNN.DSO_Update_Constants</p>
<p>Reviewed By: mikekgfb</p>
<p>Differential Revision: D49748685</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110273<br />
Approved by: https://github.com/hl475</p>]]></description>
      <pubDate>Fri, 29 Sep 2023 10:14:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d6d3f6cfe5bde945f39a3e2128d0971e24bc4284</guid>
    </item>
    <item>
      <title>[inductor] handle non-list/tuple outputs for FallbackKernel (#110145)</title>
      <link>https://github.com/pytorch/pytorch/commit/30759848faa7aca49878e464160452d698544f87</link>
      <description><![CDATA[<p>[inductor] handle non-list/tuple outputs for FallbackKernel (#110145)</p>
<p>generate_output may return non-list/tuple outputs. Let's force<br />
those to be list, because we will enumerate kernel.outputs<br />
later in the codegen.</p>
<p>Also fixed a minor issue in an assertion message.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110145<br />
Approved by: https://github.com/aakhundov</p>]]></description>
      <pubDate>Fri, 29 Sep 2023 09:13:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/30759848faa7aca49878e464160452d698544f87</guid>
    </item>
    <item>
      <title>[aotinductor] Refactor test_aot_inductor to take different devices (#110216)</title>
      <link>https://github.com/pytorch/pytorch/commit/0ff1155d3aa91e379fe2036d4f677a644d647ca6</link>
      <description><![CDATA[<p>[aotinductor] Refactor test_aot_inductor to take different devices (#110216)</p>
<p>Summary: Replace hardcoded device to self.device, to make it easier to test both cpu and cuda</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110216<br />
Approved by: https://github.com/chenyang78, https://github.com/bertmaher<br />
ghstack dependencies: #110215</p>]]></description>
      <pubDate>Fri, 29 Sep 2023 08:30:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0ff1155d3aa91e379fe2036d4f677a644d647ca6</guid>
    </item>
    <item>
      <title>[aotinductor] Refactor test_aot_inductor (#110215)</title>
      <link>https://github.com/pytorch/pytorch/commit/ce6d09a775edc4749de4ca9e0acdf63ae7696d62</link>
      <description><![CDATA[<p>[aotinductor] Refactor test_aot_inductor (#110215)</p>
<p>Summary: Remove the usage of output tensors in the test script, since AOTInductor now returns output tensors instead of taking in pre-allocated output tensors.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110215<br />
Approved by: https://github.com/angelayi, https://github.com/chenyang78</p>]]></description>
      <pubDate>Fri, 29 Sep 2023 08:30:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ce6d09a775edc4749de4ca9e0acdf63ae7696d62</guid>
    </item>
    <item>
      <title>[Dynamo] Add native support for Triton Kernels to Dynamo (#109623)</title>
      <link>https://github.com/pytorch/pytorch/commit/2d50a30d77b554bea9c1431f500e5eecaa4e6fd1</link>
      <description><![CDATA[<p>[Dynamo] Add native support for Triton Kernels to Dynamo (#109623)</p>
<p>This PR adds native support to Dynamo to detect Triton kernels and<br />
create an FX graph node out of them. AOT eager and inductor modes will<br />
be support in follow up PRs.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109623<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 29 Sep 2023 07:49:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2d50a30d77b554bea9c1431f500e5eecaa4e6fd1</guid>
    </item>
    <item>
      <title>[inductor] Add fbcode include path for cuda (#110240)</title>
      <link>https://github.com/pytorch/pytorch/commit/92f4a7b66353655127eb56e9afd1979229c5938b</link>
      <description><![CDATA[<p>[inductor] Add fbcode include path for cuda (#110240)</p>
<p>We missed the cuda include, leading to failures in cases where CUDA<br />
was not installed locally but only provided via third-party/GVFS.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49745585/">D49745585</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110240<br />
Approved by: https://github.com/hl475</p>]]></description>
      <pubDate>Fri, 29 Sep 2023 05:39:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/92f4a7b66353655127eb56e9afd1979229c5938b</guid>
    </item>
    <item>
      <title>Inductor cpp wrapper: support MkldnnRnnLayer (#107858)</title>
      <link>https://github.com/pytorch/pytorch/commit/20dabea35dfbd8fa78d1588322984cb9bbc61b73</link>
      <description><![CDATA[<p>Inductor cpp wrapper: support MkldnnRnnLayer (#107858)</p>
<ol>
<li>Directly use the <code>codegen</code> function of the parent class which already supported both python and cpp wrapper.</li>
<li>The output of the <code>at::mkldnn_rnn_layer</code> OP is actually a <code>std::tuple</code> https://github.com/pytorch/pytorch/blob/1491bae277668fac459937c874a49c3bb8adedcb/aten/src/ATen/native/mkldnn/RNN.cpp#L218 Fix the type when calling <code>MultiOutput</code>.</li>
</ol>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107858<br />
Approved by: https://github.com/jgong5, https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 28 Sep 2023 16:22:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/20dabea35dfbd8fa78d1588322984cb9bbc61b73</guid>
    </item>
    <item>
      <title>Add support for item() and nonzero() codegen in Inductor (#109893)</title>
      <link>https://github.com/pytorch/pytorch/commit/d1a13129bbe237853ae8859c2664d551b450a47b</link>
      <description><![CDATA[<p>Add support for item() and nonzero() codegen in Inductor (#109893)</p>
<p>This is another version of<br />
https://github.com/pytorch/pytorch/pull/109262 that I think is more<br />
harmonious with inductor design.</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109893<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 28 Sep 2023 15:37:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d1a13129bbe237853ae8859c2664d551b450a47b</guid>
    </item>
    <item>
      <title>[inductor] Add CI jobs to test AOTInductor (#108419)</title>
      <link>https://github.com/pytorch/pytorch/commit/f82a29e32b7df9a0623bf8c418beeae3e6be22e1</link>
      <description><![CDATA[<p>[inductor] Add CI jobs to test AOTInductor (#108419)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108419<br />
Approved by: https://github.com/angelayi, https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 28 Sep 2023 12:19:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f82a29e32b7df9a0623bf8c418beeae3e6be22e1</guid>
    </item>
    <item>
      <title>Fixed minor issues for bmm/mm decompositon (#109836)</title>
      <link>https://github.com/pytorch/pytorch/commit/aaaa3c158617d80cb6a36a33b1d316a084e7473f</link>
      <description><![CDATA[<p>Fixed minor issues for bmm/mm decompositon (#109836)</p>
<p>Summary:<br />
* Fixed minor issues for bmm/mm decompositon<br />
* enabled addmm for inductor</p>
<p>Test Plan: ci</p>
<p>Reviewed By: mikekgfb</p>
<p>Differential Revision: D49522332</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109836<br />
Approved by: https://github.com/jansel, https://github.com/mikekgfb</p>]]></description>
      <pubDate>Thu, 28 Sep 2023 10:45:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/aaaa3c158617d80cb6a36a33b1d316a084e7473f</guid>
    </item>
    <item>
      <title>[aot_inductor] Lightweight model runner (#110158)</title>
      <link>https://github.com/pytorch/pytorch/commit/5f417fd710eabee96bde76b4d6a0b6b52039c44f</link>
      <description><![CDATA[<p>[aot_inductor] Lightweight model runner (#110158)</p>
<p>It's useful to have a simple, lightweight way to run a model that adds<br />
essentially no overhead to calling the model's generated <code>run_impl</code> method.<br />
This C API is a super thin wrapper around AOTInductorModel: Create, Run, and<br />
Delete are provided, and do very little work beyond dispatch to the appropriate<br />
helpers.</p>
<p>Note the Create function also provides additional functionality beyond the<br />
Container API; it allows the user to pass in a weight map defined in userland,<br />
which is a requirement for several serving use cases.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49670711/">D49670711</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110158<br />
Approved by: https://github.com/desertfire, https://github.com/chenyang78</p>]]></description>
      <pubDate>Thu, 28 Sep 2023 06:59:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5f417fd710eabee96bde76b4d6a0b6b52039c44f</guid>
    </item>
    <item>
      <title>[torchbench] Consistent accuracy results with dynamobench (#110189)</title>
      <link>https://github.com/pytorch/pytorch/commit/ad0ba5e187bcfa32f40b8fb23b85b9e44f3e0a56</link>
      <description><![CDATA[<p>[torchbench] Consistent accuracy results with dynamobench (#110189)</p>
<p>Summary:<br />
Use the upstream <code>torch._dynamo.same</code> function in accuracy checking and remove the self-hosted version in torchbench.</p>
<p>Now cmf_10x and ads_dhen_5x can run in deterministic mode, enable deepcopy and deterministic mode.</p>
<p>Test Plan:<br />
<code>$ buck2 run mode/opt //pytorch/benchmark:run -- cmf_10x -d cuda -t train --accuracy
Running train method from cmf_10x on cuda in eager mode with input batch size 4 and precision tf32.
Accuracy:                            pass</code></p>
<p><code>$ buck2 run mode/opt //pytorch/benchmark:run -- cmf_10x -d cuda -t train --torchdynamo inductor --torchinductor_enable_batch_fusion --torchinductor_enable_split_cat_fx_pass --accuracy
Running train method from cmf_10x on cuda in dynamo inductor mode with input batch size 4 and precision tf32.
Accuracy:                            pass</code></p>
<p>Without this PR, it will print:</p>
<p><code>File "/mnt/xarfuse/uid-234232/9aa53cfe-seed-nspid4026531836_cgpid9238070-ns-4026531840/torch/_dynamo/utils.py", line 190, in time_wrapper
    r = func(*args, **kwargs)
  File "/mnt/xarfuse/uid-234232/9aa53cfe-seed-nspid4026531836_cgpid9238070-ns-4026531840/torch/_inductor/graph.py", line 464, in run
    return super().run(*args)
  File "/mnt/xarfuse/uid-234232/9aa53cfe-seed-nspid4026531836_cgpid9238070-ns-4026531840/torch/fx/interpreter.py", line 138, in run
    self.env[node] = self.run_node(node)
  File "/mnt/xarfuse/uid-234232/9aa53cfe-seed-nspid4026531836_cgpid9238070-ns-4026531840/torch/_inductor/graph.py", line 826, in run_node
    result.realize_hint()
  File "/mnt/xarfuse/uid-234232/9aa53cfe-seed-nspid4026531836_cgpid9238070-ns-4026531840/torch/_inductor/ir.py", line 5273, in realize_hint
    and self.is_pointwise_non_scalar_tensor_num_reads_larger_than_one()
  File "/mnt/xarfuse/uid-234232/9aa53cfe-seed-nspid4026531836_cgpid9238070-ns-4026531840/torch/_inductor/utils.py", line 343, in wrapper
    setattr(self, key, fn(self))
  File "/mnt/xarfuse/uid-234232/9aa53cfe-seed-nspid4026531836_cgpid9238070-ns-4026531840/torch/_inductor/ir.py", line 5332, in is_pointwise_non_scalar_tensor_num_reads_larger_than_one
    (sum(read.index != 0 for read in self.data.get_reads()) &gt; 1)
  File "/mnt/xarfuse/uid-234232/9aa53cfe-seed-nspid4026531836_cgpid9238070-ns-4026531840/torch/_inductor/ir.py", line 5332, in &lt;genexpr&gt;
    (sum(read.index != 0 for read in self.data.get_reads()) &gt; 1)
  File "/mnt/xarfuse/uid-234232/9aa53cfe-seed-nspid4026531836_cgpid9238070-ns-4026531840/torch/_inductor/dependencies.py", line 74, in index
    raise NotImplementedError("StarDep does not have an index")
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
NotImplementedError: StarDep does not have an index
Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True</code></p>
<p>Reviewed By: jackiexu1992, mengluy0125</p>
<p>Differential Revision: D49639733</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110189<br />
Approved by: https://github.com/desertfire, https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 28 Sep 2023 06:50:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ad0ba5e187bcfa32f40b8fb23b85b9e44f3e0a56</guid>
    </item>
    <item>
      <title>[inductor] Enhance an input type assertion msg (#110176)</title>
      <link>https://github.com/pytorch/pytorch/commit/8e14e76c3439caf12140a64fe139c10f72dfc557</link>
      <description><![CDATA[<p>[inductor] Enhance an input type assertion msg (#110176)</p>
<p>Summary: to address https://github.com/pytorch/pytorch/issues/110089</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110176<br />
Approved by: https://github.com/angelayi</p>]]></description>
      <pubDate>Thu, 28 Sep 2023 05:35:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8e14e76c3439caf12140a64fe139c10f72dfc557</guid>
    </item>
    <item>
      <title>[inductor] Decompose addmm if it's a dot product on cpu (#110010)</title>
      <link>https://github.com/pytorch/pytorch/commit/eb082ef604d377463750d27ab504ec57954dfcbe</link>
      <description><![CDATA[<p>[inductor] Decompose addmm if it's a dot product on cpu (#110010)</p>
<p>Generated code for dot product is often faster (on CPU) than<br />
dispatching to aten, since it avoids op dispatch overhead and allows fusion<br />
with surrounding ops, which in turn avoids allocations.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49595876/">D49595876</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110010<br />
Approved by: https://github.com/chenyang78, https://github.com/jgong5, https://github.com/mikekgfb</p>]]></description>
      <pubDate>Thu, 28 Sep 2023 05:30:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/eb082ef604d377463750d27ab504ec57954dfcbe</guid>
    </item>
    <item>
      <title>[aotinductor] Rename if name is prefixed with integer (#110113)</title>
      <link>https://github.com/pytorch/pytorch/commit/c71a64ccce7215ab9b3938582c6249b62a72be8b</link>
      <description><![CDATA[<p>[aotinductor] Rename if name is prefixed with integer (#110113)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/109894.<br />
Since in c++ we cannot have variables that start with an integer, we can do some additional handling in inductor to not produce constant tensors with names starting with integers.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110113<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Wed, 27 Sep 2023 23:26:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c71a64ccce7215ab9b3938582c6249b62a72be8b</guid>
    </item>
    <item>
      <title>Enable typechecking for _inductor/fx_passes/group_batch_fusion.py (#110111)</title>
      <link>https://github.com/pytorch/pytorch/commit/fc1fcc4d17c5a99aebcb0af0f69d7278c9a2efbe</link>
      <description><![CDATA[<p>Enable typechecking for _inductor/fx_passes/group_batch_fusion.py (#110111)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110111<br />
Approved by: https://github.com/eellison, https://github.com/Skylion007<br />
ghstack dependencies: #110109</p>]]></description>
      <pubDate>Wed, 27 Sep 2023 20:53:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fc1fcc4d17c5a99aebcb0af0f69d7278c9a2efbe</guid>
    </item>
    <item>
      <title>[inductor] Actually enable typing for sizevars.py and joint_graph.py (#110109)</title>
      <link>https://github.com/pytorch/pytorch/commit/3e7f23e04fd0e2834e5cdd32494c7456be6968d6</link>
      <description><![CDATA[<p>[inductor] Actually enable typing for sizevars.py and joint_graph.py (#110109)</p>
<p>The commit message of #107862 says it enabled mypy checking for<br />
sizevars.py, but it seems that it neglected to update .lintrunner.toml.</p>
<p>New type errors appear to have crept in since then, so I've fixed them<br />
accordingly.</p>
<p>A similar mistake happened with #109955 for joint_graph.py, though that<br />
one is more recent and so hasn't had any new type errors to fix.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110109<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Wed, 27 Sep 2023 20:53:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3e7f23e04fd0e2834e5cdd32494c7456be6968d6</guid>
    </item>
    <item>
      <title>[AOTInductor] ProxyExecutor supports custom op with tuple output (#110140)</title>
      <link>https://github.com/pytorch/pytorch/commit/7f2b51c6684198eb59631cf71c04ad27e0c65025</link>
      <description><![CDATA[<p>[AOTInductor] ProxyExecutor supports custom op with tuple output (#110140)</p>
<p>Summary:<br />
Extend ProxyExecutor to support custom ops with tuple outputs.</p>
<p>Generated wrapper code for <code>out3, out4 = torch.ops.fb.fn_with_tuple_output(out2, 1)</code></p>
<p><code>AtenTensorHandle buf5_handle;  // output buffer
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_new_uninitialized_tensor(&amp;buf5_handle));
    RAIIAtenTensorHandle buf5(buf5_handle);
    AtenTensorHandle buf6_handle;  // output buffer
    AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_new_uninitialized_tensor(&amp;buf6_handle));
    RAIIAtenTensorHandle buf6(buf6_handle);
    AtenTensorHandle tensor_args_var_3[] = {buf3.get(), buf5.get(), buf6.get()};
    int64_t int_args_var_4[] = {1};
    aoti_torch_proxy_executor_call_function(proxy_executor, 1, 1, int_args_var_4, 3, tensor_args_var_3);</code></p>
<p>Test Plan: Test</p>
<p>Differential Revision: D49673994</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110140<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Wed, 27 Sep 2023 18:50:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7f2b51c6684198eb59631cf71c04ad27e0c65025</guid>
    </item>
    <item>
      <title>[core IR] Add lift_fresh, split.Tensor, and unbind decompositions to core ATen decomp table (#110102)</title>
      <link>https://github.com/pytorch/pytorch/commit/22e706f76894a898036329256a3f2f58e79aee92</link>
      <description><![CDATA[<p>[core IR] Add lift_fresh, split.Tensor, and unbind decompositions to core ATen decomp table (#110102)</p>
<h2>Context</h2>
<p>Add existing decomps for <code>lift_fresh</code>, <code>split.Tensor</code>, and <code>unbind</code> to the core ATen decomposition table. Do not use them in inductor, since Inductor currently lowers these directly.</p>
<p>One note though is that <code>lift_fresh</code>'s decomposition has a note saying it's not correct under autograd. However, my understanding is that these decompositions are registered to the <code>"post_autograd"</code> decomposition table, meaning autograd wouldn't be a factor. Would like some confirmation that this premise is correct.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110102<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 27 Sep 2023 17:21:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/22e706f76894a898036329256a3f2f58e79aee92</guid>
    </item>
    <item>
      <title>[AOTInductor] Update regex rule for symbol (#110184)</title>
      <link>https://github.com/pytorch/pytorch/commit/840bb650f8433213da6c93fe561afa644acd6279</link>
      <description><![CDATA[<p>[AOTInductor] Update regex rule for symbol (#110184)</p>
<p>Summary:<br />
Update regex rule to match _ letter.</p>
<p>Test Plan:<br />
Included in commit</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110184<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Wed, 27 Sep 2023 17:13:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/840bb650f8433213da6c93fe561afa644acd6279</guid>
    </item>
    <item>
      <title>[AOTIndutor] Fix freeze for AOTInductor (#110055)</title>
      <link>https://github.com/pytorch/pytorch/commit/778210879216652a763bae180587ee95d3b4f48c</link>
      <description><![CDATA[<p>[AOTIndutor] Fix freeze for AOTInductor (#110055)</p>
<p>Summary:<br />
Add test for freeze graph in AOTInductor.<br />
Remove unused code path.</p>
<p>Test Plan:<br />
Included in commit.</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110055<br />
Approved by: https://github.com/angelayi</p>]]></description>
      <pubDate>Wed, 27 Sep 2023 13:21:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/778210879216652a763bae180587ee95d3b4f48c</guid>
    </item>
    <item>
      <title>chore(inductor): Simplify `will_fusion_create_cycle` and cleanup to `node.ancestors`  (#109976)</title>
      <link>https://github.com/pytorch/pytorch/commit/6aae636f69abebfb8cffea9f7694f9d87929faf9</link>
      <description><![CDATA[<p>chore(inductor): Simplify <code>will_fusion_create_cycle</code> and cleanup to <code>node.ancestors</code>  (#109976)</p>
<p>recursive_predecessors == ancestors so rename.</p>
<p>Improve comments</p>
<p>Simplify <code>will_fusion_create_cycle</code> - make it easier to read and add detailed comments.</p>
<p>Diagram to illustrate clarification of shortcut.<br />
<img alt="Inductor Deep Dive" src="https://github.com/pytorch/pytorch/assets/9093549/7a30e088-8a33-4a9c-a8a7-81199cd086e2" /></p>
<p>CC: @ngimel</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109976<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 27 Sep 2023 12:48:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6aae636f69abebfb8cffea9f7694f9d87929faf9</guid>
    </item>
    <item>
      <title>[AOTInductor] Switch ProxyExecutor to use AtenTensorHandle  (#109748)</title>
      <link>https://github.com/pytorch/pytorch/commit/ec5bbef8af62cf60aaac3994c9f2fceed08dd9c7</link>
      <description><![CDATA[<p>[AOTInductor] Switch ProxyExecutor to use AtenTensorHandle  (#109748)</p>
<p>Summary: Switch ProxyExecutor to use AtenTensorHandle.</p>
<p>Test Plan: E2E Test</p>
<p>Differential Revision: D49471659</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109748<br />
Approved by: https://github.com/yifuwang, https://github.com/desertfire, https://github.com/chenyang78</p>]]></description>
      <pubDate>Wed, 27 Sep 2023 09:51:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ec5bbef8af62cf60aaac3994c9f2fceed08dd9c7</guid>
    </item>
    <item>
      <title>Fix inductor CI (by updating graph break count) (#110160)</title>
      <link>https://github.com/pytorch/pytorch/commit/7dbdf3be1ebb529e6b19fee72f8e64931b3ddc8d</link>
      <description><![CDATA[<p>Fix inductor CI (by updating graph break count) (#110160)</p>
<p>There was a vision hash update which led to fewer graph breaks. This<br />
seems expected to me (because the hash update included<br />
https://github.com/pytorch/vision/pull/7944 and nms is used in maskrcnn).</p>
<p>Test Plan:<br />
- wait for ci</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110160<br />
Approved by: https://github.com/ezyang, https://github.com/Chillee</p>]]></description>
      <pubDate>Wed, 27 Sep 2023 06:37:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7dbdf3be1ebb529e6b19fee72f8e64931b3ddc8d</guid>
    </item>
    <item>
      <title>[core IR] Add a core decomposition for aten.all (#110093)</title>
      <link>https://github.com/pytorch/pytorch/commit/dec140f1eaa751a8ea5b03388acaeb24207a94b4</link>
      <description><![CDATA[<p>[core IR] Add a core decomposition for aten.all (#110093)</p>
<h2>Context</h2>
<p>Change the ref implementation of <code>aten.all</code> to only use other <code>torch</code> operators such that we can use it for the core ATen decomposition table. This will replace the decomposition for <code>aten.all</code> that was used specifically by Inductor.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110093<br />
Approved by: https://github.com/manuelcandales, https://github.com/peterbell10, https://github.com/lezcano</p>]]></description>
      <pubDate>Tue, 26 Sep 2023 17:31:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/dec140f1eaa751a8ea5b03388acaeb24207a94b4</guid>
    </item>
    <item>
      <title>[core IR] Add glu as a core decomposition (#110043)</title>
      <link>https://github.com/pytorch/pytorch/commit/9928c10e71774dec01d93aa0645f92633d185b24</link>
      <description><![CDATA[<p>[core IR] Add glu as a core decomposition (#110043)</p>
<h2>Context</h2>
<p>Add the decomposition for <code>aten.glu</code> as a decomposition in the core ATen decomposition table. Don't use it in the Inductor decomposition table since Inductor has a lowering for it.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110043<br />
Approved by: https://github.com/peterbell10, https://github.com/lezcano<br />
ghstack dependencies: #110046</p>]]></description>
      <pubDate>Tue, 26 Sep 2023 16:23:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9928c10e71774dec01d93aa0645f92633d185b24</guid>
    </item>
    <item>
      <title>[inductor] support _scaled_dot_product_flash_attention fallback (#110085)</title>
      <link>https://github.com/pytorch/pytorch/commit/4d0ae7c9da31cb62c1ce80b9c870f0b4dba9da53</link>
      <description><![CDATA[<p>[inductor] support _scaled_dot_product_flash_attention fallback (#110085)</p>
<p>Summary:<br />
This PR supports _scaled_dot_product_flash_attention fallback kernel.<br />
Note that in the abi_compatible mode, we retrieve outputs by passing<br />
output argument pointers rather than relying on std::get.</p>
<p>It also fixes an issue related to dynamic shapes, where we wrongfully<br />
query undefined dynamic symbols.</p>
<p>Test Plan: ci</p>
<p>Reviewed By: frank-wei</p>
<p>Differential Revision: D49620191</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110085<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 26 Sep 2023 16:09:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4d0ae7c9da31cb62c1ce80b9c870f0b4dba9da53</guid>
    </item>
    <item>
      <title>[aotinductor] Relax the CUDAGuard device index check (#110030)</title>
      <link>https://github.com/pytorch/pytorch/commit/993530ee4fe6edcdead21a264d5344602b0f9ee6</link>
      <description><![CDATA[<p>[aotinductor] Relax the CUDAGuard device index check (#110030)</p>
<p>Summary: Although AOTInductor only supports running on a single cuda device, it does work in the case where there is a mix of cpu and cuda ops. So instead of asserting if a CUDA index appears for the first time, we check if there is only one cuda device index. This solves https://github.com/pytorch/pytorch/issues/109655</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110030<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 26 Sep 2023 08:23:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/993530ee4fe6edcdead21a264d5344602b0f9ee6</guid>
    </item>
    <item>
      <title>[inductor] Fix triton compiler error in multilayer any (#109325)</title>
      <link>https://github.com/pytorch/pytorch/commit/92d86cd1ad44e74a31dd26d89940c3b75b8faa3b</link>
      <description><![CDATA[<p>[inductor] Fix triton compiler error in multilayer any (#109325)</p>
<p>Fixes #109196</p>
<p>When we have a split reduction and the tensor is not an even multiple of the split size,<br />
we use <code>ops.masked</code> to pad to an even multiple. In the case here we generated:<br />
<code>python
tmp5 = tl.where(mask, tmp4, 0)</code></p>
<p>which implicitly promotes our boolean value to <code>int32</code>. The fix is to give the default<br />
value the same dtype as <code>result</code>.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109325<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Tue, 26 Sep 2023 04:29:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/92d86cd1ad44e74a31dd26d89940c3b75b8faa3b</guid>
    </item>
    <item>
      <title>[core IR] Add a core decomposition for floor_divide (#110046)</title>
      <link>https://github.com/pytorch/pytorch/commit/5df8aca994022b8d1928a6ac665295e1250914d8</link>
      <description><![CDATA[<p>[core IR] Add a core decomposition for floor_divide (#110046)</p>
<h2>Context</h2>
<p>Introduce a core decomposition for <code>aten.floor_divide</code> into other <code>aten</code> ops, and add it to the core ATen decomposition table.</p>
<p>This replaces the decomposition of <code>floor_divide</code> that was used by Inductor. I noticed there was a note on that decomposition</p>
<p>```</p>
<h1>TorchInductor-only decomposition. It should not be taken to core.</h1>
<h1>See https://github.com/pytorch/torchdynamo/pull/1120</h1>
<p>```</p>
<p>but couldn't discern the reason why this is the case. cc: @lezcano</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110046<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Tue, 26 Sep 2023 00:39:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5df8aca994022b8d1928a6ac665295e1250914d8</guid>
    </item>
    <item>
      <title>Enable typechecking for _inductor/fx_passes/joint_graph.py (#109955)</title>
      <link>https://github.com/pytorch/pytorch/commit/41bb5c27a2977b24b3c4b6a00fd5566448aebef9</link>
      <description><![CDATA[<p>Enable typechecking for _inductor/fx_passes/joint_graph.py (#109955)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109955<br />
Approved by: https://github.com/Skylion007<br />
ghstack dependencies: #109951, #109952, #109954</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 18:49:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/41bb5c27a2977b24b3c4b6a00fd5566448aebef9</guid>
    </item>
    <item>
      <title>Enable typechecking for _inductor/fx_passes/pad_mm.py (#109954)</title>
      <link>https://github.com/pytorch/pytorch/commit/86762f33d1eefb4534c6b94f74951970461b03f6</link>
      <description><![CDATA[<p>Enable typechecking for _inductor/fx_passes/pad_mm.py (#109954)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109954<br />
Approved by: https://github.com/Skylion007<br />
ghstack dependencies: #109951, #109952</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 18:49:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/86762f33d1eefb4534c6b94f74951970461b03f6</guid>
    </item>
    <item>
      <title>Enable typechecking for _inductor/fx_passes/pre_grad.py (#109952)</title>
      <link>https://github.com/pytorch/pytorch/commit/55f85530788b0c900a971776e3f2e06f71405161</link>
      <description><![CDATA[<p>Enable typechecking for _inductor/fx_passes/pre_grad.py (#109952)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109952<br />
Approved by: https://github.com/Skylion007<br />
ghstack dependencies: #109951</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 18:49:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/55f85530788b0c900a971776e3f2e06f71405161</guid>
    </item>
    <item>
      <title>Enable typechecking for _inductor/fx_passes/split_cat.py (#109951)</title>
      <link>https://github.com/pytorch/pytorch/commit/89fc66fb36ddf8eb691b6f3ff700ea2e76baa443</link>
      <description><![CDATA[<p>Enable typechecking for _inductor/fx_passes/split_cat.py (#109951)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109951<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 18:49:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/89fc66fb36ddf8eb691b6f3ff700ea2e76baa443</guid>
    </item>
    <item>
      <title>[aotinductor] Rename aot_runtime to aoti_runtime (#110007)</title>
      <link>https://github.com/pytorch/pytorch/commit/4bf1cd6961204daaa4564c94505a0617888979b2</link>
      <description><![CDATA[<p>[aotinductor] Rename aot_runtime to aoti_runtime (#110007)</p>
<p>Summary: Make the naming more explicit</p>
<p>Differential Revision: D49593528</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110007<br />
Approved by: https://github.com/houseroad</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 16:46:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4bf1cd6961204daaa4564c94505a0617888979b2</guid>
    </item>
    <item>
      <title>[AOTInductor] Bug fix for redefining symbol name (#110041)</title>
      <link>https://github.com/pytorch/pytorch/commit/7af30ea54cc50faa220ef71e62faa564be2bbcd6</link>
      <description><![CDATA[<p>[AOTInductor] Bug fix for redefining symbol name (#110041)</p>
<p>Summary:<br />
Bug fix for redefining symbol name.</p>
<p>Test Plan:<br />
python benchmarks/dynamo/huggingface.py --bfloat16 --accuracy --inference --device cuda --export-aot-inductor --cold-start-latency --only OPTForCausalLM</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/110041<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 15:03:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7af30ea54cc50faa220ef71e62faa564be2bbcd6</guid>
    </item>
    <item>
      <title>[inductor] enable mypy checking in torch/_inductor/codegen/cpp.py (#109729)</title>
      <link>https://github.com/pytorch/pytorch/commit/7ed06e8317d6f8ac5e339d1b354cd8632ea2613d</link>
      <description><![CDATA[<p>[inductor] enable mypy checking in torch/_inductor/codegen/cpp.py (#109729)</p>
<p>Summary: Add enough typehints / ignores to enable mypy checking in torch/_inductor/codegen/cpp.py</p>
<p>Test Plan: lintrunner</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109729<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 14:53:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7ed06e8317d6f8ac5e339d1b354cd8632ea2613d</guid>
    </item>
    <item>
      <title>[aotinductor] Update benchmark to include compilation time (#109998)</title>
      <link>https://github.com/pytorch/pytorch/commit/57cdad23962a11c8ec892d32d1e3790c1d8c7d20</link>
      <description><![CDATA[<p>[aotinductor] Update benchmark to include compilation time (#109998)</p>
<p>Fixes <a href="https://github.com/pytorch/pytorch/pull/109820#pullrequestreview-1638629777">comment</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109998<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 13:30:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/57cdad23962a11c8ec892d32d1e3790c1d8c7d20</guid>
    </item>
    <item>
      <title>[RFC] Allow "spawn" start method for torchinductor workers. (#108850)</title>
      <link>https://github.com/pytorch/pytorch/commit/ab70183c533eef2533a3c2fe23eb088d834314cb</link>
      <description><![CDATA[<p>[RFC] Allow "spawn" start method for torchinductor workers. (#108850)</p>
<p>Context: https://github.com/pytorch/pytorch/issues/108586</p>
<p>This PR adds a config to torchinductor such that users can specify the multiprocessing context for TorchInductor workers in codecache.</p>
<p>This would allow users a choice of using "spawn" in multithreaded environments instead of "fork" being hardcoded as the default.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108850<br />
Approved by: https://github.com/ezyang, https://github.com/zdevito</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 13:30:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ab70183c533eef2533a3c2fe23eb088d834314cb</guid>
    </item>
    <item>
      <title>[aotinductor] Skip benchmarks with control flow (#109661)</title>
      <link>https://github.com/pytorch/pytorch/commit/a565f1bee68478c0237836d5e593601e9194083b</link>
      <description><![CDATA[<p>[aotinductor] Skip benchmarks with control flow (#109661)</p>
<p>Since AOTInductor doesn't support control flow yet, we will skip over tests that are currently failing due to containing control flow in the code. Logs taken from https://hud.pytorch.org/benchmark/compilers?startTime=Tue%2C%2012%20Sep%202023%2022%3A56%3A40%20GMT&amp;stopTime=Tue%2C%2019%20Sep%202023%2022%3A56%3A40%20GMT&amp;granularity=hour&amp;suite=torchbench&amp;mode=inference&amp;dtype=bfloat16&amp;lBranch=main&amp;lCommit=2c1554a0323107d821be3ff13df7833b9f0b960d&amp;rBranch=main&amp;rCommit=47be61e12bd51df27182343d312dc3df485d5559</p>
<p>Errors documented in https://github.com/pytorch/pytorch/issues/105217</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109661<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 10:49:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a565f1bee68478c0237836d5e593601e9194083b</guid>
    </item>
    <item>
      <title>[inductor] Do type promotion in pointless cumsum pattern replacement (#109960)</title>
      <link>https://github.com/pytorch/pytorch/commit/fe5e63f5db3e065332cc7e0dffb69df01639454d</link>
      <description><![CDATA[<p>[inductor] Do type promotion in pointless cumsum pattern replacement (#109960)</p>
<p>Fixes #109925</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109960<br />
Approved by: https://github.com/Fidget-Spinner, https://github.com/lezcano</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 10:17:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fe5e63f5db3e065332cc7e0dffb69df01639454d</guid>
    </item>
    <item>
      <title>Reland "Update AOTAutograd to use FunctionalTensorMode instead of C++ functionalization (#106406)" (#109906)</title>
      <link>https://github.com/pytorch/pytorch/commit/d0fe8fa5db6dd06adfe1246a72b6d3a5215ff86e</link>
      <description><![CDATA[<p>Reland "Update AOTAutograd to use FunctionalTensorMode instead of C++ functionalization (#106406)" (#109906)</p>
<p>I'm pretty sure this is fixed but I'll run inductor and trunk CI. The failing test in trunk previously was that the selective activation checkpointing code that landed recently assumes that it can detect whether or not AOTAutograd is running by seeing if the inputs to SAC are C++ <code>FunctionalTensorWrapper</code>s</p>
<p>previous land broke some inductor trunk tests</p>
<p>This reverts commit 629a628cc8bb1f62e2cce11bf0c8a00d3d06f896.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109906<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Mon, 25 Sep 2023 06:53:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d0fe8fa5db6dd06adfe1246a72b6d3a5215ff86e</guid>
    </item>
    <item>
      <title>[Inductor] Generalize inductor triton backend device agnostic (#109486)</title>
      <link>https://github.com/pytorch/pytorch/commit/e9c9b1ed59e74279765587ea927fb7ac067905c5</link>
      <description><![CDATA[<p>[Inductor] Generalize inductor triton backend device agnostic (#109486)</p>
<h1>Motivation</h1>
<p>@jansel As discussed before, we expected to generalize some cuda-specific code. This can make inductor more friendly to third-party backend so that we can leverage inductor code as much as possible.</p>
<h1>Solution</h1>
<p>To implement this, we give a solution to introduce device runtime abstraction. We wrapper them inside <code>DeviceInterface</code> and use <code>register_interface_for_device</code> to register each kind of device to inductor. Then use <code>get_interface_for_device</code> to fetch the corresponding runtime from device type. Then usage is like this:<br />
<code>python
device_interface = get_interface_for_device("xpu")
device_interface .is_available() # to check if XPU is available
device_interface .device_count() # to check how much XPU device is available</code><br />
The <code>DeviceInterface</code> is a simple abstraction, which enables third-party backends that implement CUDA-like semantics to be integrated with inductor. This can prevent third-party backend from using monkey patch to override some utility functions, like <code>decode_device</code> that is hard-coded with CUDA.</p>
<h1>Additional Context</h1>
<p>The main code change:<br />
- To leverage AsyncCompile, make it device-agnostic<br />
- Avoid monkey patches, make some utility functions device-agnostic</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109486<br />
Approved by: https://github.com/jansel, https://github.com/jgong5, https://github.com/EikanWang</p>]]></description>
      <pubDate>Sat, 23 Sep 2023 23:49:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e9c9b1ed59e74279765587ea927fb7ac067905c5</guid>
    </item>
    <item>
      <title>Revert "[inductor] fix a max-autotune rng state related bug (#109828)"</title>
      <link>https://github.com/pytorch/pytorch/commit/d9627c42648984526dba0c19d6d00e38ee97d3eb</link>
      <description><![CDATA[<p>Revert "[inductor] fix a max-autotune rng state related bug (#109828)"</p>
<p>This reverts commit 3663436db31bd3cebcb76efe05d8355553a05c57.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/109828 on behalf of https://github.com/huydhn due to Sorry for reverting your change, but the rocm failure looks legit. There is also another numpy import error when running dynamo test on CPU (<a href="https://github.com/pytorch/pytorch/pull/109828#issuecomment-1732423883">comment</a>)</p>]]></description>
      <pubDate>Sat, 23 Sep 2023 14:35:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d9627c42648984526dba0c19d6d00e38ee97d3eb</guid>
    </item>
    <item>
      <title>Back out "[pytorch][PR] [Inductor] Extend Pattern Matcher to Match Equivalent Function Invocation" (#109931)</title>
      <link>https://github.com/pytorch/pytorch/commit/85ddc985d098d00e81ec6539d9a006751e85d1ea</link>
      <description><![CDATA[<p>Back out "[pytorch][PR] [Inductor] Extend Pattern Matcher to Match Equivalent Function Invocation" (#109931)</p>
<p>Summary:<br />
Original commit changeset: 3466b85fe0a1</p>
<p>Original Phabricator Diff: D49433268</p>
<p>More context D49536556</p>
<p>bypass-github-pytorch-ci-checks</p>
<p>Test Plan: revertreverthammer</p>
<p>Differential Revision: D49565384</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109931<br />
Approved by: https://github.com/houseroad</p>]]></description>
      <pubDate>Fri, 22 Sep 2023 21:58:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/85ddc985d098d00e81ec6539d9a006751e85d1ea</guid>
    </item>
    <item>
      <title>[AOTInductor] Load model on arbitrary device (#109816)</title>
      <link>https://github.com/pytorch/pytorch/commit/54faedf5f2d8920d954d66f2211eb6b5a2b1acea</link>
      <description><![CDATA[<p>[AOTInductor] Load model on arbitrary device (#109816)</p>
<p>Reviewed By: desertfire</p>
<p>Differential Revision: D49402404</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109816<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Fri, 22 Sep 2023 20:45:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/54faedf5f2d8920d954d66f2211eb6b5a2b1acea</guid>
    </item>
    <item>
      <title>Basic fp8 support in Inductor (#109168)</title>
      <link>https://github.com/pytorch/pytorch/commit/bbdce9357175f6c69f481d83d8eacd8714d82f96</link>
      <description><![CDATA[<p>Basic fp8 support in Inductor (#109168)</p>
<p>Add basic fp8 support in Inductor, including:<br />
* Fix fp8 Triton codegen issues;<br />
* Add min_elements_per_thread requirement for fp8 related dtype conversions. More details on Triton implementation can be found from https://github.com/openai/triton/blob/10f59d8ce04052521c1bc0cb3a3f8b98918fc7e3/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp#L10.</p>
<p>Note that the current implementation only works for Pointwise. Will create follow-up PRs for Reduction.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109168<br />
Approved by: https://github.com/drisspg</p>]]></description>
      <pubDate>Fri, 22 Sep 2023 20:41:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bbdce9357175f6c69f481d83d8eacd8714d82f96</guid>
    </item>
    <item>
      <title>[Inductor][FX]support nn.Linear nn.ConvTransposeNd for efficient_conv_bn_eval (#109722)</title>
      <link>https://github.com/pytorch/pytorch/commit/c789ed6e62cdee9917ae08a432ab9b6be8d59e64</link>
      <description><![CDATA[<p>[Inductor][FX]support nn.Linear nn.ConvTransposeNd for efficient_conv_bn_eval (#109722)</p>
<p>Using the <code>functional_call</code> API, we can deal with nn.Linear and nn.ConvTransposeNd, just like normal conv.</p>
<p>Thanks for @albanD pointing out the API in https://github.com/pytorch/pytorch/issues/109596 .</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109722<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 22 Sep 2023 17:12:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c789ed6e62cdee9917ae08a432ab9b6be8d59e64</guid>
    </item>
    <item>
      <title>[inductor] fix a max-autotune rng state related bug (#109828)</title>
      <link>https://github.com/pytorch/pytorch/commit/3663436db31bd3cebcb76efe05d8355553a05c57</link>
      <description><![CDATA[<p>[inductor] fix a max-autotune rng state related bug (#109828)</p>
<p>Fix https://github.com/pytorch/pytorch/issues/109736 .</p>
<p>HF pin move causes regression on accuracy check for HF models on the dashboard. Manually reverting the HF PR ( https://github.com/huggingface/transformers/pull/24696/files ) could recover, but this may hide some real issue. I happen to found that using a warm matmul max-autotune cache can work around the issue. Or putting it in another way:<br />
- make all calls to check_cache cache miss repro the issue<br />
- make all cals to check_cache cache hit works around the issue</p>
<p>I did some sort of 'bisect' to force halving the amount of cache miss each time while still make sure we can repro. Luckily reducing to a single cache miss still repro the issue. With more debugging, it turns out that it's the call to <code>torch.randn</code> on cuda device causing the problem.</p>
<p>The fix is to make sure  we restore the rng state when we generate random inputs for max-autotune benchmarking.</p>
<p>TBH, I can not fully explain the root cause although I know it's caused by rng state change.  AOTAutograd already has some logic to preserve rng state. And I can not repro the issue in unit tests. I have a few guess why the RNG state is not restored in the first place after we generate random inputs for max-autotune:<br />
- maybe AOTAutograd misses some corner case to preserve the rng state<br />
- maybe for the failed models, there are some eager fallback that's not handled by inductor. And if those fallback calles random number related APIs, we will see the issue. But again I don't find a good way to simulate this.</p>
<p>Repro:</p>
<p><code>TORCHINDUCTOR_BENCHMARK_KERNEL=1 TORCHINDUCTOR_MAX_AUTOTUNE_GEMM=1 CUDA_VISIBLE_DEVICES=3 time python benchmarks/dynamo/huggingface.py --backend inductor --amp --accuracy --only PLBartForCausalLM --training --cold-start-latency</code></p>
<p>We always repro the issue without the PR but pass the accuracy check with the PR.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109828<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Fri, 22 Sep 2023 16:58:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3663436db31bd3cebcb76efe05d8355553a05c57</guid>
    </item>
    <item>
      <title>inductor: tigher upperbound for rblock scaling (#109839)</title>
      <link>https://github.com/pytorch/pytorch/commit/de1b00abdaa0a9fa72c38bcf8b281e60c862bef4</link>
      <description><![CDATA[<p>inductor: tigher upperbound for rblock scaling (#109839)</p>
<p>Previously when we deciding if dynamically scaling down rblock, we use the following formule to compute the upper bound of number of blocks per sm:<br />
<code>max_threads_per_multi_processo / (32 * num_warps)</code></p>
<p>This is correct but it's a bit loose and some times because of the loose upper bound, we skip some optimization opportunities.</p>
<p>The new upper bound is: 65536 / n_reg_used_by_each_block . This is a tighter upper bound and can be helpful if the kernel uses too many registers (i.e. much larger than 32).</p>
<p>For kernel https://gist.github.com/shunting314/59aeafd297ed8ff03aa12030a2dd41ae (this is a real kernel inductor generates for HF), the change improve its perf from:<br />
0.485ms    0.332GB    684.29GB/s<br />
to<br />
0.240ms    0.332GB    1382.70GB/s</p>
<p>. The perf is bad previsouly because of register spills</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109839<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 22 Sep 2023 12:55:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/de1b00abdaa0a9fa72c38bcf8b281e60c862bef4</guid>
    </item>
    <item>
      <title>Enable typechecking for _inductor/pattern_matcher.py (#109613)</title>
      <link>https://github.com/pytorch/pytorch/commit/2895fbd857f72c8c96842eb2235e8f9171d52fe8</link>
      <description><![CDATA[<p>Enable typechecking for _inductor/pattern_matcher.py (#109613)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109613<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Fri, 22 Sep 2023 12:50:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2895fbd857f72c8c96842eb2235e8f9171d52fe8</guid>
    </item>
    <item>
      <title>[inductor] Add back a missing header include (#109845)</title>
      <link>https://github.com/pytorch/pytorch/commit/c27c56a5c428837adfc8c3f5308b23c06dcda386</link>
      <description><![CDATA[<p>[inductor] Add back a missing header include (#109845)</p>
<p>Summary: It was removed in https://github.com/pytorch/pytorch/pull/109678, which regressed GoogleFnet in HF.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109845<br />
Approved by: https://github.com/angelayi, https://github.com/chenyang78</p>]]></description>
      <pubDate>Fri, 22 Sep 2023 09:06:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c27c56a5c428837adfc8c3f5308b23c06dcda386</guid>
    </item>
    <item>
      <title>[inductor] Refactor some libtorch c shim interfaces (#109834)</title>
      <link>https://github.com/pytorch/pytorch/commit/d7dfa91e123e88fe6b7a3cdf69d0943d86c1895d</link>
      <description><![CDATA[<p>[inductor] Refactor some libtorch c shim interfaces (#109834)</p>
<p>Summary: Change the returned values to be in the back of the parameters, because 1) it is more consistent with AOTInductor runtime API convention; 2) because the out-variant ops have the out tensor at the beginning of parameters, this makes the return values more distinguished from those</p>
<p>Test Plan:<br />
<code>buck test mode/opt caffe2/torch/fb/model_transform/experimental/benchmark/test/aotinductor:test_aot_inductor_benchmark</code></p>
<p>Differential Revision: D49522928</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109834<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Fri, 22 Sep 2023 04:45:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d7dfa91e123e88fe6b7a3cdf69d0943d86c1895d</guid>
    </item>
    <item>
      <title>[Inductor] Extend Pattern Matcher to Match Equivalent Function Invocation (#107832)</title>
      <link>https://github.com/pytorch/pytorch/commit/4ff294522a6de60c6b1d2eb42c165285967d18bc</link>
      <description><![CDATA[<p>[Inductor] Extend Pattern Matcher to Match Equivalent Function Invocation (#107832)</p>
<p>Fixes #104391</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107832<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 21 Sep 2023 21:26:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4ff294522a6de60c6b1d2eb42c165285967d18bc</guid>
    </item>
    <item>
      <title>[inductor] Change AOTInductor to return output tensors (#109790)</title>
      <link>https://github.com/pytorch/pytorch/commit/8856c1628e8ddb3fe3bd51ed8cfef5c02ad0a609</link>
      <description><![CDATA[<p>[inductor] Change AOTInductor to return output tensors (#109790)</p>
<p>Summary:<br />
Change AOTInductor to directly return output tensors instead of taking pre-allocated output tensors to return the results. This gives several benefits:</p>
<ul>
<li>It makes sure AOTInductor has the same behavior when managing the output tensors as the default Inductor, which is widely tested and thus more reliable.</li>
<li>As we have debugged before, there are cases we still have to codegen extra copy_ ops to fill the pre-allocated output tensors which doesn't make sense for performance.</li>
<li>With the coming enhanced memory planning, this again will make sure the memory planning logic is the between AOTInductor and Inductor, which will greatly simplify the problem and improve the reliability.</li>
</ul>
<p>This change also combines D49494954 from Yang and https://github.com/pytorch/pytorch/pull/109560 from Angela.</p>
<p>Differential Revision: D49502318</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109790<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Thu, 21 Sep 2023 18:31:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8856c1628e8ddb3fe3bd51ed8cfef5c02ad0a609</guid>
    </item>
    <item>
      <title>[aotinductor] Update performance benchmark code (109560) (#109820)</title>
      <link>https://github.com/pytorch/pytorch/commit/f7ddc545035992a0661a18bd75b4caccaf55fee5</link>
      <description><![CDATA[<p>[aotinductor] Update performance benchmark code (109560) (#109820)</p>
<p>Summary: Same as #109560, made a new PR because we need to land from internal</p>
<p>Previously during performance benchmark testing, we would create an AOTInductorModelContainerHandle every time the compiled function is run with new inputs. However after https://github.com/pytorch/pytorch/pull/108473 we now load the constants needed in the runtime when initializing the AOTInductorModelContainerHandle. This resulted in our benchmarks displaying a ~0.4x speedup.</p>
<p>This diff moves the initialization of AOTInductorModelContainerHandle outside of the code where we run the compiled function with different inputs.</p>
<p>For example,<br />
<code>python benchmarks/dynamo/huggingface.py --performance --cold-start-latency --inference --bfloat16 --export-aot-inductor --disable-cudagraphs --device cuda --total-partitions 3 --partition-id 0 --only AlbertForMaskedLM</code><br />
results in <code>1.359x</code> speedup.</p>
<p>Specifically, this adds a <code>create_container_handle</code> and <code>delete_container_handle</code> function which need to called before <code>run</code>. We call <code>create_container_handle</code> to initialize the AOTInductorModelContainerHandle, call <code>run</code> to run the compiled .so with different inputs, and then <code>delete_container_handle</code> to delete it.</p>
<p><a href="https://hud.pytorch.org/benchmark/compilers?startTime=Wed%2C%2013%20Sep%202023%2021%3A03%3A55%20GMT&amp;stopTime=Wed%2C%2020%20Sep%202023%2021%3A03%3A55%20GMT&amp;granularity=hour&amp;suite=torchbench&amp;mode=inference&amp;dtype=bfloat16&amp;lBranch=angelayi/aot_inductor_benchmark&amp;lCommit=f9aa49c4c9a1a140b6f0c4520d1d6d99b57e12fa&amp;rBranch=main&amp;rCommit=015be4cedba357eb931e24bf188479235db7c5c8">Updated dashboard results</a></p>
<p>Test Plan: CI</p>
<p>Differential Revision: D49513934</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109820<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 21 Sep 2023 12:49:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f7ddc545035992a0661a18bd75b4caccaf55fee5</guid>
    </item>
    <item>
      <title>Fix torchbench --multiprocess (#109657)</title>
      <link>https://github.com/pytorch/pytorch/commit/ef8d461b09d89b6495b7df822f7f69c333867000</link>
      <description><![CDATA[<p>Fix torchbench --multiprocess (#109657)</p>
<p><code>python benchmarks/dynamo/torchbench.py --multiprocess</code> currently fails due to initializing distributed multiple times:</p>
<p><code>torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:6789 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:6789
 (errno: 98 - Address already in use).</code></p>
<p>Because torchbench calls itself via mp.spawn, there is the parent run (with <code>--multiprocess</code>) and child runs (with <code>--multiprocess --only &lt;model&gt;</code>).</p>
<p>This PR addresses this by fixing two issues:<br />
1) distributed is initialized once in parent run and once in child runs, it should be initialized only in child runs where we have accurate rank and world size info<br />
2) torchbench overrides CUDA_VISIBLE_DEVICES/world_size sometimes, but it shouldn't for distributed use cases where we want to use all available gpus</p>
<p>I am also adding a CI test to cover this type of issue in #109311</p>
<h3>Test plan</h3>
<p>parent run test: <code>python benchmarks/dynamo/torchbench.py --ci --accuracy --timing --explain --inductor --device cuda --inference --bfloat16 --output /home/xmfan/local/pytorch/test/test-reports/inference_torchbench.csv --multiprocess</code><br />
child run test: <code>python benchmarks/dynamo/torchbench.py --ci --accuracy --timing --explain --inductor --device cuda --inference --bfloat16 --output /home/xmfan/local/pytorch/test/test-reports/inference_torchbench.csv --multiprocess --only simple_gpt</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109657<br />
Approved by: https://github.com/H-Huang</p>]]></description>
      <pubDate>Thu, 21 Sep 2023 08:53:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ef8d461b09d89b6495b7df822f7f69c333867000</guid>
    </item>
    <item>
      <title>Revert "[inductor] Use _unsafe_view decompostion (#109669)"</title>
      <link>https://github.com/pytorch/pytorch/commit/406b8412c2ade6857edd53ec42467b1268996cc6</link>
      <description><![CDATA[<p>Revert "[inductor] Use _unsafe_view decompostion (#109669)"</p>
<p>This reverts commit 90a2026cd12065994eb234e8c5f332143d9d9468.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/109669 on behalf of https://github.com/clee2000 due to failing internally (<a href="https://github.com/pytorch/pytorch/pull/109669#issuecomment-1729906056">comment</a>)</p>]]></description>
      <pubDate>Thu, 21 Sep 2023 08:25:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/406b8412c2ade6857edd53ec42467b1268996cc6</guid>
    </item>
    <item>
      <title>[inductor] Enable mypy checking for codegen/triton_foreach (#109643)</title>
      <link>https://github.com/pytorch/pytorch/commit/62555930a0a9ea0ee3cd07063b3b83cd0a174c99</link>
      <description><![CDATA[<p>[inductor] Enable mypy checking for codegen/triton_foreach (#109643)</p>
<p>Summary: Add enough typehints to enable mypy checking for codegen/triton_foreach. Also fixed a bug in a dtype param.</p>
<p>Test Plan:<br />
* <code>python test/inductor/test_foreach.py</code><br />
* <code>lintrunner</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109643<br />
Approved by: https://github.com/mlazos<br />
ghstack dependencies: #109146</p>]]></description>
      <pubDate>Thu, 21 Sep 2023 06:30:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/62555930a0a9ea0ee3cd07063b3b83cd0a174c99</guid>
    </item>
    <item>
      <title>[inductor] Set CUDA_VISIBLE_DEVICES for multi-device subprocess autotuning (#109500)</title>
      <link>https://github.com/pytorch/pytorch/commit/4eada253e102705122cdbc532d5a5c5fb7246384</link>
      <description><![CDATA[<p>[inductor] Set CUDA_VISIBLE_DEVICES for multi-device subprocess autotuning (#109500)</p>
<p>Summary: The curent parallel autotune implementation sets the CUDA_VISIBLE_DEVICES env var too late -- after the benchmarking subprocess has started -- and the torch libraries don't recognize the change. Since the multiprocessing library doesn't support providing an environment for the subprocess, temporarily set CUDA_VISIBLE_DEVICES in the parent process so that the change is inherited by the subprocess.</p>
<p>Test Plan:<br />
* New unit test to verify the env var is set in the sub-process and fail the benchmark if it's not.<br />
* Ran multiprocess autotuning and looked at the output from <code>nvidia-smi pmon</code> to make sure that all GPUs were assigned processes.</p>
<p>Snippet:<br />
<code>1    3442314     C     2     1     -     -   python
    2    3442318     C     2     1     -     -   python
    3    3442320     C     8     2     -     -   python
    4    3442323     C     9     4     -     -   python
    5    3442325     C    10     4     -     -   python
    6    3442327     C    10     4     -     -   python
    7    3442329     C     2     0     -     -   python
    0    3434906     C     0     0     -     -   python</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109500<br />
Approved by: https://github.com/eellison, https://github.com/shunting314</p>]]></description>
      <pubDate>Thu, 21 Sep 2023 06:29:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4eada253e102705122cdbc532d5a5c5fb7246384</guid>
    </item>
    <item>
      <title>[inductor] visualize fused ops in svg graph (#107752)</title>
      <link>https://github.com/pytorch/pytorch/commit/772e104dfdfd70c74cbc9600cfc946dc7c378f68</link>
      <description><![CDATA[<p>[inductor] visualize fused ops in svg graph (#107752)</p>
<p>example usage<br />
* <code>TORCH_COMPILE_DEBUG=1 INDUCTOR_ORIG_FX_SVG=1 INDUCTOR_POST_FUSION_SVG=1 python trig.py</code>: show original fx node name, file, and code. see snapshot 2 where we have origin_0, 1, 2<br />
* trig.py can be found in P816304818</p>
<p>Implementation<br />
* keep original fx graph in GraphLowering, <code>self.orig_gm: torch.fx.GraphModule = gm.__copy__()</code><br />
* draw original fx graph with origins ir_post_fusion <code>V.debug.draw_orig_fx_graph(self.orig_gm, self.scheduler.nodes)</code>. node.meta["buff_meta"] tracks buf_name</p>
<p><img width="350" alt="Screenshot 2023-08-29 at 12 40 24 PM" src="https://github.com/pytorch/pytorch/assets/134637289/c4e197cb-ab3b-4a09-a584-c1356376accb"></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107752<br />
Approved by: https://github.com/mlazos</p>]]></description>
      <pubDate>Thu, 21 Sep 2023 00:03:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/772e104dfdfd70c74cbc9600cfc946dc7c378f68</guid>
    </item>
    <item>
      <title>inductor: only do the conv+bn folding for the freezing path (#109587)</title>
      <link>https://github.com/pytorch/pytorch/commit/e4d8ec9fe82b6186efbff0e2a7b8393b157ef298</link>
      <description><![CDATA[<p>inductor: only do the conv+bn folding for the freezing path (#109587)</p>
<p>Re-enable PR: https://github.com/pytorch/pytorch/pull/109270</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109587<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 20 Sep 2023 16:47:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e4d8ec9fe82b6186efbff0e2a7b8393b157ef298</guid>
    </item>
    <item>
      <title>[Inductor] Break the loop fusion when node2 depends on node1 mutations (#109172)</title>
      <link>https://github.com/pytorch/pytorch/commit/9e2b07ac9d189ff05c37a5d1683e239fd36dfe5b</link>
      <description><![CDATA[<p>[Inductor] Break the loop fusion when node2 depends on node1 mutations (#109172)</p>
<p><strong>Summary</strong><br />
Fix the issue: https://github.com/pytorch/pytorch/issues/108963. After this PR, loop fusion should break when node2 depends on node1's buffer mutation. Take the UT as example:</p>
<ul>
<li>Before this PR, the generated code is:<br />
```<br />
cpp_fused_div_index_add_0 = async_compile.cpp('''</li>
</ul>
<h1>include "/tmp/torchinductor_root/ib/cibrnuq56cxamjj4krp4zpjvsirbmlolpbnmomodzyd46huzhdw7.h"</h1>
<p>extern "C" void kernel(const double<em> in_ptr0,<br />
                       const long</em> in_ptr1,<br />
                       const double<em> in_ptr2,<br />
                       double</em> out_ptr0,<br />
                       double* out_ptr1)<br />
{<br />
    {<br />
        auto tmp0 = in_ptr0[static_cast<long>(0L)];<br />
        out_ptr0[static_cast<long>(0L)] = tmp0;<br />
    }<br />
    {<br />
        auto tmp0 = in_ptr1[static_cast<long>(0L)];<br />
        auto tmp1 = in_ptr2[static_cast<long>(0L)];<br />
        auto tmp4 = out_ptr0[static_cast<long>(0L)];<br />
        auto tmp2 = static_cast<double>(2.0);<br />
        auto tmp3 = decltype(tmp1)(tmp1 * tmp2);<br />
        auto tmp5 = tmp4 / tmp2;<br />
        atomic_add(&amp;out_ptr0[static_cast<long>(0L)], tmp3);<br />
        out_ptr1[static_cast<long>(0L)] = tmp5;<br />
    }<br />
}<br />
''')<br />
```</p>
<ul>
<li>After this PR, the generated code is:<br />
```<br />
cpp_fused_div_index_add_0 = async_compile.cpp('''</li>
</ul>
<h1>include "/tmp/torchinductor_root/ib/cibrnuq56cxamjj4krp4zpjvsirbmlolpbnmomodzyd46huzhdw7.h"</h1>
<p>extern "C" void kernel(const double<em> in_ptr0,<br />
                       const long</em> in_ptr1,<br />
                       const double<em> in_ptr2,<br />
                       double</em> out_ptr0,<br />
                       double* out_ptr1)<br />
{<br />
    {<br />
        auto tmp0 = in_ptr0[static_cast<long>(0L)];<br />
        out_ptr0[static_cast<long>(0L)] = tmp0;<br />
    }<br />
    {<br />
        auto tmp0 = in_ptr1[static_cast<long>(0L)];<br />
        auto tmp1 = in_ptr2[static_cast<long>(0L)];<br />
        auto tmp2 = static_cast<double>(2.0);<br />
        auto tmp3 = decltype(tmp1)(tmp1 * tmp2);<br />
        atomic_add(&amp;out_ptr0[static_cast<long>(0L)], tmp3);<br />
    }<br />
    {<br />
        auto tmp0 = out_ptr0[static_cast<long>(0L)];<br />
        auto tmp1 = static_cast<double>(2.0);<br />
        auto tmp2 = tmp0 / tmp1;<br />
        out_ptr1[static_cast<long>(0L)] = tmp2;<br />
    }<br />
}<br />
''')<br />
```</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_torchinductor.py -k test_mutations_loop_fusion</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109172<br />
Approved by: https://github.com/jgong5, https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 20 Sep 2023 16:30:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9e2b07ac9d189ff05c37a5d1683e239fd36dfe5b</guid>
    </item>
    <item>
      <title>[inductor] Clean up AOTInductor runtime ABI (#109678)</title>
      <link>https://github.com/pytorch/pytorch/commit/9c2715bbb208fc47a2b3bab2d8fdb7e14cd82a1a</link>
      <description><![CDATA[<p>[inductor] Clean up AOTInductor runtime ABI (#109678)</p>
<p>Summary: Change the AOTInductor runtime interface to avoid referring to aten data structures directly, mostly at::Tensor and ProxyExecutor. This a combination of https://github.com/pytorch/pytorch/pull/109436,  https://github.com/pytorch/pytorch/pull/109498, https://github.com/pytorch/pytorch/pull/109450, https://github.com/pytorch/pytorch/pull/109606, plus a few internal build changes.</p>
<p>Reviewed By: frank-wei</p>
<p>Differential Revision: D49374820</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109678<br />
Approved by: https://github.com/frank-wei, https://github.com/chenyang78</p>]]></description>
      <pubDate>Wed, 20 Sep 2023 16:25:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9c2715bbb208fc47a2b3bab2d8fdb7e14cd82a1a</guid>
    </item>
    <item>
      <title>[inductor] Use _unsafe_view decompostion (#109669)</title>
      <link>https://github.com/pytorch/pytorch/commit/90a2026cd12065994eb234e8c5f332143d9d9468</link>
      <description><![CDATA[<p>[inductor] Use _unsafe_view decompostion (#109669)</p>
<p>As per the old comment, decomposing is better than lowering because patterns for<br />
<code>view</code> would apply to <code>_unsafe_view</code> as well.</p>
<p>https://github.com/salilsdesai/pytorch/blob/fc47ba2794564592a243b8b3484b5cfee1e72fda/torch/_inductor/decomposition.py#L89</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109669<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #109667, #109668</p>]]></description>
      <pubDate>Wed, 20 Sep 2023 10:45:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/90a2026cd12065994eb234e8c5f332143d9d9468</guid>
    </item>
    <item>
      <title>[aot inductor] Make unit tests work on CPU (#109625)</title>
      <link>https://github.com/pytorch/pytorch/commit/e87bd9f588d44d2d826a9791d9f1bc92adc05d9e</link>
      <description><![CDATA[<p>[aot inductor] Make unit tests work on CPU (#109625)</p>
<p>Summary: AOT inductor is only sort-of supported on CPU right now, but it works<br />
with a few hacks (the .so needs to be compiled and run with CUDA present,<br />
because we haven't excised the CUDA deps; also there's an <code>is_cpu</code> flag that<br />
needs to be plumbed into the call, or else all the weights are erroneously<br />
allocated on GPU).</p>
<p>But, with those hacks in place, it currently works, so it's worth having the<br />
current state of it continue working (and at some point we'll remove the<br />
hacks).</p>
<p>Test Plan:<br />
<code>python test_aot_inductor -k test_simple_cpu</code></p>
<p>Reviewers: binbao</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49427400">D49427400</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/109625<br />
Approved by: https://github.com/mikekgfb, https://github.com/chenyang78, https://github.com/desertfire</p>]]></description>
      <pubDate>Wed, 20 Sep 2023 06:51:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e87bd9f588d44d2d826a9791d9f1bc92adc05d9e</guid>
    </item>
    <item>
      <title>[Decomposition] baddbmm (#108534)</title>
      <link>https://github.com/pytorch/pytorch/commit/40b2c796dcae5768ff5de279b13914d5948fd414</link>
      <description><![CDATA[<p>[Decomposition] baddbmm (#108534)</p>
<p>Summary:<br />
Moving decomposition of baddbmm from _inductor/decomposition.py and include it in core_aten_decompositions</p>
<p>https://github.com/pytorch/pytorch/blob/ff38c0e2f9cae35378553c38ccf7188007fed938/torch/_inductor/decomposition.py#L203</p>
<p>Test Plan: Phabricator + OSS Tests</p>
<p>Differential Revision: D48871741</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108534<br />
Approved by: https://github.com/SherlockNoMad</p>]]></description>
      <pubDate>Wed, 20 Sep 2023 04:49:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/40b2c796dcae5768ff5de279b13914d5948fd414</guid>
    </item>
    <item>
      <title>[Inductor][FX]Support efficient conv bn eval (#108757)</title>
      <link>https://github.com/pytorch/pytorch/commit/b30ee35a6f141d3247a24fd09f96ea50a7e2b3c7</link>
      <description><![CDATA[<p>[Inductor][FX]Support efficient conv bn eval (#108757)</p>
<p>This PR adds an <code>efficient_conv_bn_eval_graph_transform</code> pass to the inductor. It tries to identify consecutive conv + bn <strong>computation</strong> with bn in eval mode, and changes it to a more efficient implementation. It does not modify parameters, which makes it <strong>support training</strong> without any pain. If no such patterns are identified, it does nothing. Therefore, it is backward compatible.</p>
<p>It has great benefit in terms of memory footprint:</p>
<p>For resnet50 with input batchsize 64, image size 224, forward + backward training:</p>
<p>| Technique                   | Memory Footprint (GB)      | Remarks                                   |<br />
|-------------------------------|----------------------------|-------------------------------------------|<br />
| Eager Mode  | 5.18          |                                           |<br />
| torch.compile                 | 5.46         | Strangely, not saving memory              |<br />
| torch.compile with this PR                       | 2.88          | **Saves about 50% memory! **         |</p>
<p>The script to measure the memory footprint:</p>
<p>```python<br />
from torchvision.models.resnet import resnet50<br />
import torch</p>
<p>net = resnet50().eval().cuda()</p>
<p>input = torch.randn(64, 3, 224, 224).cuda()</p>
<p>opt_net = torch.compile(net) # Use torch.compile</p>
<h1>opt_net = net # Eager mode</h1>
<p>current_memory = torch.cuda.memory_allocated()<br />
torch.cuda.reset_peak_memory_stats()</p>
<p>for i in range(10):<br />
    opt_net.zero_grad()<br />
    output = opt_net(input)<br />
    output.sum().backward()<br />
    del output</p>
<p>peak_memory = torch.cuda.max_memory_allocated()<br />
additional_peak_memory = peak_memory - current_memory<br />
print(f"Additional peak memory used: {additional_peak_memory / (1024 ** 3)} GB")<br />
```</p>
<p>More results can be found in the corresponding paper: (this method is called Tune Mode in the tables).</p>
<p><img width="709" alt="image" src="https://github.com/pytorch/pytorch/assets/23236638/db4815b0-d93e-4726-b1d5-e6651f256484"></p>
<p><img width="653" alt="image" src="https://github.com/pytorch/pytorch/assets/23236638/22e5e1ab-6129-4c3d-a875-3c7343293b2e"></p>
<p>Note: the difference between this PR and https://github.com/pytorch/pytorch/pull/106372 is that, https://github.com/pytorch/pytorch/pull/106372 tries to fix and change the implementation of <code>torch.fx.experimental.optimization.fuse</code>, which causes compatibility issues; this PR only introduces a new graph transform passes, and does not break the previous code.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108757<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 20 Sep 2023 00:10:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b30ee35a6f141d3247a24fd09f96ea50a7e2b3c7</guid>
    </item>
    <item>
      <title>[AOTInductor] Fix aot_inductor/test:test_custom_ops (#109660)</title>
      <link>https://github.com/pytorch/pytorch/commit/293205c54b9a0fd3c6dece70a64b6950a7c6e836</link>
      <description><![CDATA[<p>[AOTInductor] Fix aot_inductor/test:test_custom_ops (#109660)</p>
<p>Summary: Fix aot_inductor/test:test_custom_ops, which was broken by https://github.com/pytorch/pytorch/pull/109391</p>
<p>Test Plan: buck2 run mode/dev-nosan //deeplearning/aot_inductor/test:test_custom_ops</p>
<p>Differential Revision: D49438928</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109660<br />
Approved by: https://github.com/desertfire, https://github.com/chenyang78</p>]]></description>
      <pubDate>Tue, 19 Sep 2023 23:44:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/293205c54b9a0fd3c6dece70a64b6950a7c6e836</guid>
    </item>
    <item>
      <title>[inductor] Enable mypy checking for torch/_inductor/codegen/triton.py (#109146)</title>
      <link>https://github.com/pytorch/pytorch/commit/85d26f786899c8a9be09a997bc38232401cf5b70</link>
      <description><![CDATA[<p>[inductor] Enable mypy checking for torch/_inductor/codegen/triton.py (#109146)</p>
<p>Summary: enably mypy chcking for torch/_inductor/codegen/triton.py and make the minimum number of fixes / ignores to get the linter to pass</p>
<p>Test Plan: <code>lintrunner -a</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109146<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Tue, 19 Sep 2023 15:01:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/85d26f786899c8a9be09a997bc38232401cf5b70</guid>
    </item>
    <item>
      <title>[inductor] Decompose torch.ops.quantized.embedding_bag_byte_unpack (#109398)</title>
      <link>https://github.com/pytorch/pytorch/commit/1895bd9bb520f6628c6627ad283c978ae7b4bb68</link>
      <description><![CDATA[<p>[inductor] Decompose torch.ops.quantized.embedding_bag_byte_unpack (#109398)</p>
<p>This would be cleaner if we had support for u8-&gt;float32 views<br />
(bitcasts) in inductor, but it works for now.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49329910/">D49329910</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109398<br />
Approved by: https://github.com/hl475, https://github.com/jansel, https://github.com/jgong5</p>]]></description>
      <pubDate>Tue, 19 Sep 2023 06:11:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1895bd9bb520f6628c6627ad283c978ae7b4bb68</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Enable quantization dynamic batch size support (#108550)</title>
      <link>https://github.com/pytorch/pytorch/commit/4a60bd22b2a54dc21864f8f718782194fcf64ec9</link>
      <description><![CDATA[<p>[Quant][Inductor] Enable quantization dynamic batch size support (#108550)</p>
<p><strong>Summary</strong><br />
This Diff enables dynamic batch size support for quantization use case in Inductor. Take the UT in this PR as example, after this PR, the generated code will have assumption of dynamic input batch size.<br />
```<br />
cpp_fused_quantize_per_tensor_0 = async_compile.cpp('''</p>
<h1>include "/tmp/torchinductor_root/ib/cibrnuq56cxamjj4krp4zpjvsirbmlolpbnmomodzyd46huzhdw7.h"</h1>
<p>extern "C" void kernel(const float<em> in_ptr0,<br />
                       unsigned char</em> out_ptr0,<br />
                       const long ks0,<br />
                       const long ks1)<br />
{<br />
    {<br />
        #pragma GCC ivdep<br />
        for(long i0=static_cast<long>(0L); i0&lt;static_cast<long>(ks0); i0+=static_cast<long>(1L))<br />
        {<br />
            #pragma GCC ivdep<br />
            for(long i1=static_cast<long>(0L); i1&lt;static_cast<long>(3L); i1+=static_cast<long>(1L))<br />
            {<br />
                #pragma GCC ivdep<br />
                for(long i2=static_cast<long>(0L); i2&lt;static_cast<long>(static_cast<long>(ks1<em>ks1)); i2+=static_cast<long>(1L))<br />
                {<br />
                    auto tmp0 = in_ptr0[static_cast<long>(i2 + (i1</em>(static_cast<long>(ks1<em>ks1))) + (3L</em>i0<em>(static_cast<long>(ks1</em>ks1))))];<br />
                    auto tmp1 = static_cast<float>(40.36037717834931);<br />
                    auto tmp2 = decltype(tmp0)(tmp0 * tmp1);<br />
                    auto tmp3 = std::nearbyint(tmp2);<br />
                    auto tmp4 = static_cast<float>(97.0);<br />
                    auto tmp5 = tmp3 + tmp4;<br />
                    auto tmp6 = static_cast<float>(0.0);<br />
                    auto tmp7 = max_propagate_nan(tmp5, tmp6);<br />
                    auto tmp8 = static_cast<float>(255.0);<br />
                    auto tmp9 = min_propagate_nan(tmp7, tmp8);<br />
                    auto tmp10 = static_cast<unsigned char>(tmp9);<br />
                    out_ptr0[static_cast<long>(i1 + (3L<em>i2) + (3L</em>i0<em>(static_cast<long>(ks1</em>ks1))))] = tmp10;<br />
                }<br />
            }<br />
        }<br />
    }<br />
}<br />
''')</p>
<p>cpp_fused_dequantize_per_tensor_mean_quantize_per_tensor_1 = async_compile.cpp('''</p>
<h1>include "/tmp/torchinductor_root/ib/cibrnuq56cxamjj4krp4zpjvsirbmlolpbnmomodzyd46huzhdw7.h"</h1>
<p>extern "C" void kernel(const unsigned char<em> in_ptr0,<br />
                       float</em> out_ptr0,<br />
                       unsigned char<em> out_ptr1,<br />
                       const long ks0,<br />
                       const long ks1)<br />
{<br />
    {<br />
        #pragma GCC ivdep<br />
        for(long i0=static_cast<long>(0L); i0&lt;static_cast<long>(ks0); i0+=static_cast<long>(1L))<br />
        {<br />
            for(long i1=static_cast<long>(0L); i1&lt;static_cast<long>(16L); i1+=static_cast<long>(16L))<br />
            {<br />
                {<br />
                    #pragma omp declare reduction(+:at::vec::Vectorized<float>:omp_out = omp_out + omp_in) initializer(omp_priv={at::vec::Vectorized<float>(0)})<br />
                    float tmp_acc0 = 0;<br />
                    at::vec::Vectorized<float> tmp_acc0_vec = at::vec::Vectorized<float>(0);<br />
                    for(long i2=static_cast<long>(0L); i2&lt;static_cast<long>(1L + (static_cast<long>((at::native::div_floor_integer(ks1, 2L))</em>(at::native::div_floor_integer(ks1, 2L)))) + (2L<em>(at::native::div_floor_integer(ks1, 2L)))); i2+=static_cast<long>(1L))<br />
                    {<br />
                        auto tmp0 = at::vec::Vectorized<uint8_t>::loadu_one_fourth(in_ptr0 + static_cast<long>(i1 + (16L</em>i0) + (16L<em>i2) + (16L</em>i0<em>(static_cast<long>((at::native::div_floor_integer(ks1, 2L))</em>(at::native::div_floor_integer(ks1, 2L))))) + (32L<em>i0</em>(at::native::div_floor_integer(ks1, 2L)))));<br />
                        auto tmp1 = at::vec::convert_uint8_to_float(tmp0);<br />
                        auto tmp2 = at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                        auto tmp3 = tmp1 - tmp2;<br />
                        auto tmp4 = at::vec::Vectorized<float>(static_cast<float>(0.010429476387798786));<br />
                        auto tmp5 = tmp3 * tmp4;<br />
                        tmp_acc0_vec = tmp_acc0_vec + tmp5;<br />
                    }<br />
                    tmp_acc0_vec.store(out_ptr0 + static_cast<long>(i1 + (16L<em>i0)));<br />
                }<br />
            }<br />
        }<br />
    }<br />
    {<br />
        #pragma GCC ivdep<br />
        for(long i0=static_cast<long>(0L); i0&lt;static_cast<long>(16L</em>ks0); i0+=static_cast<long>(1L))<br />
        {<br />
            auto tmp0 = out_ptr0[static_cast<long>(i0)];<br />
            auto tmp1 = static_cast<float>(1L + (static_cast<long>((at::native::div_floor_integer(ks1, 2L))<em>(at::native::div_floor_integer(ks1, 2L)))) + (2L</em>(at::native::div_floor_integer(ks1, 2L))));<br />
            auto tmp2 = tmp0 / tmp1;<br />
            auto tmp3 = static_cast<float>(168.09128392896545);<br />
            auto tmp4 = decltype(tmp2)(tmp2 * tmp3);<br />
            auto tmp5 = std::nearbyint(tmp4);<br />
            auto tmp6 = static_cast<float>(0.0);<br />
            auto tmp7 = tmp5 + tmp6;<br />
            auto tmp8 = max_propagate_nan(tmp7, tmp6);<br />
            auto tmp9 = static_cast<float>(255.0);<br />
            auto tmp10 = min_propagate_nan(tmp8, tmp9);<br />
            auto tmp11 = static_cast<unsigned char>(tmp10);<br />
            out_ptr1[static_cast<long>(i0)] = tmp11;<br />
        }<br />
    }<br />
}<br />
''')</p>
<p>cpp_fused_dequantize_per_tensor_2 = async_compile.cpp('''</p>
<h1>include "/tmp/torchinductor_root/ib/cibrnuq56cxamjj4krp4zpjvsirbmlolpbnmomodzyd46huzhdw7.h"</h1>
<p>extern "C" void kernel(const unsigned char<em> in_ptr0,<br />
                       float</em> out_ptr0,<br />
                       const long ks0)<br />
{<br />
    {<br />
        for(long i0=static_cast<long>(0L); i0&lt;static_cast<long>(16L*ks0); i0+=static_cast<long>(16L))<br />
        {<br />
            auto tmp0 = at::vec::Vectorized<uint8_t>::loadu_one_fourth(in_ptr0 + static_cast<long>(i0));<br />
            auto tmp1 = at::vec::convert_uint8_to_float(tmp0);<br />
            auto tmp2 = at::vec::Vectorized<float>(static_cast<float>(100.0));<br />
            auto tmp3 = tmp1 - tmp2;<br />
            auto tmp4 = at::vec::Vectorized<float>(static_cast<float>(0.0056716203689575195));<br />
            auto tmp5 = tmp3 * tmp4;<br />
            tmp5.store(out_ptr0 + static_cast<long>(i0));<br />
        }<br />
    }<br />
}<br />
''')</p>
<p>async_compile.wait(globals())<br />
del async_compile</p>
<p>def call(args):<br />
    arg8_1, arg9_1, arg10_1 = args<br />
    args.clear()<br />
    s0 = arg8_1<br />
    s2 = arg9_1<br />
    assert_size_stride(arg10_1, (s0, 3, s2, s2), (3<em>(s2</em>s2), s2<em>s2, s2, 1))<br />
    buf0 = empty_strided((s0, 3, s2, s2), (3</em>(s2<em>s2), 1, 3</em>s2, 3), device='cpu', dtype=torch.uint8)<br />
    cpp_fused_quantize_per_tensor_0(c_void_p(arg10_1.data_ptr()), c_void_p(buf0.data_ptr()), c_long(s0), c_long(s2))<br />
    del arg10_1<br />
    buf1 = torch.ops.onednn.qconv2d_pointwise(buf0, 0.024776775389909744, 97, constant5, constant2, constant3, constant0, [1, 1], [1, 1], [1, 1], 1, 95.88209060714476, 0, False, 'relu', [], '')<br />
    assert_size_stride(buf1, (s0, 16, 1 + s2, 1 + s2), (16 + (16<em>(s2</em>s2)) + (32<em>s2), 1, 16 + (16</em>s2), 16))<br />
    del buf0<br />
    # Source Nodes: [quantize_per_tensor_default_2], Original ATen: [quantized_decomposed.quantize_per_tensor]<br />
    buf2 = torch.ops.quantized.max_pool2d(buf1, [3, 3], [2, 2], [1, 1], [1, 1], False)<br />
    del buf1<br />
    buf3 = buf2<br />
    assert_size_stride(buf3, (s0, 16, 1 + (s2 // 2), 1 + (s2 // 2)), (16 + (16<em>((s2 // 2)</em>(s2 // 2))) + (32<em>(s2 // 2)), 1, 16 + (16</em>(s2 // 2)), 16))<br />
    del buf2<br />
    buf4 = empty_strided((s0, 16, 1, 1), (16, 1, 16<em>s0, 16</em>s0), device='cpu', dtype=torch.float32)<br />
    buf5 = empty_strided((s0, 16), (16, 1), device='cpu', dtype=torch.uint8)<br />
    cpp_fused_dequantize_per_tensor_mean_quantize_per_tensor_1(c_void_p(buf3.data_ptr()), c_void_p(buf4.data_ptr()), c_void_p(buf5.data_ptr()), c_long(s0), c_long(s2))<br />
    del buf3<br />
    buf6 = torch.ops.onednn.qlinear_pointwise(buf5, 0.005949148442596197, 0, constant6, constant4, constant3, constant1, 176.31645543014483, 100, False, 'none', [], '')<br />
    assert_size_stride(buf6, (s0, 16), (16, 1))<br />
    del buf5<br />
    buf7 = reinterpret_tensor(buf4, (s0, 16), (16, 1)); del buf4  # reuse<br />
    cpp_fused_dequantize_per_tensor_2(c_void_p(buf6.data_ptr()), c_void_p(buf7.data_ptr()), c_long(s0))<br />
    return (buf7, )</p>
<p>```</p>
<p><strong>TestPlan</strong><br />
<code>python -m pytest test_mkldnn_pattern_matcher.py -k test_qconv2d_maxpool2d_linear_dynamic</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108550<br />
Approved by: https://github.com/jgong5, https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 19 Sep 2023 00:30:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4a60bd22b2a54dc21864f8f718782194fcf64ec9</guid>
    </item>
    <item>
      <title>Revert "inductor: only do the conv+bn folding for the freezing path (#109270)"</title>
      <link>https://github.com/pytorch/pytorch/commit/70ca3ee951ba3dc58e5e42a96bcca0245d8f12bc</link>
      <description><![CDATA[<p>Revert "inductor: only do the conv+bn folding for the freezing path (#109270)"</p>
<p>This reverts commit c7017fff38e73210541124739ed9404492ddd68c.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/109270 on behalf of https://github.com/malfet due to Broke slow test, see https://hud.pytorch.org/pytorch/pytorch/commit/c7017fff38e73210541124739ed9404492ddd68c (<a href="https://github.com/pytorch/pytorch/pull/109270#issuecomment-1724132526">comment</a>)</p>]]></description>
      <pubDate>Mon, 18 Sep 2023 10:15:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/70ca3ee951ba3dc58e5e42a96bcca0245d8f12bc</guid>
    </item>
    <item>
      <title>Enable typechecking for _inductor/fx_utils.py (#109415)</title>
      <link>https://github.com/pytorch/pytorch/commit/5cd8a6d40a8a68f611cff34a561ac20d2ea5e561</link>
      <description><![CDATA[<p>Enable typechecking for _inductor/fx_utils.py (#109415)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109415<br />
Approved by: https://github.com/Skylion007<br />
ghstack dependencies: #109269, #109347, #109335</p>]]></description>
      <pubDate>Mon, 18 Sep 2023 10:12:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5cd8a6d40a8a68f611cff34a561ac20d2ea5e561</guid>
    </item>
    <item>
      <title>Enable typechecking for _inductor/debug.py (#109335)</title>
      <link>https://github.com/pytorch/pytorch/commit/fe452108fbf057dd82659f3c5a6b8593f893242d</link>
      <description><![CDATA[<p>Enable typechecking for _inductor/debug.py (#109335)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109335<br />
Approved by: https://github.com/eellison<br />
ghstack dependencies: #109269, #109347</p>]]></description>
      <pubDate>Mon, 18 Sep 2023 10:12:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fe452108fbf057dd82659f3c5a6b8593f893242d</guid>
    </item>
    <item>
      <title>Enable typechecking for _inductor/freezing.py (#109269)</title>
      <link>https://github.com/pytorch/pytorch/commit/bab627073a9cc673a52c9415036e1253105b2f7e</link>
      <description><![CDATA[<p>Enable typechecking for _inductor/freezing.py (#109269)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109269<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Mon, 18 Sep 2023 10:12:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bab627073a9cc673a52c9415036e1253105b2f7e</guid>
    </item>
    <item>
      <title>[inductor] scale down RBLOCK for occupancy (#109275)</title>
      <link>https://github.com/pytorch/pytorch/commit/cddeceb6b6a64ad5dae7b19532fc5f7bfbe704fa</link>
      <description><![CDATA[<p>[inductor] scale down RBLOCK for occupancy (#109275)</p>
<p>For large reduction (with large xnumel and rnumel), we potentially need run large number of thread blocks. Occupancy matters here since with larger occupancy we can run more blocks on each SM and we may need less number of waves to run the entire kernel on the GPU.  Number of registers used by each thread can limit the occupancy. For A100, it's safe to say that register usage does not limit occupancy only if each thread use &lt;= 32 registers. This PR leverage this observation and reduce RBLOCK (thus reduce registers used by each thread) if thread usage limit occupancy for large reduction.</p>
<p>The scenario mentioned can happen for the softmax kernel used in transformers. Here are some results get from devgpu:<br />
- PLBartForCausalLM we improve from 1.88x (58.7ms) to 2.00x (55.82ms)<br />
- TrOCRForCausalLM we improve from 1.45x (92.9ms) to 1.51x (89.12ms)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109275<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 18 Sep 2023 09:29:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cddeceb6b6a64ad5dae7b19532fc5f7bfbe704fa</guid>
    </item>
    <item>
      <title>[inductor] Fix CudaStreamGuard in AOTInductor ABI compatible mode (#109471)</title>
      <link>https://github.com/pytorch/pytorch/commit/6ffa59031acf4cd20213d9d6d4cca72237207b9c</link>
      <description><![CDATA[<p>[inductor] Fix CudaStreamGuard in AOTInductor ABI compatible mode (#109471)</p>
<p>Summary: Use a RAII class to wrap around at::cuda::CUDAStreamGuard. Previous implementation didn't follow the exact CUDAStreamGuard behavior.</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D49355542</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109471<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Mon, 18 Sep 2023 07:54:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/6ffa59031acf4cd20213d9d6d4cca72237207b9c</guid>
    </item>
    <item>
      <title>[RELAND] Force synced KJT to trace unbacked SymInt (#108960) (#109216)</title>
      <link>https://github.com/pytorch/pytorch/commit/88600e7d2e3bb2c1c3db84048eaa581bc60024c0</link>
      <description><![CDATA[<p>[RELAND] Force synced KJT to trace unbacked SymInt (#108960) (#109216)</p>
<p>Summary:</p>
<p>The basic concept behind this diff is to modify Dynamo's tracing behavior when it encounters a KeyedJaggedTensor that is synced (aka has <code>_length_per_key</code> and <code>_offset_per_key</code> populated). These fields are lists of integers; ordinarily, Dynamo will optimistically try to specialize on integers, however, for KJTs, we know that these integers will definitely vary from run-to-run. Furthermore, ordinarily, we would also specialize these integers if they are 0/1, but we will frequently expect features in KJTs to be 0/1.</p>
<p>The fix is to detect KJTs and treat these integers as <em>unbacked integers</em>. This is NOT a universally sound optimization: when treating these integers as unbacked, we never report them as equal to zero or one. In return, we always generate graphs that generalize no matter the length of values on features. This is enough to trace through APS sparse arch, torchrec_dlrm and some small split-cat examples.</p>
<p>The special integer behavior is triggered by a dynamically scoped <code>force_unspec_int_unbacked_size_like</code> variable on TracingContext, which we trigger when we wrap a KJT. There probably are other ways to do this, but this was simple and worked.</p>
<p>Test Plan:<br />
<code>buck2 test mode/dev-nosan //pytorch/benchmark/fb/test_gpu:run_test_gpu</code></p>
<p>from aakhundov</p>
<ol>
<li>first build feed_lower_benchmark:<br />
<code>buck2 build --show-output mode/opt -c python.package_style=inplace -c fbcode.enable_gpu_sections=true -c fbcode.platform=platform010 -c fbcode.split-dwarf=true hpc/new/models/feed/benchmark:feed_lower_benchmark</code></li>
<li>then run the lowering of the model with it:<br />
<code>TORCHINDUCTOR_MAX_AUTOTUNE=1 TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCH_LOGS="output_code,graph_code" TORCH_COMPILE_DEBUG=1 ../buck-out/v2/gen/fbcode/79c6b019ee0f9469/hpc/new/models/feed/benchmark/__feed_lower_benchmark__/feed_lower_benchmark.par --load=manifold://ig_inference_model/tree/user/facebook/fblearner/predictor/960999465/60/gpu_lowering/input.predictor --skip-trt --skip-ait --sync-mode=0 --enable-aot-inductor --lower-presets="ig_stories" --gpu-trace</code><br />
cf https://docs.google.com/document/d/1yD30xYrdmM8r2HTdmXnZTg0-MHVexfVrAa0294m1AUE/edit?pli=1#heading=h.qiv3fp7e6zg0</li>
</ol>
<p>From torchrec: https://www.internalfb.com/intern/wiki/Torchrec/Development/Testing_production_models/</p>
<p>From ge0405<br />
baseline (without your diff): f477293168<br />
your diff: f477292363</p>
<p><code>buck2 test //caffe2/test/dynamo:test_dynamo_torchrec
buck2 run 'fbcode//mode/opt' fbcode//pytorch/benchmark/fb/test_gpu:run_test_gpu -- 'pytorch.benchmark.fb.test_gpu.test_gpu.TestBenchmarkFbGpu.test_train_blue_reels_vdd_v3_inductor_speedup'</code></p>
<p>Differential Revision: D49236757</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109216<br />
Approved by: https://github.com/voznesenskym</p>]]></description>
      <pubDate>Mon, 18 Sep 2023 06:39:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/88600e7d2e3bb2c1c3db84048eaa581bc60024c0</guid>
    </item>
    <item>
      <title>[inductor] realize_into should not alias src and dst (#108126)</title>
      <link>https://github.com/pytorch/pytorch/commit/1a361e4e9ff1a8dadd72c7301441cfb83d687f24</link>
      <description><![CDATA[<p>[inductor] realize_into should not alias src and dst (#108126)</p>
<p>Fixes #107995</p>
<p>In the reproducer we have the fx graph:<br />
```python<br />
class <lambda>(torch.nn.Module):<br />
    def forward(self, arg0_1: f32[1]):<br />
        # File: <ipython-input-1-5f62cb746ad5>:10, code: return self.layer1(inputs)<br />
        gt: b8[1] = torch.ops.aten.gt.Scalar(arg0_1, 0)<br />
        mul: f32[1] = torch.ops.aten.mul.Tensor(arg0_1, 5.2955089)<br />
        where: f32[1] = torch.ops.aten.where.self(gt, arg0_1, mul);  gt = mul = None</p>
<pre><code>    # No stacktrace found for following nodes
    copy_: f32[1] = torch.ops.aten.copy_.default(arg0_1, where);  arg0_1 = None
    return (where,)
</code></pre>
<p>```</p>
<p>The <code>where</code> node is both copied into <code>arg0_1</code> and returned as the output of the<br />
function. Currently <code>realize_into</code> converts the where's storage into a<br />
<code>MutationLayout</code> of <code>arg0_1</code>, for which no tensor named <code>buf0</code> is allocated.</p>
<p>This is incorrect as <code>where</code> and <code>arg0_1</code> shouldn't share storage. It also<br />
breaks the wrapper code generation which references <code>buf0</code> directly in the<br />
return, but never allocates a <code>buf0</code>.</p>
<p>This issue only appears for size zero tensors, because otherwise the <code>src</code><br />
buffer becomes a user of <code>arg0_1</code> which forces this copy to happen anyway.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108126<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Mon, 18 Sep 2023 06:16:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1a361e4e9ff1a8dadd72c7301441cfb83d687f24</guid>
    </item>
    <item>
      <title>[inductor] Forward fix a windows test error (#109449)</title>
      <link>https://github.com/pytorch/pytorch/commit/c8e4e08c8d10d8b43595e8900c51fb763342bcc1</link>
      <description><![CDATA[<p>[inductor] Forward fix a windows test error (#109449)</p>
<p>Summary: forward fix a windows test error from https://github.com/pytorch/pytorch/pull/109391</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109449<br />
Approved by: https://github.com/mikekgfb</p>]]></description>
      <pubDate>Sun, 17 Sep 2023 11:54:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c8e4e08c8d10d8b43595e8900c51fb763342bcc1</guid>
    </item>
    <item>
      <title>inductor: only do the conv+bn folding for the freezing path (#109270)</title>
      <link>https://github.com/pytorch/pytorch/commit/c7017fff38e73210541124739ed9404492ddd68c</link>
      <description><![CDATA[<p>inductor: only do the conv+bn folding for the freezing path (#109270)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109270<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Sun, 17 Sep 2023 04:36:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c7017fff38e73210541124739ed9404492ddd68c</guid>
    </item>
    <item>
      <title>[inductor] Add a C shim layer for libtorch (#109391)</title>
      <link>https://github.com/pytorch/pytorch/commit/0f646b1d154fdd7a567b29598d79298bbca24841</link>
      <description><![CDATA[<p>[inductor] Add a C shim layer for libtorch (#109391)</p>
<p>Summary:<br />
This PR adds a limited C shim layer for libtorch. The ultimate goal is to ban any direct reference to aten/c10 data structures or functions, to avoid ABI breakage by providing stable C interfaces.</p>
<p>To make the review and landing easier, we broke the changes into several steps. In this PR (a combination of https://github.com/pytorch/pytorch/pull/109022 and https://github.com/pytorch/pytorch/pull/109351), we add C interfaces for certain libtorch functions and modify the wrapper codegen to generate calls to those interfaces. There are a few other items to be addressed in future PRs:</p>
<ul>
<li>The AOTInductor runtime interface still takes lists of aten tensors as input and output</li>
<li>The interaction with ProxyExecutor (general fallback support) needs to move away from aten tensor</li>
<li>Remove all references to aten/c10 headers in the AOTInductor-generated code</li>
</ul>
<p>Differential Revision: D49302669</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109391<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Sat, 16 Sep 2023 08:46:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0f646b1d154fdd7a567b29598d79298bbca24841</guid>
    </item>
    <item>
      <title>[inductor] Remove a bunch of check_gradient=False in opinfo tests (#109417)</title>
      <link>https://github.com/pytorch/pytorch/commit/58bdc63dd6761cf7d50f9319024e4a9f09b6206a</link>
      <description><![CDATA[<p>[inductor] Remove a bunch of check_gradient=False in opinfo tests (#109417)</p>
<p>Despite what the comments say, they do not seem to segfault not cause<br />
CUDA errors any more.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109417<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #109359, #109416</p>]]></description>
      <pubDate>Sat, 16 Sep 2023 05:31:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/58bdc63dd6761cf7d50f9319024e4a9f09b6206a</guid>
    </item>
    <item>
      <title>Have inductor tests call output_process_fn_grad (#109416)</title>
      <link>https://github.com/pytorch/pytorch/commit/1e4f2b576dc168f63023a633dd9c24eee98dfd0a</link>
      <description><![CDATA[<p>Have inductor tests call output_process_fn_grad (#109416)</p>
<p>This is similar to what's done in test_ops.py.</p>
<p>Fixes #109353.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109416<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #109359</p>]]></description>
      <pubDate>Sat, 16 Sep 2023 05:31:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1e4f2b576dc168f63023a633dd9c24eee98dfd0a</guid>
    </item>
    <item>
      <title>Inductor: Increase multiplier to 3 for Inductor AMP benchmark correctness check (#109097)</title>
      <link>https://github.com/pytorch/pytorch/commit/55c19a3c6d38a49fe34e008c4c566445c43810f0</link>
      <description><![CDATA[<p>Inductor: Increase multiplier to 3 for Inductor AMP benchmark correctness check (#109097)</p>
<p><strong>Summary</strong><br />
As reported in https://github.com/pytorch/pytorch/issues/108333, we find some of the models have failed the benchmark's correctness check. However, the end-to-end model's accuracy (<a href="https://gist.github.com/leslie-fang-intel/aac8b3c2b450532fd0517c758bb845e0">test script</a>) when comparing AMP with FP32 is within a difference of less than 0.1%. Thus, it's possible that the correctness check failures for these models are false alarms. We use multiplier of 3 instead of 2 in this PR to avoid these false alarms. Model end-to-end accuracy test results are:</p>
<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:x="urn:schemas-microsoft-com:office:excel"
xmlns="http://www.w3.org/TR/REC-html40">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content="Microsoft Excel 15">
<link id=Main-File rel=Main-File
href="file:///C:/Users/jiahaofa/AppData/Local/Temp/msohtmlclip1/01/clip.htm">
<link rel=File-List
href="file:///C:/Users/jiahaofa/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml">
</head>

<body link="#0563C1" vlink="#954F72">

SPR |   |   |   |   |   |  
-- | -- | -- | -- | -- | -- | --
  | FP32 Imperative TOP1 Accuracy | FP32 Imperative TOP5 Accuracy | BF16 AMP Inductor TOP1 Accuracy | BF16 AMP Inductor TOP5 Accuracy | BF16/FP32 Relative Loss TOP1 Accuracy | BF16/FP32 Relative Loss TOP5 Accuracy
gluon_inception_v3 | 73.262 | 90.774 | 73.256 | 90.802 | -0.01% | 0.03%
mobilenetv2_100 | 72.89 | 90.996 | 72.826 | 90.946 | -0.09% | -0.05%
mobilenetv3_large_100 | 75.72 | 92.55 | 75.764 | 92.554 | 0.06% | 0.00%

</body>

</html>

<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109097<br />
Approved by: https://github.com/jgong5, https://github.com/jansel</p>]]></description>
      <pubDate>Sat, 16 Sep 2023 02:02:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/55c19a3c6d38a49fe34e008c4c566445c43810f0</guid>
    </item>
    <item>
      <title>Use `torch.cumsum` instead of numpy one (#109400)</title>
      <link>https://github.com/pytorch/pytorch/commit/fb58a72d968e34f6a917ce4bc47ed18cd661011b</link>
      <description><![CDATA[<p>Use <code>torch.cumsum</code> instead of numpy one (#109400)</p>
<p><code>s/list(numpy.cumsum(foo))/torch.cumsum(torch.tensor(foo), 0).tolist()/</code></p>
<p>Test plan: <code>python3 ../test/inductor/test_split_cat_fx_passes.py -v</code></p>
<p>Partially addresses https://github.com/pytorch/pytorch/issues/109387</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109400<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Fri, 15 Sep 2023 18:52:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fb58a72d968e34f6a917ce4bc47ed18cd661011b</guid>
    </item>
    <item>
      <title>Revert "[inductor][Optimus]Improve logging for group batch fusion (#109314)"</title>
      <link>https://github.com/pytorch/pytorch/commit/7af792ab05076bf68107eff87eb88d627d0c68f8</link>
      <description><![CDATA[<p>Revert "[inductor][Optimus]Improve logging for group batch fusion (#109314)"</p>
<p>This reverts commit afad0d074b5504c87aa1dc9ae352686a8dd3a8eb.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/109314 on behalf of https://github.com/facebook-github-bot due to Diff reverted internally (<a href="https://github.com/pytorch/pytorch/pull/109314#issuecomment-1722015037">comment</a>)</p>]]></description>
      <pubDate>Fri, 15 Sep 2023 15:28:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7af792ab05076bf68107eff87eb88d627d0c68f8</guid>
    </item>
    <item>
      <title>[inductor][Optimus]Improve logging for group batch fusion (#109314)</title>
      <link>https://github.com/pytorch/pytorch/commit/afad0d074b5504c87aa1dc9ae352686a8dd3a8eb</link>
      <description><![CDATA[<p>[inductor][Optimus]Improve logging for group batch fusion (#109314)</p>
<p>Summary: Log graph with Everpaste for debug and find more patterns to fuse</p>
<p>Test Plan: to add logs</p>
<p>Differential Revision: D49284640</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109314<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Fri, 15 Sep 2023 12:46:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/afad0d074b5504c87aa1dc9ae352686a8dd3a8eb</guid>
    </item>
    <item>
      <title>[AOTInductor] Do not hardcode directory with .cubin files (#109151)</title>
      <link>https://github.com/pytorch/pytorch/commit/cc03e3a8922d286931bf5468e6dea9e82ff95625</link>
      <description><![CDATA[<p>[AOTInductor] Do not hardcode directory with .cubin files (#109151)</p>
<p>Reviewed By: frank-wei, chenyang78</p>
<p>Differential Revision: D49081883</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109151<br />
Approved by: https://github.com/chenyang78</p>]]></description>
      <pubDate>Fri, 15 Sep 2023 10:38:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cc03e3a8922d286931bf5468e6dea9e82ff95625</guid>
    </item>
    <item>
      <title>[inductor] Enable mypy checking for torch/_inductor/bounds.py (#109271)</title>
      <link>https://github.com/pytorch/pytorch/commit/86e6bd3e53bdbc55449b491ae3264fe56246921a</link>
      <description><![CDATA[<p>[inductor] Enable mypy checking for torch/_inductor/bounds.py (#109271)</p>
<p>Summary: Add type hints and enable mypy checking for torch/_inductor/bounds.py</p>
<p>Test Plan: lintrunner</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109271<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Fri, 15 Sep 2023 09:47:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/86e6bd3e53bdbc55449b491ae3264fe56246921a</guid>
    </item>
    <item>
      <title>[BE] Do not use `numpy` in `torch._inductor.codegen.cpp` (#109324)</title>
      <link>https://github.com/pytorch/pytorch/commit/a9bf1031d4b4822130a67055964625c586d624e8</link>
      <description><![CDATA[<p>[BE] Do not use <code>numpy</code> in <code>torch._inductor.codegen.cpp</code> (#109324)</p>
<p><code>s/numpy.iinfo(numpy.int32)/torch.iinfo(torch.int32)/</code> as those two are interchangeable</p>
<p>Partially addresses https://github.com/pytorch/pytorch/issues/109387</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109324<br />
Approved by: https://github.com/albanD</p>]]></description>
      <pubDate>Fri, 15 Sep 2023 09:29:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a9bf1031d4b4822130a67055964625c586d624e8</guid>
    </item>
    <item>
      <title>[inductor] Lower masked_scatter on CUDA (#108803)</title>
      <link>https://github.com/pytorch/pytorch/commit/aed9bee0413dac190452fbfa9ab2a44b6e6843f5</link>
      <description><![CDATA[<p>[inductor] Lower masked_scatter on CUDA (#108803)</p>
<p>This decomposes masked_scatter into <code>aten.cumsum</code> and a single pointwise kernel,<br />
which is similar to what is done in eager. I only do this for CUDA because on CPU<br />
it isn't split into two passes like this so would cause a slowdown.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108803<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Fri, 15 Sep 2023 08:36:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/aed9bee0413dac190452fbfa9ab2a44b6e6843f5</guid>
    </item>
    <item>
      <title>Enable typing for _inductor/exc.py (#109176)</title>
      <link>https://github.com/pytorch/pytorch/commit/66fdea606df1f10b0b4785bd26a04731b3e7ecdc</link>
      <description><![CDATA[<p>Enable typing for _inductor/exc.py (#109176)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109176<br />
Approved by: https://github.com/eellison<br />
ghstack dependencies: #109173</p>]]></description>
      <pubDate>Fri, 15 Sep 2023 04:36:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/66fdea606df1f10b0b4785bd26a04731b3e7ecdc</guid>
    </item>
    <item>
      <title>Add more types for inductor_prims.py (#109173)</title>
      <link>https://github.com/pytorch/pytorch/commit/bd89f80baebbab99c2f05fbb3adbaef41ab4491f</link>
      <description><![CDATA[<p>Add more types for inductor_prims.py (#109173)</p>
<p>Also fix a grammatical issue in the docstring.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/109173<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Fri, 15 Sep 2023 04:36:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bd89f80baebbab99c2f05fbb3adbaef41ab4491f</guid>
    </item>
    <item>
      <title>[inductor] update fbcode skips for AOTInductor (#109313)</title>
      <link>https://github.com/pytorch/pytorch/commit/34ddf08f2737025fb1447070fae3087889b2e8bb</link>
      <description><![CDATA[<p>[inductor] update fbcode skips for AOTInductor (#109313)</p>
<p>Summary: seems like the <code>if __name__ == "__main__":</code> part doesn't run in fbcode; instead, just add skips</p>
<p>Differential Revision: D49258492</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109313<br />
Approved by: https://github.com/desertfire, https://github.com/chenyang78</p>]]></description>
      <pubDate>Thu, 14 Sep 2023 20:28:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/34ddf08f2737025fb1447070fae3087889b2e8bb</guid>
    </item>
    <item>
      <title>Add meta and OpInfo for _embedding_bag_dense_backward (#109211)</title>
      <link>https://github.com/pytorch/pytorch/commit/fe14e43d14420a53426215a5fff30113da6d216a</link>
      <description><![CDATA[<p>Add meta and OpInfo for _embedding_bag_dense_backward (#109211)</p>
<p>The sample inputs is a bit involved because there are a lot of<br />
shenanigans in the derivative formula.  Check comments.</p>
<p>This is exercised in vdd, internal test <code>buck2 run '@fbcode//mode/opt' fbcode//pytorch/benchmark/fb/test_gpu:run_test_gpu -- 'pytorch.benchmark.fb.test_gpu.test_gpu.TestBenchmarkFbGpu.test_train_blue_reels_vdd_v3_inductor_speedup'</code></p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/109211<br />
Approved by: https://github.com/albanD, https://github.com/zou3519</p>]]></description>
      <pubDate>Thu, 14 Sep 2023 10:49:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fe14e43d14420a53426215a5fff30113da6d216a</guid>
    </item>
    <item>
      <title>Revert "[inductor] Lower masked_scatter on CUDA (#108803)"</title>
      <link>https://github.com/pytorch/pytorch/commit/900288f138529e5e8ed83ebda2c5b782751c9b30</link>
      <description><![CDATA[<p>Revert "[inductor] Lower masked_scatter on CUDA (#108803)"</p>
<p>This reverts commit e4036ed7068cdcbe07470c1740ca25ab8ead7a3b.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/108803 on behalf of https://github.com/peterbell10 due to Bot merged after aborted rebase (<a href="https://github.com/pytorch/pytorch/pull/108803#issuecomment-1719918831">comment</a>)</p>]]></description>
      <pubDate>Thu, 14 Sep 2023 10:12:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/900288f138529e5e8ed83ebda2c5b782751c9b30</guid>
    </item>
    <item>
      <title>[dynamo] implement custom dict variable as a general solution for HF's ModelOutput class (#105044)</title>
      <link>https://github.com/pytorch/pytorch/commit/9021fb8dacb9751dd4baac90bf5f695676989bba</link>
      <description><![CDATA[<p>[dynamo] implement custom dict variable as a general solution for HF's ModelOutput class (#105044)</p>
<p>before the PR, for HF's ModelOutput class, we use dicts.py/DataClassVariable with our own implementation on <strong>getItem</strong>, <strong>setAttr</strong>, <strong>setItem</strong>. There is a risk that ModelOutput logic may change since it is a user code</p>
<p>after the PR, we inline <strong>getItem</strong>, <strong>setAttr</strong>, <strong>setItem</strong> using dicts.py/CustomizedDictVariable so the logic always keep AA</p>
<p>unit test<br />
* python test/dynamo/test_model_output.py -k test_HF_bert_model_output</p>
<p>test on HF benchmark<br />
* python benchmarks/dynamo/huggingface.py -d cuda --inference --accuracy --progress --inductor --print-dataframe-summary 2&gt;&amp;1<br />
* all metric are the same before/after the PR, including pass rate, unique_graphs, graph_breaks, unique_graph_breaks<br />
  * before the PR: P790393916<br />
  * after the PR: P790368991</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/105044<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 14 Sep 2023 09:15:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9021fb8dacb9751dd4baac90bf5f695676989bba</guid>
    </item>
    <item>
      <title>[inductor] Lower masked_scatter on CUDA (#108803)</title>
      <link>https://github.com/pytorch/pytorch/commit/e4036ed7068cdcbe07470c1740ca25ab8ead7a3b</link>
      <description><![CDATA[<p>[inductor] Lower masked_scatter on CUDA (#108803)</p>
<p>This decomposes masked_scatter into <code>aten.cumsum</code> and a single pointwise kernel,<br />
which is similar to what is done in eager. I only do this for CUDA because on CPU<br />
it isn't split into two passes like this so would cause a slowdown.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108803<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #108802</p>]]></description>
      <pubDate>Thu, 14 Sep 2023 09:07:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e4036ed7068cdcbe07470c1740ca25ab8ead7a3b</guid>
    </item>
    <item>
      <title>Revert "[inductor] Add ir.Scan and lower aten.cumsum on CUDA (#106581)"</title>
      <link>https://github.com/pytorch/pytorch/commit/800c66561872f1f66fc0850696fe231ef71401e4</link>
      <description><![CDATA[<p>Revert "[inductor] Add ir.Scan and lower aten.cumsum on CUDA (#106581)"</p>
<p>This reverts commit 5976a08eea1656a0f5420661b33e0937248f2097.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/106581 on behalf of https://github.com/peterbell10 due to This combined with #108803 uncovered a triton bug openai/triton#2298 (<a href="https://github.com/pytorch/pytorch/pull/106581#issuecomment-1719811113">comment</a>)</p>]]></description>
      <pubDate>Thu, 14 Sep 2023 08:58:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/800c66561872f1f66fc0850696fe231ef71401e4</guid>
    </item>
    <item>
      <title>Added a flag is_cpu to the AOTInductor runtime (#109300)</title>
      <link>https://github.com/pytorch/pytorch/commit/1b502139f3cc9f1d52c752dc5690538b743ba758</link>
      <description><![CDATA[<p>Added a flag is_cpu to the AOTInductor runtime (#109300)</p>
<p>Summary:<br />
added a flag is_cpu that can be specified by the user to<br />
indicate whether the AOTInductor runtime is for CPU. It's<br />
false by default.</p>
<p>Test Plan: ci</p>
<p>Reviewed By: hl475, aakhundov</p>
<p>Differential Revision: D49253826</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109300<br />
Approved by: https://github.com/aakhundov</p>]]></description>
      <pubDate>Thu, 14 Sep 2023 08:24:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1b502139f3cc9f1d52c752dc5690538b743ba758</guid>
    </item>
    <item>
      <title>[AOTInductor] Add is_cpu for AOTInductorModelContainer (#109287)</title>
      <link>https://github.com/pytorch/pytorch/commit/3acccb3aa094c92d5ed33a3ad328c05ce420f362</link>
      <description><![CDATA[<p>[AOTInductor] Add is_cpu for AOTInductorModelContainer (#109287)</p>
<p>Summary:<br />
If is_cpu is set for the model container, no need to move the weights from cpu to device.</p>
<p>Reviewed By: bertmaher</p>
<p>Differential Revision: D49252595</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109287<br />
Approved by: https://github.com/aakhundov</p>]]></description>
      <pubDate>Thu, 14 Sep 2023 08:24:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3acccb3aa094c92d5ed33a3ad328c05ce420f362</guid>
    </item>
    <item>
      <title>[dynamo] Add BACKEND_MATCH guard to detect and recompile when backend changes (#107337)</title>
      <link>https://github.com/pytorch/pytorch/commit/94a54b89aae0bec1d1a079aff247a772c75fc843</link>
      <description><![CDATA[<p>[dynamo] Add BACKEND_MATCH guard to detect and recompile when backend changes (#107337)</p>
<p><strong>Motivation:</strong><br />
We try to make torch.cond use torch.compile automatically so that we could error out when there is side-effects in the branches and correctly handle the closures.</p>
<p>Before this PR, we have a warning if we don't turn on a config raise_on_backend_change (turning it on gives us an error) for the following code:<br />
```python<br />
def foo()</p>
<h1>Inside torch.cond, we'd like to do something like</h1>
<p>torch.compile(foo, backend="eager", fullgraph=True)(...)<br />
...</p>
<h1>Users may then call torch.compile somewhere else.</h1>
<h1>Dynamo will use the cached code of foo for "eager" backend</h1>
<h1>but we expect dynamo to recompile with "inductor" backend.</h1>
<p>torch.compile(foo, backend="inductor")(...)<br />
```</p>
<p>This PR adds a BACKEND_MATCH guard. Effectively, it implements a per-backend cache. In the above example, the cached code for "eager" won't work for "inductor" due to guard check failures and the second torch.compile will do a re-compilation. In the future, it might be useful to have something like a configuration guard that guards against dynamo configuration changes across different compiles (e.g. compile a function with fullgraph=False then compile it again with fullgraph=True).</p>
<p><strong>Implementation:</strong><br />
1. We add a guarded_backend_cache and check the most_recent_backend against the backend associated with cached code. We also remove the raise_on_backend_change flag.</p>
<p>Note: More lines are printed for debug log due to newly added context manager and guard adds .</p>
<p><strong>Test Plan:</strong><br />
Removed original tests that raise on different backend and add a new test to test whether the BACKEND_MATCH guard can guard against backend change.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107337<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 14 Sep 2023 07:49:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/94a54b89aae0bec1d1a079aff247a772c75fc843</guid>
    </item>
    <item>
      <title>[AOTInductor] Skip pre_grad_passes for exported graph. (#109246)</title>
      <link>https://github.com/pytorch/pytorch/commit/7f7f6267e99f8dd5990e43dd2301ac37bb66e299</link>
      <description><![CDATA[<p>[AOTInductor] Skip pre_grad_passes for exported graph. (#109246)</p>
<p>Summary:<br />
We skip pre_grad_passes if graph comes from export (aten IR) since<br />
pre_grad_passes (i.e. remove_identity) would not preserve meta["val"] in aten IR.</p>
<p>Differential Revision: D49246374</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109246<br />
Approved by: https://github.com/aakhundov</p>]]></description>
      <pubDate>Thu, 14 Sep 2023 05:30:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7f7f6267e99f8dd5990e43dd2301ac37bb66e299</guid>
    </item>
    <item>
      <title>AOTInductor dynamic shape (#109012)</title>
      <link>https://github.com/pytorch/pytorch/commit/9cd4548f01fa9bc8d1be9a1ce298a22a04fca19c</link>
      <description><![CDATA[<p>AOTInductor dynamic shape (#109012)</p>
<p>Summary: This PR adds dynamic-shape support for AOTInductor</p>
<ul>
<li>
<p>On the runtime/interface side, we added two structs, StaticDimInfo<br />
and DynamicDimInfo, to hold values for static and dynamic dimensions,<br />
respectively. Dynamic dimensions are tracked by an unordered map field<br />
defined in AOTInductorModelBase. At inference time, the inference run<br />
method will assign the current real dimensional value to each dynamic<br />
dimension before executing any kernel.</p>
</li>
<li>
<p>On the CUDA wrapper codegen side, we generate dynamic symbols<br />
appropriately for shape computations. We simulate kernel launch grids<br />
in the C++ land by re-using the grid functions from the Python world.<br />
The returned grid configs, which may contain symbolic expressions,<br />
are printed out in their C++ forms via the CppPrinter. Note that<br />
when dynamic shapes are involved, we have to compute grid configs<br />
for each kernel at runtime in the same way as we do for launching<br />
the corresponding Triton kernel. Otherwise, we may end up with<br />
memory-access failures or mis-computations caused by invalid indices<br />
for fetching or storing data in device memory.</p>
</li>
</ul>
<p>Differential Revision: D49100472</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109012<br />
Approved by: https://github.com/khabinov, https://github.com/desertfire, https://github.com/hl475</p>]]></description>
      <pubDate>Thu, 14 Sep 2023 00:00:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/9cd4548f01fa9bc8d1be9a1ce298a22a04fca19c</guid>
    </item>
    <item>
      <title>[inductor][easy] Enable mypy checking for all inductor files that already pass (#109238)</title>
      <link>https://github.com/pytorch/pytorch/commit/50a084070fe782e898edc8ba42c9e5f945c9c149</link>
      <description><![CDATA[<p>[inductor][easy] Enable mypy checking for all inductor files that already pass (#109238)</p>
<p>Summary: Let's just enable if mypy checking already passes. I checked all entries in the exclude list and enabled any that individually pass. Also needed one trivial change to a file already enabled.</p>
<p>Test Plan: <code>lintrunner torch/_inductor/*.py torch/_inductor/*/*.py</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109238<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 13 Sep 2023 17:45:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/50a084070fe782e898edc8ba42c9e5f945c9c149</guid>
    </item>
    <item>
      <title>[ez][inductor][fx passes] quick fix for invalid nodes (#109234)</title>
      <link>https://github.com/pytorch/pytorch/commit/62732bdcdb8b6112e01366d4ad1c2a50e61da1ed</link>
      <description><![CDATA[<p>[ez][inductor][fx passes] quick fix for invalid nodes (#109234)</p>
<p>Summary: As title.Need to check whether node is valid before fusion</p>
<p>Test Plan: To add test</p>
<p>Differential Revision: D49241525</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109234<br />
Approved by: https://github.com/yanboliang</p>]]></description>
      <pubDate>Wed, 13 Sep 2023 17:40:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/62732bdcdb8b6112e01366d4ad1c2a50e61da1ed</guid>
    </item>
    <item>
      <title>[inductor] Parallelize Max Autotune step 2: Use multiple GPUs (#109127)</title>
      <link>https://github.com/pytorch/pytorch/commit/4a09ed54590f4fe2eecc9ea69d043fe6f090f3d5</link>
      <description><![CDATA[<p>[inductor] Parallelize Max Autotune step 2: Use multiple GPUs (#109127)</p>
<p>Test Plan:<br />
<code>python test/inductor/test_max_autotune.py</code><br />
<code>TORCHINDUCTOR_AUTOTUNE_IN_SUBPROC=1 TORCHINDUCTOR_MAX_AUTOTUNE=1 python benchmarks/dynamo/torchbench.py --device cuda --performance --backend inductor --inference --only hf_Bart</code><br />
<code>TORCHINDUCTOR_AUTOTUNE_MULTI_DEVICE=1 TORCHINDUCTOR_AUTOTUNE_IN_SUBPROC=1 TORCHINDUCTOR_MAX_AUTOTUNE=1 python benchmarks/dynamo/torchbench.py --device cuda --performance --backend inductor --inference --only hf_Bart</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109127<br />
Approved by: https://github.com/shunting314, https://github.com/eellison<br />
ghstack dependencies: #109126</p>]]></description>
      <pubDate>Wed, 13 Sep 2023 16:37:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4a09ed54590f4fe2eecc9ea69d043fe6f090f3d5</guid>
    </item>
    <item>
      <title>[inductor] Parallelize Max Autotune step 1: refactor autotune_process (#109126)</title>
      <link>https://github.com/pytorch/pytorch/commit/ce4283933fcd119a1bdd275b765f5105c1b8337c</link>
      <description><![CDATA[<p>[inductor] Parallelize Max Autotune step 1: refactor autotune_process (#109126)</p>
<p>Summary: Step 1 in revamping subprocess autotune to support multiple GPUs. This diff just does some refactoring to autotune_process.py in order to prepare for the next diff:<br />
* Move all logic for managing the sub-process (like detecting sub-process crashes) into the TuningProcess class.<br />
* Use log.debug statements instead of print statements</p>
<p>Test Plan: python test/inductor/test_max_autotune.py</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109126<br />
Approved by: https://github.com/shunting314, https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 13 Sep 2023 16:37:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ce4283933fcd119a1bdd275b765f5105c1b8337c</guid>
    </item>
    <item>
      <title>Fix failing inductor test (#109220)</title>
      <link>https://github.com/pytorch/pytorch/commit/aca3bd44d1b05c5c7eb694c724d745d947caa7e2</link>
      <description><![CDATA[<p>Fix failing inductor test (#109220)</p>
<p>Summary: This broke as a result of the flashv2 PR. The tests couldnt' be listed expect for a100 machine which is weird..</p>
<p>Test Plan: buck2 test 'fbcode//mode/opt' fbcode//caffe2/test/inductor:fused_attention</p>
<p>Differential Revision: D49239716</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109220<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 13 Sep 2023 15:12:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/aca3bd44d1b05c5c7eb694c724d745d947caa7e2</guid>
    </item>
    <item>
      <title>Fix issue when input/output buffer of functional collective (e.g. allreduce / allgather) is incorrectly reused later (#108811)</title>
      <link>https://github.com/pytorch/pytorch/commit/faa5985dfefacc31485eb6fdec52ed62196d7a17</link>
      <description><![CDATA[<p>Fix issue when input/output buffer of functional collective (e.g. allreduce / allgather) is incorrectly reused later (#108811)</p>
<p>For this program:<br />
<code>python
def func(a, *, tag, ranks, group_size):
    ar = torch.ops.c10d_functional.all_reduce(a, "sum", tag, ranks, group_size)
    ar = torch.ops.c10d_functional.wait_tensor(ar)
    c = torch.relu(a)
    # c = a
    d = torch.matmul(c, c)
    e = d + ar
    return (e,)</code><br />
the generated code is:<br />
<code>python
def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (4, 4), (4, 1))
    with torch.cuda._DeviceGuard(1):
        torch.cuda.set_device(1) # no-op to ensure context
        buf0 = empty_strided((4, 4), (4, 1), device='cuda', dtype=torch.float32)
        buf0.copy_(arg0_1) #no reuse
        buf1_pg = c10d._find_or_create_pg_by_ranks_and_tag('', [0, 1], 2)
        buf1 = buf0
        buf1_work = dist.all_reduce(buf1, async_op=True, group=buf1_pg, op=fun_col_impl._str_to_reduce_op('sum'))
        fun_col_impl._register_tensor_work(buf1, buf1_work)
        del buf1
        buf0 = _wait_tensor(buf0)
        buf2 = buf0
        buf3 = buf0; del buf0  # reuse
        # Source Nodes: [relu], Original ATen: [aten.relu]
        stream1 = get_cuda_stream(1)
        triton_poi_fused_relu_0.run(arg0_1, buf3, 16, grid=grid(16), stream=stream1)
        del arg0_1
        buf4 = empty_strided((4, 4), (4, 1), device='cuda', dtype=torch.float32)
        # Source Nodes: [add, relu], Original ATen: [aten.add, aten.relu]
        extern_kernels.addmm(buf2, buf3, buf3, alpha=1, beta=1, out=buf4)
        return (buf4, )</code><br />
We can notice that allreduce input (<code>buf1</code> which is alias of <code>buf0</code>) is incorrectly reused as input (<code>buf3</code>) to the triton <code>triton_poi_fused_relu_0</code> inplace kernel, diverging from eager mode logic.</p>
<p>In general, we should make it so that Inductor doesn't try to reuse the input buffer to an inplace functional collective.</p>
<p>We have a similar problem for output buffer of out-of-place functional collectives, see https://github.com/pytorch/pytorch/issues/108780#issuecomment-1714921994.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108811<br />
Approved by: https://github.com/Chillee, https://github.com/wconstab</p>]]></description>
      <pubDate>Wed, 13 Sep 2023 13:39:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/faa5985dfefacc31485eb6fdec52ed62196d7a17</guid>
    </item>
    <item>
      <title>Back out "[Inductor] Extend Pattern Matcher to Match Equivalent Function Invocation (#107832)" (#109174)</title>
      <link>https://github.com/pytorch/pytorch/commit/8851603a9c504d1a11045829ca26eae0cbb24225</link>
      <description><![CDATA[<p>Back out "[Inductor] Extend Pattern Matcher to Match Equivalent Function Invocation (#107832)" (#109174)</p>
<p>Summary:<br />
Original commit changeset: ad8e1321811a</p>
<p>Original Phabricator Diff: D49151331</p>
<p>Test Plan: Sandcastle</p>
<p>Differential Revision: D49218851</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109174<br />
Approved by: https://github.com/hl475, https://github.com/yanboliang</p>]]></description>
      <pubDate>Wed, 13 Sep 2023 10:17:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8851603a9c504d1a11045829ca26eae0cbb24225</guid>
    </item>
    <item>
      <title>Allow marking multiple unstable configs of the same job name (#109185)</title>
      <link>https://github.com/pytorch/pytorch/commit/c9fdfafb00ee9cafb6d596deb7117f449ea391d3</link>
      <description><![CDATA[<p>Allow marking multiple unstable configs of the same job name (#109185)</p>
<p>This is a bug that has stayed for a surprisingly long period of time (my fault).  When there are multiple unstable configurations (<code>inductor</code>, <code>inductor_huggingface</code>, <code>inductor_huggingface_dynamic</code>) of the same job (<code>inductor / cuda12.1-py3.10-gcc9-sm86</code>), only the first one was marked as unstable.  The for loop returned too early and missed the other twos even though they were also marked as unstable, for example https://ossci-metrics.s3.amazonaws.com/unstable-jobs.json</p>
<h3>Testing</h3>
<ul>
<li>Add an unit test</li>
<li>CI run https://github.com/pytorch/pytorch/actions/runs/6169798353 shows that the configs below are all marked as unstable:</li>
<li>https://github.com/pytorch/pytorch/issues/107079</li>
<li>https://github.com/pytorch/pytorch/issues/109153</li>
<li>https://github.com/pytorch/pytorch/issues/109154</li>
<li>Manually run the script to verify the test matrix output:<br />
<code>python .github/scripts/filter_test_configs.py \
    --workflow "inductor" \
    --job-name "cuda12.1-py3.10-gcc9-sm86 / build," \
    --test-matrix "{ include: [
    { config: "inductor", shard: 1, num_shards: 1, runner: "linux.g5.4xlarge.nvidia.gpu" },
    { config: "inductor_huggingface", shard: 1, num_shards: 1, runner: "linux.g5.4xlarge.nvidia.gpu" },
    { config: "inductor_timm", shard: 1, num_shards: 2, runner: "linux.g5.4xlarge.nvidia.gpu" },
    { config: "inductor_timm", shard: 2, num_shards: 2, runner: "linux.g5.4xlarge.nvidia.gpu" },
    { config: "inductor_torchbench", shard: 1, num_shards: 1, runner: "linux.g5.4xlarge.nvidia.gpu" },
    { config: "inductor_huggingface_dynamic", shard: 1, num_shards: 1, runner: "linux.g5.4xlarge.nvidia.gpu" },
    { config: "inductor_timm_dynamic", shard: 1, num_shards: 2, runner: "linux.g5.4xlarge.nvidia.gpu" },
    { config: "inductor_timm_dynamic", shard: 2, num_shards: 2, runner: "linux.g5.4xlarge.nvidia.gpu" },
    { config: "inductor_torchbench_dynamic", shard: 1, num_shards: 1, runner: "linux.g5.4xlarge.nvidia.gpu" },
    { config: "inductor_distributed", shard: 1, num_shards: 1, runner: "linux.g5.12xlarge.nvidia.gpu" },
  ]}
  " \
    --pr-number "" \
    --tag "" \
    --event-name "push" \
    --schedule "" \
    --branch ""
::set-output name=keep-going::False
::set-output name=is-unstable::False
::set-output name=reenabled-issues::
::set-output name=test-matrix::{"include": [{"config": "inductor", "shard": 1, "num_shards": 1, "runner": "linux.g5.4xlarge.nvidia.gpu", "unstable": "unstable"}, {"config": "inductor_huggingface", "shard": 1, "num_shards": 1, "runner": "linux.g5.4xlarge.nvidia.gpu", "unstable": "unstable"}, {"config": "inductor_timm", "shard": 1, "num_shards": 2, "runner": "linux.g5.4xlarge.nvidia.gpu"}, {"config": "inductor_timm", "shard": 2, "num_shards": 2, "runner": "linux.g5.4xlarge.nvidia.gpu"}, {"config": "inductor_torchbench", "shard": 1, "num_shards": 1, "runner": "linux.g5.4xlarge.nvidia.gpu"}, {"config": "inductor_huggingface_dynamic", "shard": 1, "num_shards": 1, "runner": "linux.g5.4xlarge.nvidia.gpu", "unstable": "unstable"}, {"config": "inductor_timm_dynamic", "shard": 1, "num_shards": 2, "runner": "linux.g5.4xlarge.nvidia.gpu"}, {"config": "inductor_timm_dynamic", "shard": 2, "num_shards": 2, "runner": "linux.g5.4xlarge.nvidia.gpu"}, {"config": "inductor_torchbench_dynamic", "shard": 1, "num_shards": 1, "runner": "linux.g5.4xlarge.nvidia.gpu"}, {"config": "inductor_distributed", "shard": 1, "num_shards": 1, "runner": "linux.g5.12xlarge.nvidia.gpu"}]}
::set-output name=is-test-matrix-empty::False</code><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/109185<br />
Approved by: https://github.com/clee2000</li>
</ul>]]></description>
      <pubDate>Wed, 13 Sep 2023 09:06:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c9fdfafb00ee9cafb6d596deb7117f449ea391d3</guid>
    </item>
    <item>
      <title>inductor/test_max_autotune serial in CI (#109209)</title>
      <link>https://github.com/pytorch/pytorch/commit/fe198f31415b2ca85b67eaaf7cd415fb85079f23</link>
      <description><![CDATA[<p>inductor/test_max_autotune serial in CI (#109209)</p>
<p>Fixes #ISSUE_NUMBER<br />
Trying to figure out why the this keeps timing out, wondering if its due to parallelization weirdness<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/109209<br />
Approved by: https://github.com/huydhn</p>]]></description>
      <pubDate>Wed, 13 Sep 2023 09:04:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fe198f31415b2ca85b67eaaf7cd415fb85079f23</guid>
    </item>
    <item>
      <title>[inductor] Enable Mypy Checking for torch/_inductor/codecache.py (#108789)</title>
      <link>https://github.com/pytorch/pytorch/commit/264f1e7b4c5bc7fee2fca1d7f3c7d8bd615ba21a</link>
      <description><![CDATA[<p>[inductor] Enable Mypy Checking for torch/_inductor/codecache.py (#108789)</p>
<p>Summary: Add type annotations to torch/_inductor/codecache.py and enable mypy checking</p>
<p>Test Plan:<br />
<code>lintrunner torch/_inductor/*.py</code><br />
<code>python test/inductor/test_max_autotune.py</code><br />
<code>python test/inductor/test_aot_inductor.py</code><br />
<code>python test/inductor/test_torchinductor.py</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108789<br />
Approved by: https://github.com/Skylion007, https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 13 Sep 2023 06:05:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/264f1e7b4c5bc7fee2fca1d7f3c7d8bd615ba21a</guid>
    </item>
    <item>
      <title>Check index size during decomp of index_add (#108826)</title>
      <link>https://github.com/pytorch/pytorch/commit/db48bc80d9a32e3508e208fd4708ea52d51fbe1e</link>
      <description><![CDATA[<p>Check index size during decomp of index_add (#108826)</p>
<p>This partially fixes the <code>test_index_add_correctness</code> test (#108181)<br />
when run under inductor: it causes an exception to be raised <a href="https://github.com/pytorch/pytorch/blob/dec2b267d4af159bd0a8669a679700f385a1dc98/test/test_torch.py#L6049">here</a><br />
as expected.</p>
<p>The test as a whole still cannot be made to pass under inductor because<br />
the <a href="https://github.com/pytorch/pytorch/blob/dec2b267d4af159bd0a8669a679700f385a1dc98/test/test_torch.py#L6051">last assert</a> still fails, likely due to #108798.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108826<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 13 Sep 2023 05:06:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/db48bc80d9a32e3508e208fd4708ea52d51fbe1e</guid>
    </item>
    <item>
      <title>Enable typechecking for _inductor/virtualized.py (#108916)</title>
      <link>https://github.com/pytorch/pytorch/commit/d2d36aad6ff9107a8360465aaa90457f3c05d90b</link>
      <description><![CDATA[<p>Enable typechecking for _inductor/virtualized.py (#108916)</p>
<p>Also add a few more type annotations to utils.py (some of its functions<br />
are called from virtualized.py)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108916<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 13 Sep 2023 05:04:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d2d36aad6ff9107a8360465aaa90457f3c05d90b</guid>
    </item>
    <item>
      <title>Update inductor ci_expected_accuracy (#109148)</title>
      <link>https://github.com/pytorch/pytorch/commit/3d8d59e68b2bb6517e8134d4246910c110f0d7a0</link>
      <description><![CDATA[<p>Update inductor ci_expected_accuracy (#109148)</p>
<p>Changes due to updating the HF pin: <a href="https://github.com/pytorch/pytorch/pull/107400">107400</a><br />
Somehow during the previous PR it didn't need these changes...probably a CI bug</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109148<br />
Approved by: https://github.com/clee2000, https://github.com/desertfire</p>]]></description>
      <pubDate>Tue, 12 Sep 2023 21:12:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3d8d59e68b2bb6517e8134d4246910c110f0d7a0</guid>
    </item>
    <item>
      <title>Revert "[inductor] Lower masked_scatter on CUDA (#108803)"</title>
      <link>https://github.com/pytorch/pytorch/commit/91aab161d05415c9c7f4519a31eb8a14018d94cd</link>
      <description><![CDATA[<p>Revert "[inductor] Lower masked_scatter on CUDA (#108803)"</p>
<p>This reverts commit c8e577bf409591910f9667a51f2cf92b3c5455e0.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/108803 on behalf of https://github.com/lezcano due to makes test_comprehensive_masked_scatter_cuda_int64 flaky (<a href="https://github.com/pytorch/pytorch/pull/108803#issuecomment-1716407433">comment</a>)</p>]]></description>
      <pubDate>Tue, 12 Sep 2023 12:49:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/91aab161d05415c9c7f4519a31eb8a14018d94cd</guid>
    </item>
    <item>
      <title>[Inductor CUTLASS backend] Step 5: Gemm CUTLASS templates (#108015)</title>
      <link>https://github.com/pytorch/pytorch/commit/a2d5f133102ca34a029a3c88b028a1b541625d75</link>
      <description><![CDATA[<p>[Inductor CUTLASS backend] Step 5: Gemm CUTLASS templates (#108015)</p>
<p>This is the step 5 to add cutlass as an alternative inductor backend.</p>
<p>Feature request: https://github.com/pytorch/pytorch/issues/106991.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108015<br />
Approved by: https://github.com/kadeng, https://github.com/jansel, https://github.com/aakhundov<br />
ghstack dependencies: #107802, #107847, #107901, #107931</p>]]></description>
      <pubDate>Tue, 12 Sep 2023 09:44:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a2d5f133102ca34a029a3c88b028a1b541625d75</guid>
    </item>
    <item>
      <title>[Inductor CUTLASS backend] Step 4: CUDA (template) kernels (#107931)</title>
      <link>https://github.com/pytorch/pytorch/commit/097fd43f8cf3ef150d91fb8b96c76071b03731bf</link>
      <description><![CDATA[<p>[Inductor CUTLASS backend] Step 4: CUDA (template) kernels (#107931)</p>
<p>This is the step 4 to add cutlass as an alternative inductor backend.<br />
Full tests can be found from the last PR in the stack.</p>
<p>Feature request: https://github.com/pytorch/pytorch/issues/106991.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107931<br />
Approved by: https://github.com/aakhundov, https://github.com/jansel, https://github.com/kadeng<br />
ghstack dependencies: #107802, #107847, #107901</p>]]></description>
      <pubDate>Tue, 12 Sep 2023 09:44:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/097fd43f8cf3ef150d91fb8b96c76071b03731bf</guid>
    </item>
    <item>
      <title>[Inductor CUTLASS backend] Step 3: autotune_process, and CUDABenchmarkRequest (#107901)</title>
      <link>https://github.com/pytorch/pytorch/commit/b2d764ece024ac3759e5de756a778c888dc85686</link>
      <description><![CDATA[<p>[Inductor CUTLASS backend] Step 3: autotune_process, and CUDABenchmarkRequest (#107901)</p>
<p>This is the step 3 to add cutlass as an alternative inductor backend.<br />
Full tests can be found from the last PR in the stack.</p>
<p>Feature request: https://github.com/pytorch/pytorch/issues/106991.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107901<br />
Approved by: https://github.com/jansel, https://github.com/aakhundov, https://github.com/kadeng<br />
ghstack dependencies: #107802, #107847</p>]]></description>
      <pubDate>Tue, 12 Sep 2023 09:44:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b2d764ece024ac3759e5de756a778c888dc85686</guid>
    </item>
    <item>
      <title>[Inductor CUTLASS backend] Step 2: CUDACodeCache (#107847)</title>
      <link>https://github.com/pytorch/pytorch/commit/102fefac21402a537f41cbdeaba09bff2be7e607</link>
      <description><![CDATA[<p>[Inductor CUTLASS backend] Step 2: CUDACodeCache (#107847)</p>
<p>This is the step 2 to add cutlass as an alternative inductor backend.<br />
Feature request: https://github.com/pytorch/pytorch/issues/106991.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107847<br />
Approved by: https://github.com/jansel, https://github.com/kadeng, https://github.com/aakhundov<br />
ghstack dependencies: #107802</p>]]></description>
      <pubDate>Tue, 12 Sep 2023 09:44:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/102fefac21402a537f41cbdeaba09bff2be7e607</guid>
    </item>
    <item>
      <title>[Inductor CUTLASS backend] Step 1: Inductor config for cuda / cutlass, util functions. (#107802)</title>
      <link>https://github.com/pytorch/pytorch/commit/a14761b68afc37fa764345d6eb3a43e6f5a87cb9</link>
      <description><![CDATA[<p>[Inductor CUTLASS backend] Step 1: Inductor config for cuda / cutlass, util functions. (#107802)</p>
<p>This is the step 1 to add cutlass as an alternative inductor backend.<br />
Feature request: https://github.com/pytorch/pytorch/issues/106991.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107802<br />
Approved by: https://github.com/jansel, https://github.com/aakhundov, https://github.com/kadeng</p>]]></description>
      <pubDate>Tue, 12 Sep 2023 09:44:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a14761b68afc37fa764345d6eb3a43e6f5a87cb9</guid>
    </item>
    <item>
      <title>[inductor] Lower masked_scatter on CUDA (#108803)</title>
      <link>https://github.com/pytorch/pytorch/commit/c8e577bf409591910f9667a51f2cf92b3c5455e0</link>
      <description><![CDATA[<p>[inductor] Lower masked_scatter on CUDA (#108803)</p>
<p>This decomposes masked_scatter into <code>aten.cumsum</code> and a single pointwise kernel,<br />
which is similar to what is done in eager. I only do this for CUDA because on CPU<br />
it isn't split into two passes like this so would cause a slowdown.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108803<br />
Approved by: https://github.com/lezcano<br />
ghstack dependencies: #108802</p>]]></description>
      <pubDate>Tue, 12 Sep 2023 08:16:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c8e577bf409591910f9667a51f2cf92b3c5455e0</guid>
    </item>
    <item>
      <title>Update HF version to commit hash (6c26faa) (#107400)</title>
      <link>https://github.com/pytorch/pytorch/commit/c3945b5f8496f068d56ea1879c10d62ee4ec1801</link>
      <description><![CDATA[<p>Update HF version to commit hash (6c26faa) (#107400)</p>
<p>Some <a href="https://ossci-raw-job-status.s3.amazonaws.com/log/15968424899">errors</a> in the <a href="https://hud.pytorch.org/benchmark/huggingface/inductor_aot_inductor?startTime=Thu,%2010%20Aug%202023%2018:05:47%20GMT&amp;stopTime=Thu,%2017%20Aug%202023%2018:05:47%20GMT&amp;granularity=hour&amp;mode=inference&amp;dtype=bfloat16&amp;lBranch=main&amp;lCommit=384e0d104fd077d31efafc564129660e9b7a0f25&amp;rBranch=main&amp;rCommit=03414081ff7ee011e17ee10f9ddb2584811bf965">torchinductor hf benchmarks</a> should be fixed in the most recent release (for example, this <a href="https://github.com/huggingface/transformers/blame/c036c814f427415db15d37fca98540d55abea492/src/transformers/models/opt/modeling_opt.py#L688">line</a> no longer exists). Additionally, I landed a <a href="https://github.com/huggingface/transformers/commit/6c26faa159b79a42d7fa46cb66e2d21523351987">commit (6c26faa)</a> to the HF transformers repro to fix one of the graph breaks. This PR results in <a href="https://hud.pytorch.org/benchmark/compilers?startTime=Thu%2C%2010%20Aug%202023%2022%3A45%3A09%20GMT&amp;stopTime=Thu%2C%2017%20Aug%202023%2022%3A45%3A09%20GMT&amp;granularity=hour&amp;suite=torchbench&amp;mode=inference&amp;dtype=bfloat16&amp;lBranch=angelayi/hf_version&amp;lCommit=0accaaca2fa70ca2f78c1a587dd4b6750448dd90&amp;rBranch=main&amp;rCommit=03414081ff7ee011e17ee10f9ddb2584811bf965">76% pass rate for the export + aot inductor HF benchmark!</a></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107400<br />
Approved by: https://github.com/ezyang, https://github.com/desertfire, https://github.com/malfet</p>]]></description>
      <pubDate>Tue, 12 Sep 2023 07:25:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c3945b5f8496f068d56ea1879c10d62ee4ec1801</guid>
    </item>
    <item>
      <title>[export] Lift constant tensors as buffes (reland) (#109040)</title>
      <link>https://github.com/pytorch/pytorch/commit/58391aeaf1f859625cf9356a2f05597864fd2e9d</link>
      <description><![CDATA[<p>[export] Lift constant tensors as buffes (reland) (#109040)</p>
<p>Summary:<br />
When we retrace the graph containing constant tensors, they get lifted as buffer inputs.<br />
AotInductor also wants to lift all the constants as inputs.<br />
If we separate the constants as a separate thing, then it adds an additional complexity where we now have to keep track of 3 inputs (params, buffers, constants).</p>
<p>Cons: People might care about specifically what buffers are/are not buffers?</p>
<p>If people want to know specifically which buffers are constants, we can add an additional field in the graph signature to mark this.</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D49153367</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/109040<br />
Approved by: https://github.com/zhxchen17</p>]]></description>
      <pubDate>Tue, 12 Sep 2023 07:23:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/58391aeaf1f859625cf9356a2f05597864fd2e9d</guid>
    </item>
    <item>
      <title>Revert "[ROCm] Add ROCm AMDGPU support for inductor cpp codegen (#105141)"</title>
      <link>https://github.com/pytorch/pytorch/commit/5a7c008b300f49b61644576f2d8e87fdd59bf210</link>
      <description><![CDATA[<p>Revert "[ROCm] Add ROCm AMDGPU support for inductor cpp codegen (#105141)"</p>
<p>This reverts commit 8ff00360a4daab7848307a9a0b1c81b1da873d0c.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/105141 on behalf of https://github.com/DanilBaibak due to Break internal build (<a href="https://github.com/pytorch/pytorch/pull/105141#issuecomment-1715629007">comment</a>)</p>]]></description>
      <pubDate>Tue, 12 Sep 2023 04:29:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5a7c008b300f49b61644576f2d8e87fdd59bf210</guid>
    </item>
    <item>
      <title>Force synced KJT to trace unbacked SymInt (#108960)</title>
      <link>https://github.com/pytorch/pytorch/commit/f9a250c35bd061e2e6f4c2d92e2b1b16390e8636</link>
      <description><![CDATA[<p>Force synced KJT to trace unbacked SymInt (#108960)</p>
<p>Summary:<br />
The basic concept behind this diff is to modify Dynamo's tracing behavior when it encounters a KeyedJaggedTensor that is synced (aka has <code>_length_per_key</code> and <code>_offset_per_key</code> populated). These fields are lists of integers; ordinarily, Dynamo will optimistically try to specialize on integers, however, for KJTs, we know that these integers will definitely vary from run-to-run. Furthermore, ordinarily, we would also specialize these integers if they are 0/1, but we will frequently expect features in KJTs to be 0/1.</p>
<p>The fix is to detect KJTs and treat these integers as <em>unbacked integers</em>. This is NOT a universally sound optimization: when treating these integers as unbacked, we never report them as equal to zero or one. In return, we always generate graphs that generalize no matter the length of values on features. This is enough to trace through APS sparse arch, torchrec_dlrm and some small split-cat examples.</p>
<p>The special integer behavior is triggered by a dynamically scoped <code>force_unspec_int_unbacked_size_like</code> variable on TracingContext, which we trigger when we wrap a KJT. There probably are other ways to do this, but this was simple and worked.</p>
<p>Test Plan:<br />
<code>buck2 test mode/dev-nosan //pytorch/benchmark/fb/test_gpu:run_test_gpu</code></p>
<p>from aakhundov</p>
<ol>
<li>first build feed_lower_benchmark:<br />
<code>buck2 build --show-output mode/opt -c python.package_style=inplace -c fbcode.enable_gpu_sections=true -c fbcode.platform=platform010 -c fbcode.split-dwarf=true hpc/new/models/feed/benchmark:feed_lower_benchmark</code></li>
<li>then run the lowering of the model with it:<br />
<code>TORCHINDUCTOR_MAX_AUTOTUNE=1 TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCH_LOGS="output_code,graph_code" TORCH_COMPILE_DEBUG=1 ../buck-out/v2/gen/fbcode/79c6b019ee0f9469/hpc/new/models/feed/benchmark/__feed_lower_benchmark__/feed_lower_benchmark.par --load=manifold://ig_inference_model/tree/user/facebook/fblearner/predictor/960999465/60/gpu_lowering/input.predictor --skip-trt --skip-ait --sync-mode=0 --enable-aot-inductor --lower-presets="ig_stories" --gpu-trace</code><br />
cf https://docs.google.com/document/d/1yD30xYrdmM8r2HTdmXnZTg0-MHVexfVrAa0294m1AUE/edit?pli=1#heading=h.qiv3fp7e6zg0</li>
</ol>
<p>From torchrec: https://www.internalfb.com/intern/wiki/Torchrec/Development/Testing_production_models/</p>
<p>From ge0405<br />
baseline (without your diff): f477293168<br />
your diff: f477292363</p>
<p>Differential Revision: D49019987</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108960<br />
Approved by: https://github.com/voznesenskym</p>]]></description>
      <pubDate>Mon, 11 Sep 2023 19:44:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/f9a250c35bd061e2e6f4c2d92e2b1b16390e8636</guid>
    </item>
    <item>
      <title>Revert "[inductor] Parallelize Max Autotune step 1: Use Popen (#107982)"</title>
      <link>https://github.com/pytorch/pytorch/commit/2039f30c062ec225bf1add9a887c7d8b30a8b6a9</link>
      <description><![CDATA[<p>Revert "[inductor] Parallelize Max Autotune step 1: Use Popen (#107982)"</p>
<p>This reverts commit d6856680039e5557b45e4cd6e95f82ca64f6435a.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/107982 on behalf of https://github.com/masnesral due to fbcode failures (<a href="https://github.com/pytorch/pytorch/pull/107982#issuecomment-1714818307">comment</a>)</p>]]></description>
      <pubDate>Mon, 11 Sep 2023 17:12:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2039f30c062ec225bf1add9a887c7d8b30a8b6a9</guid>
    </item>
    <item>
      <title>Revert "[inductor] Parallelize Max Autotune step 2: Use all GPUs (#107983)"</title>
      <link>https://github.com/pytorch/pytorch/commit/c36c2bfcb2c650488ac866cbdaf2bd717b89b5b1</link>
      <description><![CDATA[<p>Revert "[inductor] Parallelize Max Autotune step 2: Use all GPUs (#107983)"</p>
<p>This reverts commit 2c61313ff3b9ca585f04a4bb78263f301a8cec27.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/107983 on behalf of https://github.com/masnesral due to fbcode failures (<a href="https://github.com/pytorch/pytorch/pull/107983#issuecomment-1714816358">comment</a>)</p>]]></description>
      <pubDate>Mon, 11 Sep 2023 17:08:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c36c2bfcb2c650488ac866cbdaf2bd717b89b5b1</guid>
    </item>
    <item>
      <title>Speical treatment to build AOTInductor with cuda-12 from Meta internal (#108831)</title>
      <link>https://github.com/pytorch/pytorch/commit/97d918817877deba183cb22145b6cf45af2c619f</link>
      <description><![CDATA[<p>Speical treatment to build AOTInductor with cuda-12 from Meta internal (#108831)</p>
<p>Differential Revision: D49042577</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108831<br />
Approved by: https://github.com/bertmaher</p>]]></description>
      <pubDate>Mon, 11 Sep 2023 14:16:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/97d918817877deba183cb22145b6cf45af2c619f</guid>
    </item>
    <item>
      <title>[inductor] Add ir.Scan and lower aten.cumsum on CUDA (#106581)</title>
      <link>https://github.com/pytorch/pytorch/commit/5976a08eea1656a0f5420661b33e0937248f2097</link>
      <description><![CDATA[<p>[inductor] Add ir.Scan and lower aten.cumsum on CUDA (#106581)</p>
<p>This adds the <code>ir.Scan</code> node (currently only supported on CUDA) which re-uses the existing reduction kernel machinery to support different kinds of non-pointwise ops. Just like reductions it supports prologue and epilogue fusions and has both persistent and non-persistent kernel generation.</p>
<p>Currently this doesn't support the equivalent of <code>Reduction.create_multilayer</code> and will instead fall back to eager in those cases. This is because splitting into multiple kernel invocations ends up being far slower than cub's single kernel strategy which matches the performance of a copy kernel.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/106581<br />
Approved by: https://github.com/lezcano, https://github.com/atalman</p>]]></description>
      <pubDate>Mon, 11 Sep 2023 10:44:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5976a08eea1656a0f5420661b33e0937248f2097</guid>
    </item>
    <item>
      <title>Enable Mypy Checking for torch/_inductor/fx_passes/fuse_attention.py (#107369)</title>
      <link>https://github.com/pytorch/pytorch/commit/4a4a2fc1a58499a5d27b94144ac6927f6e561010</link>
      <description><![CDATA[<p>Enable Mypy Checking for torch/_inductor/fx_passes/fuse_attention.py (#107369)</p>
<p>Fixes #105230</p>
<p>Summary:</p>
<p>As suggested in https://github.com/pytorch/pytorch/issues/105230 mypy checking is enabled in torch/_inductor/fx_passes/fuse_attention.py</p>
<p>After Fix:</p>
<p>mypy --follow-imports=skip torch/_inductor/fx_passes/fuse_attention.py Success: no issues found in 1 source file</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107369<br />
Approved by: https://github.com/mikaylagawarecki</p>]]></description>
      <pubDate>Mon, 11 Sep 2023 10:08:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4a4a2fc1a58499a5d27b94144ac6927f6e561010</guid>
    </item>
    <item>
      <title>inductor: add custom pass hooks in post_grad_passes (#108615)</title>
      <link>https://github.com/pytorch/pytorch/commit/18225cc6aae109391f675373a8df71b3d8caa77f</link>
      <description><![CDATA[<p>inductor: add custom pass hooks in post_grad_passes (#108615)</p>
<p>Supports #107921</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108615<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Sun, 10 Sep 2023 20:13:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/18225cc6aae109391f675373a8df71b3d8caa77f</guid>
    </item>
    <item>
      <title>inductor: remove redundant memory copy when view a ExternKernelAlloc buffer (#108635)</title>
      <link>https://github.com/pytorch/pytorch/commit/a6b153b3119e73790b70bbc72ac474ad8f511427</link>
      <description><![CDATA[<p>inductor: remove redundant memory copy when view a ExternKernelAlloc buffer (#108635)</p>
<p>When viewing a ExternKernelAlloc buffer, there always have a redundant memory copy:<br />
```<br />
buf0: ExternKernelSchedulerNode(MKLPackedLinear)<br />
buf0.writes = [StarDep(name='buf0')]<br />
buf0.unmet_dependencies = []<br />
buf0.met_dependencies = [StarDep(name='arg1_1'), StarDep(name='constant0'), StarDep(name='constant1')]<br />
buf0.users = [NodeUser(node=SchedulerNode(name='buf1'), can_inplace=True, is_weak=False)]<br />
buf0.node.kernel = torch.ops.mkl._mkl_linear</p>
<p>buf1: SchedulerNode(ComputedBuffer)<br />
buf1.writes = [MemoryDep('buf1', c0, {c0: 64})]<br />
buf1.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 64})]<br />
buf1.met_dependencies = []<br />
buf1.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]<br />
buf1.group.device = cpu<br />
buf1.group.iteration = ((64,), ())<br />
buf1.sizes = ([64], [])<br />
class buf1_loop_body:<br />
    var_ranges = {z0: 64}<br />
    index0 = z0<br />
    def body(self, ops):<br />
        get_index = self.get_index('index0')<br />
        load = ops.load('buf0', get_index)<br />
        get_index_1 = self.get_index('index0')<br />
        store = ops.store('buf1', get_index_1, load, None)<br />
        return store<br />
```</p>
<p>and the cpp backend-generated code is:<br />
```<br />
cpp_fused_view_0 = async_compile.cpp('''</p>
<h1>include "/tmp/torchinductor_xiaobing/ib/cibrnuq56cxamjj4krp4zpjvsirbmlolpbnmomodzyd46huzhdw7.h"</h1>
<p>extern "C" void kernel(float* in_out_ptr0)<br />
{<br />
    #pragma omp parallel num_threads(40)<br />
    {<br />
        {<br />
            #pragma omp for<br />
            for(long i0=static_cast<long>(0L); i0&lt;static_cast<long>(64L); i0+=static_cast<long>(16L))<br />
            {<br />
                auto tmp0 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + static_cast<long>(i0));<br />
                tmp0.store(in_out_ptr0 + static_cast<long>(i0));<br />
            }<br />
        }<br />
    }<br />
}<br />
''')</p>
<p>async_compile.wait(globals())<br />
del async_compile</p>
<p>def call(args):<br />
    arg1_1, = args<br />
    args.clear()<br />
    assert_size_stride(arg1_1, (4, 16), (16, 1))<br />
    buf0 = torch.ops.mkl._mkl_linear(arg1_1, constant1, constant0, None, 4)<br />
    del arg1_1<br />
    buf1 = reinterpret_tensor(buf0, (4, 4, 4), (16, 4, 1)); del buf0  # reuse<br />
    cpp_fused_view_0(c_void_p(buf1.data_ptr()))<br />
    return (buf1, )<br />
```</p>
<p>For the ExternKernelAlloc buffer, we can do a real view, rather than a memory copy.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108635<br />
Approved by: https://github.com/jgong5, https://github.com/desertfire, https://github.com/jansel<br />
ghstack dependencies: #108560</p>]]></description>
      <pubDate>Sun, 10 Sep 2023 17:19:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a6b153b3119e73790b70bbc72ac474ad8f511427</guid>
    </item>
    <item>
      <title>inductor: make onednn linear inputs are always real contiguous (#108560)</title>
      <link>https://github.com/pytorch/pytorch/commit/a6ada463ec767139b950be6ffeddd2edb70f632d</link>
      <description><![CDATA[<p>inductor: make onednn linear inputs are always real contiguous (#108560)</p>
<p>For OneDNN linear, if packed linear inputs are not the default contiguous tensor, it always calls in ref pat and gets a worse performance, this PR will force its inputs to the actual default contiguous tensor.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108560<br />
Approved by: https://github.com/jgong5, https://github.com/desertfire, https://github.com/jansel</p>]]></description>
      <pubDate>Sun, 10 Sep 2023 17:11:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/a6ada463ec767139b950be6ffeddd2edb70f632d</guid>
    </item>
    <item>
      <title>[inductor] Enable Mypy Checking for torch/_inductor/codegen/triton_utils.py (#108951)</title>
      <link>https://github.com/pytorch/pytorch/commit/1a3a07ac2cbb565102c5d6be6dcb4ada2d6acb00</link>
      <description><![CDATA[<p>[inductor] Enable Mypy Checking for torch/_inductor/codegen/triton_utils.py (#108951)</p>
<p>Summary: Used monkeytype to generate the typehints and enabled mypy checking</p>
<p>Test Plan: <code>lintrunner torch/_inductor/codegen/*.py</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108951<br />
Approved by: https://github.com/Skylion007</p>]]></description>
      <pubDate>Sun, 10 Sep 2023 11:18:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1a3a07ac2cbb565102c5d6be6dcb4ada2d6acb00</guid>
    </item>
    <item>
      <title>[inductor] Parallelize Max Autotune step 2: Use all GPUs (#107983)</title>
      <link>https://github.com/pytorch/pytorch/commit/2c61313ff3b9ca585f04a4bb78263f301a8cec27</link>
      <description><![CDATA[<p>[inductor] Parallelize Max Autotune step 2: Use all GPUs (#107983)</p>
<p>Summary: Step 2 in revamping subprocess autotune to support multiple GPUs: use a pool of subprocesses and distribute benchmark calls across them.</p>
<p>Test Plan:<br />
<code>python test/inductor/test_max_autotune.py</code><br />
<code>TORCHINDUCTOR_AUTOTUNE_IN_SUBPROC=1 TORCHINDUCTOR_MAX_AUTOTUNE=1 python benchmarks/dynamo/torchbench.py --device cuda --performance --backend inductor --inference --only hf_Bart</code><br />
<code>TORCHINDUCTOR_AUTOTUNE_MULTI_DEVICE=1 TORCHINDUCTOR_AUTOTUNE_IN_SUBPROC=1 TORCHINDUCTOR_MAX_AUTOTUNE=1 python benchmarks/dynamo/torchbench.py --device cuda --performance --backend inductor --inference --only hf_Bart</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107983<br />
Approved by: https://github.com/eellison, https://github.com/shunting314<br />
ghstack dependencies: #107982</p>]]></description>
      <pubDate>Sun, 10 Sep 2023 07:43:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2c61313ff3b9ca585f04a4bb78263f301a8cec27</guid>
    </item>
    <item>
      <title>[inductor] Parallelize Max Autotune step 1: Use Popen (#107982)</title>
      <link>https://github.com/pytorch/pytorch/commit/d6856680039e5557b45e4cd6e95f82ca64f6435a</link>
      <description><![CDATA[<p>[inductor] Parallelize Max Autotune step 1: Use Popen (#107982)</p>
<p>Summary: Step 1 in revamping subprocess autotune to support multiple GPUs: use Popen to create a new process with an entry point we control so we don't reinterpret the toplevel script.</p>
<p>Test Plan: <code>python test/inductor/test_max_autotune.py</code><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/107982<br />
Approved by: https://github.com/eellison, https://github.com/shunting314</p>]]></description>
      <pubDate>Sun, 10 Sep 2023 07:43:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d6856680039e5557b45e4cd6e95f82ca64f6435a</guid>
    </item>
    <item>
      <title>[Inductor] Extend Pattern Matcher to Match Equivalent Function Invocation (#107832)</title>
      <link>https://github.com/pytorch/pytorch/commit/4c503f2451819a7a84df915871fd64a9239438b6</link>
      <description><![CDATA[<p>[Inductor] Extend Pattern Matcher to Match Equivalent Function Invocation (#107832)</p>
<p>Fixes #104391</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107832<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Sat, 09 Sep 2023 11:19:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4c503f2451819a7a84df915871fd64a9239438b6</guid>
    </item>
    <item>
      <title>[ROCm] Add ROCm AMDGPU support for inductor cpp codegen (#105141)</title>
      <link>https://github.com/pytorch/pytorch/commit/8ff00360a4daab7848307a9a0b1c81b1da873d0c</link>
      <description><![CDATA[<p>[ROCm] Add ROCm AMDGPU support for inductor cpp codegen (#105141)</p>
<p>Follows from previous enablement attempt: https://github.com/pytorch/pytorch/pull/101797</p>
<p>Adds support for hsaco binaries in inductor's cpp_wrapper codegen and enables the CUDA tests in test_cpp_wrapper.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/105141<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Sat, 09 Sep 2023 08:28:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8ff00360a4daab7848307a9a0b1c81b1da873d0c</guid>
    </item>
    <item>
      <title>[inductor] Add CPU-side profiler event names for templates and foreach kernels (#108449)</title>
      <link>https://github.com/pytorch/pytorch/commit/ed7f9cac917dd078084cd7af0e898799a5997955</link>
      <description><![CDATA[<p>[inductor] Add CPU-side profiler event names for templates and foreach kernels (#108449)</p>
<p>This passes in the descriptive kernel name as part of the triton_meta dict that gets passed to the CachingAutotuner, for foreach kernels and templates.</p>
<p>Before:<br />
<img width="684" alt="Screenshot 2023-09-01 at 11 56 02 AM" src="https://github.com/pytorch/pytorch/assets/5067123/c14e13fc-0d9e-425a-a08b-613ef42aa264"></p>
<p>After:<br />
<img width="562" alt="Screenshot 2023-09-01 at 2 13 00 PM" src="https://github.com/pytorch/pytorch/assets/5067123/551bb9a9-865b-401e-b6e0-8ebbe5431565"></p>
<p>This PR also refactors the "magic strings" (KERNEL_NAME and DESCRIPTIVE_KRNL_NAME) into an enum in utils.py.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108449<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Fri, 08 Sep 2023 18:11:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ed7f9cac917dd078084cd7af0e898799a5997955</guid>
    </item>
    <item>
      <title>Revert "Re-land: Break graph on `manual_seed`. (#108647)"</title>
      <link>https://github.com/pytorch/pytorch/commit/8caaa4f4cdac6657f72c48607b6a732a975c5d20</link>
      <description><![CDATA[<p>Revert "Re-land: Break graph on <code>manual_seed</code>. (#108647)"</p>
<p>This reverts commit c887309437817f39ea3ef484732af427b393899f.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/108647 on behalf of https://github.com/huydhn due to Ouch, we are hit again my another internal import error from https://github.com/pytorch/pytorch/blob/main/torch/_inductor/config.py#L205-L206 (<a href="https://github.com/pytorch/pytorch/pull/108647#issuecomment-1712230103">comment</a>)</p>]]></description>
      <pubDate>Fri, 08 Sep 2023 13:18:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8caaa4f4cdac6657f72c48607b6a732a975c5d20</guid>
    </item>
    <item>
      <title>[reland][inductor] Switch to use the runtime interface for AOTInductor testing (#108878)</title>
      <link>https://github.com/pytorch/pytorch/commit/e91f66471cc5c8dfd38144dcc37ac63e06d03f78</link>
      <description><![CDATA[<p>[reland][inductor] Switch to use the runtime interface for AOTInductor testing (#108878)</p>
<p>Summary: This is a reland of https://github.com/pytorch/pytorch/pull/108663</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108878<br />
Approved by: https://github.com/muchulee8</p>]]></description>
      <pubDate>Fri, 08 Sep 2023 09:58:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e91f66471cc5c8dfd38144dcc37ac63e06d03f78</guid>
    </item>
    <item>
      <title>Revert "[inductor] Switch to use the runtime interface for AOTInductor testing (#108663)"</title>
      <link>https://github.com/pytorch/pytorch/commit/428f5f9e7eb98b1c3a82f8ee22da5a8853b2dea5</link>
      <description><![CDATA[<p>Revert "[inductor] Switch to use the runtime interface for AOTInductor testing (#108663)"</p>
<p>This reverts commit 366ce589d0b6bdde8f9ca2087f224b6925841a05.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/108663 on behalf of https://github.com/Chillee due to Sorry :'( Need to revert to resolve merge conflict for another revert (<a href="https://github.com/pytorch/pytorch/pull/108663#issuecomment-1711076411">comment</a>)</p>]]></description>
      <pubDate>Thu, 07 Sep 2023 21:01:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/428f5f9e7eb98b1c3a82f8ee22da5a8853b2dea5</guid>
    </item>
    <item>
      <title>[dynamo] Move global state guards to C++ (#108624)</title>
      <link>https://github.com/pytorch/pytorch/commit/4965fffedad8f7f4d333a384d3b046b029395de5</link>
      <description><![CDATA[<p>[dynamo] Move global state guards to C++ (#108624)</p>
<p>This combines a bunch of python global state guards into a single C++ guard and switches to checking them 100% of the time.  It also adds a few new guards for things that change inductor's behavior.   Even though we are checking more things, I expect this to be much faster.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108624<br />
Approved by: https://github.com/anijain2305</p>]]></description>
      <pubDate>Thu, 07 Sep 2023 20:07:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/4965fffedad8f7f4d333a384d3b046b029395de5</guid>
    </item>
    <item>
      <title>[AOTInductor] Include constants in AOTInductor .so file. (#108473)</title>
      <link>https://github.com/pytorch/pytorch/commit/30a33b76b97b62bca6a8d81476dc050436e00e71</link>
      <description><![CDATA[<p>[AOTInductor] Include constants in AOTInductor .so file. (#108473)</p>
<p>Summary:<br />
Include constants in AOTInductor .so file.<br />
Added some difference:<br />
1) serialize with ctypes instead of the native of torch.storage<br />
2) Use the underlying for_blob instead of from_blob to construct Tensor.</p>
<p>Test Plan:<br />
Unit tests:<br />
<code>test/inductor/test_aot_inductor.py</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108473<br />
Approved by: https://github.com/angelayi</p>]]></description>
      <pubDate>Thu, 07 Sep 2023 19:49:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/30a33b76b97b62bca6a8d81476dc050436e00e71</guid>
    </item>
    <item>
      <title>Revert "[dynamo] Add BACKEND_MATCH guard to detect and recompile when backend changes (#107337)"</title>
      <link>https://github.com/pytorch/pytorch/commit/38fcf77a1bd6fd8a18a74a17ed02c19730169e6c</link>
      <description><![CDATA[<p>Revert "[dynamo] Add BACKEND_MATCH guard to detect and recompile when backend changes (#107337)"</p>
<p>This reverts commit 1a64ec7dd48408d6839a5c2cceb55b0c4be2243b.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/107337 on behalf of https://github.com/huydhn due to Sorry for reverting your change but inductor perf smoke test starts to regress after this (<a href="https://github.com/pytorch/pytorch/pull/107337#issuecomment-1710974588">comment</a>)</p>]]></description>
      <pubDate>Thu, 07 Sep 2023 18:03:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/38fcf77a1bd6fd8a18a74a17ed02c19730169e6c</guid>
    </item>
    <item>
      <title>Decompose/add reference for `view_as_complex` (#108005)</title>
      <link>https://github.com/pytorch/pytorch/commit/c458fa0d35d9e4fa610320a1ece26517eaef2fbf</link>
      <description><![CDATA[<p>Decompose/add reference for <code>view_as_complex</code> (#108005)</p>
<p>Aten source: https://github.com/pytorch/pytorch/blob/d4a99631dd589afbb972b3401e56f029192c9b0f/aten/src/ATen/native/ComplexHelper.h#L78</p>
<p>Documentation reference:<br />
https://pytorch.org/docs/stable/generated/torch.view_as_complex.html</p>
<p>Note: this adds a new primitive <code>view_of_dtype</code>, which is trivially implemented, as its meta function is already implemented elsewhere.</p>
<p>Finally, this is not registered as a decomposition (yet), because TorchInductor does not yet support complex types. It should be added once we do.</p>
<p>Closes https://github.com/pytorch/pytorch/issues/108020 as well.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108005<br />
Approved by: https://github.com/peterbell10, https://github.com/ezyang</p>]]></description>
      <pubDate>Thu, 07 Sep 2023 15:49:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/c458fa0d35d9e4fa610320a1ece26517eaef2fbf</guid>
    </item>
    <item>
      <title>[inductor] Switch to use the runtime interface for AOTInductor testing (#108663)</title>
      <link>https://github.com/pytorch/pytorch/commit/366ce589d0b6bdde8f9ca2087f224b6925841a05</link>
      <description><![CDATA[<p>[inductor] Switch to use the runtime interface for AOTInductor testing (#108663)</p>
<p>Summary: Switch AOTInductor unit tests and integration tests to invoke the same runtime interface. This is only an effort to unify the usage of the runtime. The interface scrutiny will come in later PRs.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108663<br />
Approved by: https://github.com/ezyang<br />
ghstack dependencies: #108653</p>]]></description>
      <pubDate>Thu, 07 Sep 2023 15:38:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/366ce589d0b6bdde8f9ca2087f224b6925841a05</guid>
    </item>
    <item>
      <title>[dynamo] Add BACKEND_MATCH guard to detect and recompile when backend changes (#107337)</title>
      <link>https://github.com/pytorch/pytorch/commit/1a64ec7dd48408d6839a5c2cceb55b0c4be2243b</link>
      <description><![CDATA[<p>[dynamo] Add BACKEND_MATCH guard to detect and recompile when backend changes (#107337)</p>
<p><strong>Motivation:</strong><br />
We try to make torch.cond use torch.compile automatically so that we could error out when there is side-effects in the branches and correctly handle the closures.</p>
<p>Before this PR, we have a warning if we don't turn on a config raise_on_backend_change (turning it on gives us an error) for the following code:<br />
```python<br />
def foo()</p>
<h1>Inside torch.cond, we'd like to do something like</h1>
<p>torch.compile(foo, backend="eager", fullgraph=True)(...)<br />
...</p>
<h1>Users may then call torch.compile somewhere else.</h1>
<h1>Dynamo will use the cached code of foo for "eager" backend</h1>
<h1>but we expect dynamo to recompile with "inductor" backend.</h1>
<p>torch.compile(foo, backend="inductor")(...)<br />
```</p>
<p>This PR adds a BACKEND_MATCH guard. Effectively, it implements a per-backend cache. In the above example, the cached code for "eager" won't work for "inductor" due to guard check failures and the second torch.compile will do a re-compilation. In the future, it might be useful to have something like a configuration guard that guards against dynamo configuration changes across different compiles (e.g. compile a function with fullgraph=False then compile it again with fullgraph=True).</p>
<p><strong>Implementation:</strong><br />
1. We add a guarded_backend_cache and check the most_recent_backend against the backend associated with cached code. We also remove the raise_on_backend_change flag.</p>
<ol>
<li>Then newly added context manager and guard adds more lines for debug log so we change the uppper limit from 50 to 55.</li>
</ol>
<p><strong>Test Plan:</strong><br />
Removed original tests that raise on different backend and add a new test to test whether the BACKEND_MATCH guard can guard against backend change.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107337<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 07 Sep 2023 14:45:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/1a64ec7dd48408d6839a5c2cceb55b0c4be2243b</guid>
    </item>
    <item>
      <title>Revert "[inductor] Add ir.Scan and lower aten.cumsum on CUDA (#106581)"</title>
      <link>https://github.com/pytorch/pytorch/commit/8ba23e48fa8f608a8ac6382973f9b50308658d2c</link>
      <description><![CDATA[<p>Revert "[inductor] Add ir.Scan and lower aten.cumsum on CUDA (#106581)"</p>
<p>This reverts commit 53a27021c59f1df640cba88bb48f67ee977e07f8.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/106581 on behalf of https://github.com/atalman due to Sorry for reverting your change, but it broke rocm CI (<a href="https://github.com/pytorch/pytorch/pull/106581#issuecomment-1710776610">comment</a>)</p>]]></description>
      <pubDate>Thu, 07 Sep 2023 13:13:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8ba23e48fa8f608a8ac6382973f9b50308658d2c</guid>
    </item>
    <item>
      <title>[inductor] Add ir.Scan and lower aten.cumsum on CUDA (#106581)</title>
      <link>https://github.com/pytorch/pytorch/commit/53a27021c59f1df640cba88bb48f67ee977e07f8</link>
      <description><![CDATA[<p>[inductor] Add ir.Scan and lower aten.cumsum on CUDA (#106581)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/106581<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Thu, 07 Sep 2023 09:40:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/53a27021c59f1df640cba88bb48f67ee977e07f8</guid>
    </item>
    <item>
      <title>Remove fixed skips (#108674)</title>
      <link>https://github.com/pytorch/pytorch/commit/ab9fb03d6f674e3592910a0c4cc8208517a71084</link>
      <description><![CDATA[<p>Remove fixed skips (#108674)</p>
<p>These no longer fail with TEST_WITH_TORCHINDUCTOR.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/108674<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 07 Sep 2023 09:36:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ab9fb03d6f674e3592910a0c4cc8208517a71084</guid>
    </item>
    <item>
      <title>[inductor] Refactor wrapper.py (#108653)</title>
      <link>https://github.com/pytorch/pytorch/commit/fae9547cb78add2986447fdb7227ba08a7d0419b</link>
      <description><![CDATA[<p>[inductor] Refactor wrapper.py (#108653)</p>
<p>Summary: Cherry-pick refactoring from https://github.com/pytorch/pytorch/pull/105331 to make the code review easier.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108653<br />
Approved by: https://github.com/ezyang, https://github.com/khabinov</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 21:27:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fae9547cb78add2986447fdb7227ba08a7d0419b</guid>
    </item>
    <item>
      <title>[inductor] simplify time_and_log fallback (#108489)</title>
      <link>https://github.com/pytorch/pytorch/commit/35974234c4f811e28a4a74852d2245ec4d044c2f</link>
      <description><![CDATA[<p>[inductor] simplify time_and_log fallback (#108489)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108489<br />
Approved by: https://github.com/eellison<br />
ghstack dependencies: #108468</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 20:23:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/35974234c4f811e28a4a74852d2245ec4d044c2f</guid>
    </item>
    <item>
      <title>[inductor] simplify cudagraph_fail_reasons printing (#108468)</title>
      <link>https://github.com/pytorch/pytorch/commit/96dd173fa00b427f4f082c45ee6c5bbc97c8a091</link>
      <description><![CDATA[<p>[inductor] simplify cudagraph_fail_reasons printing (#108468)</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108468<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 20:23:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/96dd173fa00b427f4f082c45ee6c5bbc97c8a091</guid>
    </item>
    <item>
      <title>[inductor][easy] Enable Mypy Checking in torch/_inductor/kernel/ (#108678)</title>
      <link>https://github.com/pytorch/pytorch/commit/275e71c562f3f1e0232e2f4a22b5388e0e468864</link>
      <description><![CDATA[<p>[inductor][easy] Enable Mypy Checking in torch/_inductor/kernel/ (#108678)</p>
<p>Summary: Looks like these already pass (and torch/_inductor/kernel/mm_plus_mm_new.py does not exist)</p>
<p>Test Plan: <code>lintrunner torch/_inductor/kernel/mm.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/__init__.py torch/_inductor/kernel/mm_plus_mm.py</code><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/108678<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 19:29:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/275e71c562f3f1e0232e2f4a22b5388e0e468864</guid>
    </item>
    <item>
      <title>[export] Lift constant tensors as buffers (#108592)</title>
      <link>https://github.com/pytorch/pytorch/commit/e3407238f6be0583fe6dac7e2c4897f6c4480ed4</link>
      <description><![CDATA[<p>[export] Lift constant tensors as buffers (#108592)</p>
<p>When we retrace the graph containing constant tensors, they get lifted as buffer inputs.<br />
AotInductor also wants to lift all the constants as inputs.<br />
If we separate the constants as a separate thing, then it adds an additional complexity where we now have to keep track of 3 inputs (params, buffers, constants).</p>
<p>Cons: People might care about specifically what buffers are/are not buffers?</p>
<p>If people want to know specifically which buffers are constants, we can add an additional field in the graph signature to mark this.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D49017872">D49017872</a><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/108592<br />
Approved by: https://github.com/zhxchen17</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 17:14:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e3407238f6be0583fe6dac7e2c4897f6c4480ed4</guid>
    </item>
    <item>
      <title>Revert "Remove fixed skips (#108674)"</title>
      <link>https://github.com/pytorch/pytorch/commit/43527d41a255322003fd806eaefce56f0de7bfb0</link>
      <description><![CDATA[<p>Revert "Remove fixed skips (#108674)"</p>
<p>This reverts commit 518cfda2dd0e940603c74717b4cb33493a9ec908.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/108674 on behalf of https://github.com/huydhn due to Sorry for reverting this, but one test is failing on inductor https://hud.pytorch.org/pytorch/pytorch/commit/518cfda2dd0e940603c74717b4cb33493a9ec908, and it seems easier to revert this than disabling the test (<a href="https://github.com/pytorch/pytorch/pull/108674#issuecomment-1709310192">comment</a>)</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 16:56:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/43527d41a255322003fd806eaefce56f0de7bfb0</guid>
    </item>
    <item>
      <title>[inductor][easy] Enable Mypy Checking for torch/_inductor/decomposition.py (#108682)</title>
      <link>https://github.com/pytorch/pytorch/commit/27fe45eaf60548f4919b398f374b031185fb19f0</link>
      <description><![CDATA[<p>[inductor][easy] Enable Mypy Checking for torch/_inductor/decomposition.py (#108682)</p>
<p>Summary: Looks like one simple type mismatch between <code>get_decompositions()</code> and <code>remove_decompositions()</code></p>
<p>Test Plan: <code>lintrunner torch/_inductor/decomposition.py</code><br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/108682<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 16:48:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/27fe45eaf60548f4919b398f374b031185fb19f0</guid>
    </item>
    <item>
      <title>Remove fixed skips (#108674)</title>
      <link>https://github.com/pytorch/pytorch/commit/518cfda2dd0e940603c74717b4cb33493a9ec908</link>
      <description><![CDATA[<p>Remove fixed skips (#108674)</p>
<p>These no longer fail with TEST_WITH_TORCHINDUCTOR.<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/108674<br />
Approved by: https://github.com/desertfire</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 14:33:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/518cfda2dd0e940603c74717b4cb33493a9ec908</guid>
    </item>
    <item>
      <title>Fix inductor `sub` with symbolic integers. (#108518)</title>
      <link>https://github.com/pytorch/pytorch/commit/089950b83a8f1a69d82fd75eb0a7e6350423b362</link>
      <description><![CDATA[<p>Fix inductor <code>sub</code> with symbolic integers. (#108518)</p>
<p>Fix: #108159</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108518<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 13:01:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/089950b83a8f1a69d82fd75eb0a7e6350423b362</guid>
    </item>
    <item>
      <title>Enable mypy checking in torch/_inductor/sizevars.py (#107862)</title>
      <link>https://github.com/pytorch/pytorch/commit/8a76f8e6fe96fe9ec74b8b2e9f4cfb0ddc0de777</link>
      <description><![CDATA[<p>Enable mypy checking in torch/_inductor/sizevars.py (#107862)</p>
<p>Fixes #105230</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107862<br />
Approved by: https://github.com/eellison</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 11:43:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/8a76f8e6fe96fe9ec74b8b2e9f4cfb0ddc0de777</guid>
    </item>
    <item>
      <title>[PT2 Inference] Prototype of Inference Runtime (#108482)</title>
      <link>https://github.com/pytorch/pytorch/commit/bee7e781304a8b916e6c892e3c041e96c5543378</link>
      <description><![CDATA[<p>[PT2 Inference] Prototype of Inference Runtime (#108482)</p>
<p>Summary:<br />
This diff demonstrates a simplified E2E workflow for PT2 Inference stack:<br />
1. Model author with <code>torch.export()</code><br />
2. Model processing with <code>aot_inductor.compile()</code><br />
3. Model served with a new Inference Runtime API, named <code>ModelRunner</code></p>
<p><code>torch.export()</code> and <code>aot_inductor.compile()</code> produces a zip file using <code>PyTorchStreamWriter</code>.<br />
Runtime reads the zip file with <code>PyTorchStreamReader</code>.<br />
The zip file contains<br />
 {F1080328179}<br />
More discussion on packaging can be found in https://docs.google.com/document/d/1C-4DP5yu7ZhX1aB1p9JcVZ5TultDKObM10AqEtmZ-nU/edit?usp=sharing</p>
<p>Runtime can now switch between two Execution modes:<br />
1. Graph Interpreter mode, implemented based on Sigmoid's Executor<br />
2. AOTInductor mode, implemented based on FBAOTInductorModel</p>
<p>Test Plan:<br />
buck2 run  mode/dev-nosan mode/inplace -c fbcode.enable_gpu_sections=True //sigmoid/inference/test:e2e_test</p>
<p>Export and Lower with AOTInductor<br />
buck2 run mode/dev-sand mode/inplace -c fbcode.enable_gpu_sections=True sigmoid/inference:export_package</p>
<p>Run with GraphInterpreter and AOTInducotr<br />
buck2 run mode/dev-nosan //sigmoid/inference:main</p>
<p>Reviewed By: suo</p>
<p>Differential Revision: D47781098</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108482<br />
Approved by: https://github.com/zhxchen17</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 11:28:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/bee7e781304a8b916e6c892e3c041e96c5543378</guid>
    </item>
    <item>
      <title>Enable mypy checking in torch/_inductor/__init__.py (#108336)</title>
      <link>https://github.com/pytorch/pytorch/commit/e471c12a016a46d7b8d1c8c83a38021f08fce078</link>
      <description><![CDATA[<p>Enable mypy checking in torch/_inductor/<strong>init</strong>.py (#108336)</p>
<p>Fixes #105230</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108336<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 09:14:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e471c12a016a46d7b8d1c8c83a38021f08fce078</guid>
    </item>
    <item>
      <title>Torchbench model tolerance changes (#108598)</title>
      <link>https://github.com/pytorch/pytorch/commit/738106c1f78c30ff2217d14efd2c78533a3aaa24</link>
      <description><![CDATA[<p>Torchbench model tolerance changes (#108598)</p>
<p>Move detectron2_fcos_r_50_fpn to amp. The minifier showed the following snippet as causing the divergence, where inductor has better numerics than eager:</p>
<p>```<br />
import torch</p>
<p>def foo(x):<br />
    return x &gt; .2</p>
<p>inp = torch.tensor([.2002], device="cuda", dtype=torch.bfloat16)<br />
print(foo(inp))</p>
<p>print(torch.compile(foo)(inp))<br />
```</p>
<p>doctr_reco_predictor had very minimal divergence (.002 vs .001 required), bumping tolerance here.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108598<br />
Approved by: https://github.com/shunting314</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 08:52:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/738106c1f78c30ff2217d14efd2c78533a3aaa24</guid>
    </item>
    <item>
      <title>Inductor cpp wrapper: fix codegen of positional args with default value (#108552)</title>
      <link>https://github.com/pytorch/pytorch/commit/ca9f4222e158ce1510c562e9f23adc6addb87fe6</link>
      <description><![CDATA[<p>Inductor cpp wrapper: fix codegen of positional args with default value (#108552)</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/108323.<br />
Cpp wrapper has functionality regression on <code>llama</code> and <code>tnt_s_patch16_224</code> due to recent support of scaled dot product flash attention in inductor.</p>
<p>The schema of this OP is as follows:<br />
<code>- func: _scaled_dot_product_flash_attention(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -&gt; (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, int max_q, int max_k, Tensor philox_seed, Tensor philox_offset, Tensor debug_attn_mask)</code></p>
<p>For <code>llama</code> and <code>tnt_s_patch16_224</code>, the OP is called in the below way, where the three positional args with default values are not passed (<code>float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False</code>).<br />
<code>python
y = torch.ops.aten._scaled_dot_product_flash_attention.default(x0, x1, x2, scale = 0.125)</code></p>
<p>This PR fixes the cpp wrapper support for this case.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108552<br />
Approved by: https://github.com/jgong5, https://github.com/desertfire, https://github.com/jansel</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 05:15:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ca9f4222e158ce1510c562e9f23adc6addb87fe6</guid>
    </item>
    <item>
      <title>[inductor] Move AOTInductor runtime headers (#108564)</title>
      <link>https://github.com/pytorch/pytorch/commit/60bd30ee0b35e4a5d96e2e65d320e99bd07c34d3</link>
      <description><![CDATA[<p>[inductor] Move AOTInductor runtime headers (#108564)</p>
<p>Summary: Move AOTInductor runtime header files into its own subdirectory, to separate them from to-be-added libtorch C interface.</p>
<p>Reviewed By: frank-wei</p>
<p>Differential Revision: D48905038</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108564<br />
Approved by: https://github.com/frank-wei</p>]]></description>
      <pubDate>Wed, 06 Sep 2023 03:50:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/60bd30ee0b35e4a5d96e2e65d320e99bd07c34d3</guid>
    </item>
    <item>
      <title>[inductor] Use empty_strided to create output tensors when testing AOTInductor (#108364)</title>
      <link>https://github.com/pytorch/pytorch/commit/28c5b622109fa99336fd10b4d1fd31c3e5ec6b8e</link>
      <description><![CDATA[<p>[inductor] Use empty_strided to create output tensors when testing AOTInductor (#108364)</p>
<p>Summary: This will fix 3 fail_accuracy failures in HF.</p>
<p>Test Plan:<br />
<code>python benchmarks/dynamo/huggingface.py --bfloat16 --accuracy --inference --device cuda --export-aot-inductor --only  T5Small</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108364<br />
Approved by: https://github.com/angelayi<br />
ghstack dependencies: #108412</p>]]></description>
      <pubDate>Tue, 05 Sep 2023 18:04:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/28c5b622109fa99336fd10b4d1fd31c3e5ec6b8e</guid>
    </item>
    <item>
      <title>error when using _dynamo.optimize_ddp=True and _inductor.keep_output_stride=False together (#108235)</title>
      <link>https://github.com/pytorch/pytorch/commit/da914aed218db6cced97431b69407c0c6f1f47d6</link>
      <description><![CDATA[<p>error when using _dynamo.optimize_ddp=True and _inductor.keep_output_stride=False together (#108235)</p>
<p>From talking to @wconstab, we agreed that because of the way DDPOptimizer is written, it is (sort of) incompatible with inductor's <code>keep_output_stride=False</code> optimizations (and will cause silent correctness problems if you use them ogether). Added an assertion.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108235<br />
Approved by: https://github.com/wconstab<br />
ghstack dependencies: #108081</p>]]></description>
      <pubDate>Tue, 05 Sep 2023 12:02:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/da914aed218db6cced97431b69407c0c6f1f47d6</guid>
    </item>
    <item>
      <title>Fix inductor &lt;&gt; ddp_optimizer issue (#108081)</title>
      <link>https://github.com/pytorch/pytorch/commit/def33d4d7a0c74fea0c845eb972af244a69be0d8</link>
      <description><![CDATA[<p>Fix inductor &lt;&gt; ddp_optimizer issue (#108081)</p>
<p>@wconstab pointed out that inductor found a graph with 6 input mutations and only 1 output, and seemed to be (incorrectly) chopping off the first "6" outputs from the graph (even though there is only 1). It looks like this is because:</p>
<p>(1) AOTAutograd has special handling for input mutations in inference vs. training graphs. In a training graph, whenever AOTAutograd sees an input mutation, it will add an <strong>extra</strong> output to the graph, corresponding to the updated input (and then at runtime, it will grab the updated input, and perform the actual mutation outside of the graph).</p>
<p>In inference, AOTAutograd is smarter and can leave the input mutations directly in the graph for inductor to optimize (doing this in training is harder). In inference, AOTAutograd will <strong>not</strong> add any extra graph outputs for input mutations.</p>
<p>It looks like inductor was unconditionally assuming that input mutations counted as extra outputs in the graph, which is wrong for the inference case.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108081<br />
Approved by: https://github.com/wconstab</p>]]></description>
      <pubDate>Tue, 05 Sep 2023 12:02:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/def33d4d7a0c74fea0c845eb972af244a69be0d8</guid>
    </item>
    <item>
      <title>[inductor] Update how AOTInductor resizes output tensors (#108412)</title>
      <link>https://github.com/pytorch/pytorch/commit/7cdfc38433424d1df8205d5ec8400814b1f190ae</link>
      <description><![CDATA[<p>[inductor] Update how AOTInductor resizes output tensors (#108412)</p>
<p>Summary: Improve https://github.com/pytorch/pytorch/pull/107848 so that there is no resize_ needed for output tensors when existing the main function.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108412<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Tue, 05 Sep 2023 11:33:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/7cdfc38433424d1df8205d5ec8400814b1f190ae</guid>
    </item>
    <item>
      <title>[inductor] Add an aot_inductor class in inductor config (#108369)</title>
      <link>https://github.com/pytorch/pytorch/commit/3d2938b1fcfa58bc17abb4014cd4f547c50d0a6f</link>
      <description><![CDATA[<p>[inductor] Add an aot_inductor class in inductor config (#108369)</p>
<p>Summary: Introduce an aot_inductor class to group AOTInductor specific configs</p>
<p>Differential Revision: D48880684</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108369<br />
Approved by: https://github.com/frank-wei</p>]]></description>
      <pubDate>Mon, 04 Sep 2023 23:11:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3d2938b1fcfa58bc17abb4014cd4f547c50d0a6f</guid>
    </item>
    <item>
      <title>[Inductor] Make aot-inductor work with pip installed torch (#108319)</title>
      <link>https://github.com/pytorch/pytorch/commit/ff38c0e2f9cae35378553c38ccf7188007fed938</link>
      <description><![CDATA[<p>[Inductor] Make aot-inductor work with pip installed torch (#108319)</p>
<p>It seems pip-installed torch is built with <code>D_GLIBCXX_USE_CXX11_ABI=0</code> and it fails the inductor/test_aot_inductor.py with:<br />
```<br />
ERROR: test_with_offset (<strong>main</strong>.AotInductorTests)</p>
<hr />
<p>Traceback (most recent call last):<br />
  File "/home/ubuntu/anaconda3/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2388, in wrapper<br />
    method(<em>args, </em>*kwargs)<br />
  File "/home/ubuntu/src/pytorch/test/inductor/test_aot_inductor.py", line 112, in test_with_offset<br />
    actual = AOTInductorModelRunner.run(model, example_inputs, expected)<br />
  File "/home/ubuntu/src/pytorch/test/inductor/test_aot_inductor.py", line 63, in run<br />
    optimized, exported, output_tensors, output_spec = AOTInductorModelRunner.load(<br />
  File "/home/ubuntu/src/pytorch/test/inductor/test_aot_inductor.py", line 50, in load<br />
    optimized = torch.utils.cpp_extension.load_inline(<br />
  File "/home/ubuntu/anaconda3/lib/python3.9/site-packages/torch/utils/cpp_extension.py", line 1635, in load_inline<br />
    return _jit_compile(<br />
  File "/home/ubuntu/anaconda3/lib/python3.9/site-packages/torch/utils/cpp_extension.py", line 1736, in _jit_compile<br />
    return _import_module_from_library(name, build_directory, is_python_module)<br />
  File "/home/ubuntu/anaconda3/lib/python3.9/site-packages/torch/utils/cpp_extension.py", line 2136, in _import_module_from_library<br />
    module = importlib.util.module_from_spec(spec)<br />
  File "<frozen importlib._bootstrap>", line 565, in module_from_spec<br />
  File "<frozen importlib._bootstrap_external>", line 1173, in create_module<br />
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed<br />
ImportError: /tmp/torchinductor_ubuntu/cqrzlw3yizrsx2us5bnjosr4tzct24h6qwb6xbbx654fxvdupoub/cr6ndwlgeorw34etxhwvs547kbnftyxtwwrsmbdraa4hjeevsvji.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE<br />
```<br />
I'm not sure how to test this in CI, maybe run tests with prebuilt wheels?</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108319<br />
Approved by: https://github.com/ezyang</p>]]></description>
      <pubDate>Mon, 04 Sep 2023 11:57:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/ff38c0e2f9cae35378553c38ccf7188007fed938</guid>
    </item>
    <item>
      <title>[AOTInductor][Reland] Proxy Executor for Extern Fallback kernels (#107279) (#108350)</title>
      <link>https://github.com/pytorch/pytorch/commit/b9dfdc091b21b1a3475baed55238b4dadbdcd7e9</link>
      <description><![CDATA[<p>[AOTInductor][Reland] Proxy Executor for Extern Fallback kernels (#107279) (#108350)</p>
<p>Summary:</p>
<p>This is a prototype for running extern fallback kernels with a host side proxy executor.</p>
<p>Sample of generated cpp wrapper call:<br />
<code>at::Tensor buf0;  // output buffer
        void* tensor_args_var_0[] = {&amp;arg0_1, &amp;arg0_1, &amp;arg1_1, &amp;arg0_1, &amp;arg1_1, &amp;buf0};
        int64_t int_args_var_1[] = {81, 81, 7, 7, 7, 81};
        proxy_executor-&gt;call_function("buf0", int_args_var_1, tensor_args_var_0);</code></p>
<ul>
<li>In my current implementation, proxy executor interprets the raw pointers according to the ops schema.<br />
This assumes that custom op MUST have a valid schema registered to Dispatcher. (I would like to validate this assumption)</li>
<li>
<p>I am using callboxed() API of the custom kernels. This is inevitable, as we wish to have a single call_function API for all possible custom kernels.</p>
</li>
<li>
<p>These are all the input argument types I have support so far.<br />
       union Argument {<br />
         # Bool value does not matter<br />
         1: bool asNone;<br />
         2: TensorArgument asTensor;<br />
         3: list<TensorArgument> asTensors;<br />
         5: i64 asInt;<br />
         7: list<i64> asInts;<br />
         8: double asFloat;<br />
         9: list<double> asFloats;<br />
         10: string asString;<br />
         10.5: list<string> asStrings;<br />
         11: SymIntArgument asSymInt;<br />
         12: list<SymIntArgument> asSymInts;<br />
         13: ScalarType asScalarType;<br />
         14: MemoryFormat asMemoryFormat;<br />
         15: Layout asLayout;<br />
         16: Device asDevice;<br />
         17: bool asBool;<br />
         18: list<bool> asBools;<br />
       }</p>
</li>
<li>
<p>Need a policy for handling unpopulated argument with default values. Here are the options, and it has BC  implications.</p>
</li>
<li>requires exported fx graph to explicitly populate default values, if users doesn't specify.</li>
<li>requires cpp wrapper to explicitly populate default values, if fx graph doesn't specify.</li>
<li>Proxy executor look up from opSchema for default values.</li>
</ul>
<p>For fixing T162112344</p>
<p>Test Plan:<br />
frontend:<br />
buck2 run mode/dev-sand mode/inplace -c fbcode.enable_gpu_sections=True sigmoid/frontend:export_main</p>
<p>test:<br />
 buck2 run mode/dev-sand //deeplearning/aot_inductor/test:test_custom_ops</p>
<p>backend:<br />
buck2 run mode/dev-nosan //deeplearning/aot_inductor/fb:main</p>
<p>buck2 test 'fbcode//mode/opt' fbcode//caffe2/torch/fb/model_transform/experimental/benchmark/test:test_aot_inductor_benchmark -- --exact 'caffe2/torch/fb/model_transform/experimental/benchmark/test:test_aot_inductor_benchmark - test_aot_inductor_benchmark_cmf30x (caffe2.torch.fb.model_transform.experimental.benchmark.test.test_aot_inductor_benchmark.AOTInductorBenchmark)'</p>
<p>Reviewed By: suo</p>
<p>Differential Revision: D48747417</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108350<br />
Approved by: https://github.com/izaitsevfb</p>]]></description>
      <pubDate>Sat, 02 Sep 2023 09:14:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/b9dfdc091b21b1a3475baed55238b4dadbdcd7e9</guid>
    </item>
    <item>
      <title>[inductor] Handle aten.full's dtype in the decomposition (#108443)</title>
      <link>https://github.com/pytorch/pytorch/commit/fa8edd93b77fcbd5326554ac86906209bd3f9465</link>
      <description><![CDATA[<p>[inductor] Handle aten.full's dtype in the decomposition (#108443)</p>
<p>In the lowering we don't have <code>SymFloat</code> and <code>SymInt</code>, we just have <code>sympy.Expr</code><br />
so it is impossible to accurately determine the expected dtype of a <code>full</code> call.<br />
For example, <code>sym_float(int_expr)</code> has <code>is_integer=True</code> but should be treated<br />
as a float. In the decomposition though, we can get this right.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108443<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Fri, 01 Sep 2023 16:53:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fa8edd93b77fcbd5326554ac86906209bd3f9465</guid>
    </item>
    <item>
      <title>[export] Properly handle duplicated params. (#108415)</title>
      <link>https://github.com/pytorch/pytorch/commit/fc1c862e624b1adddf6f58cfd58f979ec6887c16</link>
      <description><![CDATA[<p>[export] Properly handle duplicated params. (#108415)</p>
<p>Summary:</p>
<p>Test Plan:<br />
python benchmarks/dynamo/huggingface.py --bfloat16 --accuracy<br />
--inference --device cuda --export --only  BertForMaskedLM</p>
<p>python benchmarks/dynamo/huggingface.py --bfloat16 --accuracy<br />
--inference --device cuda --export-aot-inductor --only  BertForMaskedLM</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Fixes #ISSUE_NUMBER</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108415<br />
Approved by: https://github.com/angelayi</p>]]></description>
      <pubDate>Fri, 01 Sep 2023 11:44:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fc1c862e624b1adddf6f58cfd58f979ec6887c16</guid>
    </item>
    <item>
      <title>[inductor] Generalize pointless_cumsum_replacement pattern (#108373)</title>
      <link>https://github.com/pytorch/pytorch/commit/e58d3ed81d4e23b5a71b9558e04ba75b95744ae0</link>
      <description><![CDATA[<p>[inductor] Generalize pointless_cumsum_replacement pattern (#108373)</p>
<p>The current pattern transforms:<br />
<code>ones([x, y]).cumsum(1) -&gt; arange(1, 1 + y).expand([x, y])</code><br />
but this generalizes it to<br />
<code>full(shape, fill_value).cumsum(d) -&gt;
    (fill_value * arange(1, 1 + shape[d])).view([1..., shape[d], 1...]).expand(shape)</code></p>
<p>So we handle any fill value, any number of dimensions, and broadcasting to any dimension.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108373<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Fri, 01 Sep 2023 09:12:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/e58d3ed81d4e23b5a71b9558e04ba75b95744ae0</guid>
    </item>
    <item>
      <title>[export] Fix duplicated params for AOTInductor. (#108354)</title>
      <link>https://github.com/pytorch/pytorch/commit/d96446b9c2ec3741e297be576328d9951ed9e227</link>
      <description><![CDATA[<p>[export] Fix duplicated params for AOTInductor. (#108354)</p>
<p>Summary:</p>
<p>Test Plan:<br />
python benchmarks/dynamo/huggingface.py --bfloat16 --accuracy<br />
--inference --device cuda --export --only  BertForMaskedLM</p>
<p>python benchmarks/dynamo/huggingface.py --bfloat16 --accuracy --inference --device cuda --export-aot-inductor --only  BertForMaskedLM</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Fixes #ISSUE_NUMBER</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108354<br />
Approved by: https://github.com/angelayi, https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 31 Aug 2023 19:18:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d96446b9c2ec3741e297be576328d9951ed9e227</guid>
    </item>
    <item>
      <title>Revert "Flash Attention v2 (#105602)"</title>
      <link>https://github.com/pytorch/pytorch/commit/d569e506abcfb581809378c1b89bdedb86f5a640</link>
      <description><![CDATA[<p>Revert "Flash Attention v2 (#105602)"</p>
<p>This reverts commit 9df3d882c8fe1e57914315aa250664ad5003d4fd.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/105602 on behalf of https://github.com/huydhn due to I think we miss a case here for sm80 build on inductor workflow as it is now OOM on trunk https://github.com/pytorch/pytorch/actions/runs/6042843139 (<a href="https://github.com/pytorch/pytorch/pull/105602#issuecomment-1701974862">comment</a>)</p>]]></description>
      <pubDate>Thu, 31 Aug 2023 17:15:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/d569e506abcfb581809378c1b89bdedb86f5a640</guid>
    </item>
    <item>
      <title>x86_inductor_quantizer switches to new graph capture API (#108214)</title>
      <link>https://github.com/pytorch/pytorch/commit/fb808c30c776a922aa79d50834e83ffb6cd30e56</link>
      <description><![CDATA[<p>x86_inductor_quantizer switches to new graph capture API (#108214)</p>
<p><strong>Summary</strong><br />
Update <code>X86InductorQuantizer</code> and related testcase to the new graph capture API <code>capture_pre_autograd_graph</code>.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108214<br />
Approved by: https://github.com/jgong5, https://github.com/jerryzh168</p>]]></description>
      <pubDate>Thu, 31 Aug 2023 16:43:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/fb808c30c776a922aa79d50834e83ffb6cd30e56</guid>
    </item>
    <item>
      <title>Pin pandas version for inductor Docker image (#108355)</title>
      <link>https://github.com/pytorch/pytorch/commit/3e75fd06e2b5c785438702b4005d2607ac70ec17</link>
      <description><![CDATA[<p>Pin pandas version for inductor Docker image (#108355)</p>
<p>Building docker in trunk is failing atm https://github.com/pytorch/pytorch/actions/runs/6033657019/job/16370683676 with the following error:</p>
<p>```<br />
+ conda_reinstall numpy=1.24.4<br />
+ as_jenkins conda install -q -n py_3.10 -y --force-reinstall numpy=1.24.4<br />
+ sudo -E -H -u jenkins env -u SUDO_UID -u SUDO_GID -u SUDO_COMMAND -u SUDO_USER env PATH=/opt/conda/envs/py_3.10/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 conda install -q -n py_3.10 -y --force-reinstall numpy=1.24.4<br />
Collecting package metadata (current_repodata.json): ...working... done<br />
Solving environment: ...working... unsuccessful initial attempt using frozen solve. Retrying with flexible solve.<br />
Collecting package metadata (repodata.json): ...working... done<br />
Solving environment: ...working... unsuccessful initial attempt using frozen solve. Retrying with flexible solve.</p>
<p>PackagesNotFoundError: The following packages are not available from current channels:</p>
<ul>
<li>numpy=1.24.4</li>
</ul>
<p>Current channels:</p>
<ul>
<li>https://repo.anaconda.com/pkgs/main/linux-64</li>
<li>https://repo.anaconda.com/pkgs/main/noarch</li>
<li>https://repo.anaconda.com/pkgs/r/linux-64</li>
<li>https://repo.anaconda.com/pkgs/r/noarch<br />
```</li>
</ul>
<p>This was pulled in by pandas 2.1.0 released yesterday https://pypi.org/project/pandas/2.1.0<br />
Pull Request resolved: https://github.com/pytorch/pytorch/pull/108355<br />
Approved by: https://github.com/kit1980, https://github.com/atalman, https://github.com/malfet</p>]]></description>
      <pubDate>Thu, 31 Aug 2023 13:58:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/3e75fd06e2b5c785438702b4005d2607ac70ec17</guid>
    </item>
    <item>
      <title>Fallback to eager for float8 ops in inductor (#108293)</title>
      <link>https://github.com/pytorch/pytorch/commit/5b6ba4110be80934bc18461bc8f04d1cb2b6b148</link>
      <description><![CDATA[<p>Fallback to eager for float8 ops in inductor (#108293)</p>
<h1>Summary</h1>
<p>As a stop gap to supporting the FP8 Dtype within inductor we would like to fallback to eager. Currently there are 3 ops that are needed for this:<br />
<code>_scaled_mm</code> ( matmul for fp8 types)<br />
<code>clone</code> (for creating new copies of fp8 tensors)<br />
<code>to</code> ( for converting to and from fp8 types).</p>
<p>This PR registers a fallback for _scaled_mm. And adds fp8 to trigger <code>unsupported_input_tensor</code></p>
<p>Prior to these changes this was failing with:<br />
<code>Shell
  File "/home/drisspg/meta/pytorch/torch/_inductor/codegen/triton_utils.py", line 11, in signature_of
    tye = JITFunction._type_of(arg.dtype)
  File "/home/drisspg/miniconda3/envs/dev/lib/python3.10/site-packages/triton/runtime/jit.py", line 229, in _type_of
    return key if isinstance(key, str) else f"*{tys[dtype_str]}"
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
KeyError: 'float8_e4m3fn'</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108293<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Thu, 31 Aug 2023 12:48:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/5b6ba4110be80934bc18461bc8f04d1cb2b6b148</guid>
    </item>
    <item>
      <title>Fix constant folding of arithmetic operations with symbolic values. (#108160)</title>
      <link>https://github.com/pytorch/pytorch/commit/aeb4d6d5c5e4fb9aab3e372d37eae727e099fb4f</link>
      <description><![CDATA[<p>Fix constant folding of arithmetic operations with symbolic values. (#108160)</p>
<p>Partial fix: #108067</p>
<p>This PR fixes an inductor bug where it assumed the type of arithmetic nodes arguments were<br />
all <code>Tensor</code>.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108160<br />
Approved by: https://github.com/lezcano</p>]]></description>
      <pubDate>Thu, 31 Aug 2023 12:26:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/aeb4d6d5c5e4fb9aab3e372d37eae727e099fb4f</guid>
    </item>
    <item>
      <title>pass inference accuracy check for detectron2_fcos_r_50_fpn (#108328)</title>
      <link>https://github.com/pytorch/pytorch/commit/eb8659fe81f3d4b061674bf149a6805cd292db8d</link>
      <description><![CDATA[<p>pass inference accuracy check for detectron2_fcos_r_50_fpn (#108328)</p>
<p>We need a higher tolerance to pass the inference accuracy check for detectron2_fcos_r_50_fpn .</p>
<p>Command:<br />
<code>python benchmarks/dynamo/torchbench.py --backend inductor --bfloat16 --accuracy --only detectron2_fcos_r_50_fpn --disable-cudagraphs --inference</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108328<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 31 Aug 2023 12:21:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/eb8659fe81f3d4b061674bf149a6805cd292db8d</guid>
    </item>
    <item>
      <title>Enable Mypy Checking in torch/_inductor/dependencies.py (#107675)</title>
      <link>https://github.com/pytorch/pytorch/commit/877561f3881362591a5b1b77cc468ec612386250</link>
      <description><![CDATA[<p>Enable Mypy Checking in torch/_inductor/dependencies.py (#107675)</p>
<p>Fixes #105230</p>
<p>Summary:</p>
<p>As suggested in https://github.com/pytorch/pytorch/issues/105230 mypy checking is enabled in torch/_inductor/dependencies.py</p>
<p>After Fix:</p>
<p>mypy --follow-imports=skip torch/_inductor/dependencies.py Success: no issues found in 1 source file</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/107675<br />
Approved by: https://github.com/jansel</p>]]></description>
      <pubDate>Thu, 31 Aug 2023 09:36:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/877561f3881362591a5b1b77cc468ec612386250</guid>
    </item>
    <item>
      <title>Revert "Fallback to eager for float8 ops in inductor (#108293)"</title>
      <link>https://github.com/pytorch/pytorch/commit/2e1e7ed61075b544f4fab67d36be38c2c0cc3846</link>
      <description><![CDATA[<p>Revert "Fallback to eager for float8 ops in inductor (#108293)"</p>
<p>This reverts commit 98aa3745c258827cde8d081d0713ba2cd67c864e.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/108293 on behalf of https://github.com/huydhn due to Sorry for reverting your change, it is failing on ROCM https://hud.pytorch.org/pytorch/pytorch/commit/98aa3745c258827cde8d081d0713ba2cd67c864e (<a href="https://github.com/pytorch/pytorch/pull/108293#issuecomment-1701446105">comment</a>)</p>]]></description>
      <pubDate>Thu, 31 Aug 2023 09:21:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/2e1e7ed61075b544f4fab67d36be38c2c0cc3846</guid>
    </item>
    <item>
      <title>Revert "[AOTInductor] Include constants in AOTInductor .so file. (#10… (#108349)</title>
      <link>https://github.com/pytorch/pytorch/commit/06d74e6b24a6f7dfd0b8d05e2171d74def166140</link>
      <description><![CDATA[<p>Revert "[AOTInductor] Include constants in AOTInductor .so file. (#10… (#108349)</p>
<p>This reverts commit c3239442a3dd1040b251ff33bef40589cba40e1c due to internal test failures.</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108349<br />
Approved by: https://github.com/aakhundov, https://github.com/zhxchen17</p>]]></description>
      <pubDate>Thu, 31 Aug 2023 08:26:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/06d74e6b24a6f7dfd0b8d05e2171d74def166140</guid>
    </item>
    <item>
      <title>inductor: make fallback for cpu scatter_add (#108220)</title>
      <link>https://github.com/pytorch/pytorch/commit/cbf7c918837cce5b46a972a27c74b5dd1a4acc3e</link>
      <description><![CDATA[<p>inductor: make fallback for cpu scatter_add (#108220)</p>
<p>For inductor cpu backend, the scatter_add will use <code>atomic_add</code>, which get a worse performance, currently, we make fallback for it to avoid performance regression compared with eager mode(single socket of SKX):<br />
```<br />
basic_gnn_gin 1.16x(after) Vs 0.509x(before)</p>
<p>basic_gnn_sage  1.064x(after) Vs 0.496x (before)</p>
<p>basic_gnn_gcn 1.373x(aftre) Vs 0.720x(before)<br />
```</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108220<br />
Approved by: https://github.com/jgong5, https://github.com/desertfire</p>]]></description>
      <pubDate>Thu, 31 Aug 2023 08:11:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/cbf7c918837cce5b46a972a27c74b5dd1a4acc3e</guid>
    </item>
    <item>
      <title>Use ctypes to serialize raw content for tensors. (#108287)</title>
      <link>https://github.com/pytorch/pytorch/commit/43f28beffc572474e8c5f6ba6c33115e9dc69be9</link>
      <description><![CDATA[<p>Use ctypes to serialize raw content for tensors. (#108287)</p>
<p>Summary:<br />
There's a deadlock in current storage's implementation if the size of tensor is too large. Use ctypes to do serialization.</p>
<p>Test Plan:<br />
python benchmarks/dynamo/huggingface.py --bfloat16 --accuracy --inference --device cuda --export-aot-inductor --only MT5ForConditionalGeneration</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>Fixes #ISSUE_NUMBER</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108287<br />
Approved by: https://github.com/desertfire, https://github.com/malfet</p>]]></description>
      <pubDate>Wed, 30 Aug 2023 22:59:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/43f28beffc572474e8c5f6ba6c33115e9dc69be9</guid>
    </item>
    <item>
      <title>Fallback to eager for float8 ops in inductor (#108293)</title>
      <link>https://github.com/pytorch/pytorch/commit/98aa3745c258827cde8d081d0713ba2cd67c864e</link>
      <description><![CDATA[<p>Fallback to eager for float8 ops in inductor (#108293)</p>
<h1>Summary</h1>
<p>As a stop gap to supporting the FP8 Dtype within inductor we would like to fallback to eager. Currently there are 3 ops that are needed for this:<br />
<code>_scaled_mm</code> ( matmul for fp8 types)<br />
<code>clone</code> (for creating new copies of fp8 tensors)<br />
<code>to</code> ( for converting to and from fp8 types).</p>
<p>This PR registers a fallback for _scaled_mm. And adds fp8 to trigger <code>unsupported_input_tensor</code></p>
<p>Prior to these changes this was failing with:<br />
<code>Shell
  File "/home/drisspg/meta/pytorch/torch/_inductor/codegen/triton_utils.py", line 11, in signature_of
    tye = JITFunction._type_of(arg.dtype)
  File "/home/drisspg/miniconda3/envs/dev/lib/python3.10/site-packages/triton/runtime/jit.py", line 229, in _type_of
    return key if isinstance(key, str) else f"*{tys[dtype_str]}"
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
KeyError: 'float8_e4m3fn'</code></p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108293<br />
Approved by: https://github.com/peterbell10</p>]]></description>
      <pubDate>Wed, 30 Aug 2023 20:09:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/98aa3745c258827cde8d081d0713ba2cd67c864e</guid>
    </item>
    <item>
      <title>Allow registering decomps for HigherOrderOp; add decomp for out_dtype (#108080)</title>
      <link>https://github.com/pytorch/pytorch/commit/0e4752bafc24755893bd1657c42d8937c275ce74</link>
      <description><![CDATA[<p>Allow registering decomps for HigherOrderOp; add decomp for out_dtype (#108080)</p>
<p>We allow registering decomps for HigherOrderOp via the existing decomp<br />
mechanisms:<br />
- I refactored those APIs to accept torch._ops.OperatorBase, which is the base<br />
  class for torch.ops.HigherOrderOperator and torch.ops.OpOverload<br />
- HigherOrderOps must directly call maybe_handle_decomp in their<br />
  ProxyTorchDispatchMode handling in order to resolve decompositions. We<br />
  can change this in the future so that they do not need to do this.</p>
<p>Next, we add an inductor decomp for out_dtype. This decomp shouldn't be<br />
generally available because we want to preserve out_dtype to the backend<br />
for other use cases (i.e. executorch).</p>
<p>Test Plan:<br />
- new tests</p>
<p>Pull Request resolved: https://github.com/pytorch/pytorch/pull/108080<br />
Approved by: https://github.com/HDCharles</p>]]></description>
      <pubDate>Wed, 30 Aug 2023 19:15:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/0e4752bafc24755893bd1657c42d8937c275ce74</guid>
    </item>
    <item>
      <title>Revert "[BE] Pin scipy to 1.10.1 (#108270)"</title>
      <link>https://github.com/pytorch/pytorch/commit/95e3126370f9cb826875ed1629fbfb6ad9299346</link>
      <description><![CDATA[<p>Revert "[BE] Pin scipy to 1.10.1 (#108270)"</p>
<p>This reverts commit cd3860cf160838a997ecbbed1ff58823c252e5b3.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/108270 on behalf of https://github.com/huydhn due to Some inductor tests start failing after this change. The failure comes from numba so I suspect that updating Docker pulls in an unwanted dependency update again (<a href="https://github.com/pytorch/pytorch/pull/108270#issuecomment-1700302953">comment</a>)</p>]]></description>
      <pubDate>Wed, 30 Aug 2023 19:06:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/commit/95e3126370f9cb826875ed1629fbfb6ad9299346</guid>
    </item>
  </channel>
</rss>
