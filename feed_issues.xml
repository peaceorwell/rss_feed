<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[Inductor cutlass backend] Enabled nonzero workspace and Cutlass StreamK</title>
      <link>https://github.com/pytorch/pytorch/pull/125406</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124928<br />
* <strong>-&gt;</strong> #125406</p>
<p>Enable nonzero workspace and Cutlass StreamK for Inductor Cutlass GEMM ops.</p>
<p>This is a simpler rewrite of my original version of #119005 using @peterbell10 's workspace allocation mechanism from #117992</p>
<p>Test Plan:<br />
 - Additional unit test in test_cutlass_backend.py which specifically tests StreamK GEMM with workspace requirement<br />
 - CI</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 02 May 2024 08:54:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125406</guid>
    </item>
    <item>
      <title>nn parameterization causing increased compile time</title>
      <link>https://github.com/pytorch/pytorch/issues/125314</link>
      <description><![CDATA[<p>Internal link: https://fb.workplace.com/groups/1075192433118967/permalink/1421189078519299/</p>
<p>Minimal repro:<br />
```python<br />
"""<br />
torch==2.3.0.dev20240226+cu121<br />
parametrize=True: 4005ms<br />
parametrize=False: 1285ms<br />
parametrize=True: 1848ms<br />
parametrize=False: 1417ms</p>
<p>torch==2.4.0.dev20240415+cu118<br />
parametrize=True: 18487ms<br />
parametrize=False: 1064ms<br />
parametrize=True: 17213ms<br />
parametrize=False: 1064ms<br />
"""</p>
<p>import torch<br />
from torch import nn<br />
import time</p>
<p>class Module(nn.Module):<br />
    def <strong>init</strong>(self) -&gt; None:<br />
        super().<strong>init</strong>()<br />
        self.seq = nn.Sequential(*[<br />
            nn.Linear(128, 128, bias=True)<br />
            for _ in range(32)<br />
        ])</p>
<pre><code>def forward(self, x):
    return self.seq(x)
</code></pre>
<p>class Parametrization(torch.nn.Module):<br />
    def forward(self, x):<br />
        return x.half()</p>
<pre><code>def right_inverse(self, x):
    return x.float()
</code></pre>
<p>def parametrize(model: nn.Module):<br />
    mods = list(model.modules())<br />
    for mod in mods:<br />
        params = list(mod._parameters.items())<br />
        for name, p in params:<br />
            if p is not None:<br />
                torch.nn.utils.parametrize.register_parametrization(mod, name, Parametrization(), unsafe=True)</p>
<p>x = torch.randn([1, 128], device="cuda", dtype=torch.half)</p>
<h1>2 options:</h1>
<p>print(f"torch=={torch.<strong>version</strong>}")<br />
for USE_PARAMETRIZATION in [True, False, True, False]:<br />
    torch._dynamo.reset()<br />
    m = Module().cuda()<br />
    if USE_PARAMETRIZATION:<br />
        parametrize(m)<br />
    else:<br />
        m.half()<br />
    m(x)</p>
<pre><code>m.compile()

t = time.time()
m(x)
dt = time.time() - t
print(f"parametrize={USE_PARAMETRIZATION}: {int(1000 * dt)}ms")
</code></pre>
<p>```</p>
<p>Bisected to find the following commits responsible: https://github.com/pytorch/pytorch/pull/121041 (~200ms -&gt; ~4500ms locally) and https://github.com/pytorch/pytorch/pull/123804 (~4500ms -&gt; ~20000ms locally).</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Wed, 01 May 2024 10:00:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125314</guid>
    </item>
    <item>
      <title>[Inductor max autotune] Make autotune_select_algorithm more robust</title>
      <link>https://github.com/pytorch/pytorch/pull/124928</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124928<br />
* #125406</p>
<p>This diff makes sure that a custom exception is thrown when no valid<br />
choices remain during autotuning. This allows to gracefully fall back<br />
to a default choice, even if that default choice has not been passed to<br />
autotune_select_algorithm.</p>
<p>Additionally, this diff handles RuntimeErrors during autotuning gracefully, e.g. the corresponding choice is ignored but it does not lead to the compilation failure of the entire model if a problematic choice is encountered during autotuning.<br />
( An error is being logged, though).</p>
<p>Test Plan:<br />
CI</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 03:14:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124928</guid>
    </item>
  </channel>
</rss>
