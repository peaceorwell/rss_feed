<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>`torch.histc` does not work with `torch.compile(dynamic=True)`</title>
      <link>https://github.com/pytorch/pytorch/issues/124512</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>torch.histc</code> does not work with <code>torch.compile(dynamic=True)</code></p>
<p>Using <code>torch.histc</code> with <code>torch.compile(dynamic=True)</code> raises a <code>NotImplementedError</code>. See the minified repro. The code works fine without <code>dynamic=True</code>.</p>
<p>Following the link in the error message, it seems like the <code>torch.ops.aten.histc.default</code> op does not support dynamic shapes. According to <code>opcheck</code>:</p>
<p>```<br />
import torch<br />
from torch.testing._internal.optests import opcheck</p>
<p>inputs = torch.rand(3, device="cuda")<br />
opcheck(torch.ops.aten.histc.default, args=(inputs,), kwargs={"bins": 4, "min": 0, "max": 1})<br />
```</p>
<p>Error message:</p>
<p><code>...
OpCheckError: opcheck(op, ...): test_aot_dispatch_dynamic failed with aten.histc.default
...</code></p>
<p>(+ plus the same stack trace as before)</p>
<p>Note: this is also the case for <code>torch.ops.aten.histc.out</code>.</p>
<h3>Error logs</h3>
<h2>```</h2>
<p>NotImplementedError                       Traceback (most recent call last)<br />
File /scratch/pytorch/torch/_subclasses/fake_tensor.py:1432, in FakeTensorMode._dispatch_impl(self, func, types, args, kwargs)<br />
   1431     with in_kernel_invocation_manager(self):<br />
-&gt; 1432         r = func(<em>args, </em>*kwargs)<br />
   1433 except NotImplementedError as not_implemented_error:</p>
<p>File /scratch/pytorch/torch/<em>ops.py:629, in OpOverload.__call__(self</em>, <em>args, </em><em>kwargs)<br />
    626 def <strong>call</strong>(self_, </em>args, <strong>kwargs):  # noqa: B902<br />
    627     # use <code>self_</code> to avoid naming collide with aten ops arguments that<br />
    628     # are named "self". This way, all the aten ops can be called by kwargs.<br />
--&gt; 629     return self_._op(*args, </strong>kwargs)</p>
<p>NotImplementedError: aten::histc: attempted to run this operator with Meta tensors, but there was no fake impl or Meta kernel registered. You may have run into this message while using an operator with PT2 compilation APIs (torch.compile/torch.export); in order to use this operator with those APIs you'll need to add a fake impl.Please see the following doc for next steps: https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit</p>
<p>During handling of the above exception, another exception occurred:</p>
<p>UnsupportedOperatorException              Traceback (most recent call last)<br />
File /scratch/pytorch/torch/_dynamo/utils.py:1865, in run_node(tracer, node, args, kwargs, nnmodule)<br />
   1864 if op == "call_function":<br />
-&gt; 1865     return node.target(<em>args, </em>*kwargs)<br />
   1866 elif op == "call_method":</p>
<p>File /scratch/pytorch/torch/utils/_stats.py:20, in count.<locals>.wrapper(<em>args, </em><em>kwargs)<br />
     19 simple_call_counter[fn.<strong>qualname</strong>] = simple_call_counter[fn.<strong>qualname</strong>] + 1<br />
---&gt; 20 return fn(</em>args, **kwargs)</p>
<p>File /scratch/pytorch/torch/_subclasses/fake_tensor.py:870, in FakeTensorMode.<strong>torch_dispatch</strong>(self, func, types, args, kwargs)<br />
    869 try:<br />
--&gt; 870     return self.dispatch(func, types, args, kwargs)<br />
    871 except TypeError:</p>
<p>File /scratch/pytorch/torch/_subclasses/fake_tensor.py:1215, in FakeTensorMode.dispatch(self, func, types, args, kwargs)<br />
   1214 if self.cache_enabled:<br />
-&gt; 1215     return self._cached_dispatch_impl(func, types, args, kwargs)<br />
   1216 else:</p>
<p>File /scratch/pytorch/torch/_subclasses/fake_tensor.py:948, in FakeTensorMode._cached_dispatch_impl(self, func, types, args, kwargs)<br />
    947 if output is unassigned:<br />
--&gt; 948     output = self._dispatch_impl(func, types, args, kwargs)<br />
    950 return output</p>
<p>File /scratch/pytorch/torch/_subclasses/fake_tensor.py:1434, in FakeTensorMode._dispatch_impl(self, func, types, args, kwargs)<br />
   1433 except NotImplementedError as not_implemented_error:<br />
-&gt; 1434     return maybe_run_unsafe_fallback(not_implemented_error)<br />
   1436 return self.wrap_meta_outputs_with_default_device_logic(<br />
   1437     r, func, flat_args, device=kwargs.get("device")<br />
   1438 )</p>
<p>File /scratch/pytorch/torch/_subclasses/fake_tensor.py:1417, in FakeTensorMode._dispatch_impl.<locals>.maybe_run_unsafe_fallback(error)<br />
   1416 if has_symbolic_sizes or not self.can_run_unsafe_fallback(func):<br />
-&gt; 1417     raise UnsupportedOperatorException(func)<br />
   1418 if error is None:</p>
<p>UnsupportedOperatorException: aten.histc.default</p>
<p>The above exception was the direct cause of the following exception:</p>
<p>RuntimeError                              Traceback (most recent call last)<br />
File /scratch/pytorch/torch/_dynamo/utils.py:1747, in get_fake_value(node, tx, allow_non_graph_fake)<br />
   1746     with tx.fake_mode, enable_python_dispatcher():<br />
-&gt; 1747         ret_val = wrap_fake_exception(<br />
   1748             lambda: run_node(tx.output, node, args, kwargs, nnmodule)<br />
   1749         )<br />
   1750 except Unsupported:</p>
<p>File /scratch/pytorch/torch/_dynamo/utils.py:1262, in wrap_fake_exception(fn)<br />
   1261 try:<br />
-&gt; 1262     return fn()<br />
   1263 except UnsupportedFakeTensorException as e:</p>
<p>File /scratch/pytorch/torch/_dynamo/utils.py:1748, in get_fake_value.<locals>.<lambda>()<br />
   1746     with tx.fake_mode, enable_python_dispatcher():<br />
   1747         ret_val = wrap_fake_exception(<br />
-&gt; 1748             lambda: run_node(tx.output, node, args, kwargs, nnmodule)<br />
   1749         )<br />
   1750 except Unsupported:</p>
<p>File /scratch/pytorch/torch/_dynamo/utils.py:1883, in run_node(tracer, node, args, kwargs, nnmodule)<br />
   1882     except Exception as e:<br />
-&gt; 1883         raise RuntimeError(make_error_message(e)).with_traceback(<br />
   1884             e.<strong>traceback</strong><br />
   1885         ) from e<br />
   1887 raise AssertionError(op)</p>
<p>File /scratch/pytorch/torch/_dynamo/utils.py:1865, in run_node(tracer, node, args, kwargs, nnmodule)<br />
   1864 if op == "call_function":<br />
-&gt; 1865     return node.target(<em>args, </em>*kwargs)<br />
   1866 elif op == "call_method":</p>
<p>File /scratch/pytorch/torch/utils/_stats.py:20, in count.<locals>.wrapper(<em>args, </em><em>kwargs)<br />
     19 simple_call_counter[fn.<strong>qualname</strong>] = simple_call_counter[fn.<strong>qualname</strong>] + 1<br />
---&gt; 20 return fn(</em>args, **kwargs)</p>
<p>File /scratch/pytorch/torch/_subclasses/fake_tensor.py:870, in FakeTensorMode.<strong>torch_dispatch</strong>(self, func, types, args, kwargs)<br />
    869 try:<br />
--&gt; 870     return self.dispatch(func, types, args, kwargs)<br />
    871 except TypeError:</p>
<p>File /scratch/pytorch/torch/_subclasses/fake_tensor.py:1215, in FakeTensorMode.dispatch(self, func, types, args, kwargs)<br />
   1214 if self.cache_enabled:<br />
-&gt; 1215     return self._cached_dispatch_impl(func, types, args, kwargs)<br />
   1216 else:</p>
<p>File /scratch/pytorch/torch/_subclasses/fake_tensor.py:948, in FakeTensorMode._cached_dispatch_impl(self, func, types, args, kwargs)<br />
    947 if output is unassigned:<br />
--&gt; 948     output = self._dispatch_impl(func, types, args, kwargs)<br />
    950 return output</p>
<p>File /scratch/pytorch/torch/_subclasses/fake_tensor.py:1434, in FakeTensorMode._dispatch_impl(self, func, types, args, kwargs)<br />
   1433 except NotImplementedError as not_implemented_error:<br />
-&gt; 1434     return maybe_run_unsafe_fallback(not_implemented_error)<br />
   1436 return self.wrap_meta_outputs_with_default_device_logic(<br />
   1437     r, func, flat_args, device=kwargs.get("device")<br />
   1438 )</p>
<p>File /scratch/pytorch/torch/_subclasses/fake_tensor.py:1417, in FakeTensorMode._dispatch_impl.<locals>.maybe_run_unsafe_fallback(error)<br />
   1416 if has_symbolic_sizes or not self.can_run_unsafe_fallback(func):<br />
-&gt; 1417     raise UnsupportedOperatorException(func)<br />
   1418 if error is None:</p>
<p>RuntimeError: Failed running call_function <built-in method histc of type object at 0x7f7546500a20>(<em>(FakeTensor(..., device='cuda:0', size=(s0,)),), </em>*{'bins': 4, 'min': 0, 'max': 1}):<br />
aten.histc.default</p>
<p>During handling of the above exception, another exception occurred:</p>
<p>Unsupported                               Traceback (most recent call last)<br />
      3 inputs = torch.rand(3, device="cuda")<br />
      4 histc_opt = torch.compile(torch.histc, dynamic=True, fullgraph=True)<br />
----&gt; 5 histc_opt(inputs, bins=4, min=0, max=1)</p>
<p>File /scratch/pytorch/torch/_dynamo/eval_frame.py:403, in _TorchDynamoContext.<strong>call</strong>.<locals>._fn(<em>args, </em><em>kwargs)<br />
    401 prior = set_eval_frame(callback)<br />
    402 try:<br />
--&gt; 403     return fn(</em>args, **kwargs)<br />
    404 finally:<br />
    405     set_eval_frame(prior)</p>
<p>File /scratch/pytorch/torch/_dynamo/convert_frame.py:977, in catch_errors_wrapper.<locals>.catch_errors(frame, cache_entry, frame_state)<br />
    973             return hijacked_callback(frame, cache_entry, hooks, frame_state)<br />
    975 with compile_lock, _disable_current_modes():<br />
    976     # skip=1: skip this frame<br />
--&gt; 977     return callback(frame, cache_entry, hooks, frame_state, skip=1)</p>
<p>File /scratch/pytorch/torch/_dynamo/convert_frame.py:411, in convert_frame_assert.<locals>._convert_frame_assert(frame, cache_entry, hooks, frame_state, skip)<br />
    397 compile_id = CompileId(frame_id, frame_compile_id)<br />
    399 signpost_event(<br />
    400     "dynamo",<br />
    401     "_convert_frame_assert._compile",<br />
   (...)<br />
    408     },<br />
    409 )<br />
--&gt; 411 return _compile(<br />
    412     frame.f_code,<br />
    413     frame.f_globals,<br />
    414     frame.f_locals,<br />
    415     frame.f_builtins,<br />
    416     compiler_fn,<br />
    417     one_graph,<br />
    418     export,<br />
    419     export_constraints,<br />
    420     hooks,<br />
    421     cache_size,<br />
    422     frame,<br />
    423     frame_state=frame_state,<br />
    424     compile_id=compile_id,<br />
    425     skip=skip + 1,<br />
    426 )</p>
<p>File /scratch/pytorch/torch/_utils_internal.py:70, in compiletime_sl_profile_meta.<locals>.compiletime_sl_profile_inner.<locals>.wrapper_function(<em>args, </em><em>kwargs)<br />
     68 @functools.wraps(function)<br />
     69 def wrapper_function(</em>args, <strong>kwargs):<br />
---&gt; 70     return function(*args, </strong>kwargs)</p>
<p>File /scratch/.conda/envs/pytorch/lib/python3.11/contextlib.py:81, in ContextDecorator.<strong>call</strong>.<locals>.inner(<em>args, </em><em>kwds)<br />
     78 @wraps(func)<br />
     79 def inner(</em>args, <strong>kwds):<br />
     80     with self._recreate_cm():<br />
---&gt; 81         return func(*args, </strong>kwds)</p>
<p>File /scratch/pytorch/torch/_dynamo/convert_frame.py:700, in _compile(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)<br />
    698 fail_user_frame_lineno: Optional[int] = None<br />
    699 try:<br />
--&gt; 700     guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
    701     return guarded_code<br />
    702 except (<br />
    703     Unsupported,<br />
    704     TorchRuntimeError,<br />
   (...)<br />
    711     BisectValidationException,<br />
    712 ) as e:</p>
<p>File /scratch/pytorch/torch/_dynamo/utils.py:267, in dynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper(<em>args, </em><em>kwargs)<br />
    265 with torch.profiler.record_function(f"{key} (dynamo_timed)"):<br />
    266     t0 = time.time()<br />
--&gt; 267     r = func(</em>args, **kwargs)<br />
    268     time_spent = time.time() - t0<br />
    269 compilation_time_metrics[key].append(time_spent)</p>
<p>File /scratch/pytorch/torch/_dynamo/convert_frame.py:568, in _compile.<locals>.compile_inner(code, one_graph, hooks, transform)<br />
    566 CompileContext.get().attempt = attempt<br />
    567 try:<br />
--&gt; 568     out_code = transform_code_object(code, transform)<br />
    569     break<br />
    570 except exc.RestartAnalysis as e:</p>
<p>File /scratch/pytorch/torch/_dynamo/bytecode_transformation.py:1116, in transform_code_object(code, transformations, safe)<br />
   1113 instructions = cleaned_instructions(code, safe)<br />
   1114 propagate_line_nums(instructions)<br />
-&gt; 1116 transformations(instructions, code_options)<br />
   1117 return clean_and_assemble_instructions(instructions, keys, code_options)[1]</p>
<p>File /scratch/pytorch/torch/_dynamo/convert_frame.py:173, in preserve_global_state.<locals>._fn(<em>args, </em><em>kwargs)<br />
    171 cleanup = setup_compile_debug()<br />
    172 try:<br />
--&gt; 173     return fn(</em>args, **kwargs)<br />
    174 finally:<br />
    175     cleanup.close()</p>
<p>File /scratch/pytorch/torch/_dynamo/convert_frame.py:515, in _compile.<locals>.transform(instructions, code_options)<br />
    513 try:<br />
    514     with tracing(tracer.output.tracing_context), tracer.set_current_tx():<br />
--&gt; 515         tracer.run()<br />
    516 except exc.UnspecializeRestartAnalysis:<br />
    517     speculation_log.clear()</p>
<p>File /scratch/pytorch/torch/_dynamo/symbolic_convert.py:2237, in InstructionTranslator.run(self)<br />
   2236 def run(self):<br />
-&gt; 2237     super().run()</p>
<p>File /scratch/pytorch/torch/_dynamo/symbolic_convert.py:875, in InstructionTranslatorBase.run(self)<br />
    873 try:<br />
    874     self.output.push_tx(self)<br />
--&gt; 875     while self.step():<br />
    876         pass<br />
    877 except BackendCompilerFailed:</p>
<p>File /scratch/pytorch/torch/_dynamo/symbolic_convert.py:790, in InstructionTranslatorBase.step(self)<br />
    787 self.update_block_stack(inst)<br />
    789 try:<br />
--&gt; 790     self.dispatch_table<a href="self, inst">inst.opcode</a><br />
    791     return not self.output.should_exit<br />
    792 except ReturnValueOp:</p>
<p>File /scratch/pytorch/torch/_dynamo/symbolic_convert.py:492, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    490     return handle_graph_break(self, inst, speculation.reason)<br />
    491 try:<br />
--&gt; 492     return inner_fn(self, inst)<br />
    493 except Unsupported as excp:<br />
    494     if self.generic_context_manager_depth &gt; 0:<br />
    495         # We don't support graph break under GenericContextWrappingVariable,<br />
    496         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /scratch/pytorch/torch/_dynamo/symbolic_convert.py:1301, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1299 # Map to a dictionary of str -&gt; VariableTracker<br />
   1300 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1301 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /scratch/pytorch/torch/_dynamo/symbolic_convert.py:730, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    728 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    729     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 730 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /scratch/pytorch/torch/_dynamo/variables/lazy.py:131, in _create_realize_and_forward.<locals>.realize_and_forward(self, <em>args, </em><em>kwargs)<br />
    129 @functools.wraps(getattr(VariableTracker, name))<br />
    130 def realize_and_forward(self, </em>args, <strong>kwargs):<br />
--&gt; 131     return getattr(self.realize(), name)(*args, </strong>kwargs)</p>
<p>File /scratch/pytorch/torch/<em>dynamo/variables/torch.py:754, in TorchInGraphFunctionVariable.call_function(self, tx, args, kwargs)<br />
    749                 if getattr(self.value, "<strong>module</strong>", None) == "math" and hasattr(<br />
    750                     torch, torch_sym_op<br />
    751                 ):<br />
    752                     fn</em> = getattr(torch, torch_sym_op)<br />
--&gt; 754             tensor_variable = wrap_fx_proxy(<br />
    755                 tx=tx,<br />
    756                 proxy=tx.output.create_proxy(<br />
    757                     "call_function",<br />
    758                     fn_,<br />
    759                     *proxy_args_kwargs(args, kwargs),<br />
    760                 ),<br />
    761             )<br />
    763             if (<br />
    764                 isinstance(tensor_variable, TensorVariable)<br />
    765                 and "requires_grad" in kwargs<br />
    766                 and kwargs["requires_grad"].as_python_constant()<br />
    767             ):<br />
    768                 unimplemented(<br />
    769                     """factory functions that return tensors that require grad are not supported.<br />
    770 Either create the tensor outside the compiled region, or do not set the tensor to require_grad"""<br />
    771                 )</p>
<p>File /scratch/pytorch/torch/_dynamo/variables/builder.py:1435, in wrap_fx_proxy(tx, proxy, example_value, subclass_type, <strong>options)<br />
   1427 kwargs = {<br />
   1428     "tx": tx,<br />
   1429     "proxy": proxy,<br />
   (...)<br />
   1432     </strong>options,<br />
   1433 }<br />
   1434 if subclass_type is None:<br />
-&gt; 1435     return wrap_fx_proxy_cls(target_cls=TensorVariable, <strong>kwargs)<br />
   1436 else:<br />
   1437     result = wrap_fx_proxy_cls(target_cls=TensorWithTFOverrideVariable, </strong>kwargs)</p>
<p>File /scratch/pytorch/torch/_dynamo/variables/builder.py:1520, in wrap_fx_proxy_cls(target_cls, tx, proxy, example_value, subclass_type, **options)<br />
   1516 # with preserve_rng_state():<br />
   1517 if example_value is None:<br />
   1518     # only allow_non_graph_fake in this instance because we handle the non-fake<br />
   1519     # cases properly below.<br />
-&gt; 1520     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)<br />
   1522 # Handle recursive calls here<br />
   1523 elif maybe_get_fake_mode(example_value) is tx.fake_mode:</p>
<p>File /scratch/pytorch/torch/_dynamo/utils.py:1794, in get_fake_value(node, tx, allow_non_graph_fake)<br />
   1788             module, ctx = maybe_pystub<br />
   1789             import_suggestion = (<br />
   1790                 f"It's possible that the support was implemented in "<br />
   1791                 f"module <code>{module}</code> and you may need to <code>import {module}</code>"<br />
   1792                 f"({ctx}), otherwise "<br />
   1793             )<br />
-&gt; 1794     unimplemented(<br />
   1795         f"unsupported operator: {cause.func} ({import_suggestion}see "<br />
   1796         "https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.64r4npvq0w0"<br />
   1797         " for how to fix)"<br />
   1798     )<br />
   1799 elif isinstance(<br />
   1800     cause, torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode<br />
   1801 ):<br />
   1802     raise UserError(  # noqa: TRY200<br />
   1803         UserErrorType.CONSTRAINT_VIOLATION,<br />
   1804         "Tried to use data-dependent value in the subsequent computation. "<br />
   (...)<br />
   1808         case_name="constrain_as_size_example",<br />
   1809     )</p>
<p>File /scratch/pytorch/torch/_dynamo/exc.py:212, in unimplemented(msg, from_exc)<br />
    210 if from_exc is not _NOTHING:<br />
    211     raise Unsupported(msg) from from_exc<br />
--&gt; 212 raise Unsupported(msg)</p>
<p>Unsupported: unsupported operator: aten.histc.default (see https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.64r4npvq0w0 for how to fix)</p>
<p>from user code:<br />
   File "/scratch/pytorch/torch/_dynamo/external_utils.py", line 36, in inner<br />
    return fn(<em>args, </em>*kwargs)</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>
<h3>Minified repro</h3>
<p>```<br />
import torch</p>
<p>inputs = torch.rand(3, device="cuda")<br />
histc_opt = torch.compile(torch.histc, dynamic=True, fullgraph=True)<br />
histc_opt(inputs, bins=4, min=0, max=1)<br />
```</p>
<h3>Versions</h3>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] optree==0.11.0<br />
[pip3] torch==2.4.0a0+git87f651c<br />
[pip3] triton==3.0.0</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 10:43:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124512</guid>
    </item>
    <item>
      <title>DISABLED test_no_requires_grad_inplace (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124509</link>
      <description><![CDATA[<p>Platforms: linux, rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_no_requires_grad_inplace&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24033262568">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_no_requires_grad_inplace</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 10:39:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124509</guid>
    </item>
    <item>
      <title>[feature request] allow torch.compile calls with compiler options, but without modifying global dynamo options</title>
      <link>https://github.com/pytorch/pytorch/issues/124505</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>Originally discussed in: https://github.com/pytorch/pytorch/issues/106614</p>
<p>As I understood, any torch.compile call with passed options will modify global compiler options (and thus require <code>torch._dynamo.reset()</code> or sth like this). I suggest to introduce a "local mode" which applies passed options but only to a given function. Or maybe also introduce a "compile options context" - this would be a "semi-local" mode</p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>]]></description>
      <pubDate>Fri, 19 Apr 2024 09:42:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124505</guid>
    </item>
    <item>
      <title>inductor::_reinterpret_tensor() Expected a value of type 'List[int]' for argument 'size' but instead found type 'tuple'</title>
      <link>https://github.com/pytorch/pytorch/issues/124498</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When running the Dynamo hf_Whisper benchmark with CompiledDDP (CompiledAutograd + DDP Python reducer), the Inductor generates the code that cannot be run. The error is <code>inductor::_reinterpret_tensor() Expected a value of type 'List[int]' for argument 'size' but instead found type 'tuple'</code> -- the generated code contains float which does not accept by <code>inductor::_reinterpret_tensor()</code>. The gradients of this model contain dynamic shape which may be related to the error. We are unable to reproduce this error with an unittest or a smaller  model.</p>
<p>To reproduce this error, checkout https://github.com/pytorch/pytorch/pull/121315 and use the following comment:<br />
<code>python benchmarks/dynamo/torchbench.py --performance --cold-start-latency --training  --backend inductor --disable-cudagraphs --device cuda   --ddp --multiprocess --optimize-ddp-mode="python_reducer" --only hf_Whisper --compiled-autograd</code></p>
<h3>Error logs</h3>
<p><code>File "/data/users/chienchin/mywork/pytorch/torch/_dynamo/external_utils.py", line 36, in inner                                  16:08:50 [33/29551]    return fn(*args, **kwargs)
  File "/data/users/chienchin/mywork/pytorch/torch/_dynamo/utils.py", line 2689, in wrapper
    return compiled_fn(flat_args)
  File "/data/users/chienchin/mywork/pytorch/torch/_dynamo/eval_frame.py", line 410, in _fn
    return fn(*args, **kwargs)
  File "/data/users/chienchin/mywork/pytorch/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/data/users/chienchin/mywork/pytorch/torch/_functorch/aot_autograd.py", line 974, in boxed_forward
    return compiled_fn(flat_args)
  File "/data/users/chienchin/mywork/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 130, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/data/users/chienchin/mywork/pytorch/torch/_functorch/_aot_autograd/utils.py", line 116, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/data/users/chienchin/mywork/pytorch/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 181, in rng_functionalization_wrapper
    return compiled_fw(args)
  File "/data/users/chienchin/mywork/pytorch/torch/_inductor/codecache.py", line 954, in __call__
    return self.current_callable(inputs)
  File "/data/users/chienchin/mywork/pytorch/torch/_inductor/compile_fx.py", line 853, in run
    return model(new_inputs)
  File "/tmp/tmppakyeush/im/cimukl26obc7rzhlq57a7n4zu5vd3viooh7sdulbbbrvtqlskdhl.py", line 3342, in call                          16:08:50 [12/29551]    return (reinterpret_tensor(buf139, (384, ), (1, ), 0), reinterpret_tensor(buf139, (384, ), (1, ), 384), reinterpret_tensor(buf139, (384, ), (1, ), 768), reinterpret_tensor(buf139, (384, ), (1, ), 1152), reinterpret_tensor(buf139, (384, ), (1, ), 1536), reinterpret_tensor(buf139, (384, ), (1, ), 1920), reinterpret_tensor(buf139, (384, ), (1, ), 2304), reinterpret_tensor(buf139, (384, ), (1, ), 2688), reinterpret_tensor(buf139, (384, ), (1,
), 3072), reinterpret_tensor(buf139, (384, ), (1, ), 3456), reinterpret_tensor(buf139, (384, 384, 3), (1152, 3, 1), 3840), reinterpret_tensor(buf350, (384, ), (1, ), 0), reinterpret_tensor(buf350, (384, 80, 3), (240, 3, 1), 384), reinterpret_tensor(buf350, (384, ), (1, ), 92544), reinterpret_tensor(buf350, (384, ), (1, ), 92928), reinterpret_tensor(buf350, (384, 1536), (1536, 1), 93312), reinterpret_tensor(buf350, (1536, ), (1, ), 683136), reinterpret_tensor(buf350, (1536, 384), (384, 1), 684672), reinterpret_tensor(buf350, (384, ), (1, ), 1274496), reinterpret_tensor(buf350, (384, ), (1,
), 1274880), reinterpret_tensor(buf350, (384, 384), (384, 1), 1275264), reinterpret_tensor(buf350, (384, ), (1, ), 1422720), reinterpret_tensor(buf350, (384.000000000000, 384), (384, 1), 1423104.00000000), reinterpret_tensor(buf350, (384.000000000000, 384), (384, 1), 1570560.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 1718016.00000000), reinterpret_tensor(buf350, (384.000000000000, 384), (384, 1), 1718400.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 1865856.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 1866240.00000000), reinterpret_tensor(buf350, (384, 1536), (1536, 1), 1866624.00000000), reinterpret_tensor(buf350, (1536, ), (1, ), 2456448.00000000), reinterpret_tensor(buf350, (1536, 384), (384, 1), 2457984.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 3047808.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 3048192.00000000), reinterpret_tensor(buf350, (384, 384), (384, 1), 3048576.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 3196032.00000000), reinterpret_tensor(buf350, (384.000000000000, 384), (384, 1), 3196416.00000000), reinterpret_tensor(buf350, (384.000000000000, 384), (384, 1), 3343872.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 3491328.00000000), reinterpret_tensor(buf350, (384.000000000000, 384), (384, 1), 3491712.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 3639168.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 3639552.00000000), reinterpret_tensor(buf350, (384, 1536), (1536, 1),
3639936.00000000), reinterpret_tensor(buf350, (1536, ), (1, ), 4229760.00000000), reinterpret_tensor(buf350, (1536, 384), (384, 1), 4231296.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 4821120.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 4821504.00000000), reinterpret_tensor(buf350, (384, 384), (384, 1), 4821888.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 4969344.00000000), reinterpret_tensor(buf350, (384.000000000000, 384), (384, 1), 4969728.00000000), reinterpret_tensor(buf350, (384.000000000000, 384), (384, 1), 5117184.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 5264640.00000000), reinterpret_tensor(buf350, (384.000000000000, 384), (384, 1), 5265024.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 5412480.00000000), reinterpret_tensor(buf350, (384, ), (1, ), 5412864.00000000), reinterpret_tensor(buf350, (384, 1536), (1536, 1), 5413248.00000000), reinterpret_tensor(buf350, (1536, ), (1, ), 6003072.00000000), reinterpret_tensor(buf350, (1536, 384), (384, 1), 6004608.00000000), reinterpret_tensor(buf412, (384, ), (1, ), 0), reinterpret_tensor(buf412, (384, ), (1, ), 384), reinterpret_tensor(buf412, (384, 384), (384, 1), 768), reinterpret_tensor(buf412, (384, ), (1, ), 148224), reinterpret_tensor(buf412, (384.000000000000, 384), (384, 1), 148608.000000000), reinterpret_tensor(buf412, (384.000000000000, 384), (384, 1), 296064.000000000), reinterpret_tensor(buf412, (384, ), (1, ), 443520.000000000), reinterpret_tensor(buf412,
(384.000000000000, 384), (384, 1), 443904.000000000), reinterpret_tensor(buf412, (384, ), (1, ), 591360.000000000), reinterpret_tensor(buf412, (2, ), (1, ), 591744.000000000), reinterpret_tensor(buf412, (2, 256), (256, 1), 591746.000000000), reinterpret_tensor(buf412, (256, ), (1, ), 592258.000000000), reinterpret_tensor(buf412, (256, 384), (384, 1), 592514.000000000), )
  File "/data/users/chienchin/mywork/pytorch/torch/_ops.py", line 870, in __call__
    return self_._op(*args, **(kwargs or {}))
RuntimeError: inductor::_reinterpret_tensor() Expected a value of type 'List[int]' for argument 'size' but instead found type 'tuple'.
Position: 1
Value: (384.0, 384)
Declaration: inductor::_reinterpret_tensor(Tensor self, int[] size, int[] stride, int offset_increment=0) -&gt; Tensor
Cast error details: Unable to cast Python instance of type &lt;class 'tuple'&gt; to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)</code></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>https://github.com/pytorch/pytorch/pull/121315</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 08:20:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124498</guid>
    </item>
    <item>
      <title>torch.compile does not work since 2.2.1 on MacOS for some models</title>
      <link>https://github.com/pytorch/pytorch/issues/124497</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>The execution hangs when first calling the model to warm-up. <em>After</em> will never be printed. This is on an Apple M3 Max. Smaller models work sometimes.</p>
<p>```python<br />
import torch<br />
from transformers import AutoModelForImageClassification</p>
<p>neural_network = AutoModelForImageClassification.from_pretrained("microsoft/resnet-50")<br />
input_data = torch.randn(1, 3, 228, 228)<br />
print(neural_network(input_data))<br />
neural_network_c = torch.compile(neural_network, backend="inductor")<br />
print("Before")<br />
neural_network_c(input_data)<br />
print("After")<br />
```</p>
<p>The last version that worked was torch 2.2.0. </p>
<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.2.1<br />
Is debug build: False<br />
CUDA used to build PyTorch: None<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: macOS 14.4.1 (arm64)<br />
GCC version: Could not collect<br />
Clang version: 15.0.0 (clang-1500.3.9.4)<br />
CMake version: version 3.28.4<br />
Libc version: N/A</p>
<p>Python version: 3.11.8 (main, Feb  6 2024, 21:21:21) [Clang 15.0.0 (clang-1500.1.0.2.5)] (64-bit runtime)<br />
Python platform: macOS-14.4.1-arm64-arm-64bit<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Apple M3 Max</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] onnx==1.16.0<br />
[pip3] torch==2.2.1<br />
[conda] Could not collect<br />
```</p>
<p>Thanks for your help!</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @malfet @albanD @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 08:14:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124497</guid>
    </item>
    <item>
      <title>DISABLED test_multi_backward (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124491</link>
      <description><![CDATA[<p>Platforms: linux, rocm, asan, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_multi_backward&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24025614016">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_multi_backward</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 07:40:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124491</guid>
    </item>
    <item>
      <title>fix torch.compile with triton kernels under inference_mode</title>
      <link>https://github.com/pytorch/pytorch/pull/124489</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124489<br />
* #124488</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 07:25:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124489</guid>
    </item>
    <item>
      <title>DISABLED test_mark_non_differentiable_none (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124475</link>
      <description><![CDATA[<p>Platforms: linux, rocm, asan</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_mark_non_differentiable_none&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24017927632">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 18 failures and 6 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_mark_non_differentiable_none</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 04:44:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124475</guid>
    </item>
    <item>
      <title>DISABLED test_mark_non_differentiable (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124470</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_mark_non_differentiable&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24006724359">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_mark_non_differentiable</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 01:41:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124470</guid>
    </item>
    <item>
      <title>DISABLED test_isolated_node (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124460</link>
      <description><![CDATA[<p>Platforms: linux, rocm, asan, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_isolated_node&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24005337184">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 21 failures and 7 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_isolated_node</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/inductor/test_compiled_autograd.py -1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)<br />
headers: {}</p>
<p>cc @clee2000</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 22:40:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124460</guid>
    </item>
    <item>
      <title>DISABLED test_inplace (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124446</link>
      <description><![CDATA[<p>Platforms: rocm, asan, linux, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_inplace&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23996309781">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_inplace</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 16:56:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124446</guid>
    </item>
    <item>
      <title>[don't land] Test disabling parallel compile</title>
      <link>https://github.com/pytorch/pytorch/pull/124439</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124439</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 15:27:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124439</guid>
    </item>
    <item>
      <title>Make CompiledFxGraph portable between machines</title>
      <link>https://github.com/pytorch/pytorch/pull/124438</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124438</p>
<p>As we prepare FxGraphCache to move to remote, we need to make sure there's no data that is on the disk.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D56363808">D56363808</a></p>]]></description>
      <pubDate>Thu, 18 Apr 2024 15:13:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124438</guid>
    </item>
    <item>
      <title>[inductor] for UserDefinedTritonKernels don't mark all inputs as mutating</title>
      <link>https://github.com/pytorch/pytorch/pull/124425</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124425</p>
<p>Take this example:<br />
```<br />
def _mul2(x):<br />
    y = torch.empty_like(x)<br />
    mul2_kernel<a href="in_ptr0=x, out_ptr=y,
        n_elements=x.numel(), BLOCK_SIZE=1,">(10,)</a><br />
    return y</p>
<p>def f(x):<br />
    for _ in range(4):<br />
        x = _mul2(x)<br />
    return x + 1<br />
```</p>
<p>Currently, the codegen will show up like this. Notice, how we allocate 5 buffers of the same size.<br />
```</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []</h1>
<p>buf0 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=arg0_1, out_ptr=reinterpret_tensor(buf0, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_1], Original ATen: []</h1>
<p>buf4 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf0, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf4, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_2], Original ATen: []</h1>
<p>buf8 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf4, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf8, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_3], Original ATen: []</h1>
<p>buf12 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf8, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf12, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [add], Original ATen: [aten.add]</h1>
<p>buf16 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
triton_poi_fused_add_0.run(buf12, buf16, 10, grid=grid(10), stream=stream0)...)<br />
return (buf16, )<br />
```</p>
<p>With this PR, we want to see this. Notice, how we only allocate 2 buffers this time. The other 3 buffers are re-used.<br />
```</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []</h1>
<p>buf0 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=arg0_1, out_ptr=reinterpret_tensor(buf0, (10, ), (1, ), 0), ...)<br />
del arg0_1</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_1], Original ATen: []</h1>
<p>buf2 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf0, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf2, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_2], Original ATen: []</h1>
<p>buf4 = buf0; del buf0  # reuse<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf2, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf4, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_3], Original ATen: []</h1>
<p>buf6 = buf2; del buf2  # reuse<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf4, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf6, (10, ), (1, ), 0) ...)<br />
del buf4</p>
<h1>Source Nodes: [add], Original ATen: [aten.add]</h1>
<p>buf8 = buf6; del buf6  # reuse<br />
triton_poi_fused_add_0.run(buf8, 10, grid=grid(10), stream=stream0)<br />
return (buf8, )<br />
```</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 11:54:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124425</guid>
    </item>
    <item>
      <title>Make torch._inductor.dependencies.Dep a proper class</title>
      <link>https://github.com/pytorch/pytorch/pull/124407</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124407</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 07:58:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124407</guid>
    </item>
    <item>
      <title>Reimplement unbacked symbol bindings in Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/124394</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124394<br />
* #124316<br />
* #124314<br />
* #124310<br />
* #124297<br />
* #124290<br />
* #124284<br />
* #124283</p>
<p>This PR has a lot of "draw the rest of the fucking owl" energy. Here's how to break it down.</p>
<ol>
<li><strong>torch/_inductor/graph.py</strong> - We start by tightening unbacked symbol invariants. Specifically, as we lower FX nodes, we check whether or not every unbacked_binding recorded on the FX node meta, actually ends up getting bound (according to get_unbacked_symbol_defs) in all the buffers generated by the lowering. Hopefully this invariant is self evident. This leads to a lot of failures.</li>
<li><strong>torch/_inductor/ir.py</strong> - Problem 1: There is softness in how Inductor computes defs of unbacked symbols in IR node. Previously, we tried to infer it by looking at the output sizes/strides/etc and see if new unbacked symbols popped up that we hadn't seen in the inputs. I don't know exactly what was buggy about the old code, but sometimes we would fail to notice an unbacked symbol had been bound, or rebind an unbacked symbol multiple times. Fortunately, thanks to the earlier PRs in our stack, we now have a nice list of unbacked symbol bindings from FX, so we now just store it directly on ExternKernel and use it directly to report defs. This has to be done twice: once for FallbackKernel (e.g., nonzero) and once for DynamicScalar (e.g., item) (see also <strong>torch/_inductor/lowering.py</strong>, <strong>torch/_inductor/codegen/wrapper.py</strong> and  <strong>torch/_inductor/codegen/cpp_wrapper_cpu.py</strong> for the lowering and codegen changes for item)</li>
<li><strong>process_kernel</strong> - Sidequest! It turns out that Inductor lowering can reallocate unbacked symbols. This happens specifically when we repropagate fake tensors through the operator in <code>process_kernel</code>. This repropagation process is necessary because Inductor may have changed the strides of input tensors, and it must now recompute the strides so that it can continue to appropriately plan the rest of the lowering process. This is fine: we just make sure we do the rebind unbacked + compute_unbacked_bindings dance we've been doing previously in the PR stack. But instead of putting unbacked_bindings on a new FX node, they go straight into our unbacked_bindings on the Inductor IR node.<ul>
<li><strong>codegen_unbacked_symbol_defs</strong> - Sidequest! FallbackKernel lowering is done in two steps. First, you emit the FallbackKernel buffer. Then, you emit MultiOutput buffers which actually give access to the individual outputs of FallbackKernel, which may have been multi-output. There is a design decision here: does the FallbackKernel bind the unbacked symbols, or the MultiOutput buffer? Historically, we put the binding on MultiOutput buffer, because it's more convenient: the FallbackKernel buffer is fake, in fact, it doesn't even get a name in C++ codegen. But it's kind of inconsistent with the keypath model that we've been tracking unbacked bindings with: if you have a multi-output node, you'd expect a keypath like <code>[0].size()[0]</code> representing the first output's first dimension size. That suggests that it's the FallbackKernel that should define the things. So that was my first implementation. Unfortunately, the C++ codegen is too cursed and I could not understand how to make it work in that case. So now we just unsoundly assume you cannot have multi-output data dependent output, and do the codegen in MultiOutput. There are some comments explaining exactly what we are improperly assuming.</li>
</ul>
</li>
<li><strong>_rename_unbacked_to</strong> in <strong>torch/fx/experimental/symbolic_shapes.py</strong> - Previously, when we renamed unbacked symbols, we clobbered any facts we previously knew about them. So for example, if we had a replacement <code>u0 -&gt; s0</code> but then we renamed u0 to u1, we would now setup the replacement <code>u0 -&gt; u1</code>, clobbering the old replacement. This apparently didn't matter in earlier PRs in the stack, but with Inductor now on the ball, there were some tests that indicated this was a problem. The solution is easy: if u0 had a preexisting replacement, reapply it to u1. However...<ul>
<li><strong>torch/_functorch/_aot_autograd/collect_metadata_analysis.py</strong> - When we run forward analysis, this triggers fake tensor repropagation and fresh allocations. Previously, we just cleared out the pending symbols when finished the analysis. But with the change above, this would also migrate replacements to the new symbols... which are now dead. So now we explicitly suppress generation of these symbols with <code>ignore_fresh_unbacked_symbols</code> so that no rebinding happens at all.</li>
<li><strong>torch/_dynamo/eval_frame.py</strong> - same deal; I just searched for all sites we called clear() on pending</li>
</ul>
</li>
<li>The last step is fixing the long tail of extra problems that show up, now that unbacked_bindings are load bearing into Inductor<ul>
<li><strong>torch/_dynamo/eval_frame.py</strong> - Some of the exports are making copies of nodes without repropagating fake tensors, so in this case, it is important to also copy the <code>unbacked_bindings</code> (apparently this didn't matter before without the Inductor changes)</li>
<li><strong>torch/_export/pass_base.py</strong> - I discover that this is doing fake tensor repropagation via a test suite failure. Do the same playbook as AOTAutograd: PropagateUnbackedSymInts too!  Actually, they also have implemented their own tracer as well, so do the same playbook as proxy_tensor: record unbacked_bindings on the newly traced nodes. UGH code duplication.</li>
<li><strong>torch/_subclasses/fake_tensor.py</strong>, <strong>torch/_subclasses/fake_impls.py</strong> (with call site updates at  <strong>torch/_functorch/_aot_autograd/traced_function_transforms.py</strong> and <strong>torch/fx/passes/fake_tensor_prop.py</strong>) - What's this new epoch thing? I noticed that sometimes I would be retracing, call nonzero() on a fake tensor, and not allocate a new unbacked symbol. This is actually bad, because if I don't get a new unbacked symbol, I don't know there's a binding site, and <code>unbacked_bindings</code> is now missing a binding. The reason for this is memoization: if I reuse the exact same fake tensor on my retrace, it will already have an unbacked symint memoized on it and we will short circuit allocation. Well, that's no good. So I associate the memos with a fake tensor epoch, and every time you start a new fake tensor propagation from scratch, you bump the epoch so that I clear all the memos.</li>
<li><strong>torch/_inductor/scheduler.py</strong> - I notice in unit tests that V.current_node is not always set when we call process_kernel. So I save it into the IR node and restore it when we are running <code>get_estimated_runtime</code>.</li>
<li><strong>torch/fx/experimental/symbolic_shapes.py</strong> - A few things</li>
<li><strong>rebind_unbacked</strong> (re <strong>_tensor_version</strong>). Ordinarily, when you have an unbacked SymInt, you persistently hvae it all the way to the end of the program. <code>_tensor_version</code> violates this: this generates an unbacked SymInt (for reasons I don't quite understand?) and then gets rid of it later. This triggered an assert violation. I think this op is kind of misusing unbacked SymInt, but I didn't know how to refactor it, so it gets a special case.</li>
<li><strong>rebind_unbacked</strong> (re <strong>Simplify SymBool binding</strong>). Ugh, SymBool, what a pain in the butt. I have an assert that you can only rebind unbacked symbol to another unbacked symbol. This assert fails when a boolean is involved, because the result of running keypath on the result is not <code>u1</code>, it's <code>sympy.Piecewise(... sympy.Eq(u1, 1) ...)</code>. This is actually just <code>u1</code>, but Sympy doesn't know it because it doesn't know that <code>u1</code> value range is <code>[0, 1]</code>. So we manually implement the simplification needed to get the assert to pass.</li>
<li><strong>compute_unbacked_bindings</strong> (re <strong>This is pretty fragile</strong>). There is a really funny disaster involving memoization and Inductor process kernel. Ordinarily when I retrace, if there was a memo hit in the old trace, there will be a memo hit in the new trace. However, Inductor process kernel breaks this, because it recreates fake tensor inputs to the operator call from scratch (since they might have different strides), and obviously these tensor inputs don't have the memo from the old one. I tried a little bit to try to manually transplant the memo to the new fake tensor but it seemed hopeless, so I just let the fresh symbol ride, allocating a new unbacked symbol. However, in one of our tests, we rely on knowing that the first nonzero call is equal to the second (memoized) nonzero call. The equality test looked pretty easy to discharge, so I just went ahead and added a deferred runtime assert to this effect and it worked.</li>
</ul>
</li>
</ol>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 06:23:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124394</guid>
    </item>
    <item>
      <title>torch.compile: could not reconstruct view by re-applying a ViewMeta sequence</title>
      <link>https://github.com/pytorch/pytorch/issues/124382</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I got the following error when running a <code>torch.compile</code>'ed function:<br />
<code>W0418 04:04:09.562000 140622378160704 torch/_functorch/_aot_autograd/functional_utils.py:240] [__aot_joint_graph] could not reconstruct view by re-applying a ViewMeta sequence. This error is possibly caused by dynamic shapes. Fallbacking to reconstruction using as_strided. Error message: /home/weiwen/pytorch-fork/build/aten/src/ATen/RegisterCPU.cpp:13111: SymIntArrayRef expected to contain only concrete integers</code></p>
<p>This is a regression. This error is caused by commit https://github.com/pytorch/pytorch/commit/e4c887fbf6fcc9b1864d10b3ab18294e5edebdfc. Before this commit, there is no error messages. With such errors, the program is still able to run but becomes much slower than before. In the real case, the program runs 7x slower than before.</p>
<p>Expected behavior: No error messages and it has the same behavior as before this commit.</p>
<p>Reproducer:<br />
```python<br />
import torch</p>
<p>@torch.compile(dynamic=True)<br />
def func(A: torch.Tensor, threshold=0.0):<br />
    cols = A.shape[-1]<br />
    if len(A.shape) == 3:<br />
        rows = A.shape[0] * A.shape[1]<br />
    else:<br />
        assert A.dim() == 2, f"Input tensor should be 2d or 3d but got {A.dim()}d"<br />
        rows = A.shape[0]<br />
    A = A.reshape(rows, cols)</p>
<pre><code>if threshold == 0.0:
    outlier_indices = None
    outlier_coord = None
    outlier_rows = None
    outlier_cols = None
    outlier_values = None
else:
    outlier_indices = torch.abs(A) &gt;= threshold
    outlier_coord = outlier_indices.nonzero()
    outlier_rows = outlier_coord[:, 0]
    outlier_cols = outlier_coord[:, 1]
    outlier_values = A[outlier_indices]

return outlier_indices, outlier_coord, outlier_rows, outlier_cols, outlier_values
</code></pre>
<p>print('1')<br />
A = torch.randn(2048, 2048) * 3<br />
func(A)<br />
print('2')<br />
A = torch.randn(8192, 2048) * 3<br />
func(A)<br />
print('3')<br />
A = torch.randn(2048, 8192) * 3<br />
func(A)</p>
<p>print('4')<br />
A = torch.randn(2048, 2048) * 3<br />
func(A, threshold=3)<br />
print('5')<br />
A = torch.randn(8192, 2048) * 3<br />
func(A, threshold=3)<br />
print('6')<br />
A = torch.randn(2048, 8192) * 3<br />
func(A, threshold=3)</p>
<p>for i in range(10):<br />
    print('run', i)<br />
    bs = pow(2, i)<br />
    A = torch.randn(bs, 2, 2048) * 3<br />
    func(A, threshold=3)<br />
print('ok')<br />
```<br />
Please note that this producer is a simplified version of the real case.</p>
<h3>Versions</h3>
<p>PyTorch version: 2.4.0a0+git236b0d1<br />
Is debug build: False<br />
CUDA used to build PyTorch: None<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: CentOS Stream 8 (x86_64)<br />
GCC version: (conda-forge gcc 12.3.0-3) 12.3.0<br />
Clang version: Could not collect<br />
CMake version: version 3.28.1<br />
Libc version: glibc-2.28</p>
<p>Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.16.0-x86_64-with-glibc2.28<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:        x86_64<br />
CPU op-mode(s):      32-bit, 64-bit<br />
Byte Order:          Little Endian<br />
CPU(s):              240<br />
On-line CPU(s) list: 0-239<br />
Thread(s) per core:  2<br />
Core(s) per socket:  60<br />
Socket(s):           2<br />
NUMA node(s):        2<br />
Vendor ID:           GenuineIntel<br />
CPU family:          6<br />
Model:               143<br />
Model name:          Intel(R) Xeon(R) Platinum 8490H<br />
Stepping:            8<br />
CPU MHz:             1900.000<br />
CPU max MHz:         3500.0000<br />
CPU min MHz:         800.0000<br />
BogoMIPS:            3800.00<br />
Virtualization:      VT-x<br />
L1d cache:           48K<br />
L1i cache:           32K<br />
L2 cache:            2048K<br />
L3 cache:            115200K<br />
NUMA node0 CPU(s):   0-59,120-179<br />
NUMA node1 CPU(s):   60-119,180-239<br />
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr avx512_fp16 amx_tile flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==3.8.2<br />
[pip3] flake8-bugbear==20.1.4<br />
[pip3] flake8-coding==1.3.3<br />
[pip3] flake8-comprehensions==3.3.0<br />
[pip3] flake8-executable==2.0.4<br />
[pip3] flake8-pyi==20.5.0<br />
[pip3] lion-pytorch==0.1.4<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.26.2<br />
[pip3] optree==0.10.0<br />
[pip3] torch==2.4.0a0+gite4c887f<br />
[pip3] torchao==0.1<br />
[pip3] triton==2.3.0<br />
[conda] lion-pytorch              0.1.4                    pypi_0    pypi<br />
[conda] mkl                       2023.1.0         h213fc3f_46344<br />
[conda] mkl-include               2023.2.0                 pypi_0    pypi<br />
[conda] mkl-static                2023.2.0                 pypi_0    pypi<br />
[conda] numpy                     1.26.2                   pypi_0    pypi<br />
[conda] optree                    0.10.0                   pypi_0    pypi<br />
[conda] torch                     2.4.0a0+gite4c887f           dev_0    <develop><br />
[conda] torchao                   0.1                       dev_0    <develop><br />
[conda] triton                    2.3.0                    pypi_0    pypi</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 03:15:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124382</guid>
    </item>
    <item>
      <title>DISABLED test_grad_unreachable (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124347</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_grad_unreachable&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23951026825">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_grad_unreachable</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 16:56:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124347</guid>
    </item>
    <item>
      <title>[inductor] switch assume_aligned_inputs to False</title>
      <link>https://github.com/pytorch/pytorch/pull/124336</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124336<br />
* #123319</p>
<p>In #123319, we guard some behavior behind the <code>assume_aligned_inputs</code> config option. If we set this to <code>False</code>, then the behavior added in #123319 becomes the default behavior. See the referenced PR for more details about the behavior affected.</p>
<p>Side effects:<br />
* It's possible that this will hurt performance in some scenarios. For example, if an unaligned input is used in a matmul, it might be better to perform the clone to align it first.<br />
* This will occasionally cause recompiles. Specifically: the check we perform (<code>(storage_offset * get_dtype_size(dtype)) % ALIGNMENT == 0</code>) can be guarded on if the storage_offset becomes dynamic. storage_offset becomes dynamic during automatic_dynamic_shapes after a shape or stride changes. Previously, this was increasing graph breaks in cpu inductor torchbench tests (but is fixed by more carefully guarding checks on alignment, so that we don't run them and generate guards unless actually needed).</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 15:14:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124336</guid>
    </item>
    <item>
      <title>DISABLED test_grad_to_node_inplace (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124322</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, slow, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_grad_to_node_inplace&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23939510180">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 8 workflow(s) with 24 failures and 8 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_grad_to_node_inplace</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 13:39:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124322</guid>
    </item>
    <item>
      <title>[inductor] consider pointwise nodes when deciding reduction hint</title>
      <link>https://github.com/pytorch/pytorch/pull/124131</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124131</p>
<p>In certain <strong>rare</strong> scenarios, inductor can generate a reduction kernel with really bad perf. E.g., if <br />
- the reduction kernel contains a reduction node followed by a pointwise node<br />
- And the pointwise node use a transposed layout. <br />
- the reduction node is an inner reduction<br />
- and rnumel &lt;= 1024 , </p>
<p>then inductor will generate a persistent reduction kernel and it causes really bad perf when doing tl.store for the pointwise node since we use a very skinny tile <code>(XBLOCK=1, RBLOCK=next_power_of_2(rnumel))</code> .</p>
<p>I've tried a few version of fix.<br />
- The first version is, if I found any pointwise node in a reduction kernel uses a non-contiguous dependency, we use ReductionHint.DEFAULT. This cause 8s compilation time increase for huggingface with no perf wins... The reason is ReductionHint.DEFAULT does more autotunings.<br />
- Then I changed the code to be more specific. We change the hint from INNER to DEFAULT if we are sure that the pointwise kernel can use a &gt;1 stride for the lowest dimension. Kernels meet this condition should mostly have really bad perf anyways.</p>
<p>The situation mentioned above is rare. But it's reported by internal users. I'll also run one more perf test.</p>
<p>Testing script: https://gist.github.com/shunting314/9d3389891fa43633b49b8b7564ad6d8b . Something equivalent is also added as a unit test.</p>
<p>For this specific test from user reports, we improve the mentioned reduction kernels perf by <strong>4.14x</strong> (451us -&gt; 109us)</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 17:21:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124131</guid>
    </item>
    <item>
      <title>[dynamo] Disable __call__ of user compiler returned nn modules</title>
      <link>https://github.com/pytorch/pytorch/pull/124108</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124121<br />
* #124109<br />
* <strong>-&gt;</strong> #124108<br />
* #124445</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 13:37:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124108</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Disable epilogue fusions</title>
      <link>https://github.com/pytorch/pytorch/pull/124107</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* <strong>-&gt;</strong> #124107<br />
* #121734<br />
* #123932<br />
* #123930<br />
* #121497</p>
<p>This diff disables Cutlass backend EVT epilogue fusions. It does not yet contain the removal of most of the underlying implementation. </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 13:23:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124107</guid>
    </item>
    <item>
      <title>[ignore] Experiment with thread parallel compile</title>
      <link>https://github.com/pytorch/pytorch/pull/124099</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124099</p>
<p>Summary:</p>
<p>Test Plan:</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 12:27:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124099</guid>
    </item>
    <item>
      <title>[Inductor max autotune] Make autotuning robust against very slow Kernels</title>
      <link>https://github.com/pytorch/pytorch/pull/123932</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124107<br />
* #121734<br />
* <strong>-&gt;</strong> #123932<br />
* #123930<br />
* #121497</p>
<p>If a Kernel does not return in a reasonable amount of time during autotuning, it can delay inductor compilation a lot. This change introduces soft / hard kill timeouts and a mechanism to kill Kernels being profiled in subprocesses if they take too long.</p>
<p>Correspondingly, a few new config options are introduced within _inductor/config.py - all of them with inline docs.</p>
<p>Test Plan:<br />
Existing tests within test_max_autotune.py and test_cutlass_backend.py ) cover the new codepaths.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 12 Apr 2024 03:51:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123932</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Fix tests: skipIfROCm always skips when using as class annotation</title>
      <link>https://github.com/pytorch/pytorch/pull/123930</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124107<br />
* #121734<br />
* #123932<br />
* <strong>-&gt;</strong> #123930<br />
* #121497</p>
<p>I previously added @skipIfRocm as a class annotation within test/inductor/test_cutlass_backend.py - turns out this annotation always skips if applied at class level, so I need to skip Cutlass tests on ROCm differently..</p>]]></description>
      <pubDate>Fri, 12 Apr 2024 03:30:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123930</guid>
    </item>
    <item>
      <title>[aot_inductor] Enable test_aot_inductor tests for ROCm CPU</title>
      <link>https://github.com/pytorch/pytorch/pull/123393</link>
      <description><![CDATA[<p>Fixes #ISSUE_NUMBER</p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 04 Apr 2024 14:40:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123393</guid>
    </item>
    <item>
      <title>[inductor] Specialize on unguarded alignment of example inputs</title>
      <link>https://github.com/pytorch/pytorch/pull/123319</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124336<br />
* <strong>-&gt;</strong> #123319</p>
<p>When inductor generates triton code, the triton code can either assume that the inputs given to it are aligned or unaligned. If they are aligned, triton can use more efficient instructions (like vectorized loads or tensor cores). However, if we generate "aligned" code and pass in unaligned inputs, the triton code will error out; to fix this, we clone unaligned inputs that are passed to triton kernels that expect aligned inputs. This can lead to excessive clones if we have inputs that are not expected to be aligned.</p>
<p>In this PR, we use the example input to decide whether the generated triton code should assume alignment or not. If the example input is aligned, then we will generate triton code that assumes alignment; if at runtime we receive an unaligned input, we'll make a clone. Meanwhile, if the example input is not aligned, the generated triton code will not assume inputs are aligned and we won't ever need to clone.</p>
<p>Note that the alignment of the inputs is not guarded on; we found that adding guards on tensor offsets (a) was slow in cases where we do a lot of comparisons on tensor offsets, and (b) led to a lot of recompilations.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 18:06:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123319</guid>
    </item>
    <item>
      <title>[inductor][cpu]adv_inception_v3, gluon_inception_v3 and inception_v3 AMP performance regression</title>
      <link>https://github.com/pytorch/pytorch/issues/122393</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>AMP static shape performance regression in 2024-03-18</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>adv_inception_v3</td>
      <td>multiple</td>
      <td>128</td>
      <td>3.695396</td>
      <td>0.087057353</td>
      <td>0.321711394046788</td>
      <td>30.958788</td>
      <td>128</td>
      <td>4.924577</td>
      <td>0.065420102</td>
      <td>0.322166329646854</td>
      <td>33.211353</td>
      <td>0.75</td>
      <td>1.0</td>
      <td>0.75</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>multiple</td>
      <td>128</td>
      <td>3.700601</td>
      <td>0.08712846399999999</td>
      <td>0.3224276810068639</td>
      <td>30.544114</td>
      <td>128</td>
      <td>4.877504</td>
      <td>0.06528794</td>
      <td>0.31844218850176004</td>
      <td>32.577501</td>
      <td>0.76</td>
      <td>0.99</td>
      <td>0.75</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>inception_v3</td>
      <td>multiple</td>
      <td>128</td>
      <td>3.7809</td>
      <td>0.086533052</td>
      <td>0.3271728163068</td>
      <td>30.467435</td>
      <td>128</td>
      <td>4.956273</td>
      <td>0.065406192</td>
      <td>0.32417094344241604</td>
      <td>32.360837</td>
      <td>0.76</td>
      <td>0.99</td>
      <td>0.76</td>
      <td>1.06</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>adv_inception_v3</td>
      <td>single</td>
      <td>1</td>
      <td>4.391231</td>
      <td>0.020491818</td>
      <td>0.089984306447958</td>
      <td>27.533994</td>
      <td>1</td>
      <td>6.074862</td>
      <td>0.014556469</td>
      <td>0.08842854038227801</td>
      <td>29.358509</td>
      <td>0.72</td>
      <td>0.98</td>
      <td>0.71</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>single</td>
      <td>1</td>
      <td>4.406967</td>
      <td>0.020548244</td>
      <td>0.090555433215948</td>
      <td>27.204902</td>
      <td>1</td>
      <td>6.18311</td>
      <td>0.014545653</td>
      <td>0.08993737252083</td>
      <td>29.311159</td>
      <td>0.71</td>
      <td>0.99</td>
      <td>0.71</td>
      <td>1.08</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>inception_v3</td>
      <td>single</td>
      <td>1</td>
      <td>4.47947</td>
      <td>0.020806962</td>
      <td>0.09320416207014</td>
      <td>27.136373</td>
      <td>1</td>
      <td>6.288863</td>
      <td>0.014541316</td>
      <td>0.091448344163708</td>
      <td>29.336782</td>
      <td>0.71</td>
      <td>0.98</td>
      <td>0.7</td>
      <td>1.08</td>
    </tr>
  </tbody>

</table>

<p>AMP dynamic shape performance regression in 2024-03-19</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
   <tr>
      <td>timm_models</td>
      <td>adv_inception_v3</td>
      <td>single</td>
      <td>1.0</td>
      <td>4.358126</td>
      <td>0.02061505</td>
      <td>0.0898429853963</td>
      <td>27.534621</td>
      <td>1</td>
      <td>6.102014</td>
      <td>0.014657954</td>
      <td>0.089443040519356</td>
      <td>29.887889</td>
      <td>0.71</td>
      <td>1.0</td>
      <td>0.71</td>
      <td>1.09</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>single</td>
      <td>1.0</td>
      <td>4.411057</td>
      <td>0.020516146</td>
      <td>0.09049788942632199</td>
      <td>27.462184</td>
      <td>1</td>
      <td>6.181562</td>
      <td>0.014640228</td>
      <td>0.090499477076136</td>
      <td>29.459571</td>
      <td>0.71</td>
      <td>1.0</td>
      <td>0.71</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>inception_v3</td>
      <td>single</td>
      <td>1.0</td>
      <td>4.490309</td>
      <td>0.020474836</td>
      <td>0.091938340364324</td>
      <td>27.457952</td>
      <td>1</td>
      <td>6.265146</td>
      <td>0.014763517</td>
      <td>0.092495589478482</td>
      <td>29.428536</td>
      <td>0.72</td>
      <td>1.01</td>
      <td>0.72</td>
      <td>1.07</td>
    </tr>
  </tbody>


</table>
<p>SW info</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>d6015d42</td>
      <td>main</td>
      <td>1ef0a39e</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>5bc7f7f97760d2d485621f9f30d0316e1f2440c6</td>
      <td>main</td>
      <td>41286f1505ffb214d386d72e4b72ebd680a4a475</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.18.0a0+0325175</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>
</table>

<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh <strong>thread</strong> inference performance timm_models <strong>model</strong> amp first static/dynamic</p>
<p>Suspected guilty commit: https://github.com/pytorch/pytorch/commit/ffabb25c489df1dc631a577c12a0c843c8b202f3<br />
<a href="https://github.com/pytorch/pytorch/files/14693268/timm_models-adv_inception_v3-inference-amp-dynamic-default-single-performance-drop_guilty_commit.log">timm_models-adv_inception_v3-inference-amp-dynamic-default-single-performance-drop_guilty_commit.log</a></p>
<p>cc @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 02:03:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122393</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Add bmm support</title>
      <link>https://github.com/pytorch/pytorch/pull/121734</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124107<br />
* <strong>-&gt;</strong> #121734<br />
* #123932<br />
* #123930<br />
* #121497</p>
<p>Add support for bmm ( batch matrix multiply ) op through the Cutlass backend.</p>
<p>Test Plan:<br />
 * CI<br />
 * Added test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 09:23:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121734</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Tolerate dynamic shapes</title>
      <link>https://github.com/pytorch/pytorch/pull/121497</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124107<br />
* #121734<br />
* #123932<br />
* #123930<br />
* <strong>-&gt;</strong> #121497</p>
<p>Previously, when the Cutlass backend was enabled, using dynamic shapes could lead to exceptions during JIT.</p>
<p>With this change, there are guards in place to just disable the Cutlass backend if dynamic dimensions are involved.</p>
<p>In addition, if no choices for a GEMM are available using the selected backends, then an ATen Kernel is used as fallback, even if the ATen backend is not enabled.</p>
<p>Test:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 07:55:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121497</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] DO NOT REVIEW - to be split up</title>
      <link>https://github.com/pytorch/pytorch/pull/121492</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121492<br />
* #124107<br />
* #121734<br />
* #123932<br />
* #123930<br />
* #121497</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121492</guid>
    </item>
    <item>
      <title>Increased compile time max GPUs to 512. Switched to int16_t DeviceIndex.</title>
      <link>https://github.com/pytorch/pytorch/pull/119639</link>
      <description><![CDATA[<p>Fixes #115331.</p>
<p>This PR increases the number of valid GPU devices to 512 (from 64) in order to future-proof PyTorch for providers that offer <a href="https://www.tensorwave.com/">single nodes with a large device count</a>. Until now, <code>DeviceIndex</code> was an <code>int8_t</code>, thus multiple changes were necessary:</p>
<ul>
<li><code>DeviceIndex</code> changed to <code>int16_t</code>. Updated consumers that assume it to be an <code>int8_t</code>.</li>
<li>Updated bounds checking for <code>torch.device()</code> in the Python frontend. Right now, we allow funny things like <code>torch.device('cpu', 200).index == -56</code>, which is undefined behavior. I inserted some checks to only allow values between 0 and <code>c10::Device::MAX_NUM_DEVICES - 1</code>.</li>
<li>Updated the <code>ArgumentInfo</code> struct as it hardcodes the device index as 8 bit field [^1]. Might be a breaking change, not sure if users rely on this.</li>
<li>Introduced <code>c10::Device::MAX_NUM_DEVICES</code> as a replacement for the old <code>C10_COMPILE_TIME_MAX_GPUS</code></li>
</ul>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225</p>
<p>[^1]: This field was unsigned, so I guess this has also been undef behavior the whole time? Our default device index is -1, so this always wrapped around to 255 when written to the <code>ArgumentInfo</code> struct. When I switched the <code>DeviceIndex</code> to <code>int16_t</code>, it actually stayed 255 after unpacking from <code>ArgumentInfo</code> again, as the <code>DeviceIndex</code> was now wide enough that it didn't wrap back to -1.</p>]]></description>
      <pubDate>Sat, 10 Feb 2024 05:14:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119639</guid>
    </item>
    <item>
      <title>[Inductor] Kill Mutation Layout</title>
      <link>https://github.com/pytorch/pytorch/issues/118570</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>We have replaced <code>MutationLayout</code> with <code>get_mutation_names()</code> throughout must of the code base but not entirely. See, existing PR: https://github.com/pytorch/pytorch/pull/112925. </p>
<p>We should finish the migration and kill of the few remaining uses.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @oulgen </p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>]]></description>
      <pubDate>Mon, 29 Jan 2024 12:04:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/118570</guid>
    </item>
  </channel>
</rss>
