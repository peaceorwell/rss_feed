<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>Make u/int8 cat inductor fallback cpu-only</title>
      <link>https://github.com/pytorch/pytorch/pull/123278</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123278</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 10:57:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123278</guid>
    </item>
    <item>
      <title>DTensor context parallel compile support</title>
      <link>https://github.com/pytorch/pytorch/pull/123276</link>
      <description><![CDATA[<p>Fixes #ISSUE_NUMBER</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 10:12:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123276</guid>
    </item>
    <item>
      <title>[inductor] Add explicit fma in two pass variance</title>
      <link>https://github.com/pytorch/pytorch/pull/123269</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122736<br />
* #122915<br />
* #115435<br />
* <strong>-&gt;</strong> #123269<br />
* #122518<br />
* #121924</p>
<p>The <code>dx = x - mean</code> calculation in the variance is a classic<br />
big - big = small scenario where precision is at a premium.</p>
<p>Calculating it with an fma as <code>fma(sum, 1/N, -x)</code> results in 1 fewer rounding<br />
and so we can expect greater accuracy overall.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 08:27:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123269</guid>
    </item>
    <item>
      <title>[AOTInductor] Fix non-determinism in CUfunction declarations</title>
      <link>https://github.com/pytorch/pytorch/pull/123266</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #123278<br />
* <strong>-&gt;</strong> #123266</p>
<p>These use the ordering of sets and dictionaries to determine the output order,<br />
which leads to run-to-run variance in the output code.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 07:32:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123266</guid>
    </item>
    <item>
      <title>Use Vectorized Half for eager and compile</title>
      <link>https://github.com/pytorch/pytorch/pull/123260</link>
      <description><![CDATA[<p>Using implementation added by https://github.com/pytorch/pytorch/pull/122918</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 06:32:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123260</guid>
    </item>
    <item>
      <title>DISABLED test_buffer_mutation_3_abi_compatible_cuda (__main__.AOTInductorTestABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/123251</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestABICompatibleCuda%3A%3Atest_buffer_mutation_3_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 03:49:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123251</guid>
    </item>
    <item>
      <title>[aot_inductor] Fix issues in pre_grad passes </title>
      <link>https://github.com/pytorch/pytorch/pull/123181</link>
      <description><![CDATA[<p>Summary:</p>
<p>Fixed a bug in <code>sink_cat_after_pointwise</code> pass for PT IR. The root cause is asumption of existence of input in kwargs or args</p>
<p>Differential Revision: D55617545</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 02 Apr 2024 08:27:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123181</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Add qlinear_pointwise.binary op for X86Inductor backend</title>
      <link>https://github.com/pytorch/pytorch/pull/123144</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #123240<br />
* #122667<br />
* #122593<br />
* #122387<br />
* <strong>-&gt;</strong> #123144</p>
<p><strong>Note</strong>: This is a reopen of https://github.com/pytorch/pytorch/pull/122288, which was merged by <code>ghstack land</code> to its base (not main) by mistake.</p>
<p><strong>Description</strong><br />
Add qlinear_binary op for X86Inductor backend of quantization PT2E. It only supports <code>add</code> and <code>add_relu</code> now.<br />
It will use post op sum if the extra input has the same dtype as output. Otherwise, it uses binary add.<br />
<code>+-------------------+--------------+---------------+
| Extra input dtype | Output dtype | Post op       |
+-------------------+--------------+---------------+
| Fp32/bf16         | fp32/bf16    | sum or add*   |
+-------------------+--------------+---------------+
| Fp32/bf16         | int8         | add           |
+-------------------+--------------+---------------+
| int8              | fp32/bf16    | not supported |
+-------------------+--------------+---------------+
| int8              | int8         | sum           |
+-------------------+--------------+---------------+
*Use sum if extra input and output have the same dtype; otherwise use add.</code></p>
<p><strong>Test plan</strong><br />
python test_quantization.py -k test_qlinear_add_pt2e<br />
python test_quantization.py -k test_qlinear_add_relu_pt2e</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 01 Apr 2024 17:09:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123144</guid>
    </item>
    <item>
      <title>[not4land] Run CI with compile_threads=1</title>
      <link>https://github.com/pytorch/pytorch/pull/123012</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123012<br />
* #123011<br />
* #122941</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 29 Mar 2024 16:08:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123012</guid>
    </item>
    <item>
      <title>[minifier] Don't recompile for accuracy minification</title>
      <link>https://github.com/pytorch/pytorch/pull/123005</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #123006<br />
* <strong>-&gt;</strong> #123005</p>
<p><code>backend_aot_accuracy_fails</code> reruns <code>compile_fx_inner</code> on the real inputs which<br />
means the graph is recompiled with static shapes. This meant accuracy failures<br />
related to dynamic shapes would never be captured by <code>REPRO_AFTER=aot</code>.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Fri, 29 Mar 2024 15:48:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123005</guid>
    </item>
    <item>
      <title>[inductor] Move compile workers to a subprocess</title>
      <link>https://github.com/pytorch/pytorch/pull/122941</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #123012<br />
* #123011<br />
* <strong>-&gt;</strong> #122941</p>
<p>In many environments using fork-based parallelism causes issues because user processes are not fork-safe.  This moves our parallel compile work pool into a subprocess that we control (that should be fork safe).</p>
<p>Perf run: https://github.com/pytorch/pytorch/actions/runs/8486887873</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 20:34:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122941</guid>
    </item>
    <item>
      <title>[inductor] Assume non-views are aligned, views are unaligned</title>
      <link>https://github.com/pytorch/pytorch/pull/122926</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122159<br />
* <strong>-&gt;</strong> #122926</p>
<p><strong>TL;DR</strong>: After this PR, if assume_aligned_inputs=False: Inductor will generate code based on un-guarded properties of the example input tensors; specifically, if an example input is a view, we generate unaligned triton code; and if it is not a view we generate aligned triton code (and clone any inputs that aren't aligned)</p>
<p><strong>Details</strong>:<br />
* If an input is a view, we generate unaligned triton. <br />
* If an input is not a view, we generate aligned triton and do a clone at runtime.<br />
* This change is still hidden behind the <code>assume_aligned_inputs</code> flag<br />
* This does not guard on whether tensors are views. I have a version that does guard here: https://github.com/pytorch/pytorch/pull/122867 but it has a failure in <code>test_issue106555</code> which is concerning; somehow, adding additional dynamo guards (and making no changes to inductor, because all the changes are still guarded in that PR) causes a size assertion failure in the backward pass.</p>
<p><strong>Alternatives</strong><br />
* Guard on storage offset: this would be more accurate, because we would actually know whether the tensor is aligned. For example, there may be some views that are always aligned, but in this PR we will assume that they are not aligned. However, we ran into two issues with this: (1) dynamo guards took way too long, and (2) we'd start recompiling more than we'd like.<br />
* Care about the users of the input: e.g. if it's a matmul, or maybe if it is loaded a lot of times, then it might be worthwhile to do the clone; otherwise, don't clone and generate unaligned code.<br />
* Guard on tensor-is-a-view in case this property changes. As I mentioned above, I tried this in https://github.com/pytorch/pytorch/pull/122926 but ran into some issues; but I can take another look if we think this is important.<br />
* We could also make assume_aligned_inputs=False by default (although we'd have to be careful about getting rid of the config)</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 15:56:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122926</guid>
    </item>
    <item>
      <title>WIP: [inductor] Be more strict about down-casting to {b,}float16</title>
      <link>https://github.com/pytorch/pytorch/pull/122915</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122736<br />
* <strong>-&gt;</strong> #122915<br />
* #115435<br />
* #123269<br />
* #122518<br />
* #121924</p>
<p>Our triton codegen currently treats all float16 and bfloat16 intermediates<br />
as float32 and this may lead to numerical discrepancies from eager.</p>
<p>For example, say a float16 value is inlined into one kernel but also realized<br />
as an input argument to another kernel. These kernels will observe two different<br />
values for the same variable! One in full float32 precision and the other<br />
truncated to float16.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 11:37:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122915</guid>
    </item>
    <item>
      <title>Short-term fix to preserve NJT metadata cache in torch.compile</title>
      <link>https://github.com/pytorch/pytorch/pull/122836</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121947<br />
* <strong>-&gt;</strong> #122836</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D55448636">D55448636</a></p>]]></description>
      <pubDate>Wed, 27 Mar 2024 13:48:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122836</guid>
    </item>
    <item>
      <title>[inductor] modify the output_stride of ConcatKernel</title>
      <link>https://github.com/pytorch/pytorch/pull/122761</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122761</p>
<p>Fix https://github.com/pytorch/pytorch/issues/121613.<br />
Modify the <code>output_stride</code> of <code>ConcatKernel</code>: If any input to <code>Concat</code> is <code>Pointwise</code>, check the layout of all inputs to <code>Pointwise</code>, if any of the inputs is in channels_last format, set channels_last strides for the <code>output_stride</code>.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 19:57:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122761</guid>
    </item>
    <item>
      <title>[inductor] Modify the rules for freezing the layout of x.unwrap_view() in convert_to_reinterpret_view</title>
      <link>https://github.com/pytorch/pytorch/pull/122760</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122760</p>
<p>Fix https://github.com/pytorch/pytorch/issues/121607</p>
<p>Modify the rules for freezing the layout of <code>x.unwrap_view()</code> in <code>convert_to_reinterpret_view</code>: If any read of <code>x.unwrap_view()</code> is in channels_last format, freeze the layout of <code>x.unwrap_view()</code> to channels_last format. </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 19:30:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122760</guid>
    </item>
    <item>
      <title>`torch.compile` is not usable on MacOS out of box</title>
      <link>https://github.com/pytorch/pytorch/issues/122705</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Attempts to run any <code>torch.compile</code>d code fails out-of-box with 2.3.0 wheels, as it attempts to link against wrong copy of <code>libomp.dylib</code>:<br />
<code>OMP: Error #15: Initializing libomp.dylib, but found libomp.dylib already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/</code></p>
<p>This happens, because <code>delocate</code> embeds libomp.dylib into <code>torch/.libs</code> folder, but another instance is shipped in <code>torch/libs</code></p>
<h3>Versions</h3>
<p>2.3.0</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @seemethere @osalpekar @atalman @albanD @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 07:38:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122705</guid>
    </item>
    <item>
      <title>[inductor] Fix fresh_inductor_cache()</title>
      <link>https://github.com/pytorch/pytorch/pull/122661</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122661</p>
<p>Summary: Modify fresh_inductor_cache() to clear cached state before mocking the toplevel cache_dir directory. Any lru_caches (or otherwise) can use the @clear_on_fresh_inductor_cache decorator to register the cache for clearing. Also change the base inductor TestCase class to use fresh_inductor_cache(). Previously that TestCase was only mocking the subdirectory within the toplevel cache dir designated for the FX graph cache artifacts.</p>
<p>Test Plan:<br />
- New unit test<br />
- All existing inductor tests will exercise fresh_inductor_cache()</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 16:12:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122661</guid>
    </item>
    <item>
      <title>[fix] 'from' in python code generated by inductor, resulting in SyntaxError</title>
      <link>https://github.com/pytorch/pytorch/pull/122632</link>
      <description><![CDATA[<p><code>from</code> is  a python keyword.  Python code containing <code>aten.random.from</code> will result in syntax errors. This PR handle this special case in the codegen of inductor. </p>
<p>Also add fallback lowering for aten.random</p>
<p>Fixes #121621 </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 10:47:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122632</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Enable lowering of qlinear-binary(-unary) fusion for X86Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/122593</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #123240<br />
* #122667<br />
* <strong>-&gt;</strong> #122593<br />
* #122387<br />
* #123144</p>
<p><strong>Description</strong><br />
Lower the qlinear binary post op pattern to Inductor. Use post op sum (in-place) if the extra input has the same dtype as output. Otherwise, it uses binary add.</p>
<p><strong>Supported linear-binary(-unary) patterns</strong><br />
```<br />
    linear(X)   extra input<br />
           \   /<br />
            Add<br />
             |<br />
        Optional(relu)<br />
             |<br />
             Y</p>
<ol>
<li>
<p>int8-mixed-fp32<br />
+---+---------------+-----------+------------------------------+---------+<br />
| # | Add type      | Quant out | Pattern                      | Post op |<br />
+---+---------------+-----------+------------------------------+---------+<br />
| 1 | In-/out-place | Yes       | linear + fp32 -&gt; (relu) -&gt; q | add     |<br />
+---+---------------+-----------+------------------------------+---------+<br />
| 2 | In-/out-place | No        | linear + fp32 -&gt; (relu)      | sum     |<br />
+---+---------------+-----------+------------------------------+---------+</p>
</li>
<li>
<p>int8-mixed-bf16<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| # | X2 dtype | Add type      | Quant out | Pattern                                          | Post op |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 1 | BF16     | In-/out-place | Yes       | linear + bf16 -&gt; (relu) -&gt; to_fp32 -&gt; q          | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 2 | BF16     | In-/out-place | No        | linear + bf16 -&gt; (relu)                          | sum     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 3 | FP32     | Out-place     | Yes       | linear + fp32 -&gt; (relu) -&gt; q                     | add     |<br />
|   |          | In-place right|           |                                                  |         |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 4 | FP32     | Out-place     | No        | linear + fp32 -&gt; (relu)                          | sum     |<br />
|   |          | In-place right|           |                                                  |         |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 5 | FP32     | In-place left | Yes       | linear + fp32 -&gt; to_bf16 -&gt; relu -&gt; to_fp32 -&gt; q | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 6 | FP32     | In-place left | No        | linear + fp32 -&gt; to_bf16 -&gt; (relu)               | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
```<br />
Note<br />
(1) The positions of linear and the extra input can be swapped.<br />
(2) we don't insert q-dq before the extra input of linear-add by recipe. But if q-dq is found at the<br />
extra input, we don't match that pattern because we cannot match all these patterns in 3 passes.</p>
</li>
</ol>
<p><strong>Test plan</strong><br />
python test/inductor/test_mkldnn_pattern_matcher.py -k test_qlinear_add<br />
python test/inductor/test_cpu_cpp_wrapper.py -k test_qlinear_add</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Mar 2024 23:49:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122593</guid>
    </item>
    <item>
      <title>[inductor] Add explicit ops.fma and use it in softmax_backward</title>
      <link>https://github.com/pytorch/pytorch/pull/122518</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122736<br />
* #122915<br />
* #115435<br />
* #123269<br />
* <strong>-&gt;</strong> #122518<br />
* #121924</p>
<p>This allows us to generate an fma even when fp-fusion is disabled<br />
in the compiler.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 22 Mar 2024 12:48:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122518</guid>
    </item>
    <item>
      <title>[Inductor] Pass device interface to the worker compile</title>
      <link>https://github.com/pytorch/pytorch/pull/122492</link>
      <description><![CDATA[<p>Summary: In <code>codecache.py</code> pass the device_interface directly to <code>_worker_compile()</code> instead of calling <code>get_device_interface()</code> from inside the function.</p>
<p>If the device_interface is registered by an out-of-tree module then it will only be registered inside the main process and not inside the worker process. This fixes this issue. Happy to add a test if required. </p>
<p>Test plan:<br />
No tests added</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 22 Mar 2024 04:05:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122492</guid>
    </item>
    <item>
      <title>[effects] Add inductor support for tokens</title>
      <link>https://github.com/pytorch/pytorch/pull/122347</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122348<br />
* <strong>-&gt;</strong> #122347</p>
<p>Given the following code/dynamo graph:<br />
<code>class GraphModule(torch.nn.Module):
    def forward(self, L_x_ : torch.Tensor):
        l_x_ = L_x_
        _print = torch.ops.aten._print('moo')
        res = l_x_ + l_x_;  l_x_ = None
        _print_1 = torch.ops.aten._print('moo')
        return (res,)</code></p>
<p>AOTAutograd will trace the following program, threading tokens from the inputs, through the effectful operator calls (torch.ops.aten._print), and as an output:<br />
<code>class &lt;lambda&gt;(torch.nn.Module):
    def forward(self, arg0_1: "f32[0]", arg1_1: "f32[2, 3]"):
        with_effects = torch._higher_order_ops.effects.with_effects(arg0_1, torch.ops.aten._print.default, 'moo');  arg0_1 = None
        getitem: "f32[0]" = with_effects[0];  with_effects = None
        add: "f32[2, 3]" = torch.ops.aten.add.Tensor(arg1_1, arg1_1);  arg1_1 = None
        with_effects_1 = torch._higher_order_ops.effects.with_effects(getitem, torch.ops.aten._print.default, 'moo');  getitem = None
        getitem_2: "f32[0]" = with_effects_1[0];  with_effects_1 = None
        return (getitem_2, add)</code><br />
However when we get to inductor, since we want the inductor generated code to not have any token inputs/outputs for better readability, we want to modify the aten graph by removing the tokens from inputs, and creating them through <code>torch.ops.aten._make_dep_token</code>, and sinking them through the <code>torch.ops.aten._sink_tokens</code> operators.<br />
<code>class &lt;lambda&gt;(torch.nn.Module):
   def forward(self, arg1_1: "f32[2, 3]"):
       _make_dep_token_default: "f32[0]" = torch.ops.aten._make_dep_token.default()
       with_effects = torch._higher_order_ops.effects.with_effects(_make_dep_token_default, torch.ops.aten._print.default, 'moo');  _make_dep_token_default = None
       getitem: "f32[0]" = with_effects[0];  with_effects = None
       add: "f32[2, 3]" = torch.ops.aten.add.Tensor(arg1_1, arg1_1);  arg1_1 = None
       with_effects_1 = torch._higher_order_ops.effects.with_effects(getitem, torch.ops.aten._print.default, 'moo');  getitem = None
       getitem_2: "f32[0]" = with_effects_1[0];  with_effects_1 = None
       _sink_tokens_default = torch.ops.aten._sink_tokens.default((getitem_2,));  getitem_2 = None
       return (add,)</code><br />
When doing inductor lowering, we convert <code>with_effects</code> calls to an <code>EffectfulKernel</code>, which just a <code>FallbackKernel</code> but with a pointer to previous effectful operator's call. During scheduling, we will create a <code>StarDep</code> between the EffectfulKernel and its previous EffectfulKernel so that they don't get reordered.<br />
<code>def call(args):
    arg1_1, = args
    args.clear()
    assert_size_stride(arg1_1, (2, 3), (3, 1))
    # Source Nodes: [_print], Original ATen: []
    buf2 = aten._print.default('moo')
    # Source Nodes: [_print_1], Original ATen: []
    buf3 = aten._print.default('moo')
    buf4 = empty_strided_cpu((2, 3), (3, 1), torch.float32)
    cpp_fused_add_0(arg1_1, buf4)
    del arg1_1
    return (buf4, )</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 14:10:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122347</guid>
    </item>
    <item>
      <title>Missing Symbols When running AOTInductor example with Libtorch c++11 ABI</title>
      <link>https://github.com/pytorch/pytorch/issues/122313</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I am trying to run the AOTInductor example model with the c++11 libtroch with cxx11-abi-shared-with-debs , see https://download.pytorch.org/libtorch/cu118/libtorch-cxx11-abi-shared-with-deps-2.2.1%2Bcu118.zip (also with the cuda 12.1 version) but get the following unknown symbols error:</p>
<p>```<br />
root@0b74fe279d1f:/tmp/build# ./aoti_example /tmp/example/model.so <br />
terminate called after throwing an instance of 'c10::DynamicLibraryError'<br />
  what():  Error in dlopen: /tmp/example/model.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationESsb<br />
Exception raised from DynamicLibrary at ../aten/src/ATen/DynamicLibrary.cpp:38 (most recent call first):<br />
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits\<char>, std::allocator\<char> >) + 0x6c (0x7f43696d0a0c in /tmp/libtorch/lib/libc10.so)<br />
frame #1: <unknown function> + 0x1148f11 (0x7f43526e1f11 in /tmp/libtorch/lib/libtorch_cpu.so)<br />
frame #2: torch::inductor::AOTIModelContainerRunner::AOTIModelContainerRunner(char const<em>, unsigned long, bool, char const</em>) + 0x8a (0x7f43569c295a in /tmp/libtorch/lib/libtorch_cpu.so)<br />
frame #3: <unknown function> + 0xa2c4 (0x55cb652cd2c4 in ./aoti_example)<br />
frame #4: <unknown function> + 0x4af5 (0x55cb652c7af5 in ./aoti_example)<br />
frame #5: <unknown function> + 0x29d90 (0x7f42fd7b4d90 in /lib/x86_64-linux-gnu/libc.so.6)<br />
frame #6: __libc_start_main + 0x80 (0x7f42fd7b4e40 in /lib/x86_64-linux-gnu/libc.so.6)<br />
frame #7: <unknown function> + 0x4955 (0x55cb652c7955 in ./aoti_example)</p>
<p>Aborted (core dumped)<br />
<code>``
 The example work when I set the</code>-DCMAKE_PREFIX_PATH=python3 -c 'import torch;print(torch.utils.cmake_prefix_path)'`  . However, I need to use libtroch because I need to link against opencv too, which is not possible with the python-provided library. </p>
<h2>Reproduce:</h2>
<p>Basically, follow the <a href="https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html">AOTInductor example</a> and use https://download.pytorch.org/libtorch/cu118/libtorch-cxx11-abi-shared-with-deps-2.2.1%2Bcu118.zip as libtorch.</p>
<p>In more detail:<br />
- AOT Compile the model with the following code:<br />
```<br />
import os<br />
import torch</p>
<p>class Model(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.fc1 = torch.nn.Linear(10, 16)<br />
        self.relu = torch.nn.ReLU()<br />
        self.fc2 = torch.nn.Linear(16, 1)<br />
        self.sigmoid = torch.nn.Sigmoid()</p>
<pre><code>def forward(self, x):
    x = self.fc1(x)
    x = self.relu(x)
    x = self.fc2(x)
    x = self.sigmoid(x)
    return x
</code></pre>
<p>with torch.no_grad():<br />
    device = "cuda" if torch.cuda.is_available() else "cpu"<br />
    model = Model().to(device=device)<br />
    example_inputs=(torch.randn(8, 10, device=device),)<br />
    batch_dim = torch.export.Dim("batch", min=1, max=1024)<br />
    so_path = torch._export.aot_compile(<br />
        model,<br />
        example_inputs,<br />
        # Specify the first dimension of the input x as dynamic<br />
        dynamic_shapes={"x": {0: batch_dim}},<br />
        # Specify the generated shared library path<br />
        options={"aot_inductor.output_path": os.path.join(os.getcwd(), "model.so")},<br />
    )<br />
<code>- Compile the follwoing c++ code</code></p>
<h1>include <iostream></h1>
<h1>include <vector></h1>
<h1>include <torch/torch.h></h1>
<h1>include <torch/csrc/inductor/aoti_model_container_runner_cuda.h></h1>
<p>int main() {<br />
    c10::InferenceMode mode;</p>
<pre><code>torch::inductor::AOTIModelContainerRunnerCuda runner("model.so");
std::vector&lt;torch::Tensor&gt; inputs = {torch::randn({8, 10}, at::kCUDA)};
std::vector&lt;torch::Tensor&gt; outputs = runner.run(inputs);
std::cout &lt;&lt; "Result from the first inference:"&lt;&lt; std::endl;
std::cout &lt;&lt; outputs[0] &lt;&lt; std::endl;

// The second inference uses a different batch size and it works because we
// specified that dimension as dynamic when compiling model.so.
std::cout &lt;&lt; "Result from the second inference:"&lt;&lt; std::endl;
std::cout &lt;&lt; runner.run({torch::randn({2, 10}, at::kCUDA)})[0] &lt;&lt; std::endl;

return 0;
</code></pre>
<p>}<br />
<code>``
- Instantiate cmake with</code>cmake -DCMAKE_PREFIX_PATH=/workspace/libtorch/share/cmake/<code>and DO NOT USE</code>cmake -DCMAKE_PREFIX_PATH=python3 -c 'import torch;print(torch.utils.cmake_prefix_path)' ..`. <br />
- Run the compiled binary. </p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.2.0+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA RTX A5000<br />
Nvidia driver version: 535.146.02<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      43 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             128<br />
On-line CPU(s) list:                0-127<br />
Vendor ID:                          AuthenticAMD<br />
Model name:                         AMD EPYC 7542 32-Core Processor<br />
CPU family:                         23<br />
Model:                              49<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 32<br />
Socket(s):                          2<br />
Stepping:                           0<br />
Frequency boost:                    enabled<br />
CPU max MHz:                        2900.0000<br />
CPU min MHz:                        1500.0000<br />
BogoMIPS:                           5800.28<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es<br />
Virtualization:                     AMD-V<br />
L1d cache:                          2 MiB (64 instances)<br />
L1i cache:                          2 MiB (64 instances)<br />
L2 cache:                           32 MiB (64 instances)<br />
L3 cache:                           256 MiB (16 instances)<br />
NUMA node(s):                       2<br />
NUMA node0 CPU(s):                  0-31,64-95<br />
NUMA node1 CPU(s):                  32-63,96-127<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection<br />
Vulnerability Spec rstack overflow: Mitigation; safe RET<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==7.0.0<br />
[pip3] mypy==1.9.0<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.24.1<br />
[pip3] torch==2.2.0<br />
[pip3] torchaudio==2.1.0+cu118<br />
[pip3] torchdata==0.7.1<br />
[pip3] torchtext==0.17.0<br />
[pip3] torchvision==0.17.0<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @jbschlosser</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 06:43:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122313</guid>
    </item>
    <item>
      <title>[Inductor Intel GPU backend Upstream] Enable triton installation for Intel GPU</title>
      <link>https://github.com/pytorch/pytorch/pull/122254</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122866<br />
* #121895<br />
* <strong>-&gt;</strong> #122254<br />
* #121883</p>
<p>Following the RFC https://github.com/pytorch/pytorch/issues/114856, Intel GPU Inductor backend depends on triton that  functions with Intel GPUs. This PR enabled the triton installation in Intel GPU CI docker build.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 17:14:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122254</guid>
    </item>
    <item>
      <title>Update torch.compile_faq w.r.t to functorch</title>
      <link>https://github.com/pytorch/pytorch/pull/122213</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #123268<br />
* <strong>-&gt;</strong> #122213<br />
* #122212<br />
* #122211</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 09:47:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122213</guid>
    </item>
    <item>
      <title>[HOP][inductor] Support pytrees as associative_scan input</title>
      <link>https://github.com/pytorch/pytorch/pull/122137</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122137<br />
* #119430</p>
<p>This allows <code>associative_scan</code> to take an arbitrary pytree of tensors,<br />
which is flattened to their leaves before calling the <code>associative_scan</code><br />
higher order operator.</p>
<p>I also add support in inductor to generate code for scanning over sequences<br />
of tensors.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 18 Mar 2024 14:10:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122137</guid>
    </item>
    <item>
      <title>[PT2][Inductor][3/n] Customize pre grad and post grad patterns</title>
      <link>https://github.com/pytorch/pytorch/pull/121915</link>
      <description><![CDATA[<p>Summary: Currently, we only enabled the group batch fusion customization, we also enable the split cat customization.</p>
<p>Test Plan:<br />
<code>buck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode batch-split --model_type "cmf" --flow_id 524546542</code><br />
P1196013839</p>
<p>Differential Revision: D54861682</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 09:48:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121915</guid>
    </item>
    <item>
      <title>[Inductor Intel GPU backend Upstream] Add Inductor Intel GPU backend.</title>
      <link>https://github.com/pytorch/pytorch/pull/121895</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122866<br />
* <strong>-&gt;</strong> #121895<br />
* #122254<br />
* #121883</p>
<p>As the design in RFC https://github.com/pytorch/pytorch/issues/114856, this PR implemented Intel GPU Inductor backend by:<br />
- Reuse WrapperCodegen and TritonScheduling for python wrapper and kernel code generation. And implenented device-specific code generation in XPUDeviceOpOverrides<br />
- Reuse fx_pass, lowering, codecache, triton kernel auto-tuning, and compilation.</p>
<p>For the test case, this PR provided test/inductor/test_xpu_basic.py for basic inductor backend functionality testing.<br />
We'll reuse all the existing Inductor test case in the next PR.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 00:00:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121895</guid>
    </item>
    <item>
      <title>[Inductor] RBLOCK not defined</title>
      <link>https://github.com/pytorch/pytorch/issues/120857</link>
      <description><![CDATA[<p>Repro:<br />
```<br />
from typing import List</p>
<p>import torch</p>
<p>E4M3_MAX_POS = torch.finfo(torch.float8_e4m3fn).max<br />
E5M2_MAX_POS = torch.finfo(torch.float8_e5m2).max<br />
EPS = 1e-12</p>
<p>SIZES = [torch.Size([1152, 768]), torch.Size([384, 768])]</p>
<p>def to_fp8_saturated(x, float8_dtype: torch.dtype):<br />
    if float8_dtype == torch.float8_e4m3fn:<br />
        x = x.clamp(min=-1 * E4M3_MAX_POS, max=E4M3_MAX_POS)<br />
    else:<br />
        x = x.clamp(min=-1 * E5M2_MAX_POS, max=E5M2_MAX_POS)<br />
    return x.to(float8_dtype)</p>
<p>def inner_fn(weights: List[torch.Tensor]):<br />
    # All-reduce partial amaxes computed from sharded weights<br />
    abs_weights = torch._foreach_abs(weights)<br />
    partial_amax_tensor = abs_weights[0].new_empty(len(abs_weights))<br />
    for i, abs_weight in enumerate(abs_weights):<br />
        torch.max(abs_weight, out=partial_amax_tensor[i])<br />
    # NOTE: The error reproduces without this all-reduce, so let us use a<br />
    # single-GPU repro.<br />
    # replicated_amax_tensor = torch.distributed._functional_collectives.all_reduce(<br />
    #     partial_amax_tensor, "MAX", list(range(torch.distributed.get_world_size()))<br />
    # )<br />
    replicated_amax_tensor = partial_amax_tensor</p>
<pre><code># Compute scales from replicated amaxes and the fp8 bit datas
clamped_tensor = torch.clamp(replicated_amax_tensor, EPS)
scales_tensor = E4M3_MAX_POS / clamped_tensor
datas = []
scales = []
for i, weight in enumerate(weights):
    scale = scales_tensor[i]
    weight_scaled = weight * scale
    datas.append(to_fp8_saturated(weight_scaled, torch.float8_e4m3fn))
    scales.append(scale)
return datas, scales
</code></pre>
<p>weights = [torch.randn(size, device="cuda") for size in SIZES]<br />
torch.compile(inner_fn)(weights)<br />
```</p>
<p>```<br />
CompilationError: at 10:29:def triton_(in_ptr0, out_ptr1):<br />
    xpid = tl.program_id(0)<br />
    XBLOCK: tl.constexpr = 1024<br />
    if xpid &gt;= 0 and xpid &lt; 1:<br />
        xpid_offset = xpid - 0<br />
        xnumel = 108<br />
        xoffset = xpid_offset * XBLOCK<br />
        xindex = xoffset + tl.arange(0, XBLOCK)[:, None]<br />
        xmask = xindex &lt; xnumel<br />
        rbase = tl.arange(0, RBLOCK)[None, :]</p>
<p>NameError('RBLOCK is not defined')<br />
```</p>
<details>
<summary> Full stack trace </summary>

```
Traceback (most recent call last):
  File "/data/users/andgu/float8_experimental/repro_inductor_error.py", line 74, in <module>
    torch.compile(inner_fn)(weights)
  File "/data/users/andgu/pytorch/torch/_dynamo/eval_frame.py", line 455, in _fn
    return fn(*args, **kwargs)
  File "/data/users/andgu/pytorch/torch/_dynamo/convert_frame.py", line 909, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/data/users/andgu/pytorch/torch/_dynamo/convert_frame.py", line 774, in _convert_frame
    result = inner_convert(
  File "/data/users/andgu/pytorch/torch/_dynamo/convert_frame.py", line 399, in _convert_frame_assert
    return _compile(
  File "/home/andgu/.conda/envs/pytorch-3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/data/users/andgu/pytorch/torch/_dynamo/convert_frame.py", line 665, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/data/users/andgu/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/users/andgu/pytorch/torch/_dynamo/convert_frame.py", line 537, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/data/users/andgu/pytorch/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object
    transformations(instructions, code_options)
  File "/data/users/andgu/pytorch/torch/_dynamo/convert_frame.py", line 164, in _fn
    return fn(*args, **kwargs)
  File "/data/users/andgu/pytorch/torch/_dynamo/convert_frame.py", line 502, in transform
    tracer.run()
  File "/data/users/andgu/pytorch/torch/_dynamo/symbolic_convert.py", line 2174, in run
    super().run()
  File "/data/users/andgu/pytorch/torch/_dynamo/symbolic_convert.py", line 831, in run
    and self.step()
  File "/data/users/andgu/pytorch/torch/_dynamo/symbolic_convert.py", line 794, in step
    getattr(self, inst.opname)(inst)
  File "/data/users/andgu/pytorch/torch/_dynamo/symbolic_convert.py", line 2293, in RETURN_VALUE
    self.output.compile_subgraph(
  File "/data/users/andgu/pytorch/torch/_dynamo/output_graph.py", line 957, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/home/andgu/.conda/envs/pytorch-3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/data/users/andgu/pytorch/torch/_dynamo/output_graph.py", line 1102, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/data/users/andgu/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/users/andgu/pytorch/torch/_dynamo/output_graph.py", line 1175, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/data/users/andgu/pytorch/torch/_dynamo/output_graph.py", line 1156, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/data/users/andgu/pytorch/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/data/users/andgu/pytorch/torch/__init__.py", line 1729, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
  File "/home/andgu/.conda/envs/pytorch-3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/data/users/andgu/pytorch/torch/_inductor/compile_fx.py", line 1333, in compile_fx
    return aot_autograd(
  File "/data/users/andgu/pytorch/torch/_dynamo/backends/common.py", line 57, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/data/users/andgu/pytorch/torch/_functorch/aot_autograd.py", line 878, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/data/users/andgu/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/users/andgu/pytorch/torch/_functorch/aot_autograd.py", line 603, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/data/users/andgu/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 434, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/data/users/andgu/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 639, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/data/users/andgu/pytorch/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 101, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/data/users/andgu/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/users/andgu/pytorch/torch/_inductor/compile_fx.py", line 1260, in fw_compiler_base
    return inner_compile(
  File "/data/users/andgu/pytorch/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/data/users/andgu/pytorch/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/home/andgu/.conda/envs/pytorch-3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/andgu/.conda/envs/pytorch-3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/data/users/andgu/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/users/andgu/pytorch/torch/_inductor/compile_fx.py", line 430, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/data/users/andgu/pytorch/torch/_inductor/compile_fx.py", line 698, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/data/users/andgu/pytorch/torch/_inductor/graph.py", line 1279, in compile_to_fn
    return self.compile_to_module().call
  File "/data/users/andgu/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/users/andgu/pytorch/torch/_inductor/graph.py", line 1231, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/data/users/andgu/pytorch/torch/_inductor/codecache.py", line 2103, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_andgu/fj/cfjt3dz3mu2u5apyg5xstqhvbovmasuzlzs67avpdswfwaitplsj.py", line 319, in <module>
    async_compile.wait(globals())
  File "/data/users/andgu/pytorch/torch/_inductor/codecache.py", line 2658, in wait
    scope[key] = result.result()
  File "/data/users/andgu/pytorch/torch/_inductor/codecache.py", line 2465, in result
    self.future.result()
  File "/home/andgu/.conda/envs/pytorch-3.10/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/home/andgu/.conda/envs/pytorch-3.10/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
CompilationError: at 10:29:def triton_(in_ptr0, out_ptr1):
    xpid = tl.program_id(0)
    XBLOCK: tl.constexpr = 1024
    if xpid >= 0 and xpid < 1:
        xpid_offset = xpid - 0
        xnumel = 108
        xoffset = xpid_offset * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
        xmask = xindex < xnumel
        rbase = tl.arange(0, RBLOCK)[None, :]

NameError('RBLOCK is not defined')
```

</details>

<p>On commit 9c597ff137ead9f7f7ec8fdcbf473de2d328e61b (<code>viable/strict</code> as of 2/28 morning)</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @zou3519</p>]]></description>
      <pubDate>Wed, 28 Feb 2024 16:14:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120857</guid>
    </item>
    <item>
      <title>[WIP][inductor] padding</title>
      <link>https://github.com/pytorch/pytorch/pull/120758</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120758<br />
* #123076</p>
<p>This is not fully ready for review yet. Sending it out in case people want to give early feedback. Also it can allow me run CI perf test earlier.</p>
<p>By testing BlenderbotSmallForConditionalGeneration I already see 2.5ms speedup. I feel the solution can be further improved to get more speedup.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang @jansel @Chillee @eellison </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames</p>]]></description>
      <pubDate>Tue, 27 Feb 2024 17:00:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120758</guid>
    </item>
    <item>
      <title>DISABLED test_item_to_inputs_kernel_nobreak_cuda (__main__.TestInductorDynamicCUDA)</title>
      <link>https://github.com/pytorch/pytorch/issues/119538</link>
      <description><![CDATA[<p>Platforms: rocm, linux</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_item_to_inputs_kernel_nobreak_cuda&amp;suite=TestInductorDynamicCUDA&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/21391792477">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_item_to_inputs_kernel_nobreak_cuda</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_torchinductor_dynamic_shapes.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Fri, 09 Feb 2024 01:39:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/119538</guid>
    </item>
    <item>
      <title>[HOP][inductor] Add higher order associative scan operator</title>
      <link>https://github.com/pytorch/pytorch/pull/119430</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122137<br />
* <strong>-&gt;</strong> #119430</p>
<p>Currently only supports single tensor scans, e.g. <code>cumsum</code>, <code>cumprod</code>, <code>logcumsumexp</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Wed, 07 Feb 2024 17:00:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119430</guid>
    </item>
    <item>
      <title>[inductor] Disable fp contraction and add option to use precise division</title>
      <link>https://github.com/pytorch/pytorch/pull/115435</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122736<br />
* #122915<br />
* <strong>-&gt;</strong> #115435<br />
* #123269<br />
* #122518<br />
* #121924</p>
<p>Fixes #101039</p>
<p>In the issue we have something like:<br />
<code>python
(x / 1e-6) + (y / 1e-6)</code><br />
which triton turns into<br />
<code>python
fma(x, 1e6, y * 1e6)</code></p>
<p>Where we have one division calculated to float32 precision,<br />
and the other calculated to infinite precision as part of the <code>fma</code>.<br />
In the issue this leads to a bad normalization in softmax.</p>
<p>Here we disallow the "fp fusion" optimization which generates these fma instructions.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Fri, 08 Dec 2023 10:25:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/115435</guid>
    </item>
  </channel>
</rss>
