<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[inductor] Refactor runtime files into torch._inductor.runtime (part 2)</title>
      <link>https://github.com/pytorch/pytorch/pull/124553</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122941<br />
* <strong>-&gt;</strong> #124553<br />
* #124552</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sat, 20 Apr 2024 10:59:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124553</guid>
    </item>
    <item>
      <title>[inductor] Refactor runtime files into torch._inductor.runtime (part 1)</title>
      <link>https://github.com/pytorch/pytorch/pull/124552</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122941<br />
* #124553<br />
* <strong>-&gt;</strong> #124552</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sat, 20 Apr 2024 10:59:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124552</guid>
    </item>
    <item>
      <title>[inductor] for UserDefinedTritonKernels don't mark all inputs as mutating</title>
      <link>https://github.com/pytorch/pytorch/pull/124425</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124425</p>
<p>Take this example:<br />
```<br />
def _mul2(x):<br />
    y = torch.empty_like(x)<br />
    mul2_kernel<a href="in_ptr0=x, out_ptr=y,
        n_elements=x.numel(), BLOCK_SIZE=1,">(10,)</a><br />
    return y</p>
<p>def f(x):<br />
    for _ in range(4):<br />
        x = _mul2(x)<br />
    return x + 1<br />
```</p>
<p>Currently, the codegen will show up like this. Notice, how we allocate 5 buffers of the same size.<br />
```</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []</h1>
<p>buf0 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=arg0_1, out_ptr=reinterpret_tensor(buf0, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_1], Original ATen: []</h1>
<p>buf4 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf0, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf4, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_2], Original ATen: []</h1>
<p>buf8 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf4, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf8, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_3], Original ATen: []</h1>
<p>buf12 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf8, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf12, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [add], Original ATen: [aten.add]</h1>
<p>buf16 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
triton_poi_fused_add_0.run(buf12, buf16, 10, grid=grid(10), stream=stream0)...)<br />
return (buf16, )<br />
```</p>
<p>With this PR, we want to see this. Notice, how we only allocate 2 buffers this time. The other 3 buffers are re-used.<br />
```</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []</h1>
<p>buf0 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=arg0_1, out_ptr=reinterpret_tensor(buf0, (10, ), (1, ), 0), ...)<br />
del arg0_1</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_1], Original ATen: []</h1>
<p>buf2 = empty_strided_cuda((10, ), (1, ), torch.float32)<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf0, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf2, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_2], Original ATen: []</h1>
<p>buf4 = buf0; del buf0  # reuse<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf2, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf4, (10, ), (1, ), 0) ...)</p>
<h1>Source Nodes: [triton_kernel_wrapper_mutation_3], Original ATen: []</h1>
<p>buf6 = buf2; del buf2  # reuse<br />
mul2_kernel_0.run(in_ptr0=reinterpret_tensor(buf4, (10, ), (1, ), 0), out_ptr=reinterpret_tensor(buf6, (10, ), (1, ), 0) ...)<br />
del buf4</p>
<h1>Source Nodes: [add], Original ATen: [aten.add]</h1>
<p>buf8 = buf6; del buf6  # reuse<br />
triton_poi_fused_add_0.run(buf8, 10, grid=grid(10), stream=stream0)<br />
return (buf8, )<br />
```</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D56379307">D56379307</a></p>]]></description>
      <pubDate>Thu, 18 Apr 2024 11:54:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124425</guid>
    </item>
    <item>
      <title>[inductor] Move compile workers to a subprocess</title>
      <link>https://github.com/pytorch/pytorch/pull/122941</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122941<br />
* #124553<br />
* #124552</p>
<p>In many environments using fork-based parallelism causes issues because user processes are not fork-safe.  This moves our parallel compile work pool into a subprocess that we control (that should be fork safe).</p>
<p>Perf run: https://github.com/pytorch/pytorch/actions/runs/8486887873</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 20:34:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122941</guid>
    </item>
  </channel>
</rss>
