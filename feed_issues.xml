<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>get_instructions_bytes appears to be causing increased compile time</title>
      <link>https://github.com/pytorch/pytorch/issues/125314</link>
      <description><![CDATA[<p>Internal link: https://fb.workplace.com/groups/1075192433118967/permalink/1421189078519299/</p>
<p><code>get_instructions_bytes</code> was determined to cause a compile time regression - will need to repro locally first.</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Wed, 01 May 2024 10:00:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125314</guid>
    </item>
    <item>
      <title>torch.compile + constructing an nn.Parameter + mutating it can give wrong results</title>
      <link>https://github.com/pytorch/pytorch/issues/125284</link>
      <description><![CDATA[<p>Example repro:<br />
```<br />
import torch</p>
<p>def f(x):<br />
    y = x + 1<br />
    z = torch.nn.Parameter(y)<br />
    with torch.no_grad():<br />
        z.mul_(2)<br />
    return y + z</p>
<p>x = torch.ones(2, requires_grad=True)<br />
out_ref = f(x)<br />
out_test = torch.compile(f, backend='aot_eager')(x)<br />
print(out_ref)<br />
print(out_test)<br />
print(torch.allclose(out_ref, out_test))<br />
```</p>
<p>Right now, dynamo has logic to lift <code>nn.Parameter()</code> into a custom <code>autograd.Function</code>, <code>_bind_nn_parameter</code> (<a href="https://github.com/pytorch/pytorch/blob/1bcbc9158ff7093a7d8ff3304f70dcc5e6294326/torch/_dynamo/create_parameter_op.py#L30">here</a>) but this custom function removes the aliasing between y and z. If we mutate either y or z, then that mutation is not properly reflected between its alias.</p>
<p>This is difficult to fix in general with the current autograd.Function approach, because if we change the custom <code>autograd.Function</code> to alias its output, the autograd engine will error loudly during tracing when it sees the mutation.</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78</p>]]></description>
      <pubDate>Tue, 30 Apr 2024 16:31:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125284</guid>
    </item>
    <item>
      <title>WIP: [Inductor] log fusion failure due to loop orders</title>
      <link>https://github.com/pytorch/pytorch/pull/124986</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124986<br />
* #125091<br />
* #125090</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 16:25:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124986</guid>
    </item>
    <item>
      <title>PT2 Inductor ComboKernels</title>
      <link>https://github.com/pytorch/pytorch/pull/124969</link>
      <description><![CDATA[<p>Summary:<br />
A ComboKernel combines independent Inductor Triton kernels into a single one.<br />
Consolidation with Foreach kernel:<br />
1) For the scheduler node, the logic is consolidated into ForeachKernelSchedulerNode<br />
2) The backend kernel is consolidated into ComboKernel.</p>
<p>Example:<br />
- element wise kernels<br />
original Pytorch function:<br />
<code>def test_activations(a, b, c):
     a1 = torch.nn.functional.relu(a)
     b1 = torch.nn.functional.sigmoid(b)
     c1 = torch.nn.functional.tanh(c)
     return a1, b1, c1</code><br />
combokernel<br />
<code>triton_heuristics.pointwise(
    size_hints=[512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_poi_fused_0', 'mutated_arg_names': []}
)
triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, XBLOCK : tl.constexpr):
    pid = tl.program_id(0)
    if pid % 3 == 0:
        pid_offset = pid // 3
        xnumel = 100
        rnumel = 1
        xoffset = pid_offset * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex &lt; xnumel
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (x0), xmask)
        tmp1 = triton_helpers.maximum(0, tmp0)
        tl.store(out_ptr0 + (x0), tmp1, xmask)
    elif pid % 3 == 1:
        pid_offset = pid // 3
        xnumel = 400
        rnumel = 1
        xoffset = pid_offset * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex &lt; xnumel
        x1 = xindex
        tmp2 = tl.load(in_ptr1 + (x1), xmask)
        tmp3 = tl.sigmoid(tmp2)
        tl.store(out_ptr1 + (x1), tmp3, xmask)
    elif pid % 3 == 2:
        pid_offset = pid // 3
        xnumel = 100
        rnumel = 1
        xoffset = pid_offset * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex &lt; xnumel
        x2 = xindex
        tmp4 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = libdevice.tanh(tmp4)
        tl.store(out_ptr2 + (x2), tmp5, xmask)
    else:
        pass</code><br />
- reduction kernels<br />
Original Pytorch function:<br />
<code>def test_reduce(a, b, c):
     a1 = torch.sum(a, dim=0)
     b1 = torch.max(b, dim=0)
     c1 = torch.min(c, dim=0)
     return a1, b1, c1</code><br />
Generated combokernal:<br />
<code>triton_heuristics.persistent_reduction(
     size_hints=[32, 32],
     reduction_hint=ReductionHint.DEFAULT,
     filename=__file__,
     triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*i64', 5: '*fp32', 6: '*i64', 7: '*fp32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=())]},
     inductor_meta={'kernel_name': 'triton_per_fused_0', 'mutated_arg_names': []}
 )
 triton.jit
 def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, out_ptr3, out_ptr4, XBLOCK : tl.constexpr):
     pid = tl.program_id(0)
     if pid % 3 == 0:
         pid_offset = pid // 3
         xnumel = 20
         rnumel = 20
         RBLOCK_0: tl.constexpr = 32
         xoffset = pid_offset * XBLOCK
         xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
         xmask = xindex &lt; xnumel
         rindex = tl.arange(0, RBLOCK_0)[None, :]
         roffset = 0
         rmask = rindex &lt; rnumel
         r1 = rindex
         x0 = xindex
         tmp0 = tl.load(in_ptr0 + (x0 + (20*r1)), rmask &amp; xmask, other=0.0)
         tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK_0])
         tmp3 = tl.where(rmask &amp; xmask, tmp1, float("-inf"))
         tmp4 = triton_helpers.max2(tmp3, 1)[:, None]
         tmp6 = tl.broadcast_to(rindex, tmp3.shape)
         _, tmp5_tmp = triton_helpers.max_with_index(tmp3, tmp6, 1)
         tmp5 = tmp5_tmp[:, None]
         tl.store(out_ptr0 + (x0), tmp4, xmask)
         tl.store(out_ptr1 + (x0), tmp5, xmask)
     elif pid % 3 == 1:
         pid_offset = pid // 3
         xnumel = 10
         rnumel = 10
         RBLOCK_1: tl.constexpr = 16
         xoffset = pid_offset * XBLOCK
         xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
         xmask = xindex &lt; xnumel
         rindex = tl.arange(0, RBLOCK_1)[None, :]
         roffset = 0
         rmask = rindex &lt; rnumel
         r3 = rindex
         x2 = xindex
         tmp7 = tl.load(in_ptr1 + (x2 + (10*r3)), rmask &amp; xmask, other=0.0)
         tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK_1])
         tmp10 = tl.where(rmask &amp; xmask, tmp8, float("inf"))
         tmp11 = triton_helpers.min2(tmp10, 1)[:, None]
         tmp13 = tl.broadcast_to(rindex, tmp10.shape)
         _, tmp12_tmp = triton_helpers.min_with_index(tmp10, tmp13, 1)
         tmp12 = tmp12_tmp[:, None]
         tl.store(out_ptr2 + (x2), tmp11, xmask)
         tl.store(out_ptr3 + (x2), tmp12, xmask)
     elif pid % 3 == 2:
         pid_offset = pid // 3
         xnumel = 10
         rnumel = 10
         RBLOCK_2: tl.constexpr = 16
         xoffset = pid_offset * XBLOCK
         xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
         xmask = xindex &lt; xnumel
         rindex = tl.arange(0, RBLOCK_2)[None, :]
         roffset = 0
         rmask = rindex &lt; rnumel
         r5 = rindex
         x4 = xindex
         tmp14 = tl.load(in_ptr2 + (x4 + (10*r5)), rmask &amp; xmask, other=0.0)
         tmp15 = tl.broadcast_to(tmp14, [XBLOCK, RBLOCK_2])
         tmp17 = tl.where(rmask &amp; xmask, tmp15, 0)
         tmp18 = tl.sum(tmp17, 1)[:, None]
         tl.store(out_ptr4 + (x4), tmp18, xmask)
     else:
         pass</code></p>
<p>Note: ComboKernels uses masks to allow combination of kernels working with tensors of different sizes.</p>
<p>Test Plan:<br />
<code>buck2 test mode/dev-nosan caffe2/test/inductor:foreach</code><br />
<code>buck2 test mode/dev-nosan caffe2/test/inductor:combo_kernels</code></p>
<p>Differential Revision: D54134695</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 12:02:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124969</guid>
    </item>
    <item>
      <title>Nested wrapper subclasses with torch.compile is broken</title>
      <link>https://github.com/pytorch/pytorch/issues/124878</link>
      <description><![CDATA[<p>This came in in the FSDP2 workstream, which needs a DTensor that holds some sort of float8 tensor (cc @Chillee @ezyang @zou3519 @albanD @samdow @msaroufim @anijain2305 @chauhang @awgu / @drisspg ).</p>
<p>We should probably add general testing that nested wrapper subclasses work properly in compile, as part of fixing that issue. The first thing we should probably do is audit all of the places that we call <code>__tensor_flatten__</code> in dynamo/AOTAutograd, and make sure that we are recursively calling <code>__tensor_flatten__</code> when the inner tensors are also wrapper subclasses.</p>
<p>In particular, I tried a simple repro using TwoTensor's, and noticed that the gradients we produce with compile are wrong (this runs properly when you remove the compile decorator):<br />
```<br />
import torch<br />
from torch.testing._internal.two_tensor import TwoTensor</p>
<p>@torch.compile(backend='aot_eager')<br />
def f(x):<br />
    return x.sin().cos()</p>
<p>a = torch.ones(4, requires_grad=True)<br />
a2 = a.clone().detach().requires_grad_()<br />
aa = TwoTensor(a, a2)<br />
aa2 = aa.clone().detach().requires_grad_()<br />
aaaa = TwoTensor(aa, aa2)<br />
out = f(aaaa)<br />
print(type(out))<br />
print(type(out.a))<br />
print(type(out.b))<br />
print(type(out.a.a))<br />
print(type(out.a.b))<br />
print(type(out.b.a))<br />
print(type(out.b.b))<br />
out.sum().backward()</p>
<h1>aaaa.grad should be a TwoTensor(TwoTensor, TwoTensor)</h1>
<h1>but instead it is a TwoTensor(tensor, tensor)</h1>
<p>print(type(aaaa.grad))<br />
print(type(aaaa.grad.a))<br />
print(type(aaaa.grad.b))<br />
print(type(aaaa.grad.a.a))<br />
print(type(aaaa.grad.a.b))<br />
print(type(aaaa.grad.b.a))<br />
```</p>]]></description>
      <pubDate>Wed, 24 Apr 2024 12:08:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124878</guid>
    </item>
    <item>
      <title>[RFC] Add new CPP builder for inductor on pytorch Windows</title>
      <link>https://github.com/pytorch/pytorch/issues/124245</link>
      <description><![CDATA[<h3>🚀 The feature, motivation and pitch</h3>
<h1>Motivation</h1>
<p>Current torch inductor only support non-Windows OS. This RFC is proposal to add a new CPP builder which also support Windows OS.<br />
Firstly, we can list the gaps to enable inductor on Windows.<br />
1. Current CPP builder not support Windows cl compiler.<br />
2. Current ISA check use linux <code>proc file system</code>, which not support Windows also.</p>
<h1>Proposed Solution</h1>
<h2>CPP builder</h2>
<p>Current CPP builder is hard code to only support Linux likes OS, Windows has different build options and command line rules. We need design a abstract mechanism to adapt to both Windows and Linux likes OS(compiler).</p>
<h3>Build options</h3>
<h4>A. Categories:</h4>
<p>Compiler options genarally has same categories: <code>cflags</code>, <code>ldflags</code>, <code>definations</code>, <code>include_dirs</code>, <code>libraries_dirs</code>, <code>libraries</code> and some <code>_passthough_args</code>.</p>
<h4>B. Options prefix:</h4>
<p>Linux likes OS use <code>-</code> as prefix, but Windows use <code>/</code> as prefix.<br />
Example:<br />
Setup compiler optimization level as <code>O2</code>, on Windows should be <code>/O2</code>, but on Linux likes OS should be <code>-O2</code>. </p>
<h4>C. Common and especial options.</h4>
<p><strong><em>Common options:</em></strong><br />
|Options|Windows|Linux|<br />
|---|---|---|<br />
|Compile Optimization|/O2|-O2|<br />
|Add defination XXX|/D XXX|-D XXX|<br />
|Add cflag |/cflag|-cflag|<br />
|Add include dir|/I include_dir_path |-Iinclude_dir_path |</p>
<p>The common options has same build option between Windows and Linux, which is only different with prefix character. We can store the common options and then format to command as <code>prefix character</code> + <code>common option</code> in different OS.</p>
<p><strong><em>Especial options:</em></strong><br />
|Options|Windows|Linux|<br />
|---|---|---|<br />
|Enable AVX2|/arch:AVX2|-mavx2|<br />
|Enable AVX512|/arch:AVX512|-mavx512f -mavx512dq -mavx512vl -mavx512bw|</p>
<p>Especial options is different between Windows and Linux, we can pass them to command line via  <code>_passthough_args</code>.</p>
<h3>Command line rules</h3>
<h4>A. Parameters pass order</h4>
<p>Windows: <code>cl [ option... ] filename... [ /link linkoption... ]</code><br />
Linux: no parameters order requirement.<br />
Reference: <a href="https://learn.microsoft.com/en-us/cpp/build/walkthrough-compiling-a-native-cpp-program-on-the-command-line?view=msvc-170#open-a-developer-command-prompt">MSVC Cmdline</a></p>
<h4>B. Output Binary Path</h4>
<p>Windows: <code>/Fe BinaryName</code><br />
Linux: <code>-o BinaryPath</code><br />
Reference: <a href="https://learn.microsoft.com/en-us/cpp/build/reference/compiler-options-listed-by-category?view=msvc-170&amp;redirectedfrom=MSDN#output-files">MSVC Output</a></p>
<h4>C. MSVC Compiler options</h4>
<p>Please reference to this <a href="https://learn.microsoft.com/en-us/cpp/build/reference/compiler-options-listed-by-category?view=msvc-170">doc</a>.</p>
<h3>Build options orgnazation &amp; modularization</h3>
<h4>Modularization</h4>
<p>We have categories the options by types in above part. And in pytorch scenario, we can also modularization these build options. Such as <code>BuildOptionsBase</code>, <code>CppOptions</code>, <code>CppTorchOptions</code>, and <code>CppTorchCudaOptions</code>.<br />
These options will inherit build options level by level.<br />
|modularization name|inherit to|Options categories |<br />
|---|---|---|<br />
|BuildOptionsBase| NA(base implemention) | NA |<br />
|CppOptions| BuildOptionsBase | shared_lib, cc_optimze(O2), cpp_std, warning_flag|<br />
|CppTorchOptions| CppOptions | cpp_wrapper_defination, custom_generated_macros, torch_includes_and_libs, openmp|<br />
|CppTorchCudaOptions| CppTorchOptions| cuda_incs_and_libs|</p>
<p>Note: we can extend for more devices on demand, example to <code>xpu</code> and <code>mps</code>.<br />
|modularization name|inherit to|Options categories |<br />
|---|---|---|<br />
|CppTorchXpuOptions| CppTorchOptions| xpu_incs_and_libs|<br />
|CppTorchMpsOptions| CppTorchOptions| mps_incs_and_libs|</p>
<h4>Orgnazation</h4>
<p>We can use python class to organize the inherit relations.</p>
<h3>Cross OS CppBuilder</h3>
<p>It will format the compiler command line from the build options, for both Windows and Linux.</p>
<h3>Class Diagram</h3>
<p><img alt="classes_cpp_builder" src="https://github.com/pytorch/pytorch/assets/8433590/ca585173-aad3-4e01-b2e9-57128d47a4e9" /></p>
<h2>ISA checker</h2>
<ol>
<li>Export a pybind ISA checker from backend cpuinfo.</li>
<li>Switch read /proc/cpuinfo method to new pybind functions.</li>
</ol>
<h2>Implement Plan</h2>
<h3>Step 1:</h3>
<ol>
<li>Add cpp_builder code, the new cpp_builder support Windows OS.</li>
<li>Add CPU ISA checker which is cross OS and exported from backend cpuinfo.</li>
<li>Switch compiler ISA checker to new cpp_builder.</li>
<li><code>CppCodeCache</code> use the new ISA checker.</li>
</ol>
<h3>Step 2:</h3>
<ol>
<li>Switch <code>AotCodeCompiler</code> and <code>CppCodeCache</code> to new CPP builder.</li>
<li>Remove old cpp builder code.</li>
<li>Add Windows inductor UTs.</li>
</ol>
<h3>Step 3:</h3>
<ol>
<li>Fix issue for <code>fb_code</code>, which need Meta employee help on.</li>
</ol>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @vladimir-aubrecht @iremyux @Blackhex @cristianPanaite @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 17:55:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124245</guid>
    </item>
    <item>
      <title>[1/N] Non-Tensor: Scalar Support: Enable aot compile to support aten operations with scalar input like alpha</title>
      <link>https://github.com/pytorch/pytorch/pull/124177</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #125308<br />
* #124926<br />
* #124070<br />
* <strong>-&gt;</strong> #124177<br />
* #116368<br />
* #124836</p>
<p>Some operations have a scalar input parameter, like <code>torch.add(a, b, alpha=2.0)</code>.  Currently, the aot compile does not support such a case because it requires the signature of the captured graph to align with the operation's signature. This means that some inputs in the captured graph may be scalar(float, int, bool, etc.). It breaks the assumption of <code>compile_fx_aot</code> as it assumes all the example inputs are tensor - https://github.com/pytorch/pytorch/blob/0f6ce45bcbd7026c00da43db0317ede10830378b/torch/_inductor/compile_fx.py#L1048</p>
<p>This PR intends to support such cases by allowing not-aligned signature and filtering out the non-Tensor parameters. </p>
<p>Captured graph for <code>torch.add(a, b, alpha=2.0)</code></p>
<p>```<br />
opcode         name      target           args              kwargs</p>
<hr />
<p>placeholder    arg0_1    arg0_1           ()                {}<br />
placeholder    arg1_1    arg1_1           ()                {}<br />
call_function  add       aten.add.Tensor  (arg0_1, arg1_1)  {'alpha': 2.0}<br />
output         output_1  output           ((add,),)         {}<br />
```</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 07:05:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124177</guid>
    </item>
    <item>
      <title>[2/N] Non-Tensor: Scalar Support: Add scalar to the cache for eager-through-torch.compile</title>
      <link>https://github.com/pytorch/pytorch/pull/124070</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #125308<br />
* #124926<br />
* <strong>-&gt;</strong> #124070<br />
* #124177<br />
* #116368<br />
* #124836</p>
<p>Add scalar information to the kernel configuration.</p>
<h4>Additional Context</h4>
<p>Currently, the input parameters are orchestrated by input order in the kernel configuration and loaded/mapped to the kernel at runtime. For example, the cache order of the input parameters of <code>torch.add(a, b, alpha=2.0)</code> is <code>a' first, followed by</code>b<code>and then</code>alpha`. The same order is for cache loading.</p>
<p>However, the orchestration mechanism does not support kwargs because the order of kwargs is useless. For example, the <code>out</code> of <code>aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -&gt; Tensor(a!)</code> may be before <code>approximate</code>. We will support it with subsequent PRs. </p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 07:04:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124070</guid>
    </item>
    <item>
      <title>[WIP][inductor] refine loop split logic</title>
      <link>https://github.com/pytorch/pytorch/pull/124060</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124060</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 05:52:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124060</guid>
    </item>
    <item>
      <title>Add a cache mechanism to accelerate torch.compile-for-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/116368</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #125308<br />
* #124926<br />
* #124070<br />
* #124177<br />
* <strong>-&gt;</strong> #116368<br />
* #124836</p>
<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to enable a cache mechanism to accelerate <strong>eager-through-torch.compile</strong>. When <strong>eager-through-torch.compile</strong> is enabled, we will store a persistent config to cache the kernel information for the aten operation. </p>
<p>The persistent config consists of two parts - meta_info and kernel_path.</p>
<ul>
<li>meta_info: The input tensors' shape, stride, device type, data type, and symbolic flag.</li>
<li>kernel_path: The path of the kernel produced by Inductor.</li>
</ul>
<p>When an aten operation is registered, the <code>kernel_holder</code> will load the persistent config and parse it to build the cache map;  the meta_info is key, and the kernel library is the value.</p>
<p>Currently, this PR only supports static shape to guard the kernel.</p>
<p>Take a <code>mul</code> as an example.<br />
```python<br />
class MulKernel:<br />
  def <strong>init</strong>(self) -&gt; None:<br />
    pass</p>
<p>def <strong>call</strong>(self, <em>args: Any, </em><em>kwargs: Any) -&gt; Any:<br />
    with torch._C._SetExcludeDispatchKeyGuard(torch._C.DispatchKey.Python, False):<br />
      opt_fn = torch.compile(torch.ops.aten.mul, dynamic=False, options={<br />
          "aot_inductor.eager_mode": True,<br />
          "aot_inductor.eager_op_name": "mul_Tensor"<br />
        }<br />
      )<br />
      return opt_fn(</em>args, **kwargs)</p>
<p>torch_compile_op_lib_impl = torch.library.Library("aten", "IMPL")</p>
<p>_, overload_names = torch._C._jit_get_operation("aten::mul")<br />
schema = torch._C._get_schema("aten::mul", overload_name)<br />
reg_name = schema.name<br />
if schema.overload_name:<br />
  reg_name = f"{reg_name}.{schema.overload_name}"<br />
  torch_compile_op_lib_impl.impl(<br />
    reg_name,<br />
    MulKernel(),<br />
    "CUDA",<br />
    compile_mode=True)</p>
<p>a = torch.randn(1024, 1024, device=device)<br />
b = torch.randn(1024, 1024, device=device)<br />
warm_up_iter = 1000<br />
iter = 10000<br />
fn = torch.mul</p>
<h1>Warm up</h1>
<p>for _ in range(warm_up_iter):<br />
    fn(a, b)</p>
<h1>Collect performance</h1>
<p>beg = time.time()<br />
for _ in range(iter):<br />
    fn(a, b)<br />
end = time.time()<br />
print(f"E2E run: {end - beg}")<br />
```<br />
It will produce the config as follows.</p>
<p><code>json
[
    {
        "meta_info": [
            {
                "is_symbolic": false,
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": [1024, 1024],
                "strides": [1024, 1]
            },
            {
                "is_symbolic": false,
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": [1024, 1024],
                "strides": [1024, 1]
            }
        ],
        "kernel_path": "/tmp/torchinductor_eikan/e4/ce4jw46i5l2e7v3tvr2pyglpjmahnp7x3hxaqotrvxwoeh5t6qzc.so"
    }
]</code></p>
<p>Performance-wise, we collected mul.Tensor through torch.compile w/ 10000 runs(e2e). The data is as follows. And we will collect data when we support dynamic shape.</p>
<ul>
<li>Eager: ~266.11ms</li>
<li>W/O Cache: ~3455.54ms</li>
<li>W/ Cache and Cache Miss: ~3555.3ms</li>
<li>W/ Cache and Cache Hit: ~267.12ms</li>
</ul>
<p>Hardware:</p>
<ul>
<li>CPU: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz</li>
<li>GPU: CUDA A10</li>
</ul>
<p>Software:</p>
<ul>
<li>PyTorch Version: 39df084001c54cca5fe3174176f9b0206ddb7dcf</li>
<li>GPU Driver Version: 525.147.05</li>
<li>CUDA Version: 12.0</li>
</ul>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 03:21:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116368</guid>
    </item>
    <item>
      <title>[RFC] Intel GPU Inductor backend upstreaming</title>
      <link>https://github.com/pytorch/pytorch/issues/114856</link>
      <description><![CDATA[<h3>🚀 The feature, motivation and pitch</h3>
<h3>Motivation</h3>
<p>As the RFC <a href="https://github.com/pytorch/pytorch/issues/114723">Intel GPU Upstreaming</a> mentioned, to ensure <code>torch.compile</code> to support Intel GPU, it’s crucial to provide the Intel GPU backend for Inductor.</p>
<h3>Design</h3>
<p>The existing Triton backend within Inductor already supports GPUs, including our integration enabling Triton to function with Intel GPUs. Consequently, extending Inductor to include support for Intel GPUs becomes a streamlined process by leveraging the foundation of the current Triton backend. Only minimal code design and changes would be required in the Inductor codebase itself to add Intel GPU support. The design or changes contain 3 components as follows:<br />
1.  Backend registration.<br />
2.  Code generation.<br />
3.  Graph fusion.</p>
<h4>1. Backend registration</h4>
<p><img alt="image" src="https://github.com/intel-innersource/frameworks.ai.pytorch.private-gpu/assets/2347459/cd85c770-12bb-4744-aa2e-eb99a47d8a3f" /></p>
<p>Inductor has provided a clear and simple mechanism for backend integration by registering two essential classes at runtime - <code>BaseScheduling</code> and <code>WrapperCodegen</code>:<br />
- <code>BaseScheduling</code> is the interface for kernel code generation.<br />
- <code>WrapperCodegen</code> for generating wrapper code that glue, compile, and bench the kernel. </p>
<p>For Intel GPU backend, we will reuse <code>WrapperCodegen</code> and <code>TritonScheduling</code> as we have enabled Triton to support Intel GPU, and then register to Inductor through the <code>register_backend_for_device</code> Inductor interface.</p>
<p><code>python
register_backend_for_device("xpu", TritonScheduling, WrapperCodeGen)</code></p>
<h4>2. Codegen</h4>
<p>From the design perspective, <code>WrapperCodegen</code> and <code>TritonScheduling</code> are device-agnostic to generate Python wrapper code and Triton kernel code. Regarding the detailed implementation, there are still some device-bias codes. For instance, WrapperCodegen invariably embeds the <a href="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/codegen/wrapper.py#L460"><code>synchronize()</code></a> literal directly into the kernel code, irrespective of whether the current device backend actually supports this feature.<br />
To address these frictions, we intend to provide a general design for device backend specific implementation. We would like to abstract the device backend bias code generation into a common class interface like <code>DeviceOpOverrides</code> and leave the flexibility to the device backends in their inherited class. Shown in the following graph:</p>
<p><img alt="image" src="https://github.com/pytorch/pytorch/assets/2347459/1a618f8a-aa5a-42a1-8a65-a9d600ff0a38" /></p>
<p>Besides the device bias codes in the generated wrapper and kernel code, Inductor relies on some particular runtime functions per device during generation process. Take <code>CachingAutotuner</code> in triton_heuristic.py as an example, it invokes<a href="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/triton_heuristics.py#L303"> <code>current_device()</code></a> and <a href="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/triton_heuristics.py#L303"><code>synchronize()</code></a> directly by hard-coding device type. <br />
Since the<a href="https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/device_interface.py#L34"> general device interface</a> has been ready on Dynamo side, it’s straightforward to generalize the device-specific code by utilizing the general device interface. The devices biases code can be revised as:</p>
<p><code>python
torch_device = get_interface_for_device(device_type) 
with torch_device.device(compile_meta["device"]):
        torch_device.synchronize(torch_device.current_device())</code></p>
<h4>3. Graph fusion</h4>
<p>We will extend the CPU fusion patterns defined in <code>fx_passes/mkldnn_fusion.py</code> with minimal code change and reuse them for Intel GPU. </p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @frank-wei @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Thu, 30 Nov 2023 00:25:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/114856</guid>
    </item>
    <item>
      <title>[RFC] Building system for SYCL and limited number of SYCL kernels for ATen fallbacks of TorchInductor</title>
      <link>https://github.com/pytorch/pytorch/issues/114835</link>
      <description><![CDATA[<h1>Background</h1>
<p>We are upstreaming for Intel GPU (<a href="https://github.com/pytorch/pytorch/issues/114723">[RFC] Intel GPU Upstreaming · Issue #114723 · pytorch/pytorch (github.com)</a>). For the first step, targeting recent popular and typical DL workloads, we plan to enable Intel GPU backend by using <code>torch.compile(backend=Inductor)</code> on HuggingFace, TIMM, and TorchBench. Thus, main PRs will cover Intel GPU runtime, Intel GPU backend for TorchInductor (Triton) and ATen fallbacks of TorchInductor (oneDNN integration and limited number of SYCL kernels).</p>
<h1>Motivation</h1>
<p>Where the RFC is,<br />
<img width="458" alt="image" src="https://github.com/pytorch/pytorch/assets/28250760/1b836fff-a337-45d8-9e44-79e29271b7b6"><br />
The RFC focuses on introducing SYCL kernels to PyTorch. In addition to limited number of SYCL kernels, we will introduce building system for SYCL as well. Here is summary of RFC,<br />
1. Building system for SYCL. The design of setups is aligned as existing building system for CUDA.<br />
2. Limited number of SYCL kernels will be added for fallbacks of TorchInductor. We collected TorchInductor fallback ATen operators and corresponding kernels by going through HuggingFace, TIMM and TorchBench models. Minimal set of ATen operators (about 8 SYCL kernel templates) will be added, including elementwise, reduce, random, concat, scan, indexing, sort and arange.</p>
<h1>Feature Details</h1>
<h3>Building system for SYCL</h3>
<p>Building and running SYCL kernels depends on,<br />
1. Intel Graphics driver installation.<br />
2. Intel DPCPP compiler/compiler-rt tool kit installation (an Intel SYCL compiler and runtime).<br />
<img width="461" alt="image" src="https://github.com/pytorch/pytorch/assets/28250760/821cd94f-461f-44cd-b590-aea8320fb584"></p>
<p>The design of building system for SYCL is aligned with existing building system for CUDA in PyTorch. But in the RFC, targeting SYCL kernel compilation only, compared with setups in building system for CUDA, we will have part of them. Other components will be added gradually according to what Intel GPU backend will need in future. Building system for SYCL in PyTorch will include,<br />
1. Compiler tool chain version detection.<br />
    - To verify SYCL compiler version and SYCL includes version. These two versions should be same. <br />
    - To define macro for source code usage to isolate incompatible SYCL host/device APIs or intrinsic among different compiler versions.<br />
2. Host compiler setup.<br />
    - ABI compatib ility. At the first stage, PyTorch built with Intel GPU backend (will use <code>USE_XPU</code> as building flag for Intel GPU) have to be built with ABI=1, due to ABI=0 still not supported in DPCPP runtime library. We will timely uplift DPCPP runtime library and retrieve the compilation option to align with other PyTorch building targets after DPCPP runtime library supports it (planed timeline: before PyTorch 2.5 cut out).<br />
    - Requirement of host compiler. We will target GCC as 3rd party host compiler only before PyTorch2.5. Align (or won’t enlarge restriction) with existing PyTorch compiler versions.<br />
3. Architecture detection for AOT compilation.<br />
   - From source: enable AOT build for Intel Data Center Max 1550/1450/1100 GPU by default.<br />
   - For release binary: enable AOT build for Intel Data Center Max 1500/1450/1100 GPU only.</p>
<h3>SYCL kernels for TorchInductor ATen fallback</h3>
<p>Additions come from three parts,<br />
1. Add ATen operators’ registration for PyTorch XPU dispatch key.<br />
    - The list of ATen operators is collected by going through HuggingFace, TIMM and TorchBench models with <code>torch.compile(backend=inductor)</code>.<br />
    - Registration strategy. There are two common ways to register backend for ATen operators.<br />
        - Mark backend for operators in native_functions.yaml. Source code of registration will be generated automatically.<br />
        - Not use native_functions.yaml. Write source code for registration with ATen registration API (Like <code>TORCH_LIBRARY_IMPL(aten, XPU, m)</code>) in operators’ implementation source files.<br />
    - Regarding minimal changing and impact, for current stage, we will propose the second one.<br />
2. Add corresponding SYCL kernel implementation. Multiple ATen operators can share same SYCL kernel. Kernel generalization aligns with CUDA kernels. Generalization for,<br />
    - Operator semantics. (E.g. broadcast, transposed in elementwise.)<br />
    - Data type. (PyTorch defined data type required.)<br />
    - Function generalization. (E.g. sum/mean/var share same reduce backbone.) <br />
3. Add unit test cases accordingly. Reuse as-is.<br />
Each following PRs targets a SYCL kernel, and accordingly includes ATen operator registration and unit test cases. Here is ‘SYCL kernels - ATen operators’ correspondence table,<br />
<img width="433" alt="image" src="https://github.com/pytorch/pytorch/assets/28250760/a8d78949-d8a9-46d2-9a8f-f0a2f3c91c97"></p>
<h3>TODO PRs list [PR num]</h3>
<ul>
<li>[ ] Building system for SYCL: [ ]</li>
<li>[ ] Elementwise (Loops): [ ]</li>
<li>[ ] Reduce: [ ]</li>
<li>[ ] Random: [ ]</li>
<li>[ ] Indexing: [ ]</li>
<li>[ ] Concat: [ ]</li>
<li>[ ] Scan: [ ]</li>
<li>[ ] Sort: [ ]</li>
<li>[ ] Arange: [ ]</li>
</ul>
<p>cc @frank-wei @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Wed, 29 Nov 2023 19:44:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/114835</guid>
    </item>
    <item>
      <title>RFC: Integrating oneDNN Graph Compiler into Inductor C++/OpenMP Backend for Enhanced Graph Fusion and Performance</title>
      <link>https://github.com/pytorch/pytorch/issues/105582</link>
      <description><![CDATA[<h3>🚀 The feature, motivation and pitch</h3>
<p>Integrating oneDNN Graph Compiler into Inductor C++ Backend enables enhanced pattern fusion and performance for CPU. <br />
<img src="https://github.com/pytorch/pytorch/assets/19395079/47275ec4-f7ea-425b-9656-97a6a33b1844" alt="design" width="500"/></p>
<h3>Motivation</h3>
<p>Recent developments on the Inductor C++ backend have demonstrated promising performance on DL inference workloads with CPU, thanks to optimizations like Conv/GEMM + post-op fusions and vectorization (see <a href="https://dev-discuss.pytorch.org/t/Inductor-update-4-cpu-backend-started-to-show-promising-performance-boost/874">this</a> and <a href="https://dev-discuss.pytorch.org/t/torchinductor-update-5-cpu-backend-backend-performance-update-and-deep-dive-on-key-optimizations/1117">this</a>). </p>
<p><a href="https://spec.oneapi.io/onednn-graph/latest/introduction.html">oneDNN Graph API</a> (codename LLGA) extents oneDNN with a high-level graph API. It goes beyond Conv/GEMM post-op fusions and supports <a href="http://oneapi-src.github.io/oneDNN/dev_guide_graph_fusion_patterns.html#aggressive-fusion-patterns">aggressive fusion patterns</a> such as MultiheadAttention, MLP blocks, and more (with its <a href="http://oneapi-src.github.io/oneDNN/dev_guide_graph_compiler.html">graph compiler backend</a>). Other features include  <a href="http://oneapi-src.github.io/oneDNN/dev_guide_graph_low_precision.html">low precision</a>. Since PyTorch 1.12, this API has been added in <a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-onednn-graph-with-torchscript-for-inference">TorchScript JIT fuser path</a> showing promising performance <a href="https://github.com/pytorch/pytorch/issues/49444">#49444</a>.  </p>
<p>Integrating the oneDNN Graph Compiler with Inductor C++ backend offers further performance enhancements. Additionally, adopting the oneDNN Graph API simplifies design and development.</p>
<h3>Plan</h3>
<p>Our long-term goal is to use oneDNN Graph fusion by default, replacing post-op fusions. Starting as an experimental feature, we would add a <code>onednn_graph_fusion</code> pass into the Inductor post-graph passes, enabled by an inductor.cpp config option. We hope it will eventually become the default option after validation of no performance regression from Inductor C++ backend.</p>
<p>In PyTorch 2.3, oneDNN Graph Inductor pass would be experimental, and would support FP32 &amp; BF16 (with AMP).<br />
PR - #120301<br />
In PyTorch 2.4, we would add int8 support, as Quantization 2.0 would have matured by then.<br />
In PyTorch 2.5, we may add support for generic dynamic-shape-supporting kernels, if oneDNN would have added their support by then.</p>
<h3>Implementation</h3>
<h4>Inductor joint graph passes</h4>
<p>We introduce the <code>onednn_graph_fusion</code> pass directly takes the FX graph from AOTAutograd as input. The FX graph is used to construct an LLGA Graph by lowering aten/prims IR to LLGA IR and by lowering <code>FakeTensor</code> to LLGA <code>LogicalTensor</code>. LLGA identifies fusion opportunities in the Graph and returns a list of LLGA <code>partition</code>s, each represents a set of fused operations. </p>
<p>To enable the desired fusion, we use the Inductor pattern-matcher to replacing a pattern with a target fusion, which calls an ATen op, such as <code>torch.ops.mkldnn._graph_sdpa_pattern_1</code>:</p>
<p><img width="1069" alt="image" src="https://github.com/pytorch/pytorch/assets/90872293/7991c50c-29df-4a50-846d-1053196b9217"></p>
<h3>User interface:</h3>
<p><code>torch.compile(options={"cpp.onednn_graph": True})</code></p>]]></description>
      <pubDate>Wed, 19 Jul 2023 10:53:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/105582</guid>
    </item>
  </channel>
</rss>
