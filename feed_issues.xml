<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[AOTInductor, SymbolicShape] Pass shape_env.var_to_range to BoundVars</title>
      <link>https://github.com/pytorch/pytorch/pull/121832</link>
      <description><![CDATA[<p>Summary:<br />
In production's setup, AOTInductor would compile an ExportedProgram deserialized from the archive. In such case, <code>torch._guards.TracingContext</code> is not available. However, AOTI is heavily relying on <code>TracingContext</code> in its current implementation. </p>
<p>For example, as shown in this diff, <code>BoundVars</code> is reading from <code>TracingContext</code> to get the shape_env's <code>var_to_range</code> field. </p>
<p>As a workaround, I am passing V.graph.shape_env.var_to_range to <code>BoundVars</code> instead.</p>
<p>Test Plan: buck2 test  mode/dev-nosan caffe2/test/inductor/fb:test_aot_inductor_pt2_inference</p>
<p>Differential Revision: D54563608</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 09:34:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121832</guid>
    </item>
    <item>
      <title>`fresh_inductor_cache()` does not work due to functools.lru_cache</title>
      <link>https://github.com/pytorch/pytorch/issues/121831</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>If you write unit test with <code>fresh_inductor_cache()</code>, this does not actually use a fresh cache since this function updates <code>TORCHINDUCTOR_CACHE_DIR</code> however, the <code>cache_dir()</code> function that reads this global variable has a <code>functools.lru_cache</code> so the updated value never gets read.</p>
<p>Unfortunately, calling cache_dir.cache_clear is not enough as there are many more functools.lru_caches as well as codecache.py class member variables that cache the path to tmp dir.</p>
<h3>Versions</h3>
<p>main</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 09:29:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121831</guid>
    </item>
    <item>
      <title>Log graph break reasons and wasted compile time in CompilationMetrics</title>
      <link>https://github.com/pytorch/pytorch/pull/121827</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121827</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 08:52:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121827</guid>
    </item>
    <item>
      <title>Fix torch.compile links</title>
      <link>https://github.com/pytorch/pytorch/pull/121824</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121824<br />
* #121823</p>
<p>Fixes https://github.com/pytorch/pytorch.github.io/issues/1567</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 08:04:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121824</guid>
    </item>
    <item>
      <title>Workaround for nvrtcCompileProgram changing locale</title>
      <link>https://github.com/pytorch/pytorch/pull/121822</link>
      <description><![CDATA[<p>There is a bug in CUDA 11.7 through at least CUDA 12.4 which changes the current thread locale when calling nvrtcCompileProgram.<br />
See e.g. https://stackoverflow.com/questions/74044994 This also includes the encoding used by Python by default for e.g. subsequent invocations of <code>subprocess</code> calls. When the user environment is now set to e.g. UTF-8 and changed by CUDA (to ASCII/ANSI_X3.4-1968) Python will fail to decode UTF-8 output from programs invoked.<br />
This happens e.g. in <code>test_torch</code> which calls <code>from scipy import stats</code> which runs <code>lscpu</code> and errors with something like</p>
<blockquote>
<p>UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 96: ordinal not in range(128)</p>
</blockquote>
<p>Fix by wrapping the nvrtcCompileProgram saving and restoring the thread locale.</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 07:47:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121822</guid>
    </item>
    <item>
      <title>[FSDP2] Disabled compile for  `pre_forward`</title>
      <link>https://github.com/pytorch/pytorch/pull/121816</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120952<br />
* #121680<br />
* <strong>-&gt;</strong> #121816<br />
* #121747</p>
<p>Without this, we can see some tracing errors when tracing <code>pre_forward</code> for FSDP2 + SP.</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 07:18:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121816</guid>
    </item>
    <item>
      <title>[Inductor] Fix for WrapperCodeGen.statically_known_int_or_none</title>
      <link>https://github.com/pytorch/pytorch/pull/121808</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121808</p>
<p>There's obviously a small typo in WrapperCodeGen.statically_known_int_or_none,<br />
where the return value of a call to V.graph._shape_env._maybe_evaluate_static<br />
is being discarded.</p>
<p>This fix changes that to work how it was likely intended to.</p>
<p>Test Plan:<br />
CI</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 05:33:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121808</guid>
    </item>
    <item>
      <title>[dynamo] Compile time optimizations in tx.step()</title>
      <link>https://github.com/pytorch/pytorch/pull/121790</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121810<br />
* <strong>-&gt;</strong> #121790</p>
<p><code>python benchmarks/dynamo/microbenchmarks/dynamo_microbenchmarks.py</code><br />
- Before: <code>symbolic_convert_overhead_stress_test: 10.7s</code><br />
- After: <code>symbolic_convert_overhead_stress_test: 8.6s</code></p>
<p><code>tx.step()</code> is a small part of that benchmark, so likely the speedup in that isolated function is larger than the top line.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 21:22:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121790</guid>
    </item>
    <item>
      <title>[cpu] Modify inductor opt flag --- ftree-loop-vectorize</title>
      <link>https://github.com/pytorch/pytorch/pull/121782</link>
      <description><![CDATA[<p>Fixes #115261, #113017.</p>
<p>For CPU inductor path, remove <code>-funsafe-math-optimizations</code> from optimization flags to fix functional issues.</p>
<h3>Validation on 3 benchmark suites</h3>
<h4>FP32</h4>
<p>WIP</p>
<h4>BF16</h4>
<p>WIP</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 18:46:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121782</guid>
    </item>
    <item>
      <title>[Export, AOTInductor] Populate ShapeEnv's var_to_val during deserialization</title>
      <link>https://github.com/pytorch/pytorch/pull/121759</link>
      <description><![CDATA[<p>Summary:<br />
Deserialization didn't populate ShapeEnv's <code>var_to_val</code> field properly, and AOTInductor is relying on this field to compile dynamic shape properly. <br />
As a result, when AOTI failed at compiling a deserialized ExportedProgram.</p>
<p>Test Plan: buck2 test  mode/dev-nosan caffe2/test/inductor/fb:test_aot_inductor_pt2_inference</p>
<p>Differential Revision: D54559494</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 13:48:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121759</guid>
    </item>
    <item>
      <title>[Inductor] Update the cpp_wrapper entry function signature</title>
      <link>https://github.com/pytorch/pytorch/pull/121745</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121745<br />
* #121744<br />
* #121743<br />
* #121523</p>
<p>Summary: Update the entry function to use AtenTensorHandle instead of at::Tensor. This makes the compilation of the generated cpp wrapper code much faster: test_cpu_cpp_wrapper.py from 35 min to 21 min, and test_cuda_cpp_wrapper.py from 21 min to 14 min.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54818715">D54818715</a></p>]]></description>
      <pubDate>Tue, 12 Mar 2024 11:20:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121745</guid>
    </item>
    <item>
      <title>[Compiled Autograd] Reorder accumulate grad nodes</title>
      <link>https://github.com/pytorch/pytorch/pull/121735</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121610<br />
* #120965<br />
* <strong>-&gt;</strong> #121735</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 09:32:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121735</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Add bmm support</title>
      <link>https://github.com/pytorch/pytorch/pull/121734</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* <strong>-&gt;</strong> #121734<br />
* #121497<br />
* #121491<br />
* #121490<br />
* #121489</p>
<p>Add support for bmm ( batch matrix multiply ) op through the Cutlass backend.</p>
<p>Test Plan:<br />
 * CI<br />
 * Added test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 09:23:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121734</guid>
    </item>
    <item>
      <title>Enable FX graph caching in another batch of inductor tests</title>
      <link>https://github.com/pytorch/pytorch/pull/121697</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121697</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 17:25:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121697</guid>
    </item>
    <item>
      <title>Enable FX graph cache for a batch of inductor tests</title>
      <link>https://github.com/pytorch/pytorch/pull/121696</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121696</p>
<p>Summary: Get more FX graph cache coverage by enabling it for these unit tests</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 17:25:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121696</guid>
    </item>
    <item>
      <title>Warning static library kineto_LIBRARY-NOTFOUND not found when trying build tutorial torch.compiler_aot_inductor.html</title>
      <link>https://github.com/pytorch/pytorch/issues/121668</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I observe following error when try to follow this tutorial:<br />
https://pytorch.org/docs/main/torch.compiler_aot_inductor.html</p>
<p>Here is the output log:</p>
<p>```<br />
CMAKE_PREFIX_PATH=/home/atalman/miniconda3/envs/py310/lib/python3.10/site-packages/torch/share/cmake cmake ..<br />
-- The C compiler identification is GNU 9.4.0<br />
-- The CXX compiler identification is GNU 9.4.0<br />
-- Detecting C compiler ABI info<br />
-- Detecting C compiler ABI info - done<br />
-- Check for working C compiler: /usr/bin/cc - skipped<br />
-- Detecting C compile features<br />
-- Detecting C compile features - done<br />
-- Detecting CXX compiler ABI info<br />
-- Detecting CXX compiler ABI info - done<br />
-- Check for working CXX compiler: /usr/bin/c++ - skipped<br />
-- Detecting CXX compile features<br />
-- Detecting CXX compile features - done<br />
-- Found CUDA: /usr/local/cuda (found version "12.1") <br />
-- The CUDA compiler identification is NVIDIA 12.1.105<br />
-- Detecting CUDA compiler ABI info<br />
-- Detecting CUDA compiler ABI info - done<br />
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped<br />
-- Detecting CUDA compile features<br />
-- Detecting CUDA compile features - done<br />
-- Found CUDAToolkit: /usr/local/cuda/include (found version "12.1.105") <br />
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD<br />
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed<br />
-- Looking for pthread_create in pthreads<br />
-- Looking for pthread_create in pthreads - not found<br />
-- Looking for pthread_create in pthread<br />
-- Looking for pthread_create in pthread - found<br />
-- Found Threads: TRUE<br />
-- Caffe2: CUDA detected: 12.1<br />
-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc<br />
-- Caffe2: CUDA toolkit directory: /usr/local/cuda<br />
-- Caffe2: Header version is: 12.1<br />
-- /usr/local/cuda/lib64/libnvrtc.so shorthash is b51b459d<br />
-- USE_CUDNN is set to 0. Compiling without cuDNN support<br />
-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support<br />
-- Autodetected CUDA architecture(s):  8.0 8.0 8.0 8.0 8.0 8.0 8.0 8.0<br />
-- Added CUDA NVCC flags for: -gencode;arch=compute_80,code=sm_80<br />
CMake Warning at /home/atalman/miniconda3/envs/py310/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):<br />
  static library kineto_LIBRARY-NOTFOUND not found.<br />
Call Stack (most recent call first):<br />
  /home/atalman/miniconda3/envs/py310/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:127 (append_torchlib_if_found)<br />
  CMakeLists.txt:4 (find_package)</p>
<p>-- Found Torch: /home/atalman/miniconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libtorch.so<br />
-- Configuring done<br />
-- Generating done<br />
-- Build files have been written to: /data/home/atalman/aot_ind/build<br />
```</p>
<p>However looking at the Wheel build log here:<br />
https://github.com/pytorch/pytorch/actions/runs/8229185939/job/22499908747</p>
<p>I do see we include kineto in the build:<br />
<code>Configuring Kineto dependency:
2024-03-11T07:42:39.8183621Z --   KINETO_SOURCE_DIR = /pytorch/third_party/kineto/libkineto
2024-03-11T07:42:39.8184187Z --   KINETO_BUILD_TESTS = OFF
2024-03-11T07:42:39.8184596Z --   KINETO_LIBRARY_TYPE = static</code></p>
<p>Without the correct settings people won't be able to use profiling in CPP code. Our nightlies and release binaries should generate the correct settings.</p>
<p>cc @seemethere @malfet @osalpekar @jbschlosser @chauhang </p>
<h3>Versions</h3>
<p>2.3.0</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 12:55:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121668</guid>
    </item>
    <item>
      <title>[export] Remove aot_compile in benchmarks</title>
      <link>https://github.com/pytorch/pytorch/pull/121643</link>
      <description><![CDATA[<p>Fixes #ISSUE_NUMBER</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 09:26:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121643</guid>
    </item>
    <item>
      <title>[Inductor] [ReImplement] Outer Loop Fusion for CPP Backend</title>
      <link>https://github.com/pytorch/pytorch/pull/121625</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121625</p>
<p><strong>Summary</strong><br />
Re-implement of https://github.com/pytorch/pytorch/pull/121064</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_outer_loop_fusion</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 10 Mar 2024 23:44:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121625</guid>
    </item>
    <item>
      <title>SIGILL / ILL_ILLOPN crash when using AOT Inductor pre-compiled model (torch 2.2)</title>
      <link>https://github.com/pytorch/pytorch/issues/121516</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>We are getting a crash with this backtrace:</p>
<p>torch::inductor::AOTIModelContainerRunner::AOTIModelContainerRunner <br />
at::DynamicLibrary::sym <br />
(dso) (no symbol)<br />
(libc) (no symbol)<br />
(dso) -&gt; crash</p>
<p>We checked the processors flags were the problem happened (it doesn't happen everywhere), and the machine were we compile has more flags (avx512xxx) than the machines where it runs.</p>
<p>I think our next step will be to add a wrapper around g++, that disable some avx instructions, and point CXX to it, but maybe there's a better way.</p>
<p>(I noticed an 'ISA' option but I'm not sure on how to use it).</p>
<p>```python</p>
<h1>config specific to codegen/cpp.py</h1>
<p>class cpp:<br />
    # set to torch.get_num_threads()<br />
    threads = -1</p>
<pre><code># Do not generate loops when the condition doesn't hold, like:
# for(long i0=4096; i0&lt;4096; i0+=1)
no_redundant_loops = True

# Assume number of threads is dynamic, don't specialize thread number.
# Kernels don't recompile on thread number changes with this flag on.
# For single-threaded workload, turning it on would incur a slight
# performance degradation.
dynamic_threads = False

simdlen: Optional[int] = None
min_chunk_size = 4096
cxx = (
    None,  # download gcc12 from conda-forge if conda is installed
    # "g++-12",
    # "g++-11",
    # "g++-10",
    # "clang++",
    os.environ.get("CXX", "clang++" if sys.platform == "darwin" else "g++"),
    # "g++.par",
)
# Allow kernel performance profiling via PyTorch profiler
enable_kernel_profile = False

# enable weight prepacking to get a better performance; may lead to large memory footprint
weight_prepack = True

# Inject a bug into our relu implementation; useful for testing our repro
# extraction and minification functionality.
# Valid values: "compile_error", "runtime_error", "accuracy"
inject_relu_bug_TESTING_ONLY: Optional[str] = None
inject_log1p_bug_TESTING_ONLY: Optional[str] = None

# If None, autodetect whether or not AVX512/AVX2 can be used.  Otherwise,
# force usage as specified, without testing.
vec_isa_ok: Optional[bool] = None
</code></pre>
<p>```</p>
<p>We could also try to hack and remove this:</p>
<p><code>if sys.platform == "darwin":
        # Per https://mac.r-project.org/openmp/ right way to pass `openmp` flags to MacOS is via `-Xclang`
        # Also, `-march=native` is unrecognized option on M1
        base_flags += " -Xclang"
    else:
        if platform.machine() == "ppc64le":
            base_flags += " -mcpu=native"
        else:
            base_flags += " -march=native"</code></p>
<p>(-march=native)</p>
<h3>Versions</h3>
<p>pytorch-2.2.0</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 10:12:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121516</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Tolerate dynamic shapes</title>
      <link>https://github.com/pytorch/pytorch/pull/121497</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121734<br />
* <strong>-&gt;</strong> #121497<br />
* #121491<br />
* #121490<br />
* #121489</p>
<p>Previously, when the Cutlass backend was enabled, using dynamic shapes could lead to exceptions during JIT.</p>
<p>With this change, there are guards in place to just disable the Cutlass backend if dynamic dimensions are involved.</p>
<p>In addition, if no choices for a GEMM are available using the selected backends, then an ATen Kernel is used as fallback, even if the ATen backend is not enabled.</p>
<p>Test:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 07:55:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121497</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] GEMM size threshold for Cutlass backend usage</title>
      <link>https://github.com/pytorch/pytorch/pull/121491</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121734<br />
* #121497<br />
* <strong>-&gt;</strong> #121491<br />
* #121490<br />
* #121489</p>
<ul>
<li>
<p>Adds a configurable GEMM size threshold for the usage of Cutlass GEMM Kernels <strong>_inductor.config.cutlass_backend_min_gemm_size</strong></p>
</li>
<li>
<p>During GEMM algorithm choice generation: <strong>if no viable choices can be generated using the configured backends, the ATen backend will be used as a fallback backend</strong>, even if it is not enabled in <strong>_inductor.config.max_autotune_gemm_backends</strong></p>
</li>
</ul>
<p>Test plan:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121491</guid>
    </item>
    <item>
      <title>[Inductor] Make codecache CUDA compilation more robust &amp; flexible</title>
      <link>https://github.com/pytorch/pytorch/pull/121490</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121734<br />
* #121497<br />
* #121491<br />
* <strong>-&gt;</strong> #121490<br />
* #121489</p>
<p>Minor changes which make the CUDA compilation within _inductor/codecache.py<br />
more robust and flexible.</p>
<p>Test plan:<br />
CI<br />
Additional test in test_codecache.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121490</guid>
    </item>
    <item>
      <title>[WIP] Add registration API for torch.compile-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/121387</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to provide a registration mode to implement a single aten operation on the top of <code>torch.compile</code> and then register to aten. </p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #116449<br />
* #116368<br />
* #121727<br />
* <strong>-&gt;</strong> #121387<br />
* #120595<br />
* #121296</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121387</guid>
    </item>
    <item>
      <title>[compiled autograd] support custom ops backed by c++ autograd::Function</title>
      <link>https://github.com/pytorch/pytorch/pull/120681</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120681</p>
<ul>
<li>Adds support for custom ops backed by c++ custom autograd functions, e.g. fbgemm</li>
<li>Include files more granularly to avoid namespace pollution and circular imports</li>
</ul>
<p>limitations:<br />
- requires user to audit their code and opt-in their custom autograd::Function via autograd::Function::is_traceable and maybe additional compiled_args + apply_with_saved implementation. this was the only way I can think of for soundness<br />
- will throw if we can't hash the saved_data i.e. for any non implemented type other than list and dict in at::IValue::hash https://github.com/pytorch/pytorch/blob/b0cfa96e82d7fbd02f5dbcef2632714caf89615d/aten/src/ATen/core/ivalue.cpp#L364<br />
- can technically silently fail if both the typeid hash and the typeid string name of the custom autograd::Function collide at the same time, and an identical autograd graph containing a different custom autograd::Function, yet that has an identical implementation, is called. this case seems extremely unlikely, and the only alternative to hash collision i can think of is compiling with reflection<br />
- tensors not saved via save_variables are not lifted, and are specialized on TensorImpl*'s hash (treated as a memory address). if needed, we can lift them.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54818488">D54818488</a></p>]]></description>
      <pubDate>Mon, 26 Feb 2024 17:38:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120681</guid>
    </item>
    <item>
      <title>[DDP][PT2D][2D] Enable DDP + TP and add test for compiled DDP + TP</title>
      <link>https://github.com/pytorch/pytorch/pull/120479</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120479<br />
* #113209</p>
<p>This PR enables DDP + TP using a TP internal API. This should not be the final implementation. A more sound implementation is to inline the TP internal API in DDP. In other words, DDP needs to be aware of DTensor so that we can support 2D state_dict.</p>
<p>This PR adds a compiled DDP + TP test to ensure the new compiled DDP fusion doesn't break TP all_reduce.</p>
<p><strong>TODOs</strong></p>
<ul>
<li>[x] Implement DDP allreduce fusion algorithm for Inductor post_grad pass.</li>
<li>[x] Add unit tests to ensure the fusion doesn't DDP + TP.</li>
<li>[ ] Group different PG and data type of all_reduces.</li>
<li>[ ] Mixed precision supports and tests</li>
<li>[ ] Implement the fusions with Inductor IR.</li>
<li>[ ] Add auto bucketing based on Inductor profiling.</li>
</ul>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54105050/">D54105050</a></p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225</p>]]></description>
      <pubDate>Thu, 22 Feb 2024 23:46:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120479</guid>
    </item>
    <item>
      <title>Add CUTLASS kernel as choice for _int_mm() Inductor autotuning</title>
      <link>https://github.com/pytorch/pytorch/pull/119685</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #119986<br />
* <strong>-&gt;</strong> #119685</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 12 Feb 2024 07:05:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119685</guid>
    </item>
    <item>
      <title>test_inductor_layout_optimization_input_mutations_cuda, AssertionError: Tensor-likes are not close! </title>
      <link>https://github.com/pytorch/pytorch/issues/117364</link>
      <description><![CDATA[<p>Repro:<br />
<code>TORCHINDUCTOR_CPP_WRAPPER=1 python test/inductor/test_torchinductor.py -k test_inductor_l
ayout_optimization_input_mutations_cuda</code></p>
<p>Error:<br />
```<br />
Traceback (most recent call last):<br />
  File "/data/users/binbao/pytorch/torch/testing/_internal/common_utils.py", line 2674, in wrapper<br />
    method(<em>args, </em><em>kwargs)<br />
  File "/data/users/binbao/pytorch/test/inductor/test_torchinductor.py", line 8195, in new_test<br />
    return value(self)<br />
  File "/home/binbao/local/miniconda3/envs/pytorch-3.10/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(</em>args, **kwds)<br />
  File "/data/users/binbao/pytorch/test/inductor/test_torchinductor.py", line 7077, in test_inductor_layout_optimization_input_mutations<br />
    self.assertEqual(out_ref, out_test)<br />
  File "/data/users/binbao/pytorch/torch/testing/_internal/common_utils.py", line 3518, in assertEqual<br />
    raise error_metas.pop()[0].to_error(<br />
AssertionError: Tensor-likes are not close!</p>
<p>Mismatched elements: 4194256 / 4194304 (100.0%)<br />
Greatest absolute difference: 2.568821430206299 at index (1, 68, 121, 49) (up to 1e-05 allowed)<br />
Greatest relative difference: 0.5 at index (0, 0, 0, 0) (up to 1.3e-06 allowed)</p>
<p>```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @chenyang78</p>]]></description>
      <pubDate>Fri, 12 Jan 2024 08:04:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/117364</guid>
    </item>
    <item>
      <title>[WIP] Add a cache mechanism to accelerate torch.compile-for-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/116368</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to provide a cache mechanism to accelerate torch.compile-for-eager. </p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #116449<br />
* <strong>-&gt;</strong> #116368<br />
* #121727<br />
* #121387<br />
* #120595<br />
* #121296</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 03:21:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116368</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Enable quantization linear pattern fusion with int8_mixed_bf16 for gelu</title>
      <link>https://github.com/pytorch/pytorch/pull/116004</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #116004<br />
* #114854<br />
* #114853</p>
<p><strong>Summary</strong><br />
Enable QLinear Unary pattern for gelu with int8_mix_bf16</p>
<p><strong>Test plan</strong><br />
python test/inductor/test_mkldnn_pattern_matcher.py -k test_qlinear_gelu_int8_mixed_bf16</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 17 Dec 2023 23:53:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116004</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Enable quantization linear pattern fusion for gelu inside inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/114854</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #116004<br />
* <strong>-&gt;</strong> #114854<br />
* #114853</p>
<p><strong>Summary</strong><br />
Enable QLinear Unary pattern for gelu with int8 </p>
<p><strong>Test plan</strong><br />
python test/inductor/test_mkldnn_pattern_matcher.py -k test_qlinear_gelu_cpu</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 29 Nov 2023 23:53:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/114854</guid>
    </item>
    <item>
      <title>[Quant][PT2E] Enable linear and linear-unary post-op gelu quant recipe for x86 inductor quantizer</title>
      <link>https://github.com/pytorch/pytorch/pull/114853</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #116004<br />
* #114854<br />
* <strong>-&gt;</strong> #114853</p>
<p><strong>Summary</strong><br />
Add Gelu for linear-unary post-op quantization recipe to x86 inductor quantizer. </p>
<p><strong>Test plan</strong><br />
python -m pytest test/quantization/pt2e/test_x86inductor_quantizer.py -k test_linear_unary_gelu<br />
python test/test_quantization.py -k test_linear_unary_with_quantizer_api</p>]]></description>
      <pubDate>Wed, 29 Nov 2023 23:53:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/114853</guid>
    </item>
    <item>
      <title>[compile] Tracker for `torchrec_dlrm` issues</title>
      <link>https://github.com/pytorch/pytorch/issues/101918</link>
      <description><![CDATA[<p>First issue<br />
~~~<br />
import torch<br />
from dataclasses import dataclass, field<br />
from typing import Any</p>
<p>@dataclass<br />
class DClass():<br />
   sharding_contexts: Any = field(default_factory=list)<br />
   a: int = 1</p>
<p>def fn(x):<br />
    return DClass().a * x</p>
<p>opt_fn = torch.compile(fn)<br />
x = torch.randn(4)<br />
opt_fn(x)</p>
<p>~~~</p>
<hr />
<p>Second issue</p>
<p>~~~<br />
from math import inf<br />
import torch<br />
from torch import tensor, device<br />
import torch.fx as fx<br />
import torch._dynamo<br />
from torch._dynamo.testing import rand_strided<br />
from torch._dynamo.debug_utils import run_fwd_maybe_bwd</p>
<p>import torch._dynamo.config<br />
import torch._inductor.config<br />
import torch._functorch.config<br />
import fbgemm_gpu</p>
<p>torch._inductor.config.fallback_random = True</p>
<p>from torch.nn import *<br />
class Repro(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()</p>
<pre><code>def forward(self, L_L_indices_ : torch.Tensor, L_L_offsets_ : torch.Tensor, L_L_self_rows_per_table_ : torch.Tensor, L_L_self_bounds_check_warning_ : torch.Tensor):
    l_l_indices_ = L_L_indices_
    l_l_offsets_ = L_L_offsets_
    l_l_self_rows_per_table_ = L_L_self_rows_per_table_
    l_l_self_bounds_check_warning_ = L_L_self_bounds_check_warning_
    long = l_l_indices_.long();  l_l_indices_ = None
    long_1 = l_l_offsets_.long();  l_l_offsets_ = None
    bounds_check_indices = torch.ops.fbgemm.bounds_check_indices(l_l_self_rows_per_table_, long, long_1, 1, l_l_self_bounds_check_warning_, None);  l_l_self_rows_per_table_ = long = l_l_self_bounds_check_warning_ = None
    return [long_1]
</code></pre>
<p>mod = Repro()</p>
<p>def load_args(reader):<br />
    buf0 = reader.storage('b34ae4d796654d6804a16e92852d0395c29d3ea7', 832, device=device(type='cuda', index=0), dtype_hint=torch.int64)<br />
    reader.tensor(buf0, (104,), dtype=torch.int64)  # L_L_indices_<br />
    buf1 = reader.storage('0aca75493e9fed822d70e8524e19324441bfaa4d', 840, device=device(type='cuda', index=0), dtype_hint=torch.int64)<br />
    reader.tensor(buf1, (105,), dtype=torch.int64)  # L_L_offsets_<br />
    buf2 = reader.storage('d0cca63d71b4e5a07f9cb3f2fbd5e3aa1c5e4c35', 208, device=device(type='cuda', index=0), dtype_hint=torch.int64)<br />
    reader.tensor(buf2, (26,), dtype=torch.int64)  # L_L_self_rows_per_table_<br />
    buf3 = reader.storage('05fe405753166f125559e7c9ac558654f107c7e9', 8, device=device(type='cuda', index=0), dtype_hint=torch.int64)<br />
    reader.tensor(buf3, (1,), dtype=torch.int64)  # L_L_self_bounds_check_warning_<br />
load_args._version = 0</p>
<p>if <strong>name</strong> == '<strong>main</strong>':<br />
    from torch._dynamo.repro.after_dynamo import run_repro<br />
    run_repro(mod, load_args, accuracy=False, command='run',<br />
        save_dir='/scratch/anijain/work/gcc-10/pytorch/checkpoints', autocast=False, backend='aot_eager')</p>
<p>~~~</p>
<p>~~~<br />
RuntimeError: Found a custom (non-ATen) operator that either mutates or its inputs: fbgemm::bounds_check_indices.. Getting these operators to work with functionalization requires some extra work. For mutable ops you need to register a corresponding out-of-place variant of the op, and you also need to register a Functionalization kernel that performs some boilerplate, telling functionalization to map from the mutable op to the out-of-place op. See a more complete example of how to do this at https://gist.github.com/bdhirsh/7dadbf6296f8f7d1abcf4c482f438aaa. Please file a GitHub issue if you run into any problems.<br />
~~~</p>
<hr />
<p>Third-issue - Bad Fx Graph<br />
Repro - <code>TORCHDYNAMO_DEBUG_FUNCTION="invoke" TORCH_LOGS="+dynamo" python benchmarks/dynamo/torchbench.py  --backend=aot_eager --float32 --inference --device cuda  --accuracy --only torchrec_dlrm</code></p>
<p>The <code>TORCHDYNAMO_DEBUG_FUNCTION</code> flag calls Dynamo only on the relevant function.</p>
<hr />
<p>cc @bdhirsh @ezyang @msaroufim @wconstab @zou3519 @ngimel</p>]]></description>
      <pubDate>Fri, 19 May 2023 16:22:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/101918</guid>
    </item>
    <item>
      <title>RuntimeError: Triton Error [CUDA]: device-side assert triggered when trying torch.compile max-autotune on nanoGPT</title>
      <link>https://github.com/pytorch/pytorch/issues/101444</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Tried to run <code>torch.compile(model, mode='max-autotune')</code> on nanoGPT, got some cuda errors such as <code>Triton Error [CUDA]: device-side assert triggered</code> when doing inference.  I originally found this when experimenting with quantization, but it reproduces without quantization.</p>
<p>Repro script (requires nanoGPT repo): https://gist.github.com/vkuzo/7352cf7b18fd2602668fb3d43ad9467a<br />
Stack trace: https://gist.github.com/vkuzo/57225adfacd7779309fd5444a5f80bcf</p>
<h3>Versions</h3>
<p>https://gist.github.com/vkuzo/a5fbfc5905f9bd33732f4f73ec6ce0f4</p>
<p>cc @ezyang @msaroufim @wconstab @ngimel @bdhirsh @anijain2305</p>]]></description>
      <pubDate>Mon, 15 May 2023 12:30:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/101444</guid>
    </item>
    <item>
      <title>Fine-tuning HuggingFace wav2vec 2.0 with `torch.compile`</title>
      <link>https://github.com/pytorch/pytorch/issues/101160</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I followed the example to fine-tune HuggingFace's wav2vec 2.0 for speech recognition, using <code>torch.compile</code>, aiming to get faster training. However, I ran into an issue as outlined in the error logs.</p>
<p>I suspect that HuggingFace's wav2vec 2.0 is not yet supported in PyTorch 2.0 and needs some modification to ensure compatibility when running <code>torch.compile</code>. It's mostly related to creating mask tensors for SpecAugment.</p>
<p>This issue seems to also be related: <a href="https://github.com/pytorch/pytorch/issues/97511">fairseq Hubert with torch.compile</a>. Same issue also raised in <a href="https://github.com/huggingface/transformers/issues/22849">HuggingFace</a>.</p>
<h3>Error logs</h3>
<p>```<br />
<strong><em>*</em> Running training </strong><strong><em><br />
  Num examples = 3,478<br />
  Num Epochs = 15<br />
  Instantaneous batch size per device = 4<br />
  Total train batch size (w. parallel, distributed &amp; accumulation) = 4<br />
  Gradient Accumulation steps = 1<br />
  Total optimization steps = 13,050<br />
  Number of trainable parameters = 311,270,569<br />
  0%|                                                                                                                                                                    | 0/13050 [00:00&lt;?, ?it/sTraceback (most recent call last):<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/<em>dynamo/output_graph.py", line 670, in call_user_compiler<br />
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py", line 1055, in debug_wrapper<br />
    compiled_gm = compiler_fn(gm, example_inputs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/<strong>init</strong>.py", line 1390, in <strong>call</strong><br />
    return compile_fx(model</em>, inputs_, config_patches=self.config)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 455, in compile_fx<br />
    return aot_autograd(<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/backends/common.py", line 48, in compiler_fn<br />
    cg = aot_module_simplified(gm, example_inputs, </em><em>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 2822, in aot_module_simplified<br />
    compiled_fn = create_aot_dispatcher_function(<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
    r = func(</em>args, </strong>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 2515, in create_aot_dispatcher_function<br />
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1715, in aot_wrapper_dedupe<br />
    return compiler_fn(flat_fn, leaf_flat_args, aot_config)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 2104, in aot_dispatch_autograd<br />
    fx_g = make_fx(joint_forward_backward, aot_config.decompositions)(<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 714, in wrapped<br />
    t = dispatch_trace(wrap_key(func, args, fx_tracer), tracer=fx_tracer, concrete_args=tuple(phs))<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 443, in dispatch_trace<br />
    graph = tracer.trace(root, concrete_args)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py", line 778, in trace<br />
    (self.create_arg(fn(<em>args)),),<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py", line 652, in flatten_fn<br />
    tree_out = root_fn(</em>tree_args)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 459, in wrapped<br />
    out = f(<em>tensors)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1158, in traced_joint<br />
    return functionalized_f_helper(primals, tangents)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1110, in functionalized_f_helper<br />
    f_outs = flat_fn_no_input_mutations(fn, f_primals, f_tangents, meta, keep_input_mutations)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1078, in flat_fn_no_input_mutations<br />
    outs = flat_fn_with_synthetic_bases_expanded(fn, primals, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1050, in flat_fn_with_synthetic_bases_expanded<br />
    outs = forward_or_joint(fn, primals_before_cloning, primals, maybe_tangents, meta, keep_input_mutations)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1019, in forward_or_joint<br />
    backward_out = torch.autograd.grad(<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/autograd/<strong>init</strong>.py", line 269, in grad<br />
    return handle_torch_function(<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/overrides.py", line 1534, in handle_torch_function<br />
    result = mode.<strong>torch_function</strong>(public_api, types, args, kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_inductor/overrides.py", line 38, in <strong>torch_function</strong><br />
    return func(</em>args, </strong>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/autograd/<strong>init</strong>.py", line 303, in grad<br />
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/utils/_stats.py", line 20, in wrapper<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 487, in <strong>torch_dispatch</strong><br />
    return self.inner_torch_dispatch(func, types, args, kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 512, in inner_torch_dispatch<br />
    out = proxy_call(self, func, args, kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 345, in proxy_call<br />
    out = func(</em>args, <strong>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_ops.py", line 287, in <strong>call</strong><br />
    return self._op(*args, </strong>kwargs or {})<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/utils/_stats.py", line 20, in wrapper<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py", line 987, in <strong>torch_dispatch</strong><br />
    return self.dispatch(func, types, args, kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py", line 1162, in dispatch<br />
    op_impl_out = op_impl(self, func, </em>args, <strong>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py", line 453, in index_tensor<br />
    check_no_bool_index_tensors(func, *args, </strong>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py", line 432, in check_no_bool_index_tensors<br />
    raise DynamicOutputShapeException(func)<br />
torch._subclasses.fake_tensor.DynamicOutputShapeException: aten.index.Tensor</p>
<p>The above exception was the direct cause of the following exception:</p>
<p>Traceback (most recent call last):<br />
  File "/home/wilson_bookbotkids_com/transformers/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py", line 775, in <module><br />
    main()<br />
  File "/home/wilson_bookbotkids_com/transformers/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py", line 723, in main<br />
    train_result = trainer.train(resume_from_checkpoint=checkpoint)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/trainer.py", line 1664, in train<br />
    return inner_training_loop(<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/trainer.py", line 1940, in _inner_training_loop<br />
    tr_loss_step = self.training_step(model, inputs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/trainer.py", line 2735, in training_step<br />
    loss = self.compute_loss(model, inputs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/trainer.py", line 2767, in compute_loss<br />
    outputs = model(<strong>inputs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 82, in forward<br />
    return self.dynamo_ctx(self._orig_mod.forward)(<em>args, </em><em>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1684, in forward<br />
    outputs = self.wav2vec2(<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1316, in forward<br />
    hidden_states = self._mask_hidden_states(<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1249, in _mask_hidden_states<br />
    if not getattr(self.config, "apply_spec_augment", True):<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1259, in <graph break in _mask_hidden_states><br />
    mask_time_indices = _compute_mask_indices(<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1266, in <graph break in _mask_hidden_states><br />
    mask_time_indices = torch.tensor(mask_time_indices, device=hidden_states.device, dtype=torch.bool)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 337, in catch_errors<br />
    return callback(frame, cache_size, hooks)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 404, in _convert_frame<br />
    result = inner_convert(frame, cache_size, hooks)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 104, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 262, in _convert_frame_assert<br />
    return _compile(<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 324, in _compile<br />
    out_code = transform_code_object(code, transform)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py", line 445, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 311, in transform<br />
    tracer.run()<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 1726, in run<br />
    super().run()<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 576, in run<br />
    and self.step()<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 540, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 1792, in RETURN_VALUE<br />
    self.output.compile_subgraph(<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 517, in compile_subgraph<br />
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 588, in compile_and_call_fx_graph<br />
    compiled_fn = self.call_user_compiler(gm)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 675, in call_user_compiler<br />
    raise BackendCompilerFailed(self.compiler_fn, e) from e<br />
torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised DynamicOutputShapeException: aten.index.Tensor</p>
<p>Set torch._dynamo.config.verbose=True for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>
<h3>Minified repro</h3>
<p>Install HuggingFace Transformers from source:<br />
<code>git clone https://github.com/huggingface/transformers.git
pip install transformers/</code></p>
<p>Run training script<br />
<code>sh
python transformers/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py \
    --dataset_name="common_voice" \
    --model_name_or_path="facebook/wav2vec2-large-xlsr-53" \
    --dataset_config_name="tr" \
    --output_dir="./wav2vec2-common_voice-tr-demo-dist" \
    --preprocessing_num_workers="16" \
    --overwrite_output_dir \
    --num_train_epochs="15" \
    --per_device_train_batch_size="4" \
    --gradient_accumulation_steps="1" \
    --learning_rate="3e-4" \
    --warmup_steps="500" \
    --evaluation_strategy="steps" \
    --text_column_name="sentence" \
    --save_steps="400" \
    --eval_steps="100" \
    --logging_steps="1" \
    --layerdrop="0.0" \
    --save_total_limit="3" \
    --freeze_feature_encoder \
    --gradient_checkpointing \
    --chars_to_ignore , ? . ! - \; \: \" ‚Äú % ‚Äò ‚Äù ÔøΩ \
    --fp16 \
    --group_by_length \
    --do_train --do_eval \
    --torch_compile True</code></p>
<h3>Versions</h3>
<details>
<summary>Versions</summary>

Collecting environment information...
PyTorch version: 2.0.1+cu117
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Debian GNU/Linux 10 (buster) (x86_64)
GCC version: (Debian 8.3.0-6) 8.3.0
Clang version: Could not collect
CMake version: version 3.26.3
Libc version: glibc-2.28

Python version: 3.9.0 | packaged by conda-forge | (default, Nov 26 2020, 07:57:39)  [GCC 9.3.0] (64-bit runtime)
Python platform: Linux-4.19.0-23-cloud-amd64-x86_64-with-glibc2.28
Is CUDA available: True
CUDA runtime version: 12.0.76
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB
Nvidia driver version: 525.60.13
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
Address sizes:       46 bits physical, 48 bits virtual
CPU(s):              12
On-line CPU(s) list: 0-11
Thread(s) per core:  2
Core(s) per socket:  6
Socket(s):           1
NUMA node(s):        1
Vendor ID:           GenuineIntel
CPU family:          6
Model:               85
Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz
Stepping:            7
CPU MHz:             2200.152
BogoMIPS:            4400.30
Hypervisor vendor:   KVM
Virtualization type: full
L1d cache:           32K
L1i cache:           32K
L2 cache:            1024K
L3 cache:            39424K
NUMA node0 CPU(s):   0-11
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==2.0.1+cu117
[pip3] torchaudio==2.0.2+cu117
[pip3] triton==2.0.0
[conda] numpy                     1.24.3                   pypi_0    pypi
[conda] torch                     2.0.1+cu117              pypi_0    pypi
[conda] torchaudio                2.0.2+cu117              pypi_0    pypi
[conda] triton                    2.0.0                    pypi_0    pypi

</details>

<p>cc @ezyang @gchanan @zou3519 @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @msaroufim @wconstab @ngimel @bdhirsh @anijain2305 @soumith</p>]]></description>
      <pubDate>Wed, 10 May 2023 22:33:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/101160</guid>
    </item>
    <item>
      <title>Make compiled models serializable</title>
      <link>https://github.com/pytorch/pytorch/issues/101107</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Serializing a compiled model with <code>pickle</code> fails with <code>Can't pickle local object 'convert_frame.&lt;locals&gt;._convert_frame'</code> and <code>cannot pickle 'ConfigModuleInstance' object</code> when using <code>dill</code>.</p>
<p>A Colab with an example: <br />
https://colab.research.google.com/drive/1v6jUUq86ql1Era4X47cIDj7bzrrz2RZe?usp=sharing</p>
<p>In Hugging Face Datasets, this error stops us from generating (deterministic) hashes for transforms (functions) that reference a compiled model, meaning such transforms cannot be cached and must be re-computed each time when transforming a dataset.   </p>
<p>(The "export" API for the compiled models would also work for us.)</p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<details>
<summary>Colab env with torch 2.0.1 installed</summary>

```
PyTorch version: 2.0.1+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: 10.0.0-4ubuntu1 
CMake version: version 3.25.2
Libc version: glibc-2.31

Python version: 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.10.147+-x86_64-with-glibc2.31
Is CUDA available: False
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.7.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          2
On-line CPU(s) list:             0,1
Thread(s) per core:              2
Core(s) per socket:              1
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           79
Model name:                      Intel(R) Xeon(R) CPU @ 2.20GHz
Stepping:                        0
CPU MHz:                         2200.196
BogoMIPS:                        4400.39
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       32 KiB
L1i cache:                       32 KiB
L2 cache:                        256 KiB
L3 cache:                        55 MiB
NUMA node0 CPU(s):               0,1
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Mitigation; PTE Inversion
Vulnerability Mds:               Vulnerable; SMT Host state unknown
Vulnerability Meltdown:          Vulnerable
Vulnerability Mmio stale data:   Vulnerable
Vulnerability Retbleed:          Vulnerable
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Vulnerable
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities

Versions of relevant libraries:
[pip3] numpy==1.22.4
[pip3] torch==2.0.1+cu118
[pip3] torchaudio==2.0.2+cu118
[pip3] torchdata==0.6.0
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.15.1
[pip3] torchvision==0.15.2+cu118
[pip3] triton==2.0.0
[conda] Could not collect
```
</details>

<p>cc @ezyang @soumith @msaroufim @wconstab @ngimel @bdhirsh @anijain2305</p>]]></description>
      <pubDate>Wed, 10 May 2023 10:48:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/101107</guid>
    </item>
    <item>
      <title>[torch.compile] the sum of `softmax` isn't `1` on cuda</title>
      <link>https://github.com/pytorch/pytorch/issues/101039</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>torch.compile</code> returns the sum of <code>softmax</code> isn't equal to <code>1</code> on cuda</p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(420)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()
    self.softmax = torch.nn.Softmax(dim=-1)

def forward(self, query, key):
    y = torch.matmul(query, key.transpose(-2, -1))
    z = y.div(1e-06)
    w = self.softmax(z)
    return w
</code></pre>
<p>device = 'cuda'<br />
func = Model().to(device)</p>
<p>query = torch.randn(1, 10, 40).to(device)</p>
<p>key = torch.randn(1, 2, 40).to(device)</p>
<p>jit_func = torch.compile(func)</p>
<p>res1 = func(query, key) # without jit<br />
print(res1)</p>
<h1>tensor([[[0., 1.],</h1>
<h1>[0., 1.],</h1>
<h1>[0., 1.],</h1>
<h1>[0., 1.],</h1>
<h1>[0., 1.],</h1>
<h1>[1., 0.],</h1>
<h1>[1., 0.],</h1>
<h1>[0., 1.],</h1>
<h1>[0., 1.],</h1>
<h1>[0., 1.]]], device='cuda:0')</h1>
<p>res2 = jit_func(query, key)<br />
print(res2)</p>
<h1>tensor([[[0.0000, 0.9804],</h1>
<h1>[0.0000, 1.0561],</h1>
<h1>[0.0000, 1.1190],</h1>
<h1>[0.0000, 0.9721],</h1>
<h1>[0.0000, 1.1999],</h1>
<h1>[0.9972, 0.0000],</h1>
<h1>[0.8769, 0.0000],</h1>
<h1>[0.0000, 1.0315],</h1>
<h1>[0.0000, 1.0398],</h1>
<h1>[0.0000, 0.8039]]], device='cuda:0')</h1>
<p>```</p>
<h3>Versions</h3>
<details>
<summary>Click to expand</summary>

```
Collecting environment information...
PyTorch version: 2.1.0.dev20230503+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.1 LTS (x86_64)
GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
Clang version: 14.0.0-1ubuntu1
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.19.5-051905-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 11.5.119
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060
Nvidia driver version: 510.108.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   46 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          24
On-line CPU(s) list:             0-23
Vendor ID:                       GenuineIntel
Model name:                      12th Gen Intel(R) Core(TM) i9-12900K
CPU family:                      6
Model:                           151
Thread(s) per core:              2
Core(s) per socket:              16
Socket(s):                       1
Stepping:                        2
CPU max MHz:                     6700.0000
CPU min MHz:                     800.0000
BogoMIPS:                        6374.40
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 invpcid_single cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities
Virtualization:                  VT-x
L1d cache:                       640 KiB (16 instances)
L1i cache:                       768 KiB (16 instances)
L2 cache:                        14 MiB (10 instances)
L3 cache:                        30 MiB (1 instance)
NUMA node(s):                    1
NUMA node0 CPU(s):               0-23
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Not affected
Vulnerability Retbleed:          Not affected
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.24.2
[pip3] pytorch-triton==2.1.0+7d1a95b046
[pip3] torch==2.1.0.dev20230503+cu118
[pip3] torchaudio==2.1.0.dev20230503+cu118
[pip3] torchvision==0.16.0.dev20230503+cu118
[conda] numpy                     1.24.2                   pypi_0    pypi
[conda] pytorch-triton            2.1.0+7d1a95b046          pypi_0    pypi
[conda] torch                     2.1.0.dev20230503+cu118          pypi_0    pypi
[conda] torchaudio                2.1.0.dev20230503+cu118          pypi_0    pypi
[conda] torchvision               0.16.0.dev20230503+cu118          pypi_0    pypi
```

</details>

<p>cc @ezyang @msaroufim @wconstab @ngimel @bdhirsh @anijain2305 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @soumith</p>]]></description>
      <pubDate>Tue, 09 May 2023 17:24:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/101039</guid>
    </item>
    <item>
      <title>[compile] output does not match eager mode</title>
      <link>https://github.com/pytorch/pytorch/issues/100075</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>```python<br />
import torch<br />
from torch import nn<br />
from torchvision.models import resnet18<br />
from torch._dynamo import allow_in_graph<br />
from functools import wraps<br />
from functorch import make_functional_with_buffers, vmap, grad</p>
<p>def traceable(f):<br />
    f = allow_in_graph(f)</p>
<pre><code>@wraps(f)
def wrapper(*args, **kwargs):
    return f(*args, **kwargs)

return wrapper
</code></pre>
<p>torch.manual_seed(42)<br />
device = 'cpu'  # also fails on CUDA</p>
<p>model = resnet18(pretrained=False, norm_layer=(lambda c: nn.GroupNorm(min(c, 32), c)))<br />
model.to(device)<br />
model.eval()</p>
<p>fnet, params, buffers = make_functional_with_buffers(model)</p>
<p>x = torch.randn(10, 3, 224, 224, device=device)<br />
f = lambda p, b, x : fnet(p, b, x).sum()</p>
<h1>Works for this simpler function</h1>
<h1>f = lambda p, b, x: (torch.sin(x) + torch.cos(x) + torch.exp(x)).sum()</h1>
<p>f = grad(f)</p>
<p>expected = f(params, buffers, x)<br />
actual = torch.compile(traceable(f))(params, buffers, x)</p>
<p>torch.testing.assert_close(actual, expected)<br />
```</p>
<p>Output<br />
```<br />
AssertionError: Tensor-likes are not close!</p>
<p>Mismatched elements: 9406 / 9408 (100.0%)<br />
Greatest absolute difference: 0.12333643436431885 at index (0, 2, 3, 4) (up to 1e-05 allowed)<br />
Greatest relative difference: 41.227630615234375 at index (5, 0, 0, 6) (up to 1.3e-06 allowed)</p>
<p>```</p>
<h3>Versions</h3>
<p>master</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @Chillee @samdow @janeyx99 @SherlockNoMad @wconstab @soumith @ngimel</p>]]></description>
      <pubDate>Tue, 25 Apr 2023 23:25:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/100075</guid>
    </item>
    <item>
      <title>torch.compile() drops the performance of validation / Dynamo is not guarding on attributes on NN modules</title>
      <link>https://github.com/pytorch/pytorch/issues/100061</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I encountered that after adding torch.compile(model), the performance of the model on the verification set has been declining as the training progresses.<br />
<img alt="image" src="https://user-images.githubusercontent.com/55650657/234467056-5434eaf2-1c7c-43ac-bbab-adeaa26a4660.png" /></p>
<p>When I remove torch.compile(model), the model is back to normal:<br />
epoch 1/1000, train: loss=0.0767, val: psnr=27.5813, 1.4m 1.4m/24.2h<br />
epoch 2/1000, train: loss=0.0438, val: psnr=28.2431, 1.6m 3.0m/25.3h<br />
epoch 3/1000, train: loss=0.0428, val: psnr=28.2909, 1.6m 4.6m/25.5h<br />
epoch 4/1000, train: loss=0.0417, val: psnr=28.4744, 1.6m 6.2m/25.9h<br />
epoch 5/1000, train: loss=0.0406, val: psnr=28.7785, 1.6m 7.8m/26.1h<br />
epoch 6/1000, train: loss=0.0404, val: psnr=28.5917, 1.6m 9.5m/26.3h<br />
epoch 7/1000, train: loss=0.0395, val: psnr=28.6923, 1.6m 11.1m/26.3h<br />
epoch 8/1000, train: loss=0.0393, val: psnr=28.9986, 1.5m 12.6m/26.2h<br />
epoch 9/1000, train: loss=0.0382, val: psnr=28.8534, 1.7m 14.2m/26.4h</p>
<h3>Error logs</h3>
<p>Those two things do not work. You can reproduce the similar result with this <a href="https://github.com/XieQi2015/F-Conv/tree/main/MinistExp">repro</a>.</p>
<p>You can implement with the simple case, by adding the model = torch.compile(model) below <a href="https://github.com/XieQi2015/F-Conv/blob/c55c4184a0cf6e7a30be62217c781264853a0515/MinistExp/Rotated_MNIST_simpleCase_Main.py#L46">here</a>.</p>
<p>After training 30 epochs, the model begin to test. If you add the torch.compile, the test accuracy will not increase. While for the version without torch.compile is normal.</p>
<h3>Minified repro</h3>
<p>env TORCHDYNAMO_REPRO_AFTER="aot" TORCHDYNAMO_REPRO_LEVEL=4</p>
<p>System prompts the body is too long, so I unload the repro.py in  <a href="https://drive.google.com/file/d/1cD1NV3ucDpHxn9unu04kmlTnjNVAgWRt/view">here</a></p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.0.0<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: Could not collect<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.4.0-42-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.1.74<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA GeForce RTX 4090<br />
GPU 1: NVIDIA GeForce RTX 4090<br />
GPU 2: NVIDIA GeForce RTX 4090<br />
GPU 3: NVIDIA GeForce RTX 4090<br />
GPU 4: NVIDIA GeForce RTX 4090<br />
GPU 5: NVIDIA GeForce RTX 4090<br />
GPU 6: NVIDIA GeForce RTX 4090<br />
GPU 7: NVIDIA GeForce RTX 4090</p>
<p>Nvidia driver version: 525.105.17<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   48 bits physical, 48 bits virtual<br />
CPU(s):                          256<br />
On-line CPU(s) list:             0-254<br />
Off-line CPU(s) list:            255<br />
Thread(s) per core:              1<br />
Core(s) per socket:              64<br />
Socket(s):                       2<br />
NUMA node(s):                    8<br />
Vendor ID:                       AuthenticAMD<br />
CPU family:                      25<br />
Model:                           1<br />
Model name:                      AMD EPYC 7763 64-Core Processor<br />
Stepping:                        1<br />
Frequency boost:                 enabled<br />
CPU MHz:                         1502.643<br />
CPU max MHz:                     2450.0000<br />
CPU min MHz:                     1500.0000<br />
BogoMIPS:                        4900.14<br />
Virtualization:                  AMD-V<br />
L1d cache:                       2 MiB<br />
L1i cache:                       2 MiB<br />
L2 cache:                        32 MiB<br />
L3 cache:                        256 MiB<br />
NUMA node0 CPU(s):               0-15,128-143<br />
NUMA node1 CPU(s):               16-31,144-159<br />
NUMA node2 CPU(s):               32-47,160-175<br />
NUMA node3 CPU(s):               48-63,176-191<br />
NUMA node4 CPU(s):               64-79,192-207<br />
NUMA node5 CPU(s):               80-95,208-223<br />
NUMA node6 CPU(s):               96-111,224-239<br />
NUMA node7 CPU(s):               112-127,240-254<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Full AMD retpoline, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca</p>
<p>Versions of relevant libraries:<br />
[pip3] natten==0.14.6+torch200cu118<br />
[pip3] numpy==1.23.5<br />
[pip3] pytorch-ssim==0.1<br />
[pip3] torch==2.0.0<br />
[pip3] torchaudio==2.0.0<br />
[pip3] torchvision==0.15.0<br />
[pip3] triton==2.0.0<br />
[conda] blas                      1.0                         mkl<br />
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch<br />
[conda] mkl                       2021.4.0           h06a4308_640<br />
[conda] mkl-service               2.4.0           py310h7f8727e_0<br />
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0<br />
[conda] mkl_random                1.2.2           py310h00e6091_0<br />
[conda] natten                    0.14.6+torch200cu118          pypi_0    pypi<br />
[conda] numpy                     1.23.5          py310hd5efca6_0<br />
[conda] numpy-base                1.23.5          py310h8e6c178_0<br />
[conda] pytorch                   2.0.0           py3.10_cuda11.8_cudnn8.7.0_0    pytorch<br />
[conda] pytorch-cuda              11.8                 h7e8668a_3    pytorch-nightly<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch<br />
[conda] pytorch-ssim              0.1                      pypi_0    pypi<br />
[conda] torchaudio                2.0.0               py310_cu118    pytorch<br />
[conda] torchtriton               2.0.0                     py310    pytorch<br />
[conda] torchvision               0.15.0              py310_cu118    pytorch</p>
<p>cc @ezyang @gchanan @zou3519 @msaroufim @wconstab @ngimel @bdhirsh @anijain2305 @soumith</p>]]></description>
      <pubDate>Tue, 25 Apr 2023 20:18:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/100061</guid>
    </item>
    <item>
      <title>[functorch] functorch_maml_omniglot - fails under torch.compile</title>
      <link>https://github.com/pytorch/pytorch/issues/98825</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Running <code>functorch_maml_omniglot</code> from <code>torchbench</code> with following patch<br />
```patch<br />
diff --git a/torchbenchmark/models/functorch_maml_omniglot/<strong>init</strong>.py b/torchbenchmark/models/functorch_maml_omniglot/<strong>init</strong>.py<br />
index faf16d73..430eaadf 100644<br />
--- a/torchbenchmark/models/functorch_maml_omniglot/<strong>init</strong>.py<br />
+++ b/torchbenchmark/models/functorch_maml_omniglot/<strong>init</strong>.py<br />
@@ -10,6 +10,17 @@ from typing import Tuple<br />
 from ...util.model import BenchmarkModel<br />
 from torchbenchmark.tasks import OTHER</p>
<p>+from torch._dynamo import allow_in_graph<br />
+from functools import wraps<br />
+<br />
+def traceable(f):<br />
+    f = allow_in_graph(f)<br />
+<br />
+    @wraps(f)<br />
+    def wrapper(<em>args, </em><em>kwargs):<br />
+        return f(</em>args, **kwargs)<br />
+<br />
+    return wrapper</p>
<p>def loss_for_task(net, n_inner_iter, x_spt, y_spt, x_qry, y_qry):<br />
     params, buffers, fnet = net<br />
@@ -66,7 +77,7 @@ class Model(BenchmarkModel):<br />
         self.model = net</p>
<pre><code>     root = str(Path(__file__).parent.parent)
</code></pre>
<ul>
<li>self.meta_inputs = torch.load(f'{root}/maml_omniglot/batch.pt')</li>
<li>self.meta_inputs = torch.load(f'{root}/functorch_maml_omniglot/batch.pt')<br />
         self.meta_inputs = tuple([torch.from_numpy(i).to(self.device) for i in self.meta_inputs])<br />
         self.example_inputs = (self.meta_inputs[0][0],)</li>
</ul>
<p>@@ -90,7 +101,9 @@ class Model(BenchmarkModel):<br />
         # In parallel, trains one model per task. There is a support (x, y)<br />
         # for each task and a query (x, y) for each task.<br />
         compute_loss_for_task = functools.partial(loss_for_task, net, n_inner_iter)<br />
-        qry_losses, qry_accs = vmap(compute_loss_for_task)(x_spt, y_spt, x_qry, y_qry)<br />
+        fn = vmap(compute_loss_for_task)<br />
+        fn = torch.compile(traceable(fn))<br />
+        qry_losses, qry_accs = fn(x_spt, y_spt, x_qry, y_qry)</p>
<pre><code>     # Compute the maml loss by summing together the returned losses.
     qry_losses.sum().backward()
</code></pre>
<p>```</p>
<p>Leads to failure:<br />
<code>Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.convolution.default(*(FakeTensor(FakeTensor(..., device='meta', size=(160, 1, 28, 28)), cpu),</code></p>
<h3>Versions</h3>
<p>master</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @Chillee @samdow @janeyx99 @soumith @ngimel</p>]]></description>
      <pubDate>Mon, 10 Apr 2023 22:54:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/98825</guid>
    </item>
    <item>
      <title>[functorch] torch.compile - functorch transforms Interaction</title>
      <link>https://github.com/pytorch/pytorch/issues/98822</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>This is an umbrella issue for all the issues related to functorch - torch.compile interaction.</p>
<p>Currently, functorch transforms can be compiled under torch.compile with undocumented API. However, there are still a few issues which require resolution.</p>
<p>Example of compiling transform<br />
```python<br />
import torch<br />
from torch._dynamo import allow_in_graph<br />
from functools import wraps</p>
<p>def traceable(f):<br />
    f = allow_in_graph(f)</p>
<pre><code>@wraps(f)
def wrapper(*args, **kwargs):
    return f(*args, **kwargs)

return wrapper
</code></pre>
<p>def fn(x):<br />
    return vmap(torch.sin)(x)</p>
<p>opt_fn = torch.compile(traceable(fn))<br />
opt_fn(torch.randn(3, 3))</p>
<p>```</p>
<p>torchbench issues:</p>
<ul>
<li>[ ] https://github.com/pytorch/pytorch/issues/98825</li>
<li>[ ] https://github.com/pytorch/pytorch/issues/98827</li>
</ul>
<p>user reported issues:<br />
- [x] https://github.com/pytorch/pytorch/issues/97425<br />
- [ ] https://github.com/pytorch/pytorch/issues/100105<br />
- [ ] https://github.com/pytorch/pytorch/issues/100075</p>
<h3>Versions</h3>
<p>master</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @Chillee @samdow @janeyx99 @soumith @ngimel</p>]]></description>
      <pubDate>Mon, 10 Apr 2023 22:40:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/98822</guid>
    </item>
    <item>
      <title>Cannot use `checkpoint_sequential` with `torch.compile`</title>
      <link>https://github.com/pytorch/pytorch/issues/98668</link>
      <description><![CDATA[<h2>Issue description</h2>
<p>I have a number of classes that derive directly from <code>nn.Sequential</code>, when I <code>torch.compile</code> models containing these classes and attempt to use them in conjunction with <code>checkpoint_sequential</code> execution immediately aborts with a TypeError exception (@ line 352 in torch/utils/checkpoint.py):</p>
<p><code>segment_size = len(functions) // segments
TypeError: object of type 'OptimizedModule' has no len()</code></p>
<p>I could imagine that perhaps the two are not meant to interoperate, but it's not clear whether that's the case.</p>
<p>I have seen this with both the stable 2.0.0 CUDA 11.7 release as well as the nightly CUDA 11.8 release (system info below)</p>
<h2>Code example</h2>
<p>I have managed to reproduce the issue in a minimal way with this code snippet:</p>
<p>```python<br />
import torch<br />
import torch.nn as nn<br />
import torch.utils.checkpoint as ckpt<br />
import collections</p>
<p>class Sequence(nn.Sequential):<br />
    def <strong>init</strong>(self) -&gt; None:<br />
        builder = collections.OrderedDict()<br />
        builder['linear_1'] = nn.Linear(32, 32)<br />
        builder['linear_2'] = nn.Linear(32, 32)<br />
        super(Sequence, self).<strong>init</strong>(builder)</p>
<p>m = Sequence()<br />
n = torch.compile(m)<br />
x = torch.randn(32)<br />
y = ckpt.checkpoint_sequential(n, segments=2, input=x, use_reentrant=False)<br />
```</p>
<h2>System Info</h2>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.1.0.dev20230406+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Arch Linux (x86_64)<br />
GCC version: (GCC) 12.2.1 20230201<br />
Clang version: 15.0.7<br />
CMake version: Could not collect<br />
Libc version: glibc-2.37</p>
<p>Python version: 3.10.10 (main, Mar  5 2023, 22:26:53) [GCC 12.2.1 20230201] (64-bit runtime)<br />
Python platform: Linux-6.2.9-arch1-1-x86_64-with-glibc2.37<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090<br />
Nvidia driver version: 530.41.03<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Address sizes:                   39 bits physical, 48 bits virtual<br />
Byte Order:                      Little Endian<br />
CPU(s):                          12<br />
On-line CPU(s) list:             0-11<br />
Vendor ID:                       GenuineIntel<br />
Model name:                      12th Gen Intel(R) Core(TM) i5-12400F<br />
CPU family:                      6<br />
Model:                           151<br />
Thread(s) per core:              2<br />
Core(s) per socket:              6<br />
Socket(s):                       1<br />
Stepping:                        5<br />
CPU(s) scaling MHz:              35%<br />
CPU max MHz:                     5600.0000<br />
CPU min MHz:                     800.0000<br />
BogoMIPS:                        4993.00<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l2 invpcid_single cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize arch_lbr flush_l1d arch_capabilities<br />
Virtualization:                  VT-x<br />
L1d cache:                       288 KiB (6 instances)<br />
L1i cache:                       192 KiB (6 instances)<br />
L2 cache:                        7.5 MiB (6 instances)<br />
L3 cache:                        18 MiB (1 instance)<br />
NUMA node(s):                    1<br />
NUMA node0 CPU(s):               0-11<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Not affected<br />
Vulnerability Retbleed:          Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.1<br />
[pip3] pytorch-triton==2.1.0+46672772b4<br />
[pip3] torch==2.1.0.dev20230406+cu118<br />
[pip3] torchaudio==2.1.0.dev20230406+cu118<br />
[pip3] torchdata==0.7.0.dev20230406<br />
[pip3] torchvision==0.16.0.dev20230406+cu118<br />
[conda] Could not collect<br />
```</p>
<p>cc @ezyang @soumith @msaroufim @wconstab @ngimel @bdhirsh</p>]]></description>
      <pubDate>Fri, 07 Apr 2023 22:01:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/98668</guid>
    </item>
    <item>
      <title>Tracker - Failing models in the torch.compile dashboard</title>
      <link>https://github.com/pytorch/pytorch/issues/98561</link>
      <description><![CDATA[<h1>Torchbench models</h1>
<ul>
<li>
<p>[ ] @bdhirsh - Inductor(default) fails for <code>hf_Longformer</code> - Repro at https://github.com/pytorch/pytorch/issues/100067<br />
Cmd - <code>python benchmarks/dynamo/torchbench.py --accuracy --training --amp --backend inductor --disable-cudagraphs --device cuda --total-partitions 3 --partition-id 1 --only=hf_Longformer</code><br />
Possible fix - https://github.com/pytorch/pytorch/pull/100115</p>
</li>
<li>
<p>[x] @wconstab   - https://github.com/pytorch/pytorch/issues/103385</p>
</li>
<li>
<p>[x] @williamwen42 - <code>hf_T5_base</code> is failing</p>
</li>
<li>
<p>[x] @yanboliang - detectron2_maskrcnn - They dont show up in dashboard because they are skipped - https://github.com/pytorch/pytorch/issues/99665</p>
</li>
</ul>
<h1>TIMM models</h1>
<ul>
<li>[x] @eellison - Inductor (w/ cudagraphs) OOM for <code>cait_m36_384</code></li>
</ul>
<h1>Huggingface models</h1>
<ul>
<li>
<p>[ ] (<strong>Up for Grabs</strong>)  -  Inductor (default) accuracy failure with <code>AlbertForQuestionAnswering</code> - Can't repro on AWS machine<br />
Next steps - Repro this on GCP machine, check the offending tensors, increase tolerance if needed.</p>
</li>
<li>
<p>[x] @eellison - Inductor (w/ cudagraphs) OOM for <code>DebertaV2ForQuestionAnswering</code> </p>
</li>
</ul>
<h2>Dynamic shapes (NOT POPULATED YET)</h2>
<ul>
<li>
<p>[ ] @ezyang  - Inductor (dynamic) w/eval fails for <code>hf_BigBird</code> <br />
Cmd - <code>python benchmarks/dynamo/torchbench.py --accuracy --inference --amp --backend inductor --dynamic-shapes --dynamic-batch-only --disable-cudagraphs --device cuda --only=hf_BigBird</code><br />
~~~<br />
2023-04-24T18:32:37.8937866Z     expr = pexpr(V.graph.sizevars.simplify(self.shape))<br />
2023-04-24T18:32:37.8938395Z   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/sympy/printing/printer.py", line 292, in doprint<br />
2023-04-24T18:32:37.8938757Z     return self._str(self._print(expr))<br />
2023-04-24T18:32:37.8939235Z   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/sympy/printing/printer.py", line 331, in _print<br />
2023-04-24T18:32:37.8939711Z     return printmethod(expr, **kwargs)<br />
2023-04-24T18:32:37.8940289Z   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codegen/common.py", line 191, in _print_Pow<br />
2023-04-24T18:32:37.8940643Z     assert exp.is_integer<br />
2023-04-24T18:32:37.8941042Z torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
2023-04-24T18:32:37.8941357Z AssertionError: <br />
~~~</p>
</li>
<li>
<p>[ ] @bdhirsh - Inductor(default) w/ training fails <br />
Cmd - <code>python benchmarks/dynamo/torchbench.py --accuracy --training --amp --backend inductor --disable-cudagraphs --device cuda --total-partitions 3 --partition-id 1 --only=hf_BigBird</code><br />
Error<br />
~~~<br />
2023-04-24T19:31:30.2966005Z   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 567, in bigbird_block_sparse_attention<br />
2023-04-24T19:31:30.2966675Z     np.random.seed(seed)<br />
2023-04-24T19:31:30.2967733Z   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 569, in <resume in bigbird_block_sparse_attention><br />
2023-04-24T19:31:30.2968585Z     rand_attn = [<br />
2023-04-24T19:31:30.2969590Z   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 591, in <resume in bigbird_block_sparse_attention><br />
2023-04-24T19:31:30.2970293Z     rand_attn = np.stack(rand_attn, axis=0)<br />
2023-04-24T19:31:30.2971020Z   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 592, in <resume in bigbird_block_sparse_attention><br />
2023-04-24T19:31:30.2971769Z     rand_attn = torch.tensor(rand_attn, device=query_layer.device, dtype=torch.long)<br />
2023-04-24T19:31:30.2972585Z   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 629, in <resume in bigbird_block_sparse_attention><br />
2023-04-24T19:31:30.2972999Z     first_context_layer.unsqueeze_(2)<br />
2023-04-24T19:31:30.2974281Z RuntimeError: Output 0 of CompiledFunctionBackward is a view and is being modified inplace. This view was created inside a custom Function (or because an input was returned as-is) and the autograd logic to handle view+inplace would override the custom backward associated with the custom Function, leading to incorrect gradients. This behavior is forbidden. You can fix this by cloning the output of the custom Function.<br />
~~~</p>
</li>
<li>
<p>[ ] @anijain2305 + @ezyang - Inductor (w/ dynamic) fails for <code>convit_base</code> for sympy error</p>
</li>
</ul>
<p>~~~<br />
2023-04-24T18:14:43.2692442Z cuda train convit_base                         WARNING:common:fp64 golden ref were not generated for convit_base. Setting accuracy check to cosine<br />
2023-04-24T18:14:58.0197194Z [2023-04-24 18:14:58,017] torch.fx.experimental.symbolic_shapes: [WARNING] 13.0: RecursionError in sympy.solve(floor(s0<em><em>0.5) - 14, s0)<br />
2023-04-24T18:15:01.5186560Z ERROR:common:backend='inductor' raised:<br />
2023-04-24T18:15:01.5187148Z CppCompileError: C++ compile error<br />
2023-04-24T18:15:01.5187414Z <br />
2023-04-24T18:15:01.5187542Z Command:<br />
2023-04-24T18:15:01.5190876Z g++ /tmp/torchinductor_jenkins/za/czarprmgsfhybui3toxkkq3vz6vil3qetzl7exfphbvz2kjjt4tr.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/py_3.10/include/python3.10 -lgomp -O3 -ffast-math -fno-finite-math-only -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_jenkins/za/czarprmgsfhybui3toxkkq3vz6vil3qetzl7exfphbvz2kjjt4tr.so<br />
2023-04-24T18:15:01.5192575Z <br />
2023-04-24T18:15:01.5192698Z Output:<br />
2023-04-24T18:15:01.5193959Z /tmp/torchinductor_jenkins/za/czarprmgsfhybui3toxkkq3vz6vil3qetzl7exfphbvz2kjjt4tr.cpp: In function √¢‚Ç¨Àúvoid kernel(float</em>, long int</em>, long int<em>, long int)√¢‚Ç¨‚Ñ¢:<br />
2023-04-24T18:15:01.5195123Z /tmp/torchinductor_jenkins/za/czarprmgsfhybui3toxkkq3vz6vil3qetzl7exfphbvz2kjjt4tr.cpp:42:72: error: invalid operands of types √¢‚Ç¨Àúlong int√¢‚Ç¨‚Ñ¢ and √¢‚Ç¨Àúdouble√¢‚Ç¨‚Ñ¢ to binary √¢‚Ç¨Àúoperator%√¢‚Ç¨‚Ñ¢<br />
2023-04-24T18:15:01.5196985Z                      auto tmp0 = out_ptr1[static_cast<long>((((i0 / 1L) % (std::floor(std::sqrt(ks0))))</em>(std::floor(std::sqrt(ks0)))) + ((i1 / 1L) % (std::floor(std::sqrt(ks0)))))];<br />
2023-04-24T18:15:01.5197557Z                                                               ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br />
2023-04-24T18:15:01.5198643Z /tmp/torchinductor_jenkins/za/czarprmgsfhybui3toxkkq3vz6vil3qetzl7exfphbvz2kjjt4tr.cpp:42:147: error: invalid operands of types √¢‚Ç¨Àúlong int√¢‚Ç¨‚Ñ¢ and √¢‚Ç¨Àúdouble√¢‚Ç¨‚Ñ¢ to binary √¢‚Ç¨Àúoperator%√¢‚Ç¨‚Ñ¢<br />
2023-04-24T18:15:01.5202118Z                      auto tmp0 = out_ptr1[static_cast<long>((((i0 / 1L) % (std::floor(std::sqrt(ks0))))*(std::floor(std::sqrt(ks0)))) + ((i1 / 1L) % (std::floor(std::sqrt(ks0)))))];<br />
~~~<br />
Next steps<br />
* @anijain2305 - Use this opportunity to test minifier with dynamic shapes<br />
* @ezyang to fix/assign the owner to fix the issue</p>
<h2>Completed</h2>
<ul>
<li>
<p>[x] Inductor (default) accuracy flakiness for <code>sebotnet33ts_256</code> - Fixed by @anijain2305 https://github.com/pytorch/pytorch/pull/99851</p>
</li>
<li>
<p>[x] Inductor (default) w/eval fails for <code>hf_BigBird</code> - Fixed by @anijain2305</p>
</li>
</ul>
<p>cc @ezyang @soumith @msaroufim @wconstab @ngimel @bdhirsh</p>]]></description>
      <pubDate>Thu, 06 Apr 2023 16:18:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/98561</guid>
    </item>
    <item>
      <title>Dynamo compiled graph gets overwritten by eager in a data dependent branch when False branch is empty</title>
      <link>https://github.com/pytorch/pytorch/issues/98533</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I have encountered an unexpected dynamo capture behavior that is related to a data dependent branch. While the result of the code execution is correct, the way it executes is unexpected -- an already compiled graph got silently dropped and fall back to eager if an empty false branch is run.</p>
<p>It is best illustrated using an example. The example is a simplified version of running HuggingFace Whisper model.</p>
<p>```<br />
import torch<br />
import torch._dynamo as dynamo</p>
<p>global_dict = {<br />
  3: 3<br />
}</p>
<p>def f(x, y):</p>
<pre><code># begin graph 0
x_len = x.shape[-1]
update_idx = global_dict.get(x_len, None)
# end graph 0

if update_idx is not None:
    # begin graph 1
    y[update_idx] += 1
    # end graph 1
return y
</code></pre>
<p>class DebugCompiler:<br />
    def <strong>init</strong>(self):<br />
        self.count = 0</p>
<pre><code>def __call__(self, gm, example_inputs):
    id = self.count
    print(f'compiling graph {id}')
    gm.graph.print_tabular()

    def run(*args, **kwargs):
        print(f'running graph {id}')
        return gm.forward(*args, **kwargs)

    self.count += 1
    return run
</code></pre>
<p>f = dynamo.optimize(DebugCompiler(), dynamic=True)(f)</p>
<p>def test(x_sizes):<br />
    y = torch.zeros(5)<br />
    for size_of_x in x_sizes:<br />
        x = torch.tensor(range(size_of_x))<br />
        y = f(x, y)<br />
        print(y)</p>
<h1>Expected output:</h1>
<h1>```</h1>
<h1>compiling graph 0</h1>
<h1>running graph 0</h1>
<h1>compiling graph 1</h1>
<h1>running graph 1</h1>
<h1>running graph 0</h1>
<h1>running graph 1</h1>
<h1>```</h1>
<p>test([3, 3])</p>
<h1>Expected output:</h1>
<h1>```</h1>
<h1>running graph 0</h1>
<h1>running graph 1</h1>
<h1>running graph 0</h1>
<h1>running graph 0</h1>
<h1>running graph 1</h1>
<h1>running graph 0</h1>
<h1>running graph 1</h1>
<h1>running graph 0</h1>
<h1>running graph 1</h1>
<h1>```</h1>
<h1>Actual output:</h1>
<h1>```</h1>
<h1>running graph 0</h1>
<h1>running graph 1</h1>
<h1>running graph 0</h1>
<h1>running graph 0</h1>
<h1>running graph 0</h1>
<h1>running graph 0</h1>
<h1>```</h1>
<p>test([3, 4, 3, 3, 3])<br />
```</p>
<p>There are two subgraphs in <code>f</code>, <code>graph0</code> checks the shape of <code>x</code> and compare against a dictionary. <code>graph1</code> updates <code>y</code>. The execution of <code>graph1</code> depends on the output of the check of <code>graph1</code>.</p>
<p>The unexpected behavior is that once the "False" branch in <code>f</code> is triggered, <code>graph1</code> is never used. The code for updating <code>y</code> silently runs in eager mode. This is proven in the 2nd test case where for the first <code>3</code>, graph 1 is used, then the input <code>4</code> triggers the False branch, and afterwards, even running with <code>3</code> does not trigger the compiled graph anymore. The output <code>y</code> for each run is still correct however, which means, the code for updating y is running in eager mode instead of using <code>graph1</code>.</p>
<p>On the other hand if there is tensor computation in the False branch this problem does not happen.</p>
<p><code>def f(x, y):
    # begin graph 0
    x_len = x.shape[-1]
    update_idx = global_dict.get(x_len, None)
    # end graph 0
    if update_idx is not None:
        # begin graph 1
        y[update_idx] += 1
        # end graph 1
    else:
       # begin graph 2
        y[update_idx] -= 1
       # end graph 2
    return y</code></p>
<h3>Versions</h3>
<p>torch==2.1.0.dev20230331+cu117</p>
<p>cc @ezyang @soumith @msaroufim @wconstab @ngimel @bdhirsh</p>]]></description>
      <pubDate>Thu, 06 Apr 2023 12:07:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/98533</guid>
    </item>
    <item>
      <title>TORCH_COMPILE_DEBUG and TORCH_LOGS interact badly</title>
      <link>https://github.com/pytorch/pytorch/issues/98413</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Here's an example log: https://gist.github.com/ezyang/6e2904c8ecbd863eefcbee7456ada544 for this run:</p>
<p><code>TORCH_COMPILE_DEBUG=1 PYTHONUNBUFFERED=1 WANDB_DISABLED=true TORCH_LOGS=dynamo,inductor,guards CUDA_VISIBLE_DEVICES=3 PYTHONPATH=src pp python examples/pytorch/translation/run_translation.py --model_name_or_path t5-base --do_train --source_lang en --target_lang de --source_prefix 'translate English to German: ' --dataset_name stas/wmt14-en-de-pre-processed --output_dir /tmp/tst-translation --num_train_epochs 1 --per_device_train_batch_size=1 --max_train_samples 1000 --overwrite_output_dir --seed 1137 --per_device_eval_batch_size 1 --fp16 --torch_compile 2&gt;&amp;1 | tee comp.log</code></p>
<p>Some things to note:</p>
<ol>
<li>Despite not asking for it, I'm still getting inductor DEBUG logs printed to stderr:</li>
</ol>
<p><code>[2023-04-05 06:43:18,087] torch._inductor.codegen.triton.__schedule: [DEBUG] Schedule:</code></p>
<p>My intention for TORCH_COMPILE_DEBUG was to get the directory dump; I think it shouldn't interact with console output</p>
<ol>
<li>It keeps repeatedly printing this:</li>
</ol>
<p><code>04/05/2023 06:43:27 - WARNING - torch._logging._internal - Using TORCH_LOGS environment variable for log settings, ignoring call to set_logs
04/05/2023 06:43:27 - WARNING - torch._logging._internal - Using TORCH_LOGS environment variable for log settings, ignoring call to set_logs
04/05/2023 06:43:27 - WARNING - torch._logging._internal - Using TORCH_LOGS environment variable for log settings, ignoring call to set_logs</code></p>
<h3>Versions</h3>
<p>master</p>
<p>cc @soumith @msaroufim @wconstab @ngimel @bdhirsh</p>]]></description>
      <pubDate>Wed, 05 Apr 2023 06:03:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/98413</guid>
    </item>
    <item>
      <title>torch.compile not compatible with multiprocessing pool</title>
      <link>https://github.com/pytorch/pytorch/issues/97992</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Attempting to call <code>torch.compile()</code> within a multiprocessing pool results in:</p>
<p><code>sh
torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised AssertionError: daemonic processes are not allowed to have children</code></p>
<p>I am using <code>transformers</code> <code>ViTModel</code> to encode images and cache them to disk inside my DataLoader transform. To improve performance, I was hoping to use <code>torch.compile()</code> since I perform this pre-processing transform on CPU. However, compiling in my transform's <code>__init__</code> runs into pickling issues when the DataLoader forks the workers. As for compiling after the workers have forked, I run into the issue above.</p>
<h3>Error logs</h3>
<p><code>sh
RuntimeError: Caught BackendCompilerFailed in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 670, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py", line 1055, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/__init__.py", line 1388, in __call__
    from torch._inductor.compile_fx import compile_fx
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 21, in &lt;module&gt;
    from . import config, metrics, overrides, pattern_matcher
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/pattern_matcher.py", line 19, in &lt;module&gt;
    from .lowering import lowerings as L
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 3868, in &lt;module&gt;
    import_submodule(kernel)
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 1304, in import_submodule
    importlib.import_module(f"{mod.__name__}.{filename[:-3]}")
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/kernel/bmm.py", line 4, in &lt;module&gt;
    from ..select_algorithm import (
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/select_algorithm.py", line 20, in &lt;module&gt;
    from .codecache import code_hash, DiskCache, PyCodeCache
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 721, in &lt;module&gt;
    AsyncCompile.warm_pool()
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 660, in warm_pool
    pool._adjust_process_count()
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/concurrent/futures/process.py", line 692, in _adjust_process_count
    self._spawn_process()
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/concurrent/futures/process.py", line 709, in _spawn_process
    p.start()
  File "/home/jovyan/.conda/envs/videocv/lib/python3.10/multiprocessing/process.py", line 118, in start
    assert not _current_process._config.get('daemon'), \
AssertionError: daemonic processes are not allowed to have children</code></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>```sh<br />
PyTorch version: 2.0.0+cu117<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.7<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.26.0<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0] (64-bit runtime)<br />
Python platform: Linux-5.4.0-121-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.6.112<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA A40<br />
Nvidia driver version: 470.57.02<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.3.3<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.3.3<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.3.3<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.3.3<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.3.3<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.3.3<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.3.3<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU: removed</p>
<p>Versions of relevant libraries:<br />
[pip3] mypy==1.0.1<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.24.2<br />
[pip3] pytorch-optimizer==2.5.0<br />
[pip3] pytorch-triton==2.0.0+b8b470bc59<br />
[pip3] torch==2.0.0<br />
[pip3] torchinfo==1.7.2<br />
[pip3] torchvision==0.15.1<br />
[pip3] triton==2.0.0<br />
[conda] numpy                     1.24.2                   pypi_0    pypi<br />
[conda] pytorch-optimizer         2.5.0                    pypi_0    pypi<br />
[conda] pytorch-triton            2.0.0+b8b470bc59          pypi_0    pypi<br />
[conda] torch                     2.0.0                    pypi_0    pypi<br />
[conda] torchinfo                 1.7.2                    pypi_0    pypi<br />
[conda] torchvision               0.15.1                   pypi_0    pypi<br />
[conda] triton                    2.0.0                    pypi_0    pypi<br />
```</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @soumith @ngimel @Xia-Weiwen @desertfire</p>]]></description>
      <pubDate>Thu, 30 Mar 2023 08:45:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/97992</guid>
    </item>
    <item>
      <title>The first epoch is very slow when using torch.compile</title>
      <link>https://github.com/pytorch/pytorch/issues/97783</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>We tried using torch.compile on our training code and the first epoch was very slow.</p>
<h3>Error logs</h3>
<p>```bash</p>
<h1>uncompiled mode: out = model(x)</h1>
<p>$ python torch2_compile.py<br />
```<br />
2.0.0+cu117<br />
/mnt/nfs/env/py-venv/py39_cuda117_torch200/lib/python3.9/site-packages/torch/autograd/<strong>init</strong>.py:200: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)<br />
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass<br />
<strong>Epoch 0/10 time: 0.09859752655029297</strong><br />
Epoch 1/10 time: 0.09461498260498047<br />
Epoch 2/10 time: 0.09749722480773926<br />
Epoch 3/10 time: 0.09946417808532715<br />
Epoch 4/10 time: 0.09831523895263672<br />
Epoch 5/10 time: 0.0978553295135498<br />
Epoch 6/10 time: 0.09766864776611328<br />
Epoch 7/10 time: 0.0983278751373291<br />
Epoch 8/10 time: 0.09094047546386719<br />
Epoch 9/10 time: 0.09506630897521973<br />
Epoch avg time: 0.09683477878570557</p>
<p>```bash</p>
<h1>compiled mode: out = compiled_model(x)</h1>
<p>$ python torch2_compile.py<br />
```<br />
2.0.0+cu117<br />
/mnt/nfs/env/py-venv/py39_cuda117_torch200/lib/python3.9/site-packages/torch/cuda/<strong>init</strong>.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)<br />
  return torch._C._cuda_getDeviceCount() &gt; 0<br />
No CUDA runtime is found, using CUDA_HOME='/mnt/nfs/env/cuda/cuda-11.7'<br />
[2023-03-28 22:20:57,063] torch._inductor.utils: [WARNING] using triton random, expect difference from eager<br />
<strong>Epoch 0/10 time: 18.54069495201111</strong><br />
Epoch 1/10 time: 0.10959434509277344<br />
Epoch 2/10 time: 0.10520339012145996<br />
Epoch 3/10 time: 0.10396909713745117<br />
Epoch 4/10 time: 0.10267758369445801<br />
Epoch 5/10 time: 0.10393261909484863<br />
Epoch 6/10 time: 0.10507917404174805<br />
Epoch 7/10 time: 0.10506296157836914<br />
Epoch 8/10 time: 0.10470414161682129<br />
Epoch 9/10 time: 0.10574936866760254<br />
Epoch avg time: 1.948666763305664</p>
<h3>Minified repro</h3>
<p><code>bash
$ CUDA_VISIBLE_DEVICES=0
$ python torch2_compile.py</code></p>
<p>```bash<br />
$ cat torch2_compile.py</p>
<h1>!/usr/bin/env python3</h1>
<p>import time<br />
import torch<br />
import torch._dynamo as dynamo<br />
import torchvision.models as models</p>
<p>model = models.alexnet()</p>
<p>optimizer = torch.optim.SGD(model.parameters(), lr=0.1)<br />
compiled_model = torch.compile(model)</p>
<p>x = torch.randn(16, 3, 224, 224)<br />
optimizer.zero_grad()</p>
<p>epoches=10<br />
count = []<br />
for epoch in range(epoches):<br />
    start = time.time()</p>
<pre><code>#out = model(x)
out = compiled_model(x)
out.sum().backward()
optimizer.step()

end = time.time()
count.append(end - start)
print(f"Epoch {epoch}/{epoches} time: {end - start}")
</code></pre>
<p>print(f"Epoch avg time: {sum(count)/len(count)}")</p>
<p>```</p>
<h3>Versions</h3>
<p>$ python ~/env/bin/collect_env.py<br />
Collecting environment information...<br />
/mnt/nfs/env/py-venv/py39_cuda117_torch200/lib/python3.9/site-packages/torch/cuda/<strong>init</strong>.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)<br />
  return torch._C._cuda_getDeviceCount() &gt; 0<br />
PyTorch version: 2.0.0+cu117<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.7<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: CentOS Linux release 7.3.1611 (Core)  (x86_64)<br />
GCC version: (GCC) 7.5.0<br />
Clang version: Could not collect<br />
CMake version: version 3.26.1<br />
Libc version: glibc-2.17</p>
<p>Python version: 3.9.15 (main, Nov 12 2022, 16:52:40)  [GCC 5.5.0] (64-bit runtime)<br />
Python platform: Linux-3.10.0-693.21.1.std7a.el7.0.x86_64-x86_64-with-glibc2.17<br />
Is CUDA available: False<br />
CUDA runtime version: 11.7.64<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A100-SXM4-80GB<br />
GPU 1: NVIDIA A100-SXM4-80GB<br />
GPU 2: NVIDIA A100-SXM4-80GB<br />
GPU 3: NVIDIA A100-SXM4-80GB<br />
GPU 4: NVIDIA A100-SXM4-80GB<br />
GPU 5: NVIDIA A100-SXM4-80GB<br />
GPU 6: NVIDIA A100-SXM4-80GB<br />
GPU 7: NVIDIA A100-SXM4-80GB</p>
<p>Nvidia driver version: 530.30.02<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:          x86_64<br />
CPU op-mode(s):        32-bit, 64-bit<br />
Byte Order:            Little Endian<br />
CPU(s):                128<br />
On-line CPU(s) list:   0-127<br />
Thread(s) per core:    2<br />
Core(s) per socket:    32<br />
Socket(s):             2<br />
NUMA node(s):          2<br />
Vendor ID:             GenuineIntel<br />
CPU family:            6<br />
Model:                 106<br />
Model name:            Intel(R) Xeon(R) Platinum 8350C CPU @ 2.60GHz<br />
Stepping:              6<br />
CPU MHz:               3500.000<br />
BogoMIPS:              5206.54<br />
Virtualization:        VT-x<br />
L1d cache:             48K<br />
L1i cache:             32K<br />
L2 cache:              1280K<br />
L3 cache:              49152K<br />
NUMA node0 CPU(s):     0-31,64-95<br />
NUMA node1 CPU(s):     32-63,96-127</p>
<p>Versions of relevant libraries:<br />
[pip3] audio2numpy==0.1.2<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.23.5<br />
[pip3] torch==2.0.0<br />
[pip3] torchaudio==2.0.1<br />
[pip3] torchvision==0.15.1<br />
[pip3] triton==2.0.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @soumith @msaroufim @wconstab @ngimel @bdhirsh</p>]]></description>
      <pubDate>Tue, 28 Mar 2023 06:49:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/97783</guid>
    </item>
    <item>
      <title>torch.compile not work in WSL</title>
      <link>https://github.com/pytorch/pytorch/issues/97501</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>`</code><br />
<code></code>import torch<br />
class TestSig(torch.nn.Module):<br />
   def <strong>init</strong>(self):<br />
      super().<strong>init</strong>()<br />
   def forward(self, x):<br />
      return torch.sigmoid(x)</p>
<p>torch._dynamo.config.verbose=True<br />
opt_cpu = torch.compile(TestSig())<br />
print("cpu:", opt_cpu(torch.randn(1)))<br />
cuda_eager = TestSig().cuda()<br />
print("cuda eager:", cuda_eager(torch.randn(1).cuda()))<br />
opt_cuda = torch.compile(TestSig()).cuda() #torch.compile(TestSig().cuda()) also fails<br />
print("cuda opt:", opt_cuda(torch.randn(1).cuda()))`</p>
<p>and I get:<br />
cpu: tensor([0.3198])<br />
cuda eager: tensor([0.4224], device='cuda:0')<br />
/usr/bin/ld: cannot find -lcuda: No such file or directory<br />
collect2: error: ld returned 1 exit status<br />
concurrent.futures.process.<em>RemoteTraceback: <br />
"""<br />
Traceback (most recent call last):<br />
  File "/root/anaconda3/lib/python3.9/concurrent/futures/process.py", line 246, in _process_worker<br />
    r = call_item.fn(<em>call_item.args, </em>*call_item.kwargs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 549, in _worker_compile<br />
    kernel.precompile(warm_cache_only_with_cc=cc)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/triton_ops/autotune.py", line 69, in precompile<br />
    self.launchers = [<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/triton_ops/autotune.py", line 70, in <listcomp><br />
    self._precompile_config(c, warm_cache_only_with_cc)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/triton_ops/autotune.py", line 83, in _precompile_config<br />
    triton.compile(<br />
  File "/root/anaconda3/lib/python3.9/site-packages/triton/compiler.py", line 1587, in compile<br />
    so_path = make_stub(name, signature, constants)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/triton/compiler.py", line 1476, in make_stub<br />
    so = _build(name, src_path, tmpdir)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/triton/compiler.py", line 1391, in _build<br />
    ret = subprocess.check_call(cc_cmd)<br />
  File "/root/anaconda3/lib/python3.9/subprocess.py", line 373, in check_call<br />
    raise CalledProcessError(retcode, cmd)<br />
subprocess.CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmphttesy02/main.c', '-O3', '-I/root/anaconda3/lib/python3.9/site-packages/triton/third_party/cuda/include', '-I/root/anaconda3/include/python3.9', '-I/tmp/tmphttesy02', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmphttesy02/triton</em>.cpython-39-x86_64-linux-gnu.so']' returned non-zero exit status 1.<br />
"""</p>
<p>The above exception was the direct cause of the following exception:</p>
<p>Traceback (most recent call last):<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/<em>dynamo/output_graph.py", line 670, in call_user_compiler<br />
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py", line 1055, in debug_wrapper<br />
    compiled_gm = compiler_fn(gm, example_inputs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/<strong>init</strong>.py", line 1390, in <strong>call</strong><br />
    return compile_fx(model</em>, inputs_, config_patches=self.config)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/<em>inductor/compile_fx.py", line 455, in compile_fx<br />
    return aot_autograd(<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/backends/common.py", line 48, in compiler_fn<br />
    cg = aot_module_simplified(gm, example_inputs, <strong>kwargs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 2805, in aot_module_simplified<br />
    compiled_fn = create_aot_dispatcher_function(<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 2498, in create_aot_dispatcher_function<br />
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1713, in aot_wrapper_dedupe<br />
    return compiler_fn(flat_fn, leaf_flat_args, aot_config)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1326, in aot_dispatch_base<br />
    compiled_fw = aot_config.fw_compiler(fw_module, flat_args_with_views_handled)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
    r = func(<em>args, </em><em>kwargs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 430, in fw_compiler<br />
    return inner_compile(<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py", line 595, in debug_wrapper<br />
    compiled_fn = compiler_fn(gm, example_inputs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/debug.py", line 239, in inner<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/root/anaconda3/lib/python3.9/contextlib.py", line 79, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 177, in compile_fx_inner<br />
    compiled_fn = graph.compile_to_fn()<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/graph.py", line 586, in compile_to_fn<br />
    return self.compile_to_module().call<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
    r = func(<em>args, </em>*kwargs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/graph.py", line 575, in compile_to_module<br />
    mod = PyCodeCache.load(code)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 528, in load<br />
    exec(code, mod.<strong>dict</strong>, mod.<strong>dict</strong>)<br />
  File "/tmp/torchinductor_root/fl/cflyzetaelrdigwqk7eeqcd4ltjygnu2ngsoprkrcxeecyg274xg.py", line 42, in <module><br />
    async_compile.wait(globals())<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 715, in wait<br />
    scope[key] = result.result()<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 573, in result<br />
    self.future.result()<br />
  File "/root/anaconda3/lib/python3.9/concurrent/futures/_base.py", line 446, in result<br />
    return self.__get_result()<br />
  File "/root/anaconda3/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result<br />
    raise self._exception<br />
subprocess.CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmphttesy02/main.c', '-O3', '-I/root/anaconda3/lib/python3.9/site-packages/triton/third_party/cuda/include', '-I/root/anaconda3/include/python3.9', '-I/tmp/tmphttesy02', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmphttesy02/triton</em>.cpython-39-x86_64-linux-gnu.so']' returned non-zero exit status 1.</p>
<p>The above exception was the direct cause of the following exception:</p>
<p>Traceback (most recent call last):<br />
  File "/data/py_projects/pegasus/test4.py", line 15, in <module><br />
    print("cuda opt:", opt_cuda(torch.randn(1).cuda()))<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in <em>call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 82, in forward<br />
    return self.dynamo_ctx(self._orig_mod.forward)(</em>args, <strong>kwargs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 337, in catch_errors<br />
    return callback(frame, cache_size, hooks)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 404, in _convert_frame<br />
    result = inner_convert(frame, cache_size, hooks)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 104, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 262, in _convert_frame_assert<br />
    return _compile(<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 324, in _compile<br />
    out_code = transform_code_object(code, transform)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py", line 445, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 311, in transform<br />
    tracer.run()<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 1726, in run<br />
    super().run()<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 576, in run<br />
    and self.step()<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 540, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 1792, in RETURN_VALUE<br />
    self.output.compile_subgraph(<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 517, in compile_subgraph<br />
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 588, in compile_and_call_fx_graph<br />
    compiled_fn = self.call_user_compiler(gm)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/root/anaconda3/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 675, in call_user_compiler<br />
    raise BackendCompilerFailed(self.compiler_fn, e) from e<br />
torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmphttesy02/main.c', '-O3', '-I/root/anaconda3/lib/python3.9/site-packages/triton/third_party/cuda/include', '-I/root/anaconda3/include/python3.9', '-I/tmp/tmphttesy02', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmphttesy02/triton</em>.cpython-39-x86_64-linux-gnu.so']' returned non-zero exit status 1.</p>
<p>Set torch._dynamo.config.verbose=True for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    torch._dynamo.config.suppress_errors = True</p>
<h3>Versions</h3>
<p>wsl2 pytorch2.0 cuda11.7</p>
<p>cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @vladimir-aubrecht @iremyux @Blackhex @cristianPanaite @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @soumith @ngimel</p>]]></description>
      <pubDate>Thu, 23 Mar 2023 22:17:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/97501</guid>
    </item>
    <item>
      <title>[compile] KeyError: example_value</title>
      <link>https://github.com/pytorch/pytorch/issues/97079</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>model.compile()</code> (defaults) gives me:</p>
<p><code>stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing forward
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing wrapped_fn
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing range_push
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in range_push&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in wrapped_fn&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing _pre_forward_module_hook
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing pre_sub_module_forward_function
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing record_module
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in pre_sub_module_forward_function&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing wrapped_fn
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in range_push&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in wrapped_fn&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing fetch_sub_module
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing iter_params
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing wrapped_fn
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in range_push&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in wrapped_fn&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing get_all_parameters
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing external_parameters
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing range_pop
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in range_pop&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in fetch_sub_module&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing is_initialized
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing get_rank
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo done tracing get_rank (RETURN_VALUE)
stderr:  - 5597 - torch._dynamo.output_graph - Step 2: calling compiler function debug_wrapper
stderr:  - 5597 - torch._dynamo.output_graph - Step 2: done compiler function debug_wrapper
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in range_push&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing current_stream
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo done tracing current_stream (RETURN_VALUE)
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in pre_sub_module_forward_function&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing _post_backward_module_hook
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing _post_backward_module_hook
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing _apply_to_tensors_only
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing forward
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in forward&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo done tracing &lt;graph break in forward&gt; (RETURN_VALUE)
stderr:  - 5597 - torch._dynamo.output_graph - Step 2: calling compiler function debug_wrapper
stderr:  - 5597 - torch._inductor.compile_fx - Step 3: torchinductor compiling FORWARDS graph 1
stderr:  - 5597 - torch._inductor.compile_fx - Step 3: torchinductor done compiling FORWARDS graph 1
stderr:  - 5597 - torch._dynamo.output_graph - Step 2: done compiler function debug_wrapper
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing is_zero_param
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing forward
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing _pre_forward_module_hook
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing pre_sub_module_forward_function
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing record_module
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in pre_sub_module_forward_function&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing fetch_sub_module
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing iter_params
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing get_all_parameters
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing register_external_parameter
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in register_external_parameter&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing all_gather
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in range_push&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing _all_gather
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing _ensure_availability_of_partitioned_params
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in _all_gather&gt;
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing _allgather_params
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in _allgather_params&gt;
stderr:  - 5597 - torch._dynamo.output_graph - Step 2: calling compiler function debug_wrapper
stderr:  - 5597 - torch._inductor.compile_fx - Step 3: torchinductor compiling FORWARDS graph 2
stderr:  - 5597 - torch._inductor.compile_fx - Step 3: torchinductor done compiling FORWARDS graph 2
stderr:  - 5597 - torch._dynamo.output_graph - Step 2: done compiler function debug_wrapper
stderr:  - 5597 - torch._dynamo.symbolic_convert - Step 1: torchdynamo start tracing &lt;graph break in _allgather_params&gt;
stderr: Traceback (most recent call last):
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py", line 324, in _compile
stderr:     out_code = transform_code_object(code, transform)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/bytecode_transformation.py", line 445, in transform_code_object
stderr:     transformations(instructions, code_options)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py", line 311, in transform
stderr:     tracer.run()
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 1726, in run
stderr:     super().run()
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 576, in run
stderr:     and self.step()
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 540, in step
stderr:     getattr(self, inst.opname)(inst)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 342, in wrapper
stderr:     return inner_fn(self, inst)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 965, in CALL_FUNCTION
stderr:     self.call_function(fn, args, {})
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 474, in call_function
stderr:     self.push(fn.call_function(self, args, kwargs))
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/misc.py", line 744, in call_function
stderr:     return self.obj.call_method(tx, self.name, args, kwargs).add_options(self)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/tensor.py", line 424, in call_method
stderr:     return wrap_fx_proxy(
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/builder.py", line 754, in wrap_fx_proxy
stderr:     return wrap_fx_proxy_cls(
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/builder.py", line 789, in wrap_fx_proxy_cls
stderr:     example_value = get_fake_value(proxy.node, tx)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py", line 1134, in get_fake_value
stderr:     args, kwargs = torch.fx.node.map_arg((node.args, node.kwargs), visit)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/fx/node.py", line 621, in map_arg
stderr:     return map_aggregate(a, lambda x: fn(x) if isinstance(x, Node) else x)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/fx/node.py", line 629, in map_aggregate
stderr:     t = tuple(map_aggregate(elem, fn) for elem in a)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/fx/node.py", line 629, in &lt;genexpr&gt;
stderr:     t = tuple(map_aggregate(elem, fn) for elem in a)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/fx/node.py", line 629, in map_aggregate
stderr:     t = tuple(map_aggregate(elem, fn) for elem in a)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/fx/node.py", line 629, in &lt;genexpr&gt;
stderr:     t = tuple(map_aggregate(elem, fn) for elem in a)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/fx/node.py", line 639, in map_aggregate
stderr:     return fn(a)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/fx/node.py", line 621, in &lt;lambda&gt;
stderr:     return map_aggregate(a, lambda x: fn(x) if isinstance(x, Node) else x)
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py", line 1132, in visit
stderr:     return n.meta["example_value"]
stderr: KeyError: example_value
stderr: 
stderr: from user code:
stderr:    File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partition_parameters.py", line 1333, in &lt;graph break in _allgather_params&gt;
stderr:     partitions[i].narrow(0,</code></p>
<h3>Versions</h3>
<p>pt-2.0</p>
<p>@Chillee </p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @soumith @ngimel</p>]]></description>
      <pubDate>Fri, 17 Mar 2023 21:59:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/97079</guid>
    </item>
    <item>
      <title>[compile] TypeError: __init__() missing 1 required positional argument: 'parent_module'</title>
      <link>https://github.com/pytorch/pytorch/issues/97078</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>model.compile()</code> (defaults) gives me:</p>
<p>```<br />
stderr:  - 5543 - torch.<em>dynamo.symbolic_convert - Step 1: torchdynamo start tracing forward<br />
stderr: Traceback (most recent call last):<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py", line 324, in _compile<br />
stderr:     out_code = transform_code_object(code, transform)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/bytecode_transformation.py", line 445, in transform_code_object<br />
stderr:     transformations(instructions, code_options)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py", line 311, in transform<br />
stderr:     tracer.run()<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 1726, in run<br />
stderr:     super().run()<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 576, in run<br />
stderr:     and self.step()<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 540, in step<br />
stderr:     getattr(self, inst.opname)(inst)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 342, in wrapper<br />
stderr:     return inner_fn(self, inst)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 1014, in CALL_FUNCTION_KW<br />
stderr:     self.call_function(fn, args, kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 474, in call_function<br />
stderr:     self.push(fn.call_function(self, args, kwargs))<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/nn_module.py", line 244, in call_function<br />
stderr:     return tx.inline_user_function_return(<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 510, in inline_user_function_return<br />
stderr:     result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 1806, in inline_call<br />
stderr:     return cls.inline_call</em>(parent, func, args, kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/<em>dynamo/symbolic_convert.py", line 1862, in inline_call</em><br />
stderr:     tracer.run()<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/<em>dynamo/symbolic_convert.py", line 576, in run<br />
stderr:     and self.step()<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 540, in step<br />
stderr:     getattr(self, inst.opname)(inst)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 342, in wrapper<br />
stderr:     return inner_fn(self, inst)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 1014, in CALL_FUNCTION_KW<br />
stderr:     self.call_function(fn, args, kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 474, in call_function<br />
stderr:     self.push(fn.call_function(self, args, kwargs))<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/nn_module.py", line 244, in call_function<br />
stderr:     return tx.inline_user_function_return(<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 510, in inline_user_function_return<br />
stderr:     result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 1806, in inline_call<br />
stderr:     return cls.inline_call</em>(parent, func, args, kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/<em>dynamo/symbolic_convert.py", line 1862, in inline_call</em><br />
stderr:     tracer.run()<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/<em>dynamo/symbolic_convert.py", line 576, in run<br />
stderr:     and self.step()<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 540, in step<br />
stderr:     getattr(self, inst.opname)(inst)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 342, in wrapper<br />
stderr:     return inner_fn(self, inst)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 965, in CALL_FUNCTION<br />
stderr:     self.call_function(fn, args, {})<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 474, in call_function<br />
stderr:     self.push(fn.call_function(self, args, kwargs))<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/nn_module.py", line 244, in call_function<br />
stderr:     return tx.inline_user_function_return(<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 510, in inline_user_function_return<br />
stderr:     result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 1806, in inline_call<br />
stderr:     return cls.inline_call</em>(parent, func, args, kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/<em>dynamo/symbolic_convert.py", line 1862, in inline_call</em><br />
stderr:     tracer.run()<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 576, in run<br />
stderr:     and self.step()<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 540, in step<br />
stderr:     getattr(self, inst.opname)(inst)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 342, in wrapper<br />
stderr:     return inner_fn(self, inst)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 965, in CALL_FUNCTION<br />
stderr:     self.call_function(fn, args, {})<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py", line 474, in call_function<br />
stderr:     self.push(fn.call_function(self, args, kwargs))<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/nn_module.py", line 203, in call_function<br />
stderr:     return wrap_fx_proxy(<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/builder.py", line 754, in wrap_fx_proxy<br />
stderr:     return wrap_fx_proxy_cls(<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/variables/builder.py", line 789, in wrap_fx_proxy_cls<br />
stderr:     example_value = get_fake_value(proxy.node, tx)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py", line 1143, in get_fake_value<br />
stderr:     nnmodule = deepcopy_to_fake_tensor(nnmodule, tx.fake_mode)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py", line 819, in deepcopy_to_fake_tensor<br />
stderr:     return wrap_fake_exception(lambda: copy.deepcopy(obj))<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py", line 808, in wrap_fake_exception<br />
stderr:     return fn()<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py", line 819, in <lambda><br />
stderr:     return wrap_fake_exception(lambda: copy.deepcopy(obj))<br />
stderr:   File "/usr/lib/python3.8/copy.py", line 172, in deepcopy<br />
stderr:     y = _reconstruct(x, memo, <em>rv)<br />
stderr:   File "/usr/lib/python3.8/copy.py", line 270, in _reconstruct<br />
stderr:     state = deepcopy(state, memo)<br />
stderr:   File "/usr/lib/python3.8/copy.py", line 146, in deepcopy<br />
stderr:     y = copier(x, memo)<br />
stderr:   File "/usr/lib/python3.8/copy.py", line 230, in _deepcopy_dict<br />
stderr:     y[deepcopy(key, memo)] = deepcopy(value, memo)<br />
stderr:   File "/usr/lib/python3.8/copy.py", line 172, in deepcopy<br />
stderr:     y = _reconstruct(x, memo, </em>rv)<br />
stderr:   File "/usr/lib/python3.8/copy.py", line 264, in _reconstruct<br />
stderr:     y = func(*args)<br />
stderr: TypeError: <strong>init</strong>() missing 1 required positional argument: 'parent_module'</p>
<p>tderr: Set torch._dynamo.config.verbose=True for more information<br />
stderr: <br />
stderr: <br />
stderr: You can suppress this exception and fall back to eager by setting:<br />
stderr:     torch._dynamo.config.suppress_errors = True<br />
stderr: <br />
stderr: <br />
stderr: The above exception was the direct cause of the following exception:<br />
stderr: <br />
stderr: Traceback (most recent call last):<br />
stderr:   File "/actions-runner/_work/m4/m4/m4/training/main.py", line 176, in <module><br />
stderr:     train_logs = trainer.train()<br />
stderr:   File "/actions-runner/_work/m4/m4/m4/training/trainer.py", line 1438, in train<br />
stderr:     ) = self._do_batch(<br />
stderr:   File "/actions-runner/_work/m4/m4/m4/training/trainer.py", line 560, in _do_batch<br />
stderr:     vl_output = self.vl_model(<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl<br />
stderr:     return forward_call(<em>args, </em><em>kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn<br />
stderr:     ret_val = func(</em>args, <strong>kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1832, in forward<br />
stderr:     loss = self.module(*inputs, </strong>kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
stderr:     result = forward_call(<em>args, </em><em>kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 82, in forward<br />
stderr:     return self.dynamo_ctx(self._orig_mod.forward)(</em>args, <strong>kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn<br />
stderr:     return fn(*args, </strong>kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 337, in catch_errors<br />
stderr:     return callback(frame, cache_size, hooks)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py", line 404, in _convert_frame<br />
stderr:     result = inner_convert(frame, cache_size, hooks)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py", line 104, in _fn<br />
stderr:     return fn(<em>args, </em><em>kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py", line 262, in _convert_frame_assert<br />
stderr:     return _compile(<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
stderr:     r = func(</em>args, **kwargs)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py", line 394, in _compile<br />
stderr:     raise InternalTorchDynamoError() from e<br />
stderr: torch._dynamo.exc.InternalTorchDynamoError<br />
stderr: ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 6786) of binary: /usr/bin/python3<br />
stderr: Traceback (most recent call last):<br />
stderr:   File "/usr/local/bin/accelerate", line 8, in <module><br />
stderr:     sys.exit(main())<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/accelerate/commands/accelerate_cli.py", line 45, in main<br />
stderr:     args.func(args)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/accelerate/commands/launch.py", line 900, in launch_command<br />
stderr:     deepspeed_launcher(args)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/accelerate/commands/launch.py", line 643, in deepspeed_launcher<br />
stderr:     distrib_run.run(args)<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/distributed/run.py", line 785, in run<br />
stderr:     elastic_launch(<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/distributed/launcher/api.py", line 134, in <strong>call</strong><br />
stderr:     return launch_agent(self._config, self._entrypoint, list(args))<br />
stderr:   File "/usr/local/lib/python3.8/dist-packages/torch/distributed/launcher/api.py", line 250, in launch_agent<br />
stderr:     raise ChildFailedError(<br />
stderr: torch.distributed.elastic.multiprocessing.errors.ChildFailedError:</p>
<p>```</p>
<h3>Versions</h3>
<p>pt-2.0</p>
<p>@Chillee </p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @soumith @ngimel</p>]]></description>
      <pubDate>Fri, 17 Mar 2023 21:55:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/97078</guid>
    </item>
    <item>
      <title>torch.compile mode="max-autotune" precision appears to be lower</title>
      <link>https://github.com/pytorch/pytorch/issues/96693</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I'm sorry, this is going to be a terrible bug report, but I wasn't able to simplify the problem. </p>
<p>I'm running a training run with a model that is compiled with <code>torch.compile</code>. The blue line below is loss with the <code>default</code> mode, the red line below is loss with <code>max-autotune</code>, the training runs are otherwise identical:</p>
<p><img alt="spiky_comparison" src="https://user-images.githubusercontent.com/22680696/224837823-788ccd48-6fb0-4099-9c87-46926dca134f.png" /></p>
<p>It appears as if precision/stability is noticeably lower on the max-autotune training run. This is running on an RTXA600 GPU. The forward pass is wrapped in <br />
<code>torch.autocast</code>, there are no graph breaks or warning messages. Max-autotune code is also noticeably a bit faster, so it is definitely doing something that the default mode is not.</p>
<p>Feel free to close this issue if this is not enough info. I could also test more specific things or try to make a mini version to get to the bottom of this, but I have no idea what to look for, or which part would be relevant.</p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.1.0.dev20230307<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Pop!_OS 22.04 LTS (x86_64)<br />
GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0<br />
Clang version: Could not collect<br />
CMake version: Could not collect<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-6.2.0-76060200-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU<br />
Nvidia driver version: 525.85.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>Versions of relevant libraries:<br />
[pip3] lion-pytorch==0.0.7<br />
[pip3] numpy==1.23.5<br />
[pip3] torch==2.1.0.dev20230307<br />
[pip3] torchaudio==2.0.0.dev20230307<br />
[pip3] torchvision==0.15.0.dev20230307<br />
[conda] blas                      1.0                         mkl<br />
[conda] lion-pytorch              0.0.7                    pypi_0    pypi<br />
[conda] mkl                       2021.4.0           h06a4308_640<br />
[conda] mkl-service               2.4.0           py310h7f8727e_0<br />
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0<br />
[conda] mkl_random                1.2.2           py310h00e6091_0<br />
[conda] numpy                     1.23.5          py310hd5efca6_0<br />
[conda] numpy-base                1.23.5          py310h8e6c178_0<br />
[conda] pytorch                   2.1.0.dev20230307 py3.10_cuda11.8_cudnn8.7.0_0    pytorch-nightly<br />
[conda] pytorch-cuda              11.8                 h7e8668a_3    pytorch-nightly<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly<br />
[conda] torchaudio                2.0.0.dev20230307     py310_cu118    pytorch-nightly<br />
[conda] torchtriton               2.0.0+b8b470bc59           py310    pytorch-nightly<br />
[conda] torchvision               0.15.0.dev20230307     py310_cu118    pytorch-nightly</p>
<p>```</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @soumith @ngimel</p>]]></description>
      <pubDate>Mon, 13 Mar 2023 13:46:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/96693</guid>
    </item>
  </channel>
</rss>
