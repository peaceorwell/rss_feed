<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[Inductor] Update the cpp_wrapper entry function signature</title>
      <link>https://github.com/pytorch/pytorch/pull/121745</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121745<br />
* #121744<br />
* #121743<br />
* #121523</p>
<p>Summary: Update the entry function to use AtenTensorHandle instead of at::Tensor. This makes the compilation of the generated cpp wrapper code much faster: test_cpu_cpp_wrapper.py from 35 min to 21 min, and test_cuda_cpp_wrapper.py from 21 min to 14 min.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54818715">D54818715</a></p>]]></description>
      <pubDate>Tue, 12 Mar 2024 11:20:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121745</guid>
    </item>
    <item>
      <title>[Compiled Autograd] Reorder accumulate grad nodes</title>
      <link>https://github.com/pytorch/pytorch/pull/121735</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121610<br />
* #120965<br />
* <strong>-&gt;</strong> #121735</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 09:32:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121735</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Add bmm support</title>
      <link>https://github.com/pytorch/pytorch/pull/121734</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* <strong>-&gt;</strong> #121734<br />
* #121497<br />
* #121491<br />
* #121490<br />
* #121489</p>
<p>Add support for bmm ( batch matrix multiply ) op through the Cutlass backend.</p>
<p>Test Plan:<br />
 * CI<br />
 * Added test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 09:23:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121734</guid>
    </item>
    <item>
      <title>[compiled autograd] free stack objects before calling compiled graph</title>
      <link>https://github.com/pytorch/pytorch/pull/121707</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121707<br />
* #121698</p>
<p>Moved compilation code into _compiled_autograd_impl, frees stack allocated objects e.g. AutogradCompilerCall</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 20:45:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121707</guid>
    </item>
    <item>
      <title>Enable FX graph caching in another batch of inductor tests</title>
      <link>https://github.com/pytorch/pytorch/pull/121697</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121697</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 17:25:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121697</guid>
    </item>
    <item>
      <title>Enable FX graph cache for a batch of inductor tests</title>
      <link>https://github.com/pytorch/pytorch/pull/121696</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121696</p>
<p>Summary: Get more FX graph cache coverage by enabling it for these unit tests</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 17:25:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121696</guid>
    </item>
    <item>
      <title>Support `triton.language.dtype` with `torch.compile`</title>
      <link>https://github.com/pytorch/pytorch/pull/121690</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121739<br />
* <strong>-&gt;</strong> #121690</p>
<p>Putting this PR as an RFC since I have resorted to some horrible hacks in order to make this work.<br />
<code>(Pdb) p triton.language.float32
triton.language.fp32
(Pdb) p str(triton.language.float32)
'fp32'
(Pdb) p repr(triton.language.float32)
'triton.language.fp32'</code><br />
This means that we need to "rewrite" them for fx graph and inductor execution.</p>
<p>This PR allows Mamba2 to work with <code>torch.compile</code>.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 16:03:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121690</guid>
    </item>
    <item>
      <title>[export] Remove aot_compile in benchmarks</title>
      <link>https://github.com/pytorch/pytorch/pull/121643</link>
      <description><![CDATA[<p>Fixes #ISSUE_NUMBER</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 09:26:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121643</guid>
    </item>
    <item>
      <title>[Inductor] [ReImplement] Outer Loop Fusion for CPP Backend</title>
      <link>https://github.com/pytorch/pytorch/pull/121625</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121625</p>
<p><strong>Summary</strong><br />
Re-implement of https://github.com/pytorch/pytorch/pull/121064</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_outer_loop_fusion</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 10 Mar 2024 23:44:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121625</guid>
    </item>
    <item>
      <title>[dynamo] Refactor TorchInGraphFunctionVariable for compile time</title>
      <link>https://github.com/pytorch/pytorch/pull/121616</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121616<br />
* #121615</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Sun, 10 Mar 2024 21:06:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121616</guid>
    </item>
    <item>
      <title>[dynamo] Minor compile time optimizations in torch.py</title>
      <link>https://github.com/pytorch/pytorch/pull/121615</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121616<br />
* <strong>-&gt;</strong> #121615</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Sun, 10 Mar 2024 21:06:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121615</guid>
    </item>
    <item>
      <title>SIGILL / ILL_ILLOPN crash when using AOT Inductor pre-compiled model (torch 2.2)</title>
      <link>https://github.com/pytorch/pytorch/issues/121516</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>We are getting a crash with this backtrace:</p>
<p>torch::inductor::AOTIModelContainerRunner::AOTIModelContainerRunner <br />
at::DynamicLibrary::sym <br />
(dso) (no symbol)<br />
(libc) (no symbol)<br />
(dso) -&gt; crash</p>
<p>We checked the processors flags were the problem happened (it doesn't happen everywhere), and the machine were we compile has more flags (avx512xxx) than the machines where it runs.</p>
<p>I think our next step will be to add a wrapper around g++, that disable some avx instructions, and point CXX to it, but maybe there's a better way.</p>
<p>(I noticed an 'ISA' option but I'm not sure on how to use it).</p>
<p>```python</p>
<h1>config specific to codegen/cpp.py</h1>
<p>class cpp:<br />
    # set to torch.get_num_threads()<br />
    threads = -1</p>
<pre><code># Do not generate loops when the condition doesn't hold, like:
# for(long i0=4096; i0&lt;4096; i0+=1)
no_redundant_loops = True

# Assume number of threads is dynamic, don't specialize thread number.
# Kernels don't recompile on thread number changes with this flag on.
# For single-threaded workload, turning it on would incur a slight
# performance degradation.
dynamic_threads = False

simdlen: Optional[int] = None
min_chunk_size = 4096
cxx = (
    None,  # download gcc12 from conda-forge if conda is installed
    # "g++-12",
    # "g++-11",
    # "g++-10",
    # "clang++",
    os.environ.get("CXX", "clang++" if sys.platform == "darwin" else "g++"),
    # "g++.par",
)
# Allow kernel performance profiling via PyTorch profiler
enable_kernel_profile = False

# enable weight prepacking to get a better performance; may lead to large memory footprint
weight_prepack = True

# Inject a bug into our relu implementation; useful for testing our repro
# extraction and minification functionality.
# Valid values: "compile_error", "runtime_error", "accuracy"
inject_relu_bug_TESTING_ONLY: Optional[str] = None
inject_log1p_bug_TESTING_ONLY: Optional[str] = None

# If None, autodetect whether or not AVX512/AVX2 can be used.  Otherwise,
# force usage as specified, without testing.
vec_isa_ok: Optional[bool] = None
</code></pre>
<p>```</p>
<p>We could also try to hack and remove this:</p>
<p><code>if sys.platform == "darwin":
        # Per https://mac.r-project.org/openmp/ right way to pass `openmp` flags to MacOS is via `-Xclang`
        # Also, `-march=native` is unrecognized option on M1
        base_flags += " -Xclang"
    else:
        if platform.machine() == "ppc64le":
            base_flags += " -mcpu=native"
        else:
            base_flags += " -march=native"</code></p>
<p>(-march=native)</p>
<h3>Versions</h3>
<p>pytorch-2.2.0</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 10:12:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121516</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Tolerate dynamic shapes</title>
      <link>https://github.com/pytorch/pytorch/pull/121497</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121734<br />
* <strong>-&gt;</strong> #121497<br />
* #121491<br />
* #121490<br />
* #121489</p>
<p>Previously, when the Cutlass backend was enabled, using dynamic shapes could lead to exceptions during JIT.</p>
<p>With this change, there are guards in place to just disable the Cutlass backend if dynamic dimensions are involved.</p>
<p>In addition, if no choices for a GEMM are available using the selected backends, then an ATen Kernel is used as fallback, even if the ATen backend is not enabled.</p>
<p>Test:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 07:55:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121497</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] DO NOT REVIEW - to be split up</title>
      <link>https://github.com/pytorch/pytorch/pull/121492</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121492<br />
* #121734<br />
* #121497<br />
* #121491<br />
* #121490<br />
* #121489</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121492</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] GEMM size threshold for Cutlass backend usage</title>
      <link>https://github.com/pytorch/pytorch/pull/121491</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121734<br />
* #121497<br />
* <strong>-&gt;</strong> #121491<br />
* #121490<br />
* #121489</p>
<ul>
<li>
<p>Adds a configurable GEMM size threshold for the usage of Cutlass GEMM Kernels <strong>_inductor.config.cutlass_backend_min_gemm_size</strong></p>
</li>
<li>
<p>During GEMM algorithm choice generation: <strong>if no viable choices can be generated using the configured backends, the ATen backend will be used as a fallback backend</strong>, even if it is not enabled in <strong>_inductor.config.max_autotune_gemm_backends</strong></p>
</li>
</ul>
<p>Test plan:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121491</guid>
    </item>
    <item>
      <title>[Inductor] Make codecache CUDA compilation more robust &amp; flexible</title>
      <link>https://github.com/pytorch/pytorch/pull/121490</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121734<br />
* #121497<br />
* #121491<br />
* <strong>-&gt;</strong> #121490<br />
* #121489</p>
<p>Minor changes which make the CUDA compilation within _inductor/codecache.py<br />
more robust and flexible.</p>
<p>Test plan:<br />
CI<br />
Additional test in test_codecache.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121490</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Move tests to separate file</title>
      <link>https://github.com/pytorch/pytorch/pull/121489</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121734<br />
* #121497<br />
* #121491<br />
* #121490<br />
* <strong>-&gt;</strong> #121489</p>
<p>Move Cutlass backend related tests to test/inductor/test_cutlass_backend.py - no changes to the tests themselves.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121489</guid>
    </item>
    <item>
      <title>[WIP] Add registration API for torch.compile-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/121387</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to provide a registration mode to implement a single aten operation on the top of <code>torch.compile</code> and then register to aten. </p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #116368<br />
* #121727<br />
* <strong>-&gt;</strong> #121387<br />
* #120595<br />
* #121296</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121387</guid>
    </item>
    <item>
      <title>[inductor] Update triton pin</title>
      <link>https://github.com/pytorch/pytorch/pull/121268</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121568<br />
* #121733<br />
* <strong>-&gt;</strong> #121268<br />
* #121556</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 13:25:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121268</guid>
    </item>
    <item>
      <title>[torch.compile] `index_select` out of bound read</title>
      <link>https://github.com/pytorch/pytorch/issues/121251</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When using <code>torch.compile</code>, <code>index_select</code> will out-of-bound read</p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(42)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()

def forward(self, x1):
    v = torch.index_select(x1, 0, torch.tensor([2]))
    return v
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(2, 2, 2, 2)</p>
<p>with torch.no_grad():<br />
    func1 = torch.compile(func)<br />
    print(func1(x.clone()))<br />
    # tensor([[[[1.0743e+23, 3.0618e-41],<br />
    #       [9.1084e-44, 0.0000e+00]],</p>
<pre><code>#      [[3.2230e-44, 0.0000e+00],
#       [3.2230e-44, 0.0000e+00]]]])

print(func(x.clone()))
# IndexError: index out of range in self
</code></pre>
<p>```</p>
<h3>Versions</h3>
<p>```<br />
Versions<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 10:35:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121251</guid>
    </item>
    <item>
      <title>[`torch.compile`] Inductor gets stuck, unbounded RAM usage</title>
      <link>https://github.com/pytorch/pytorch/issues/121197</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When I run a GNN workload with a specific dataset, the compilation process gets stuck on step 2.</p>
<h3>Error logs</h3>
<p><code>[2024-03-05 04:00:37,435] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env
[2024-03-05 04:00:37,668] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s0 = 341523 for L['x'].size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,669] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s1 = 128 for L['x'].size()[1] [2, 9223372036854775806]
[2024-03-05 04:00:37,853] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s2 = 471405 for L['subgraphs'][0].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,856] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s3 = 95082 for L['subgraphs'][0].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,884] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s3 - 1 &lt;= s0 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:37,951] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s3 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:37,951] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s3 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] set_replacement s1 = 128
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] set_replacement s1 = 128
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s1, 128) [guard added] at torch_geometric/nn/dense/linear.py:147 in forward (_meta_registrations.py:1980 in meta_mm)
[2024-03-05 04:00:38,017] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s4 = 124196 for L['subgraphs'][1].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,020] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s5 = 12758 for L['subgraphs'][1].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,039] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s5 - 1 &lt;= s3 - 1 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:38,098] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s5 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:38,099] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s5 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:38,143] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s6 = 12033 for L['subgraphs'][2].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,147] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s7 = 1025 for L['subgraphs'][2].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,165] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s7 - 1 &lt;= s5 - 1 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:38,229] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s7 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:38,230] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s7 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:38,277] [0/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2024-03-05 04:00:38,283] [0/0] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor</code></p>
<h3>Minified repro</h3>
<p>Running with the environment variable gets stuck too, producing no output.</p>
<p>The code in question is here: https://github.com/dmlc/dgl/pull/7183</p>
<p>You can install DGL nightly build and install torch_geometric via pip.</p>
<p><code>bash
pip install --pre dgl -f https://data.dgl.ai/wheels-test/cu121/repo.html
pip install torch_geometric
python dgl/examples/sampling/graphbolt/pyg/node_classification_advanced.py --dataset=ogbn-papers100M --torch-compile</code></p>
<h3>Versions</h3>
<details><summary>Details</summary>
<p>

Collecting environment information...
PyTorch version: 2.3.0a0+ebedce2
Is debug build: False
CUDA used to build PyTorch: 12.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.28.1
Libc version: glibc-2.35

Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.3.107
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100-SXM4-80GB
GPU 1: NVIDIA A100-SXM4-80GB
GPU 2: NVIDIA A100-SXM4-80GB
GPU 3: NVIDIA A100-SXM4-80GB

Nvidia driver version: 545.23.08
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      43 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             256
On-line CPU(s) list:                0-255
Vendor ID:                          AuthenticAMD
Model name:                         AMD EPYC 7702 64-Core Processor
CPU family:                         23
Model:                              49
Thread(s) per core:                 2
Core(s) per socket:                 64
Socket(s):                          2
Stepping:                           0
Frequency boost:                    enabled
CPU max MHz:                        2183.5930
CPU min MHz:                        1500.0000
BogoMIPS:                           4000.17
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es
Virtualization:                     AMD-V
L1d cache:                          4 MiB (128 instances)
L1i cache:                          4 MiB (128 instances)
L2 cache:                           64 MiB (128 instances)
L3 cache:                           512 MiB (32 instances)
NUMA node(s):                       2
NUMA node0 CPU(s):                  0-63,128-191
NUMA node1 CPU(s):                  64-127,192-255
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow: Mitigation; safe RET
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.24.4
[pip3] onnx==1.15.0rc2
[pip3] optree==0.10.0
[pip3] pytorch-quantization==2.1.2
[pip3] torch==2.3.0a0+ebedce2
[pip3] torch_geometric==2.5.0
[pip3] torch-tensorrt==2.3.0a0
[pip3] torchdata==0.7.1a0
[pip3] torchmetrics==1.3.1
[pip3] torchtext==0.17.0a0
[pip3] torchvision==0.18.0a0
[pip3] triton==2.2.0+e28a256
[conda] Could not collect


</p>
</details>

<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Mon, 04 Mar 2024 20:12:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121197</guid>
    </item>
    <item>
      <title>'torch.compile()' causes 'Accuracy failed: uint8 tensor did not match'</title>
      <link>https://github.com/pytorch/pytorch/issues/121069</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I encountered an accuracy error when calling <code>torch.compile</code> to train moco on V100.  While there is no problem with the model running on normal calls, the model accuracy returned by torch.compile is poor.</p>
<p>I tried:<br />
<code>$ TORCHDYNAMO_REPRO_AFTER="aot"  TORCHDYNAMO_REPRO_LEVEL=4  python script.py
  ...
  torch._dynamo.debug_utils.AccuracyError: Bad accuracy detected</code><br />
```<br />
$ python run_2024_03_02_07_19_53_491950-pid_111736/minifier/minifier_launcher.py<br />
  Loading inputs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:00&lt;00:00, 108.27it/s]</p>
<blockquote>
<blockquote>
<p>[2024-03-02 07:31:19,733] torch._inductor.debug: [WARNING] model<strong><em>0 debug trace: /tmp/torchinductor</em>root/fd/cfdnftfl2xq7j6wlxq6fpsufxfazwh2vaquexbcmnsd67rujg3ph.debug<br />
 [2024-03-02 07:31:25,343] torch._inductor.debug: [WARNING] model</strong>_0 debug trace: /tmp/torchinductor_root/fd/cfdnftfl2xq7j6wlxq6fpsufxfazwh2vaquexbcmnsd67rujg3ph.debug<br />
 [2024-03-02 07:31:26,150] torch._dynamo.utils: [ERROR] Accuracy failed: uint8 tensor did not match<br />
  ...<br />
RuntimeError: Input graph did not fail the tester<br />
```</p>
</blockquote>
</blockquote>
<p>I tried below instead:<br />
<code>$ TORCHDYNAMO_REPRO_AFTER="dynamo" TORCHDYNAMO_REPRO_LEVEL=4 python script.py
 ...
 [rank0]:[2024-03-02 07:51:03,687] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='compile_fn' raised:
[rank0]:[2024-03-02 07:51:03,687] torch._dynamo.convert_frame: [WARNING] AccuracyError: Bad accuracy detected.
...</code></p>
<p><code>python repro.py 
Loading inputs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 104/104 [00:00&lt;00:00, 758.15it/s]
[2024-03-02 08:00:59,878] torch._dynamo.utils: [ERROR] Accuracy failed: uint8 tensor did not match
Traceback (most recent call last):
  File "repro.py", line 756, in &lt;module&gt;
    run_repro(mod, load_args, accuracy=True, command='run', save_dir='/public/moco/torch_compile_debug/run_2024_03_02_07_19_53_491950-pid_111736/minifier/checkpoints', tracing_mode='real', check_str=None)
  File "/usr/local/lib/python3.8/site-packages/torch/_dynamo/repro/after_aot.py", line 927, in run_repro
    COMMAND_FNS[options.command](options, mod, load_args)
  File "/usr/local/lib/python3.8/site-packages/torch/_dynamo/repro/after_aot.py", line 694, in repro_run
    raise AccuracyError("Bad accuracy detected")
torch._dynamo.debug_utils.AccuracyError: Bad accuracy detected</code></p>
<h3>Versions</h3>
<p>PyTorch version: 2.1.0+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.5 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.27.7<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10)  [GCC 10.3.0] (64-bit runtime)<br />
Python platform: Linux-4.18.0-372.9.1.el8.x86_64-x86_64-with-glibc2.10<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A800 80GB PCIe<br />
GPU 1: NVIDIA A800 80GB PCIe<br />
GPU 2: NVIDIA A800 80GB PCIe<br />
GPU 3: Tesla V100S-PCIE-32GB<br />
GPU 4: Tesla V100S-PCIE-32GB<br />
GPU 5: Tesla V100S-PCIE-32GB<br />
GPU 6: NVIDIA A800 80GB PCIe</p>
<p>Nvidia driver version: 535.54.03<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.6.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   43 bits physical, 48 bits virtual<br />
CPU(s):                          128<br />
On-line CPU(s) list:             0-127<br />
Thread(s) per core:              2<br />
Core(s) per socket:              32<br />
Socket(s):                       2<br />
NUMA node(s):                    8<br />
Vendor ID:                       HygonGenuine<br />
CPU family:                      24<br />
Model:                           2<br />
Model name:                      Hygon C86 7385 32-core Processor<br />
Stepping:                        2<br />
CPU MHz:                         2439.193<br />
BogoMIPS:                        3999.97<br />
Virtualization:                  AMD-V<br />
L1d cache:                       2 MiB<br />
L1i cache:                       4 MiB<br />
L2 cache:                        32 MiB<br />
L3 cache:                        128 MiB<br />
NUMA node0 CPU(s):               0-7,64-71<br />
NUMA node1 CPU(s):               8-15,72-79<br />
NUMA node2 CPU(s):               16-23,80-87<br />
NUMA node3 CPU(s):               24-31,88-95<br />
NUMA node4 CPU(s):               32-39,96-103<br />
NUMA node5 CPU(s):               40-47,104-111<br />
NUMA node6 CPU(s):               48-55,112-119<br />
NUMA node7 CPU(s):               56-63,120-127<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP disabled, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev sev_es</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==3.7.9<br />
[pip3] functorch==0.3.0a0<br />
[pip3] numpy==1.22.2<br />
[pip3] onnx==1.12.0<br />
[pip3] pytorch-quantization==2.1.2<br />
[pip3] torch==2.1.0<br />
[pip3] torch-tensorrt==1.3.0a0<br />
[pip3] torchaudio==2.0.1<br />
[pip3] torchtext==0.11.0a0<br />
[pip3] torchvision==0.15.1<br />
[pip3] triton==2.1.0<br />
[conda] functorch                 0.3.0a0                  pypi_0    pypi<br />
[conda] mkl                       2020.4             h726a3e6_304    conda-forge<br />
[conda] mkl-include               2020.4             h726a3e6_304    conda-forge<br />
[conda] numpy                     1.22.2           py38h6ae9a64_0    conda-forge<br />
[conda] pytorch-quantization      2.1.2                    pypi_0    pypi<br />
[conda] torch                     2.1.0                    pypi_0    pypi<br />
[conda] torch-tensorrt            1.3.0a0                  pypi_0    pypi<br />
[conda] torchaudio                2.0.1                    pypi_0    pypi<br />
[conda] torchtext                 0.11.0a0                 pypi_0    pypi<br />
[conda] torchvision               0.15.1                   pypi_0    pypi<br />
[conda] triton                    2.1.0                    pypi_0    pypi</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @wconstab</p>]]></description>
      <pubDate>Sat, 02 Mar 2024 00:06:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121069</guid>
    </item>
    <item>
      <title>torch.compile fails with .view(dtype)</title>
      <link>https://github.com/pytorch/pytorch/issues/120998</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>torch.compile</code> fails when there's a dtype view call from float to int:</p>
<p>Here's a minimal code to reproduce it:<br />
``` Python<br />
import torch<br />
@torch.compile()<br />
def call(data):<br />
    return data.view(torch.uint8) + 1</p>
<p>data = torch.randint(0, 2**4, [4096, 4096], device='cuda', dtype=torch.uint8)<br />
out = call(data) #OK <br />
out = call(data.view(torch.float16)) #Error <br />
<code></code> Python<br />
BackendCompilerFailed: backend='inductor' raised:<br />
LoweringException: NotImplementedError: bitcast torch.float16 to different bitwidth type torch.uint8 is not supported yet.<br />
```</p>
<p>Wrapping the view call inside another function with <code>torch.jit.ignore</code> doesn't work either. </p>
<p>Is there by any chance a way to avoid this without converting types back and forth?<br />
Thanks a lot in advance!</p>
<h3>Versions</h3>
<p>```<br />
Versions of relevant libraries:<br />
PyTorch version: 2.1.2+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>Nvidia driver version: 535.129.03<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: Could not collect<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-89-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.2.140<br />
CUDA_MODULE_LOADING set to: LAZY</p>
<p>[pip3] numpy==1.26.4<br />
[pip3] onnxruntime==1.17.1<br />
[pip3] open-clip-torch==2.24.0<br />
[pip3] pytorch-triton==2.2.0+e28a256d71<br />
[pip3] torch==2.1.2<br />
[pip3] torchaudio==2.2.0.dev20231221+cu121<br />
[pip3] torchvision==0.16.2<br />
[pip3] triton==2.1.0<br />
[conda] Could not collect<br />
```</p>
<p>cc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Fri, 01 Mar 2024 06:56:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120998</guid>
    </item>
    <item>
      <title>[compiled autograd] support custom ops backed by c++ autograd::Function</title>
      <link>https://github.com/pytorch/pytorch/pull/120681</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120681</p>
<ul>
<li>Adds support for custom ops backed by c++ custom autograd functions, e.g. fbgemm</li>
<li>Include files more granularly to avoid namespace pollution and circular imports</li>
</ul>
<p>limitations:<br />
- requires user to audit their code and opt-in their custom autograd::Function via autograd::Function::is_traceable and maybe additional compiled_args + apply_with_saved implementation. this was the only way I can think of for soundness<br />
- will throw if we can't hash the saved_data i.e. for any non implemented type other than list and dict in at::IValue::hash https://github.com/pytorch/pytorch/blob/b0cfa96e82d7fbd02f5dbcef2632714caf89615d/aten/src/ATen/core/ivalue.cpp#L364<br />
- can technically silently fail if both the typeid hash and the typeid string name of the custom autograd::Function collide at the same time, and an identical autograd graph containing a different custom autograd::Function, yet that has an identical implementation, is called. this case seems extremely unlikely, and the only alternative to hash collision i can think of is compiling with reflection<br />
- tensors not saved via save_variables are not lifted, and are specialized on TensorImpl*'s hash (treated as a memory address). if needed, we can lift them.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54818488">D54818488</a></p>]]></description>
      <pubDate>Mon, 26 Feb 2024 17:38:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120681</guid>
    </item>
    <item>
      <title>Add CUTLASS kernel as choice for _int_mm() Inductor autotuning</title>
      <link>https://github.com/pytorch/pytorch/pull/119685</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #119986<br />
* <strong>-&gt;</strong> #119685</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 12 Feb 2024 07:05:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119685</guid>
    </item>
    <item>
      <title>Inductor sizevars wrapper assignment DCE hazard</title>
      <link>https://github.com/pytorch/pytorch/issues/118385</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Internal xref: https://fb.workplace.com/groups/1405155842844877/posts/7944646722229057</p>
<p>In wrapper.py, we only generate assignments for symbolic variables that are free:</p>
<p><code># Assign all symbolic shapes needed to local variables
        needed = V.graph.sizevars.free_symbols()</code></p>
<p>Where free is defined as</p>
<p><code>def free_symbols(self) -&gt; Set[sympy.Symbol]:
        return set(self.var_to_val.keys()) - set(self.replacements.keys())</code></p>
<p>This means that if we refine a symint like <code>s0 = s1 - 5</code>, we will no longer generate an assignment for s0.</p>
<p>This leads to a hazard: it may be possible for there to still be an occurrence of s0 in the program (for example, we may have discovered this replacement very late during Inductor compilation), and if during code generation we do not appropriately call <code>V.graph.sizevars.simplify</code> on the computation, we can still end up with a reference to s0, which we have DCE'd.</p>
<p>How to fix? The obvious choices:</p>
<ul>
<li>Arrange so that during codegen it is guaranteed that you simplify sizevars before printing them. This is tricky, because printing the sizevar directly will typically work and only fail in obscure situations. So we need some way of making sure people don't do it wrong</li>
<li>Unconditionally generate assignments for all sizevars, even if we think they are dead</li>
</ul>
<h3>Versions</h3>
<p>main</p>
<p>cc @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Fri, 26 Jan 2024 06:58:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/118385</guid>
    </item>
    <item>
      <title>[WIP] Add a simple cache mechanism to accelerate torch.compile-for-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/116368</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to provide a cache mechanism to accelerate torch.compile-for-eager. </p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #116368<br />
* #121727<br />
* #121387<br />
* #120595<br />
* #121296</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 03:21:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116368</guid>
    </item>
    <item>
      <title>DDPOptimizer lazy compile causes shape mismatch error</title>
      <link>https://github.com/pytorch/pytorch/issues/116300</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>After https://github.com/pytorch/pytorch/pull/116292 is landed, running unit test <code>test_graph_split_inductor_transpose</code> with <code>torch._dynamo.config.optimize_ddp_lazy_compile=True</code> gives this error:</p>
<p><code>File "pytorch/test/distributed/test_dynamo_distributed.py", line 736, in test_graph_split_inductor_transpose
    self.assertTrue(same(mod(x_2), ddp_compiled_mod(x_2)))
                                   ^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 434, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pytorch/test/distributed/test_dynamo_distributed.py", line 719, in forward
    def forward(self, x):
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 434, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/fx/graph_module.py", line 737, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/fx/graph_module.py", line 317, in __call__
    raise e
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/fx/graph_module.py", line 304, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "&lt;eval_with_key&gt;.74", line 6, in forward
    submod_0 = self.compiled_submod_0(l_x_);  l_x_ = None
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_dynamo/backends/distributed.py", line 344, in forward
    x = self.submod(*args)
        ^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 434, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py", line 901, in forward
    return compiled_fn(full_args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 81, in g
    return f(*args)
           ^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 83, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 105, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 81, in g
    return f(*args)
           ^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/autograd/function.py", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 408, in forward
    fw_outs = call_func_at_runtime_with_args(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 105, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_inductor/codecache.py", line 863, in __call__
    return self.get_current_callable()(inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 608, in run
    return model(new_inputs)
           ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_inductor/codecache.py", line 891, in _run_from_cache
    return compiled_graph.compiled_artifact(inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/tmp/torchinductor_willfeng/ph/cphrafcnfqdnbchgy2koxsawdvv6mpopghjmn2p5lgdrxy2yttqb.py", line 110, in call
    assert_size_stride(primals_3, (200, 30, 50), (1500, 50, 1))
AssertionError: expected size 300==200, stride 1500==1500 at dim=0</code><br />
However, we do what to turn on <code>optimize_ddp_lazy_compile</code> by default as soon as possible, to fix stride mismatch errors in other cases (see motivation: https://github.com/pytorch/pytorch/pull/114154). So we need to find the fix for this shape mismatch as soon as possible.</p>
<h3>Versions</h3>
<p>PyTorch nightly</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov</p>]]></description>
      <pubDate>Thu, 21 Dec 2023 14:14:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/116300</guid>
    </item>
    <item>
      <title>[Inductor][cpu][miscompile] Outputs of torch.matmul abnormally change with extra outputs</title>
      <link>https://github.com/pytorch/pytorch/issues/115261</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When adding extra nodes of <code>torch.add</code> and <code>torch.cat</code> as output in this model:<br />
<code>python
class Model0(torch.nn.Module):
    def forward(self, *args):
        max_1 = torch.max(args[1], args[0])
        max_2 = torch.max(max_1, args[2])
        flatten = max_2.flatten()
        matmul = torch.matmul(flatten, flatten)
        return (matmul,)</code><br />
New:<br />
<code>python
class Model1(torch.nn.Module):
    def forward(self, *args):
        max_1 = torch.max(args[1], args[0])
        add = torch.add(max_1, args[0])
        cat = torch.cat((args[2], args[2]), dim = 0)
        max_2 = torch.max(max_1, args[2])
        flatten = max_2.flatten()
        matmul = torch.matmul(flatten, flatten)
        return (add, cat, matmul)</code><br />
The output of <code>torch.matmul</code> is expected to be the same for the same input in this 2 graphs. However, it mismatched bewteew the 2 models.<br />
This mismatch is seen only on <strong>cpu</strong>.</p>
<h3>Error logs</h3>
<h1>```</h1>
<p>torch_complie triggers assertion</p>
<p>Not equal to tolerance rtol=1e-07, atol=1e-06<br />
at v2_0, v13_0<br />
Mismatched elements: 1 / 1 (100%)<br />
 x: array(230, dtype=uint8)<br />
 y: array(36, dtype=uint8)<br />
=========================<br />
=========================<br />
torch_eager does not trigger assertion<br />
=========================<br />
```</p>
<h3>Minified repro</h3>
<p>```python<br />
import numpy as np<br />
from numpy import testing<br />
import torch</p>
<p>DEVICE='cpu'</p>
<p>class Model0(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()</p>
<pre><code>def forward(self, *args):
    max_1 = torch.max(args[1], args[0])
    max_2 = torch.max(max_1, args[2])
    flatten = max_2.flatten()
    matmul = torch.matmul(flatten, flatten)
    return (matmul,)
</code></pre>
<p>model_0 = Model0()<br />
output_names_0 = ['v2_0']</p>
<p>class Model1(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()</p>
<pre><code>def forward(self, *args):
    max_1 = torch.max(args[1], args[0])
    add = torch.add(max_1, args[0])
    cat = torch.cat((args[2], args[2]), dim = 0)
    max_2 = torch.max(max_1, args[2])
    flatten = max_2.flatten()
    matmul = torch.matmul(flatten, flatten)
    return (add, cat, matmul)
</code></pre>
<p>model_1 = Model1()<br />
output_names_1 = ['v7_0', 'v10_0', 'v13_0']</p>
<p>data_0 = np.array(6, dtype=np.uint8)<br />
data_1 = np.random.normal(5, 1, size=(43, 1, 32, 24)).astype(np.uint8)<br />
data_2 = np.random.normal(5, 1, size=(1, 1, 1)).astype(np.uint8)<br />
input_data_0 = [data_0,data_1,data_2,]</p>
<p>optmodel_0 = torch.compile(model_0, fullgraph=True, backend='inductor', mode=None)<br />
model_out_0 = optmodel_0(*[torch.from_numpy(v).to(DEVICE) for v in input_data_0])<br />
model_out_0 = [v.to(DEVICE).detach() for v in model_out_0] if isinstance(model_out_0, tuple) else [model_out_0.to(DEVICE).detach()]<br />
model_out_0 = [v.cpu().resolve_conj().numpy() if v.is_conj() else v.cpu().numpy() for v in model_out_0]<br />
output_0 = dict(zip(output_names_0, model_out_0))</p>
<p>input_data_1 = input_data_0</p>
<p>optmodel_1 = torch.compile(model_1, fullgraph=True, backend='inductor', mode=None)<br />
model_out_1 = optmodel_1(*[torch.from_numpy(v).to(DEVICE) for v in input_data_1])<br />
model_out_1 = [v.to(DEVICE).detach() for v in model_out_1] if isinstance(model_out_1, tuple) else [model_out_1.to(DEVICE).detach()]<br />
model_out_1 = [v.cpu().resolve_conj().numpy() if v.is_conj() else v.cpu().numpy() for v in model_out_1]<br />
output_1 = dict(zip(output_names_1, model_out_1))<br />
output_name_dict = {'v2_0': 'v13_0'}</p>
<p>print('=========================')<br />
try:<br />
    for tensor_name_0, tensor_name_1 in output_name_dict.items():<br />
        testing.assert_allclose(output_0[tensor_name_0], output_1[tensor_name_1], atol=1e-6, err_msg=f'at {tensor_name_0}, {tensor_name_1}')<br />
    print("torch_complie does not trigger assertion")<br />
except AssertionError as e:<br />
    print("torch_complie triggers assertion")<br />
    print(e)<br />
print('=========================')</p>
<p>model_out_0 = model_0(*[torch.from_numpy(v).to(DEVICE) for v in input_data_0])<br />
model_out_0 = [v.to(DEVICE).detach() for v in model_out_0] if isinstance(model_out_0, tuple) else [model_out_0.to(DEVICE).detach()]<br />
model_out_0 = [v.cpu().resolve_conj().numpy() if v.is_conj() else v.cpu().numpy() for v in model_out_0]<br />
output_0 = dict(zip(output_names_0, model_out_0))</p>
<p>model_out_1 = model_1(*[torch.from_numpy(v).to(DEVICE) for v in input_data_1])<br />
model_out_1 = [v.to(DEVICE).detach() for v in model_out_1] if isinstance(model_out_1, tuple) else [model_out_1.to(DEVICE).detach()]<br />
model_out_1 = [v.cpu().resolve_conj().numpy() if v.is_conj() else v.cpu().numpy() for v in model_out_1]<br />
output_1 = dict(zip(output_names_1, model_out_1))</p>
<p>print('=========================')<br />
try:<br />
    for tensor_name_0, tensor_name_1 in output_name_dict.items():<br />
        testing.assert_allclose(output_0[tensor_name_0], output_1[tensor_name_1], atol=1e-6, err_msg=f'at {tensor_name_0}, {tensor_name_1}')<br />
    print("torch_eager does not trigger assertion")<br />
except AssertionError as e:<br />
    print("torch_eager triggers assertion")<br />
    print(e)<br />
print('=========================')<br />
```</p>
<h3>Versions</h3>
<p>PyTorch version: 2.2.0.dev20231205+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-6.2.0-37-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: Tesla V100S-PCIE-32GB<br />
GPU 1: Tesla V100S-PCIE-32GB<br />
GPU 2: Tesla V100S-PCIE-32GB<br />
GPU 3: Tesla V100S-PCIE-32GB<br />
GPU 4: Tesla V100S-PCIE-32GB<br />
GPU 5: Tesla V100S-PCIE-32GB<br />
GPU 6: Tesla V100S-PCIE-32GB<br />
GPU 7: Tesla V100S-PCIE-32GB</p>
<p>Nvidia driver version: 525.147.05<br />
cuDNN version: Probably one of the following:<br />
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn.so<br />
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn.so.8<br />
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8<br />
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_train.so.8<br />
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8<br />
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8<br />
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8<br />
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_train.so.8<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      48 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             256<br />
On-line CPU(s) list:                0-255<br />
Vendor ID:                          AuthenticAMD<br />
Model name:                         AMD EPYC 7763 64-Core Processor<br />
CPU family:                         25<br />
Model:                              1<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 64<br />
Socket(s):                          2<br />
Stepping:                           1<br />
Frequency boost:                    enabled<br />
CPU max MHz:                        3529.0520<br />
CPU min MHz:                        1500.0000<br />
BogoMIPS:                           4890.78<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm<br />
Virtualization:                     AMD-V<br />
L1d cache:                          4 MiB (128 instances)<br />
L1i cache:                          4 MiB (128 instances)<br />
L2 cache:                           64 MiB (128 instances)<br />
L3 cache:                           512 MiB (16 instances)<br />
NUMA node(s):                       2<br />
NUMA node0 CPU(s):                  0-63,128-191<br />
NUMA node1 CPU(s):                  64-127,192-255<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Mitigation; safe RET<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.3<br />
[pip3] onnx==1.14.1<br />
[pip3] onnxruntime==1.15.1<br />
[pip3] onnxsim==0.4.35<br />
[pip3] pytorch-triton==2.1.0+bcad9dabe1<br />
[pip3] torch==2.2.0.dev20231205+cu118<br />
[pip3] torchaudio==2.2.0.dev20231205+cu118<br />
[pip3] torchvision==0.17.0.dev20231205+cu118<br />
[pip3] triton==2.1.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Wed, 06 Dec 2023 05:41:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/115261</guid>
    </item>
    <item>
      <title>Torch compile generates incorrect graph on Llama model</title>
      <link>https://github.com/pytorch/pytorch/issues/108442</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>We have Dynamo backend defined similar to IPEX which traces and freezes the model (however the problem is general):</p>
<p>```python<br />
import importlib<br />
import logging</p>
<p>import torch<br />
from torch._dynamo import register_backend<br />
from .common import fake_tensor_unsupported</p>
<p>@register_backend<br />
@fake_tensor_unsupported<br />
def aio(model, input):<br />
    model.print_readable()<br />
    try:<br />
        with torch.no_grad():<br />
            traced_model = torch.jit.trace(model.eval(), inputs)<br />
            frozen_model = torch.jit.freeze(traced_model)<br />
        return frozen_model<br />
    except Exception as ex:<br />
        log.warning("JIT trace failed during the optimize process.")<br />
        log.warning(print(ex))<br />
        return model<br />
```</p>
<p>I'm running the Llama model from Transformers repo tag <code>tag: v4.30.1</code> with following script:</p>
<p>```python<br />
import argparse<br />
import os<br />
import sys<br />
import time<br />
import datetime<br />
import torch._dynamo.config<br />
import transformers<br />
import torch<br />
import torch._dynamo</p>
<p>from torch.autograd.profiler import profile<br />
import traceback as tb<br />
import logging</p>
<p>default_input_texts = ("Below is an instruction that describes a task." <br />
                      "Write a response that appropriately completes the request.\r\n\r\n" <br />
                      "### Instruction:\r\nList three technologies that make life easier.\r\n\r\n### Response:")</p>
<p>def parse_args():<br />
  parser = argparse.ArgumentParser()<br />
  parser.add_argument("-m", "--model_path",<br />
                      type=str, required=None,<br />
                      help="Recovered Model path")<br />
  parser.add_argument("-a", "--aio",<br />
                      dest="aio", action='store_true',<br />
                      help="Use AIO backend")<br />
  parser.set_defaults(aio=False)<br />
  parser.add_argument("-i", "--input_prompt",<br />
                      type=str, default=default_input_texts,<br />
                      help="Input prompt")<br />
  return parser.parse_args()</p>
<p>def main():<br />
  args = parse_args()</p>
<p>torch._dynamo.config.cache_size_limit = 128</p>
<p>print("Loading model and tokenizer...")<br />
  alpaca_model = transformers.LlamaForCausalLM.from_pretrained(args.model_path)<br />
  alpaca_tokenizer = transformers.LlamaTokenizer.from_pretrained(args.model_path)<br />
  alpaca_model.config.pad_token_id = alpaca_tokenizer.pad_token_id = 0 #unk<br />
  alpaca_model.config.bos_token_id = 1<br />
  alpaca_model.config.eos_token_id = 2</p>
<p>print("Torch compile...")<br />
  alpaca_model = alpaca_model.eval()<br />
  alpaca_model = torch.compile(alpaca_model, backend="air", dynamic=True, fullgraph=False)</p>
<p>inputs = alpaca_tokenizer(args.input_prompt, return_tensors="pt")<br />
  outputs = alpaca_model.generate(inputs=inputs.input_ids, max_new_tokens=100)</p>
<p>output_text = alpaca_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]</p>
<p>print("-- Alpaca output --")<br />
  print("{}\n\n".format(output_text))</p>
<p>```</p>
<p>When model gets to the our dynamo backend Pytorch throws an error (inside <code>torch.jit.trace</code>):</p>
<p>```<br />
INTERNAL ASSERT FAILED at "../torch/csrc/jit/ir/alias_analysis.cpp":621, please report a bug to PyTorch. We don't have an op for aten::<strong>and</strong> but it isn't a special case.  Argument types: Tensor, bool, </p>
<p>Candidates:<br />
    aten::<strong>and</strong>.Scalar(Tensor self, Scalar other) -&gt; Tensor<br />
    aten::<strong>and</strong>.Tensor(Tensor self, Tensor other) -&gt; Tensor<br />
    aten::<strong>and</strong>.bool(bool a, bool b) -&gt; bool<br />
    aten::<strong>and</strong>.int(int a, int b) -&gt; int</p>
<p>```</p>
<p>The problem lies in this part (I printed the model with <code>print_readable()</code> call in our backend:</p>
<p><code># File: /onspecta/transformers/src/transformers/models/llama/modeling_llama.py:234, code: if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
        size_12 = matmul_1.size()
        getitem_47 = size_12[2];  size_12 = None
        eq_4 = getitem_47 == getitem_7;  getitem_47 = None
        and__6 = True &amp; True
        and__7 = 1 &amp; eq_4;  eq_4 = None
        and__8 = and__7 &amp; True;  and__7 = None
        not__2 = _operator_not_(and__8);  and__8 = None</code></p>
<p>and__7 = 1 &amp; eq_4;  eq_4 = None &lt;----- this line is wrong there is an Tensor on left hand side and bool on the right hand side, the code generated by torch.dynamo is incorrect. Problem doesn't occur on <code>v2.0.0</code> tag, but happens on <code>400c4de53bb7b36066aef381313ed71e4a877e95</code></p>
<p>The original code from the model in this place is:</p>
<p><code>if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )</code></p>
<h3>Versions</h3>
<p>main branch</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @wconstab @bdhirsh @anijain2305</p>]]></description>
      <pubDate>Fri, 01 Sep 2023 12:04:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/108442</guid>
    </item>
    <item>
      <title>[Inductor] Add support for NEON ISA in the Inductor C++ backend</title>
      <link>https://github.com/pytorch/pytorch/pull/105590</link>
      <description><![CDATA[<p>Fixes #104729</p>
<p>As suggested in the <a href="https://dev-discuss.pytorch.org/t/torchinductor-update-5-cpu-backend-backend-performance-update-and-deep-dive-on-key-optimizations/1117#:~:text=It%20can%20be,sub%2Dclasses.">blog</a>, I subclassed the <code>VecISA</code> class and implemented a NEON version of the <code>vec_reduce_all()</code> function, to go along with the existing AVX2 and AVX512 versions. Any operation that calls <code>vec_reduce_all()</code> will also take the NEON path and benefit from its vectorization.</p>
<p>The <code>vec_reduce_all()</code> is invoked by Softmax and other operations like norms. Using the fast path results in 30% time savings for Softmax as compared to the previously taken slow path.</p>
<p>| Slow path | Fast path (NEON intrinsics)<br />
-- | -- | --<br />
Softmax (100 passes, 1024 dimension) | 623.706ms | 452.011ms</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang @Xia-Weiwen @ngimel @malfet </p>]]></description>
      <pubDate>Wed, 19 Jul 2023 11:39:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/105590</guid>
    </item>
    <item>
      <title>[Inductor] [CPU] Huggingface model BartForCausalLM &amp; MBartForCausalLM &amp; OPTForCausalLM &amp; PLBartForCausalLM performance regression &gt; 10% on 2023-04-02 nightly release</title>
      <link>https://github.com/pytorch/pytorch/issues/98273</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Compare with the 2023-03-29, there is a performance regression on huggingface model<strong>BartForCausalLM &amp; MBartForCausalLM &amp; OPTForCausalLM &amp; PLBartForCausalLM</strong> on <a href="https://github.com/pytorch/pytorch/issues/93531#issuecomment-1495275117">TorchInductor CPU Performance Dashboard</a> on 2023-04-02 as bellow:</p>
<p>|     |  2023-04-02  |   |     |   |  2023-03-29  |   |     |   |  Result Comp  |     |   |<br />
|  ----  |  ----  | ----  |  ----  | ----  |  ----  | ----  |  ----  | ----  | ----  |  ----  | ----  |<br />
| model  | batch_size  | speedup | inductor  | eager | batch_size  | speedup | inductor  | eager | speedup ratio | eager ratio  | inductor ratio |<br />
| BartForCausalLM   | 1 | 0.8682    | 3.6489689 | 3.168034799   | 1 | 1.0814    | 2.9063651 | 3.142943219   | 0.8   | 0.99  | 0.8<br />
| MBartForCausalLM  | 1 | 0.8722    | 3.6418691 | 3.176438229   | 1 | 1.08  | 2.9201856 | 3.153800448   | 0.81  | 0.99  | 0.8<br />
| OPTForCausalLM    | 1 | 0.7925    | 7.1054617 | 5.631078397   | 1 | 1.1534    | 4.8903451 | 5.640524038   | 0.69  | 1 | 0.69<br />
| PLBartForCausalLM | 1 | 0.8884    | 1.4350074 | 1.274860574   | 1 | 1.1012    | 1.1629979 | 1.280693287   | 0.81  | 1 | 0.81</p>
<p>2023-04-02 nightly release SW information:<br />
SW  | Nightly commit    | Master/Main commit<br />
-- | -- | --<br />
Pytorch|<a href="https://github.com/pytorch/pytorch/commit/5775e1c1">5775e1c1</a>|<a href="https://github.com/pytorch/pytorch/commit/7fcff01">7fcff01</a><br />
Torchbench|/|<a href="https://github.com/pytorch/benchmark/commit/83a316df">83a316df</a><br />
torchaudio|<a href="https://github.com/pytorch/audio/commit/375e751">375e751</a>|<a href="https://github.com/pytorch/audio/commit/a8f4e97">a8f4e97</a><br />
torchtext|<a href="https://github.com/pytorch/text/commit/9749082">9749082</a>| <a href="https://github.com/pytorch/text/commit/46e7eef">46e7eef</a><br />
torchvision|<a href="https://github.com/pytorch/vision/commit/8d15ca7">8d15ca7</a>|<a href="https://github.com/pytorch/vision/commit/98c5815">98c5815</a><br />
torchdata|<a href="https://github.com/pytorch/data/commit/b3048d5">b3048d5</a>|<a href="https://github.com/pytorch/data/commit/f1283eb">f1283eb</a><br />
dynamo_benchmarks|<a href="https://github.com/pytorch/pytorch/commit/1238ae3">1238ae3</a>|/</p>
<p>2023-03-29 nightly release SW information:<br />
SW  | Nightly commit    | Master/Main commit<br />
-- | -- | --<br />
Pytorch|<a href="https://github.com/pytorch/pytorch/commit/f1f0a4f">f1f0a4f</a>|<a href="https://github.com/pytorch/pytorch/commit/91166ef">91166ef</a><br />
Torchbench|/|<a href="https://github.com/pytorch/benchmark/commit/83a316df">83a316df</a><br />
torchaudio|<a href="https://github.com/pytorch/audio/commit/375e751">375e751</a>|<a href="https://github.com/pytorch/audio/commit/a8f4e97">a8f4e97</a><br />
torchtext|<a href="https://github.com/pytorch/text/commit/9749082">9749082</a>| <a href="https://github.com/pytorch/text/commit/46e7eef">46e7eef</a><br />
torchvision|<a href="https://github.com/pytorch/vision/commit/8d15ca7">8d15ca7</a>|<a href="https://github.com/pytorch/vision/commit/98c5815">98c5815</a><br />
torchdata|<a href="https://github.com/pytorch/data/commit/b3048d5">b3048d5</a>|<a href="https://github.com/pytorch/data/commit/f1283eb">f1283eb</a><br />
dynamo_benchmarks|<a href="https://github.com/pytorch/pytorch/commit/1238ae3">1238ae3</a>|/</p>
<p>Graph dump by cosim:</p>
<h3>Versions</h3>
<p>Minified repro:</p>
<p><code>python -m torch.backends.xeon.run_cpu --core_list 0 --ncores_per_instance 1 benchmarks/dynamo/huggingface.py --performance --float32 -dcpu -n50 --inductor  --no-skip --dashboard --only BartForCausalLM  --cold_start_latency --batch_size 1 --threads 1</code></p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @soumith @ngimel @Xia-Weiwen @desertfire</p>]]></description>
      <pubDate>Mon, 03 Apr 2023 19:36:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/98273</guid>
    </item>
  </channel>
</rss>
