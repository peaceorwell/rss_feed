<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[AOTInductor] Support quantized linear on CPU with fbgemm</title>
      <link>https://github.com/pytorch/pytorch/pull/122820</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122820<br />
* #122763<br />
* #122762</p>
<p>Summary:<br />
Added support for quantized linear on CPU with fbgemm.<br />
Specifically, for torch.ops.quantized.linear_unpacked_dynamic_fp16, we<br />
decompose it into two steps, pack weight, and fbgemm's qlinear with<br />
packed weight.</p>
<p>Test Plan:<br />
Included in commit.<br />
test_aot_inductor::test_quantized_linear</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 11:29:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122820</guid>
    </item>
    <item>
      <title>Deprecate accessing FakeTensor outside of torch.compile</title>
      <link>https://github.com/pytorch/pytorch/issues/122806</link>
      <description><![CDATA[<p>We'd like to make FakeTensor semantics the same inside and outside of torch.compile. </p>]]></description>
      <pubDate>Wed, 27 Mar 2024 09:52:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122806</guid>
    </item>
    <item>
      <title>In Inductor-wrapped tests, reset() before and after each test and turn off suppress_errors=True</title>
      <link>https://github.com/pytorch/pytorch/issues/122804</link>
      <description><![CDATA[<p>We did this for the Dynamo-wrapped (i.e. PYTORCH_TEST_WITH_DYNAMO=1) tests but didn't get around to the Inductor-wrapped tests (i.e. PYTORCH_TEST_WITH_INDUCTOR=1).</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 09:51:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122804</guid>
    </item>
    <item>
      <title>[DCP] `set_model_state_dict` errors on compiled module with non-persistent buffer</title>
      <link>https://github.com/pytorch/pytorch/issues/122792</link>
      <description><![CDATA[<p>```<br />
"""<br />
torchrun --standalone --nproc_per_node=2 repro_dcp_compile.py<br />
"""<br />
import os<br />
import torch<br />
import torch.nn as nn<br />
import torch.distributed as dist<br />
from torch.distributed.checkpoint.state_dict import get_model_state_dict, set_model_state_dict</p>
<p>class Model(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.lin1 = nn.Linear(4, 4)<br />
        self.lin2 = nn.Linear(4, 4)<br />
        self.register_buffer("buf", torch.randn((4,)), persistent=False)<br />
        self.weight = nn.Parameter(torch.randn((4, 4)))</p>
<p>if <strong>name</strong> == "<strong>main</strong>":<br />
    dist.init_process_group(backend="nccl")<br />
    gpu_id = int(os.environ["LOCAL_RANK"])<br />
    device = f"cuda:{gpu_id}"<br />
    torch.cuda.set_device(device)</p>
<pre><code>model = Model()
model = torch.compile(model)

sharded_sd = get_model_state_dict(model)
set_model_state_dict(model, sharded_sd)
</code></pre>
<p>```</p>
<p>```<br />
<a href="KeyError:" title="buf">rank0</a>: Traceback (most recent call last):<br />
<a href="KeyError:" title="buf">rank0</a>:   File "/data/users/andgu/pytorch/repro_dcp_compile.py", line 36, in <module></p>
<p><a href="KeyError:" title="buf">rank0</a>:   File "/data/users/andgu/pytorch/torch/distributed/checkpoint/state_dict.py", line 853, in set_model_state_dict<br />
<a href="KeyError:" title="buf">rank0</a>:     return _load_model_state_dict(model, model_state_dict, info)<br />
<a href="KeyError:" title="buf">rank0</a>:   File "/data/users/andgu/pytorch/torch/distributed/checkpoint/state_dict.py", line 416, in _load_model_state_dict<br />
<a href="KeyError:" title="buf">rank0</a>:     state_dict[fqn_with_prefix] = state_dict.pop(fqn)</p>
<p>```</p>
<p><code>set_model_state_dict</code> calls into <code>_load_model_state_dict</code>, which iterates over <code>named_buffers()</code>. For a compiled module, <code>fqns</code> and <code>fqns_with_prefix</code> always mismatch, so <code>_load_model_state_dict</code> will try to reassign from the FQN without prefix to the one with prefix. However, this does not account for non-persistent buffers not existing in the state dict.</p>
<p>One solution could be just to continue <code>if fqn not in state_dict</code>.</p>
<p>cc @LucasLLC</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 07:56:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122792</guid>
    </item>
    <item>
      <title>[dynamo] Emit FUNCTORCH_STACK_MATCH guard in vmap(compile(f)) case</title>
      <link>https://github.com/pytorch/pytorch/pull/122786</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122786</p>
<p>Fixes: #122201</p>
<p>cc @zou3519 @Chillee @samdow @kshitij12345 @janeyx99 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 06:02:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122786</guid>
    </item>
    <item>
      <title>Add Matmul recipe into x86_inductor_quantizer</title>
      <link>https://github.com/pytorch/pytorch/pull/122776</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122777<br />
* <strong>-&gt;</strong> #122776<br />
* #122775</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 01:18:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122776</guid>
    </item>
    <item>
      <title>Add Quantization recipe filter per operator type for x86_inductor_quantizer</title>
      <link>https://github.com/pytorch/pytorch/pull/122775</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122777<br />
* #122776<br />
* <strong>-&gt;</strong> #122775</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 01:18:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122775</guid>
    </item>
    <item>
      <title>Erroneous interaction between torch.compile, fx Interpreter, and type cast</title>
      <link>https://github.com/pytorch/pytorch/issues/122759</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Not sure if this is an "officially sanctioned" use of <code>torch.fx.Interpreter</code> but I am working on an inspection tool to try to capture the lowest level of torch op calls for arbitrary torch code.</p>
<p>Basically I use <code>torch.compile(fn_or_module, backend=custom_compile)</code> where <code>custom_compile</code> looks something like this:<br />
```python<br />
class Interpreter(torch.fx.Interpreter):<br />
    def <strong>init</strong>(self, gm):<br />
        super().<strong>init</strong>(gm)</p>
<pre><code># ... Hooks to capture calls etc ...
</code></pre>
<p>def custom_compile(gm : torch.fx.GraphModule, _):<br />
    def wrapper(<em>args, </em><em>kwargs):<br />
        return Interpreter(gm).run(</em>args, **kwargs)</p>
<pre><code>return make_boxed_func(wrapper)
</code></pre>
<p>```</p>
<p>What I notice is that if I try to compile a module that does a type cast (E.g.  Half to Float), it causes a strange behavior that ultimately ends up dropping the outer most dimension of a tensor:</p>
<p>```python<br />
class Wat(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()</p>
<pre><code>def forward(self, x):
    return x.float()
</code></pre>
<p>x = torch.randn((8, 128), dtype=torch.float16, device='cuda')<br />
print('Input shape:', x.shape)</p>
<p>wat = Wat()<br />
compiled_wat = torch.compile(wat, backend=custom_compile)</p>
<p>y = compiled_wat(x)<br />
print('Wat:', y.shape)<br />
```</p>
<p>If I run the above I get the following output:<br />
<code>bash
$ python test.py 
Input shape: torch.Size([8, 128])
Wat: torch.Size([128])</code></p>
<p>If I instead remove the <code>.float()</code> cast (leaving me with an identity module), I get the following code and output:<br />
```python<br />
class Identity(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()</p>
<pre><code>def forward(self, x):
    return x
</code></pre>
<p>x = torch.randn((8, 128), dtype=torch.float16, device='cuda')<br />
print('Input shape:', x.shape)</p>
<p>ident = Identity()<br />
compiled_ident = torch.compile(ident, backend=custom_compile)</p>
<p>y = compiled_ident(x)<br />
print('Ident:', y.shape)<br />
```</p>
<p><code>bash
$ python test.py
Input shape: torch.Size([8, 128])
Ident: torch.Size([8, 128])</code></p>
<p>From what I can tell, in the faulty case, <code>wrapper</code> inside <code>custom_compile</code> somehow receives 8 arguments (I.e. <code>x</code> is somehow split along the outer dimension into 8 tensors of shape <code>(128,)</code>.)</p>
<p>I am somewhat familiar with the torch codebase, so if anyone has a suggestion on where I can start looking to further root cause this, I'd be happy to look myself. Else, would be grateful if anyone knows offhand what might be causing this.</p>
<p>Thanks!</p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.1.2+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: Could not collect<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090<br />
Nvidia driver version: 546.65<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             24<br />
On-line CPU(s) list:                0-23<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family:                         6<br />
Model:                              151<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 12<br />
Socket(s):                          1<br />
Stepping:                           2<br />
BogoMIPS:                           6374.39<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni umip waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
Hypervisor vendor:                  Microsoft<br />
Virtualization type:                full<br />
L1d cache:                          576 KiB (12 instances)<br />
L1i cache:                          384 KiB (12 instances)<br />
L2 cache:                           15 MiB (12 instances)<br />
L3 cache:                           30 MiB (1 instance)<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Mitigation; Enhanced IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==6.0.0<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.24.3<br />
[pip3] numpydoc==1.5.0<br />
[pip3] torch==2.1.2<br />
[pip3] torch-tb-profiler==0.4.3<br />
[pip3] torchaudio==2.1.2<br />
[pip3] torchvision==0.16.2<br />
[pip3] triton==2.1.0<br />
[conda] _anaconda_depends         2023.09             py311_mkl_1<br />
[conda] blas                      1.0                         mkl<br />
[conda] mkl                       2023.1.0         h213fc3f_46343<br />
[conda] mkl-service               2.4.0           py311h5eee18b_1<br />
[conda] mkl_fft                   1.3.8           py311h5eee18b_0<br />
[conda] mkl_random                1.2.4           py311hdb19cb5_0<br />
[conda] numpy                     1.24.3          py311h08b1b3b_1<br />
[conda] numpy-base                1.24.3          py311hf175353_1<br />
[conda] numpydoc                  1.5.0           py311h06a4308_0<br />
[conda] torch                     2.1.2                    pypi_0    pypi<br />
[conda] torch-tb-profiler         0.4.3                    pypi_0    pypi<br />
[conda] torchaudio                2.1.2                    pypi_0    pypi<br />
[conda] torchvision               0.16.2                   pypi_0    pypi<br />
[conda] triton                    2.1.0                    pypi_0    pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 19:17:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122759</guid>
    </item>
    <item>
      <title>[Inductor] require channels last output for channels last input for max_pool2d_backward</title>
      <link>https://github.com/pytorch/pytorch/pull/122749</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122749</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Previously we fell back on max_pool2d_with_indices_backward for channels last.. Turns out this was slow because we were inferring a contiguous output for channels last inputs. Fixing the layout and lowering gives a 1-2% TIMM win. It will also unblock saving the indices as int8 kernel offsets since we now lower channels last output.</p>
<p>cc @amjames </p>]]></description>
      <pubDate>Tue, 26 Mar 2024 15:46:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122749</guid>
    </item>
    <item>
      <title>[Inductor] `test_mixed_mm_dynamic_shapes_cuda` is broken on multiple architectures</title>
      <link>https://github.com/pytorch/pytorch/issues/122747</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>observed<br />
<code>python test/inductor/test_torchinductor_codegen_dynamic_shapes.py -k test_mixed_mm_dynamic_shapes_cuda</code> failing on A2 (sm86/GA107) and V100 (sm70/GV100), among others.</p>
<h3>Versions</h3>
<p>Nightly build as of 3/26.</p>
<p>cc @ptrblck @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 15:33:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122747</guid>
    </item>
    <item>
      <title>[inductor] Add FileLock around V.debug.copy</title>
      <link>https://github.com/pytorch/pytorch/pull/122665</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122665</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 17:24:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122665</guid>
    </item>
    <item>
      <title>[inductor] Fix fresh_inductor_cache()</title>
      <link>https://github.com/pytorch/pytorch/pull/122661</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122661</p>
<p>Summary: Modify fresh_inductor_cache() to clear cached state before mocking the toplevel cache_dir directory. Any lru_caches (or otherwise) can use the @clear_on_fresh_inductor_cache decorator to register the cache for clearing. Also change the base inductor TestCase class to use fresh_inductor_cache(). Previously that TestCase was only mocking the subdirectory within the toplevel cache dir designated for the FX graph cache artifacts.</p>
<p>Test Plan:<br />
- New unit test<br />
- All existing inductor tests will exercise fresh_inductor_cache()</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 16:12:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122661</guid>
    </item>
    <item>
      <title>Dont precompile already seen keys, limit epilogue choices</title>
      <link>https://github.com/pytorch/pytorch/pull/122642</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122643<br />
* <strong>-&gt;</strong> #122642<br />
* #121999</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 12:24:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122642</guid>
    </item>
    <item>
      <title>[Inductor] Run pattern matcher over the original graph</title>
      <link>https://github.com/pytorch/pytorch/pull/122519</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122519</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D55429070">D55429070</a></p>]]></description>
      <pubDate>Fri, 22 Mar 2024 13:00:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122519</guid>
    </item>
    <item>
      <title>[inductor][cpu]adv_inception_v3, gluon_inception_v3 and inception_v3 AMP performance regression</title>
      <link>https://github.com/pytorch/pytorch/issues/122393</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>AMP static shape performance regression in 2024-03-18</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>adv_inception_v3</td>
      <td>multiple</td>
      <td>128</td>
      <td>3.695396</td>
      <td>0.087057353</td>
      <td>0.321711394046788</td>
      <td>30.958788</td>
      <td>128</td>
      <td>4.924577</td>
      <td>0.065420102</td>
      <td>0.322166329646854</td>
      <td>33.211353</td>
      <td>0.75</td>
      <td>1.0</td>
      <td>0.75</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>multiple</td>
      <td>128</td>
      <td>3.700601</td>
      <td>0.08712846399999999</td>
      <td>0.3224276810068639</td>
      <td>30.544114</td>
      <td>128</td>
      <td>4.877504</td>
      <td>0.06528794</td>
      <td>0.31844218850176004</td>
      <td>32.577501</td>
      <td>0.76</td>
      <td>0.99</td>
      <td>0.75</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>inception_v3</td>
      <td>multiple</td>
      <td>128</td>
      <td>3.7809</td>
      <td>0.086533052</td>
      <td>0.3271728163068</td>
      <td>30.467435</td>
      <td>128</td>
      <td>4.956273</td>
      <td>0.065406192</td>
      <td>0.32417094344241604</td>
      <td>32.360837</td>
      <td>0.76</td>
      <td>0.99</td>
      <td>0.76</td>
      <td>1.06</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>adv_inception_v3</td>
      <td>single</td>
      <td>1</td>
      <td>4.391231</td>
      <td>0.020491818</td>
      <td>0.089984306447958</td>
      <td>27.533994</td>
      <td>1</td>
      <td>6.074862</td>
      <td>0.014556469</td>
      <td>0.08842854038227801</td>
      <td>29.358509</td>
      <td>0.72</td>
      <td>0.98</td>
      <td>0.71</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>single</td>
      <td>1</td>
      <td>4.406967</td>
      <td>0.020548244</td>
      <td>0.090555433215948</td>
      <td>27.204902</td>
      <td>1</td>
      <td>6.18311</td>
      <td>0.014545653</td>
      <td>0.08993737252083</td>
      <td>29.311159</td>
      <td>0.71</td>
      <td>0.99</td>
      <td>0.71</td>
      <td>1.08</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>inception_v3</td>
      <td>single</td>
      <td>1</td>
      <td>4.47947</td>
      <td>0.020806962</td>
      <td>0.09320416207014</td>
      <td>27.136373</td>
      <td>1</td>
      <td>6.288863</td>
      <td>0.014541316</td>
      <td>0.091448344163708</td>
      <td>29.336782</td>
      <td>0.71</td>
      <td>0.98</td>
      <td>0.7</td>
      <td>1.08</td>
    </tr>
  </tbody>

</table>

<p>AMP dynamic shape performance regression in 2024-03-19</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
   <tr>
      <td>timm_models</td>
      <td>adv_inception_v3</td>
      <td>single</td>
      <td>1.0</td>
      <td>4.358126</td>
      <td>0.02061505</td>
      <td>0.0898429853963</td>
      <td>27.534621</td>
      <td>1</td>
      <td>6.102014</td>
      <td>0.014657954</td>
      <td>0.089443040519356</td>
      <td>29.887889</td>
      <td>0.71</td>
      <td>1.0</td>
      <td>0.71</td>
      <td>1.09</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>single</td>
      <td>1.0</td>
      <td>4.411057</td>
      <td>0.020516146</td>
      <td>0.09049788942632199</td>
      <td>27.462184</td>
      <td>1</td>
      <td>6.181562</td>
      <td>0.014640228</td>
      <td>0.090499477076136</td>
      <td>29.459571</td>
      <td>0.71</td>
      <td>1.0</td>
      <td>0.71</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>inception_v3</td>
      <td>single</td>
      <td>1.0</td>
      <td>4.490309</td>
      <td>0.020474836</td>
      <td>0.091938340364324</td>
      <td>27.457952</td>
      <td>1</td>
      <td>6.265146</td>
      <td>0.014763517</td>
      <td>0.092495589478482</td>
      <td>29.428536</td>
      <td>0.72</td>
      <td>1.01</td>
      <td>0.72</td>
      <td>1.07</td>
    </tr>
  </tbody>


</table>
<p>SW info</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>d6015d42</td>
      <td>main</td>
      <td>1ef0a39e</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>5bc7f7f97760d2d485621f9f30d0316e1f2440c6</td>
      <td>main</td>
      <td>41286f1505ffb214d386d72e4b72ebd680a4a475</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.18.0a0+0325175</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>
</table>

<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh <strong>thread</strong> inference performance timm_models <strong>model</strong> amp first static/dynamic</p>
<p>Suspected guilty commit: https://github.com/pytorch/pytorch/commit/ffabb25c489df1dc631a577c12a0c843c8b202f3<br />
<a href="https://github.com/pytorch/pytorch/files/14693268/timm_models-adv_inception_v3-inference-amp-dynamic-default-single-performance-drop_guilty_commit.log">timm_models-adv_inception_v3-inference-amp-dynamic-default-single-performance-drop_guilty_commit.log</a></p>
<p>cc @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 02:03:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122393</guid>
    </item>
    <item>
      <title>[inductor][cpu] maml_omniglot fp32 Dynamic shape default wrapper accuracy crashed</title>
      <link>https://github.com/pytorch/pytorch/issues/122285</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>new_failures in 2024-02-25<br />
| suite | name | thread | accuracy | perf | reason(reference only)<br />
-- | -- | -- | -- | -- | --<br />
torchbench | maml_omniglot | multiple | X | ‚àö | maml_omniglot, AssertionError: expected size 64==64 stride 676==1 at dim=1<br />
torchbench | maml_omniglot | single | X | ‚àö | maml_omniglot, AssertionError: expected size 64==64 stride 676==1 at dim=1</p>
<p>Error:<br />
```<br />
loading model: 0it [00:00, ?it/s]cpu  eval  maml_omniglot                      </p>
<p>skipping cudagraphs due to skipping cudagraphs due to cpu device. Found from : <br />
   File "benchmarks/dynamo/torchbench.py", line 350, in forward_pass<br />
    return mod(*inputs)</p>
<p>ERROR:common:Backend dynamo failed in warmup()<br />
Traceback (most recent call last):<br />
  File "/workspace/pytorch/benchmarks/dynamo/common.py", line 2626, in warmup<br />
    fn(model, example_inputs)<br />
  File "/workspace/pytorch/torch/_dynamo/eval_frame.py", line 455, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "benchmarks/dynamo/torchbench.py", line 348, in forward_pass<br />
    def forward_pass(self, mod, inputs, collect_outputs=True):<br />
  File "/workspace/pytorch/torch/_dynamo/eval_frame.py", line 455, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/workspace/pytorch/torch/_dynamo/external_utils.py", line 25, in inner<br />
    return fn(*args, </strong>kwargs)<br />
  File "/workspace/pytorch/torch/_functorch/aot_autograd.py", line 892, in forward<br />
    return compiled_fn(full_args)<br />
  File "/workspace/pytorch/torch/_functorch/_aot_autograd/utils.py", line 79, in g<br />
    return f(*args)<br />
  File "/workspace/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 101, in runtime_wrapper<br />
    all_outs = call_func_at_runtime_with_args(<br />
  File "/workspace/pytorch/torch/_functorch/_aot_autograd/utils.py", line 103, in call_func_at_runtime_with_args<br />
    out = normalize_as_list(f(args))<br />
  File "/workspace/pytorch/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 122, in rng_functionalization_wrapper<br />
    return compiled_fw(args)<br />
  File "/workspace/pytorch/torch/_inductor/compile_fx.py", line 1088, in wrapper<br />
    return optimized_function(args_new)<br />
  File "/workspace/pytorch/torch/_inductor/codecache.py", line 825, in <strong>call</strong><br />
    return self.get_current_callable()(inputs)<br />
  File "/workspace/pytorch/torch/_inductor/codecache.py", line 853, in _run_from_cache<br />
    return compiled_graph.compiled_artifact(inputs)<br />
  File "/tmp/tmpi0xi5r_c/fd/cfdr2bfakywvszeearvqvdyltpxlafqhpy7iri2miwsd2uusmngm.py", line 163, in call<br />
    assert_size_stride(buf0, (s0, 64, 26, 26), (43264, 1, 1664, 64))<br />
AssertionError: expected size 64==64, stride 676==1 at dim=1<br />
Run failed with return code:  255<br />
Output:  None<br />
Error:  None<br />
```</p>
<h3>Versions</h3>
<p>SW info<br />
| name | target_branch | target_commit | refer_branch | refer_commit<br />
-- | -- | -- | -- | --<br />
torchbench | main | ff42d907 | main | ff42d907<br />
torch | main | 5c7b761f8e748fe45c8e2e29563df637ae651917 | main | becfda005e524f93b1efed64917a129ef6778135<br />
torchvision | main | 0.18.0a0+a52607e | main | 0.18.0a0+a52607e<br />
torchtext | main | 0.16.0a0+b0ebddc | main | 0.16.0a0+b0ebddc<br />
torchaudio | main | 2.2.0a0+5286f9f | main | 2.2.0a0+5286f9f<br />
torchdata | main | 0.7.1a0+0790338 | main | 0.7.1a0+0790338<br />
dynamo_benchmarks | main | nightly | main | nightly</p>
<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a></p>
<p><code>shell
bash inductor_single_run.sh single inference accuracy torchbench maml_omniglot float32 first dynamic default 0</code><br />
<code>maml_omniglot</code> looks like a new enabled model, so we did not find the guilty commit.</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @WeizhuoZhang-intel @chuanqi129 @zxd1997066 </p>]]></description>
      <pubDate>Tue, 19 Mar 2024 22:38:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122285</guid>
    </item>
    <item>
      <title>[Inductor][Triton] test_mixed_mm2_cuda fails on Hopper</title>
      <link>https://github.com/pytorch/pytorch/issues/122227</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>We are seeing some segfault failures on Hopper GPUs on all Triton 3.0.0 versions:</p>
<p><code>pytorch_triton-3.0.0+989adb9a29</code>:<br />
<code>python
python test/inductor/test_torchinductor.py -v -k test_mixed_mm2_cuda
test_mixed_mm2_cuda (__main__.GPUTests) ... python: /source/llvm-project/mlir/lib/IR/TypeRange.cpp:20: mlir::TypeRange::TypeRange(ArrayRef&lt;mlir::Type&gt;): Assertion `llvm::all_of(types, [](Type t) { return t; }) &amp;&amp; "attempting to construct a TypeRange with null types"' failed.</code></p>
<p><code>pytorch_triton-3.0.0+a9bc1a3647</code>:<br />
<code>python
python test/inductor/test_torchinductor.py -v -k test_mixed_mm2_cuda
test_mixed_mm2_cuda (__main__.GPUTests) ... [d8aa2091f3fe:962  :0:962] Caught signal 11 (Segmentation fault: Sent by the kernel at address (nil))</code></p>
<p><code>pytorch_triton-3.0.0+901819d2b6</code><br />
<code>python
python test/inductor/test_torchinductor.py -v -k test_mixed_mm2_cuda
test_mixed_mm2_cuda (__main__.GPUTests) ... [d8aa2091f3fe:1275 :0:1275] Caught signal 11 (Segmentation fault: invalid permissions for mapped object at address 0x7f745aeec067)</code></p>
<h3>Versions</h3>
<p>PyTorch version: 2.4.0a0+git0d845f7b<br />
NVIDIA H100 80GB</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 12:44:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122227</guid>
    </item>
    <item>
      <title>Need to FUNCTORCH_STACK_MATCH in the vmap(compile(f)) case</title>
      <link>https://github.com/pytorch/pytorch/issues/122201</link>
      <description><![CDATA[<p>When entering torch.compile, if the functorch state is already active, then we need to emit this guard</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @Chillee @samdow @kshitij12345 @janeyx99 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 07:41:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122201</guid>
    </item>
    <item>
      <title>[`torch.compile`] When PT2 compile threads are forked after user code spawns threads, gets stuck, unbounded RAM usage</title>
      <link>https://github.com/pytorch/pytorch/issues/121197</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When I run a GNN workload with a specific dataset, the compilation process gets stuck on step 2.</p>
<h3>Error logs</h3>
<p><code>[2024-03-05 04:00:37,435] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env
[2024-03-05 04:00:37,668] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s0 = 341523 for L['x'].size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,669] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s1 = 128 for L['x'].size()[1] [2, 9223372036854775806]
[2024-03-05 04:00:37,853] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s2 = 471405 for L['subgraphs'][0].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,856] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s3 = 95082 for L['subgraphs'][0].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,884] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s3 - 1 &lt;= s0 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:37,951] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s3 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:37,951] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s3 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] set_replacement s1 = 128
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] set_replacement s1 = 128
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s1, 128) [guard added] at torch_geometric/nn/dense/linear.py:147 in forward (_meta_registrations.py:1980 in meta_mm)
[2024-03-05 04:00:38,017] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s4 = 124196 for L['subgraphs'][1].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,020] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s5 = 12758 for L['subgraphs'][1].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,039] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s5 - 1 &lt;= s3 - 1 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:38,098] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s5 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:38,099] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s5 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:38,143] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s6 = 12033 for L['subgraphs'][2].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,147] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s7 = 1025 for L['subgraphs'][2].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,165] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s7 - 1 &lt;= s5 - 1 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:38,229] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s7 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:38,230] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s7 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:38,277] [0/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2024-03-05 04:00:38,283] [0/0] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor</code></p>
<h3>Minified repro</h3>
<p>Running with the environment variable gets stuck too, producing no output.</p>
<p>The code in question is here: https://github.com/dmlc/dgl/pull/7183</p>
<p>You can install DGL nightly build and install torch_geometric via pip.</p>
<p><code>bash
pip install --pre dgl -f https://data.dgl.ai/wheels-test/cu121/repo.html
pip install torch_geometric
python dgl/examples/sampling/graphbolt/pyg/node_classification_advanced.py --dataset=ogbn-papers100M --torch-compile</code></p>
<h3>Versions</h3>
<details><summary>Details</summary>
<p>

Collecting environment information...
PyTorch version: 2.3.0a0+ebedce2
Is debug build: False
CUDA used to build PyTorch: 12.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.28.1
Libc version: glibc-2.35

Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.3.107
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100-SXM4-80GB
GPU 1: NVIDIA A100-SXM4-80GB
GPU 2: NVIDIA A100-SXM4-80GB
GPU 3: NVIDIA A100-SXM4-80GB

Nvidia driver version: 545.23.08
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      43 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             256
On-line CPU(s) list:                0-255
Vendor ID:                          AuthenticAMD
Model name:                         AMD EPYC 7702 64-Core Processor
CPU family:                         23
Model:                              49
Thread(s) per core:                 2
Core(s) per socket:                 64
Socket(s):                          2
Stepping:                           0
Frequency boost:                    enabled
CPU max MHz:                        2183.5930
CPU min MHz:                        1500.0000
BogoMIPS:                           4000.17
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es
Virtualization:                     AMD-V
L1d cache:                          4 MiB (128 instances)
L1i cache:                          4 MiB (128 instances)
L2 cache:                           64 MiB (128 instances)
L3 cache:                           512 MiB (32 instances)
NUMA node(s):                       2
NUMA node0 CPU(s):                  0-63,128-191
NUMA node1 CPU(s):                  64-127,192-255
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow: Mitigation; safe RET
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.24.4
[pip3] onnx==1.15.0rc2
[pip3] optree==0.10.0
[pip3] pytorch-quantization==2.1.2
[pip3] torch==2.3.0a0+ebedce2
[pip3] torch_geometric==2.5.0
[pip3] torch-tensorrt==2.3.0a0
[pip3] torchdata==0.7.1a0
[pip3] torchmetrics==1.3.1
[pip3] torchtext==0.17.0a0
[pip3] torchvision==0.18.0a0
[pip3] triton==2.2.0+e28a256
[conda] Could not collect


</p>
</details>

<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 04 Mar 2024 20:12:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121197</guid>
    </item>
    <item>
      <title>torch._export.aot_compile doesnt preserve buffer mutations</title>
      <link>https://github.com/pytorch/pytorch/issues/120424</link>
      <description><![CDATA[<p>repro:<br />
```<br />
import torch</p>
<p>class Mod(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.register_buffer("buf", torch.arange(100, device='cuda') * 2)<br />
    def forward(self, x):<br />
        self.buf.mul_(2)<br />
        return x + self.buf</p>
<p>x = torch.ones(100, device='cuda')<br />
m = Mod()</p>
<h1>generated triton code mutates the buffer</h1>
<p>out = torch.compile(m)(x)</p>
<h1>generated triton code does NOT mutate the buffer</h1>
<p>so = torch._export.aot_compile(m, (x,))<br />
```</p>
<p>If you run with <code>TORCH_LOGS="aot,output_code"</code>, you will see that the generated triton code in the aot_compile case does not actually mutate the input buffer.</p>
<p>cc @ezyang @msaroufim @anijain2305 @zou3519 @chauhang @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4 @desertfire @chenyang78</p>]]></description>
      <pubDate>Thu, 22 Feb 2024 11:15:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120424</guid>
    </item>
    <item>
      <title>[inductor][cpp] unified the vectorized conversion with `at::vec::convert` for all data types</title>
      <link>https://github.com/pytorch/pytorch/pull/119979</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #119979</p>
<p>This PR unified the vectorized conversion with <code>at::vec::convert</code> for all vectorized data types. The intrinsics implementations are implemented as a specialization and moved to their own arch-specific files. The vectorized conversion logic in cpp Inductor is simplified.</p>
<p>cc @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 14 Feb 2024 21:46:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119979</guid>
    </item>
    <item>
      <title>Increased compile time max GPUs to 512. Switched to int16_t DeviceIndex.</title>
      <link>https://github.com/pytorch/pytorch/pull/119639</link>
      <description><![CDATA[<p>Fixes #115331.</p>
<p>This PR increases the number of valid GPU devices to 512 (from 64) in order to future-proof PyTorch for providers that offer <a href="https://www.tensorwave.com/">single nodes with a large device count</a>. Until now, <code>DeviceIndex</code> was an <code>int8_t</code>, thus multiple changes were necessary:</p>
<ul>
<li><code>DeviceIndex</code> changed to <code>int16_t</code>. Updated consumers that assume it to be an <code>int8_t</code>.</li>
<li>Updated bounds checking for <code>torch.device()</code> in the Python frontend. Right now, we allow funny things like <code>torch.device('cpu', 200).index == -56</code>, which is undefined behavior. I inserted some checks to only allow values between 0 and <code>c10::Device::MAX_NUM_DEVICES - 1</code>.</li>
<li>Updated the <code>ArgumentInfo</code> struct as it hardcodes the device index as 8 bit field [^1]. Might be a breaking change, not sure if users rely on this.</li>
<li>Introduced <code>c10::Device::MAX_NUM_DEVICES</code> as a replacement for the old <code>C10_COMPILE_TIME_MAX_GPUS</code></li>
</ul>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225</p>
<p>[^1]: This field was unsigned, so I guess this has also been undef behavior the whole time? Our default device index is -1, so this always wrapped around to 255 when written to the <code>ArgumentInfo</code> struct. When I switched the <code>DeviceIndex</code> to <code>int16_t</code>, it actually stayed 255 after unpacking from <code>ArgumentInfo</code> again, as the <code>DeviceIndex</code> was now wide enough that it didn't wrap back to -1.</p>]]></description>
      <pubDate>Sat, 10 Feb 2024 05:14:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119639</guid>
    </item>
    <item>
      <title>[Inductor][Optimus] Enable pointwise op broadcast to batch with larger size</title>
      <link>https://github.com/pytorch/pytorch/pull/119422</link>
      <description><![CDATA[<p>Summary:<br />
Recently, we observed a lot of mul op is not batched in BAU AFOC 30x model. After looked into its trace, we see a lot of mul ops are not batched due to different shapes, thus we enable the broadcast to improve fusion performance.<br />
https://pxl.cl/4jS6N<br />
Currently, we do not turn on the pass in default since the data copy introduced from repeat may be too expensive to see any fusion benefits.</p>
<p>Test Plan:</p>
<h1>unit test</h1>
<p>Buck UI: https://www.internalfb.com/buck2/86308f02-f64a-4247-bd36-e33fecb9428d<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/12103424025715455<br />
Network: Up: 13MiB  Down: 144MiB  (reSessionID-7cbeae0b-2e32-4432-a033-724fe2baa70f)<br />
Jobs completed: 263437. Time elapsed: 3:56.1s.<br />
Cache hits: 99%. Commands: 103672 (cached: 103662, remote: 10, local: 0)<br />
Tests finished: Pass 9. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<h1>local reproduce</h1>
<p><code>buck2 run @mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode batch-split --model_type "afoc" --flow_id 529557090</code><br />
P1201029889<br />
<code>optimus_scuba_log:  {'before_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GB8dhharTWRRo3YDABQwNoEnozImbr0LAAAz', 'BatchLayernormFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GK9POBQTOvI2Js0LAMlsd1UfgE0kbr0LAAAz', 'normalization_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GC5r_RY-KRCx5WUDAPMAZ4V7Ojpcbr0LAAAz', 'remove_split_with_size_one_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GOhNmhawVtn7OVgBAHCJ5lMz4R8mbr0LAAAz', 'after_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GAIkmxZxz5TfuRIBADyUBYGAsL4pbr0LAAAz', 'before_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GKAeuwS_GfWdwyIBAIfmeXP4ko5Qbr0LAAAz', 'PostGradBatchLinearFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GEAUrhZz0v5mH_MAAN6hfn-7ZHoObr0LAAAz', 'BatchMulPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GHFkQxXq6F6Cvl4BAHEPAkIQufdsbr0LAAAz', 'BatchSubPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GLRLMBZVkEraBc4CAF4J2rYofBZdbr0LAAAz', 'after_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GMvpKxfa1CJSzIYBABVGWLcO7A5xbr0LAAAz', 'inductor': Counter({'pattern_matcher_nodes': 1721, 'pattern_matcher_count': 1531, 'normalization_pass': 642, 'remove_split_with_size_one_pass': 629, 'batch_aten_mul_broadcast': 15, 'batch_aten_mul': 13, 'scmerge_split_sections_removed': 11, 'scmerge_cat_removed': 6, 'scmerge_cat_added': 5, 'batch_aten_add_broadcast': 4, 'scmerge_split_removed': 3, 'batch_linear_post_grad': 2, 'batch_aten_sub': 2, 'batch_layernorm': 1}), 'BatchAddBroadcastPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GHvyQRkauPU815IBAC13D3y3eOZEbr0LAAAz', 'BatchMulBroadcastPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GBTvvhZQAWVuI-IAAMFAdbuxCh5Ibr0LAAAz'}</code></p>
<h1>e2e</h1>
<p>training_platform:374b5b38116b4de5e921d3ec3063e19e</p>
<h3>BAU AFOC 30x (3k)</h3>
<p>baseline:<br />
f529554280<br />
proposal:<br />
f529596191</p>
<h3>BAU AFOC 30x (2k)</h3>
<p>baseline:<br />
f529557090<br />
proposal:<br />
f529597295</p>
<h3>FIRST + CMF</h3>
<p>baseline:<br />
f529583511<br />
proposal:<br />
f529583764</p>
<h3>ICVR</h3>
<p>baseline:<br />
f529587181<br />
proposal:<br />
f529587775</p>
<h3>IG_CTR</h3>
<p>baseline:<br />
f529590319<br />
proposal:<br />
f529590519</p>
<h3>MAI</h3>
<p>baseline:<br />
f529591434<br />
proposal:<br />
f529592306</p>
<h3>DPA</h3>
<p>baseline:<br />
f529593554<br />
proposal:<br />
f529594305</p>
<p>{F1454547965}{F1454547968}{F1454547967}<br />
Differential Revision: D53549537</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Wed, 07 Feb 2024 15:52:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119422</guid>
    </item>
    <item>
      <title>CPU memory cannot get released after `torch.compile` (caused by importing `AsyncCompile`)</title>
      <link>https://github.com/pytorch/pytorch/issues/109442</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I noticed that the following line (presumably called upon first compilation) create child processes that never terminate. This cause problems as (CPU) memory released in the main process (but created before creating the child processes) will still be referenced in the child processes.<br />
<code>python
from torch._inductor.codecache import AsyncCompile</code></p>
<h3>Error logs</h3>
<p>NA</p>
<h3>Minified repro</h3>
<p>NA</p>
<h3>Versions</h3>
<p>```<br />
root@92383b32f00e:~# CUDA_VISIBLE_DEVICES="" python collect_env.py <br />
Collecting environment information...<br />
PyTorch version: 2.0.1+cu117<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.7<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 18.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0<br />
Clang version: Could not collect<br />
CMake version: version 3.27.0<br />
Libc version: glibc-2.27</p>
<p>Python version: 3.10.9 (main, Mar  8 2023, 10:47:38) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.4.0-153-generic-x86_64-with-glibc2.27<br />
Is CUDA available: False<br />
CUDA runtime version: 11.7.99<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A100 80GB PCIe<br />
GPU 1: NVIDIA A100 80GB PCIe<br />
GPU 2: NVIDIA A100 80GB PCIe<br />
GPU 3: NVIDIA A100 80GB PCIe<br />
GPU 4: NVIDIA A100 80GB PCIe</p>
<p>Nvidia driver version: 525.125.06<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.5.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:        x86_64<br />
CPU op-mode(s):      32-bit, 64-bit<br />
Byte Order:          Little Endian<br />
CPU(s):              80<br />
On-line CPU(s) list: 0-79<br />
Thread(s) per core:  2<br />
Core(s) per socket:  20<br />
Socket(s):           2<br />
NUMA node(s):        2<br />
Vendor ID:           GenuineIntel<br />
CPU family:          6<br />
Model:               85<br />
Model name:          Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz<br />
Stepping:            7<br />
CPU MHz:             3783.358<br />
CPU max MHz:         4000.0000<br />
CPU min MHz:         800.0000<br />
BogoMIPS:            4200.00<br />
Virtualization:      VT-x<br />
L1d cache:           32K<br />
L1i cache:           32K<br />
L2 cache:            1024K<br />
L3 cache:            28160K<br />
NUMA node0 CPU(s):   0-19,40-59<br />
NUMA node1 CPU(s):   20-39,60-79<br />
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] mypy==1.5.1<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.25.2<br />
[pip3] pytorch-minimize==0.0.2<br />
[pip3] pytorch-quantization==2.1.2<br />
[pip3] torch==2.0.1<br />
[pip3] torchaudio==2.0.0<br />
[pip3] torchdata==0.6.0<br />
[pip3] torchelastic==0.2.2<br />
[pip3] torchtext==0.15.0<br />
[pip3] torchvision==0.15.0<br />
[pip3] triton==2.0.0<br />
[conda] blas                      1.0                         mkl<br />
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch<br />
[conda] mkl                       2021.4.0           h06a4308_640<br />
[conda] mkl-service               2.4.0           py310h7f8727e_0<br />
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0<br />
[conda] mkl_random                1.2.2           py310h00e6091_0<br />
[conda] numpy                     1.25.2                   pypi_0    pypi<br />
[conda] pytorch-cuda              11.7                 h778d358_3    pytorch<br />
[conda] pytorch-minimize          0.0.2                    pypi_0    pypi<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch<br />
[conda] pytorch-quantization      2.1.2                    pypi_0    pypi<br />
[conda] torch                     2.0.1                    pypi_0    pypi<br />
[conda] torchaudio                2.0.0               py310_cu117    pytorch<br />
[conda] torchdata                 0.6.0                     py310    pytorch<br />
[conda] torchelastic              0.2.2                    pypi_0    pypi<br />
[conda] torchtext                 0.15.0                    py310    pytorch<br />
[conda] torchtriton               2.0.0                     py310    pytorch<br />
[conda] torchvision               0.15.0              py310_cu117    pytorch<br />
```</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov</p>]]></description>
      <pubDate>Sat, 16 Sep 2023 10:13:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/109442</guid>
    </item>
    <item>
      <title>torch.compile/triton holding GIL during compilation and CompiledKernel call results in deadlocks during distributed training</title>
      <link>https://github.com/pytorch/pytorch/issues/109074</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>We are seeing our training processes get deadlocked as follows. There is a NCCL error causing the allreduce in the backward pass to get stuck. When this occurs, typically the GPU locks up until a <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/comms.html#ncclcommabort">ncclCommAbort</a> is called and even simple cuda calls like cudaLaunchKernel, cudaEventQuery etc. get stuck on the host side.  As a result, the main thread is stuck as follows:</p>
<p><code>Thread 432919 (idle): "MainThread"
    0x155555277197 (libc.so.6)
    pthread_cond_wait (libc.so.6)
    __gnu_cxx::__nothrow_wait_cv::wait (compatibility-condvar.cc:100)
    torch::autograd::ReadyQueue::pop (libtorch_cpu.so)
    torch::autograd::Engine::thread_main (libtorch_cpu.so)
    torch::autograd::Engine::execute_with_graph_task (libtorch_cpu.so)
    torch::autograd::python::PythonEngine::execute_with_graph_task (libtorch_python.so)
    torch::autograd::Engine::execute (libtorch_cpu.so)
    torch::autograd::python::PythonEngine::execute (libtorch_python.so)
    THPEngine_run_backward (libtorch_python.so)
    backward (torch/autograd/__init__.py:200)
    backward (torch/_tensor.py:503)</code></p>
<p>Simultaneously, triton is stuck due to the above NCCL issue while trying to call a compiled kernel. The bad part here though is that triton is holding GIL (see active+gil in the trace below) and is stuck. As a result, the entire Python process freezes up and no other python threads are able to execute:</p>
<p><code>Thread 493983 (active+gil): "Dummy-20"
    sched_yield (libc.so.6)
    0x15550de3cbdb (libcuda.so.535.54.03)
    0x15550e10adb5 (libcuda.so.535.54.03)
    0x15550e10b32a (libcuda.so.535.54.03)
    0x15550df18454 (libcuda.so.535.54.03)
    0x15550ddf7836 (libcuda.so.535.54.03)
    0x15550ddf7f3d (libcuda.so.535.54.03)
    0x15550ddfaa27 (libcuda.so.535.54.03)
    0x15550dfe9b3e (libcuda.so.535.54.03)
    launch (triton_.so)
    launcher (&lt;string&gt;:6)
    run (torch/_inductor/triton_ops/autotune.py:190)
    call (coksmpcybzxz4bxfvl56kcd6k5czlqpovjyjwg4yry6do7p7hcim.py:871)
    run (torch/_inductor/compile_fx.py:248)
    _fn (torch/_dynamo/eval_frame.py:209)
    call_func_with_args (torch/_functorch/aot_autograd.py:1249)
    call_compiled_backward (torch/_functorch/aot_autograd.py:2341)
    backward (torch/_functorch/aot_autograd.py:2365)</code></p>
<p>Another example is that triton also get stuck during compilation as well due to a <code>torch.cuda.synchronize</code> getting stuck as NCCL kernels are stuck. Even in this case, triton is stuck while holding the GIL and results in the entire Python process to lock up:</p>
<p><code>Thread 1957540 (active+gil): "Dummy-20"
    synchronize (torch/cuda/__init__.py:688)
    _precompile_config (torch/_inductor/triton_ops/autotune.py:94)
    &lt;listcomp&gt; (torch/_inductor/triton_ops/autotune.py:70)
    precompile (torch/_inductor/triton_ops/autotune.py:69)
    _load_kernel (torch/_inductor/codecache.py:554)
    result (torch/_inductor/codecache.py:574)
    wait (torch/_inductor/codecache.py:715)
    &lt;module&gt; (cpb2lazn3hvywa5maow5ngritvyt6pwlcq6ela3fl5rbblyj636x.py:1578)
    load (torch/_inductor/codecache.py:528)
    compile_to_module (torch/_inductor/graph.py:575)
    time_wrapper (torch/_dynamo/utils.py:163)
    compile_to_fn (torch/_inductor/graph.py:586)
    compile_fx_inner (torch/_inductor/compile_fx.py:177)
    inner (contextlib.py:75)
    inner (torch/_inductor/debug.py:239)
    debug_wrapper (torch/_dynamo/debug_utils.py:595)
    bw_compiler (torch/_inductor/compile_fx.py:441)
    time_wrapper (torch/_dynamo/utils.py:163)
    _fn (torch/_dynamo/eval_frame.py:209)
    _wrapped_bw_compiler (torch/_dynamo/backends/common.py:38)
    call_compiled_backward (torch/_functorch/aot_autograd.py:2336)
    backward (torch/_functorch/aot_autograd.py:2365)
    apply (torch/autograd/function.py:274)</code></p>
<p>We have some custom logic in our application to recover from NCCL errors by calling <code>_abort</code> on the ProcessGroup here: https://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/init.cpp#L2226 (We set <code>NCCL_ASYNC_ERROR_HANDLING=0</code> and manage errors ourselves). This logic is usually able to recover from the NCCL errors. However, now since triton is holding GIL and is stuck, the entire Python process is frozen and none our custom logic in python can run and recover from the situation.</p>
<p>I'm wondering if it is possible to release GIL before entering C/C++ land (similar to other PyTorch operations). This would resolve the issue and allow other python threads in the application to recover from the situation.</p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>PyTorch 2.0.1</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @wconstab @bdhirsh @anijain2305 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @Xia-Weiwen @ngimel</p>]]></description>
      <pubDate>Mon, 11 Sep 2023 14:32:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/109074</guid>
    </item>
    <item>
      <title>TorchInductor Hack-a-Day on July 19th</title>
      <link>https://github.com/pytorch/pytorch/issues/105328</link>
      <description><![CDATA[<h3>Better Engineering</h3>
<ul>
<li>[x] #105571</li>
<li>[ ] #105572</li>
</ul>
<h3>Unit test</h3>
<ul>
<li>[ ] skipIfTorchInductor tracker: https://github.com/pytorch/pytorch/issues/102207</li>
<li>[x] #105534</li>
</ul>
<h3>AOTInductor</h3>
<ul>
<li>[x] #105552</li>
<li>[x] #105553</li>
<li>[ ] #105554</li>
<li>[ ] Support rand fallback, https://github.com/pytorch/pytorch/issues/103415</li>
<li>[x] #105555</li>
</ul>
<h3>Decompositions (write decomp, verify that Inductor has good performance on them and fuses as expected):</h3>
<ul>
<li>[x] #105556</li>
<li>[x] #105557</li>
<li>[x] #105558</li>
<li>[x] #105559</li>
<li>[ ] #105560</li>
<li>[ ] #105561</li>
<li>[ ] #105562</li>
<li>[x] #105563</li>
<li>[x] #105564</li>
<li>[x] #105565</li>
<li>[ ] #105566</li>
<li>[ ] #105567</li>
<li>[ ] #105568</li>
</ul>
<h3>Decomposition optimizations</h3>
<ul>
<li>[ ] Lowering matmuls to pointwise operators when they‚Äôre small or bandwidth bound (https://github.com/pytorch/pytorch/issues/103313) </li>
<li>[ ] #105569</li>
</ul>
<h3>Lowering:</h3>
<ul>
<li>[ ] #105570<br />
aten.grid_sampler_2d_backward (investigate using decomposition for backward as well)<br />
aten.avg_pool3d</li>
</ul>
<h3>Performance:</h3>
<ul>
<li>[ ] Improve concat fusion with matmuls,   https://github.com/pytorch/pytorch/issues/102804</li>
<li>[ ] Do smarter layout planning with concatenate, https://github.com/pytorch/pytorch/issues/102805</li>
<li>[ ] Do concatenate copies with a foreach kernel if applicable, https://github.com/pytorch/pytorch/issues/103475</li>
<li>[ ] Pattern-match operators into foreach kernels</li>
<li>[ ] Stop zero-ing out non-differentiable outputs in AOTAutograd, https://github.com/pytorch/pytorch/issues/104272 </li>
</ul>
<h3>Compilation time</h3>
<ul>
<li>[ ] <code>will_fusion_create_cycle</code> takes a long time  , https://github.com/pytorch/pytorch/issues/98467 </li>
</ul>]]></description>
      <pubDate>Mon, 17 Jul 2023 07:13:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/105328</guid>
    </item>
    <item>
      <title>torch.compiled model output gets overwritten despite tensor.detach()</title>
      <link>https://github.com/pytorch/pytorch/issues/104435</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>related to https://github.com/pytorch/pytorch/blob/5ab1d2c2cc4e9c83b15c98974d6610a03322f40e/torch/_inductor/cudagraph_trees.py#L1889-L1893</p>
<p>at times when you would get this error, if instead of doing out = model(input), you do out = model(input).detach() to try to fix the error, you suppress the error message while not fixing the problem. Specifically the value of out will change if you run model(input).detach() again. you have to do model(input)+0 or something similar to actually fix the problem.</p>
<p>at a high level i think this bug is either <br />
A) about tensor.detach() suppressing an error message without fixing the error.<br />
B) model outputs getting overwritten despite tensor.detach()<br />
depending on whether B is expected or not.</p>
<p>either the error message should not be suppressed or the output value should function as expected.</p>
<p>@eellison </p>
<h3>Error logs</h3>
<p>n/a</p>
<h3>Minified repro</h3>
<p>n/a</p>
<p>my own repro, try running with/without @torch.compile() and with/without .detach()<br />
running as is should either throw the error message or give the same result as running without @torch.compile</p>
<p>@torch.compile()<br />
def foo(x):<br />
    return x * x * x</p>
<p>inp = torch.rand([2], device="cuda")<br />
out = foo(inp).detach()<br />
sum_val_1 = out+out<br />
out2 = foo(inp).detach()<br />
sum_val_2 = out+out<br />
print(sum_val_1, sum_val_2, out2 + out2)<br />
assert  sum_val_1.sum()==sum_val_2.sum()</p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.1.0a0+git1dba81f<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.7<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.5 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0<br />
Clang version: 10.0.0-4ubuntu1 <br />
CMake version: version 3.26.1<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.9.5 (default, Jun  4 2021, 12:28:51)  [GCC 7.5.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-1019-aws-x86_64-with-glibc2.31<br />
Is CUDA available: False<br />
CUDA runtime version: 11.7.64<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
CPU(s):                          96<br />
On-line CPU(s) list:             0-95<br />
Thread(s) per core:              2<br />
Core(s) per socket:              24<br />
Socket(s):                       2<br />
NUMA node(s):                    2<br />
Vendor ID:                       GenuineIntel<br />
CPU family:                      6<br />
Model:                           85<br />
Model name:                      Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz<br />
Stepping:                        7<br />
CPU MHz:                         2500.000<br />
BogoMIPS:                        5000.00<br />
Hypervisor vendor:               KVM<br />
Virtualization type:             full<br />
L1d cache:                       1.5 MiB<br />
L1i cache:                       1.5 MiB<br />
L2 cache:                        48 MiB<br />
L3 cache:                        71.5 MiB<br />
NUMA node0 CPU(s):               0-23,48-71<br />
NUMA node1 CPU(s):               24-47,72-95<br />
Vulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported<br />
Vulnerability L1tf:              Mitigation; PTE Inversion<br />
Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown<br />
Vulnerability Meltdown:          Mitigation; PTI<br />
Vulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown<br />
Vulnerability Retbleed:          Vulnerable<br />
Vulnerability Spec store bypass: Vulnerable<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==6.0.0<br />
[pip3] flake8-bugbear==23.3.23<br />
[pip3] flake8-comprehensions==3.12.0<br />
[pip3] flake8-executable==2.1.3<br />
[pip3] flake8-logging-format==0.9.0<br />
[pip3] flake8-pyi==23.3.1<br />
[pip3] flake8-simplify==0.19.3<br />
[pip3] mypy==0.960<br />
[pip3] mypy-extensions==0.4.3<br />
[pip3] numpy==1.23.1<br />
[pip3] pytorch-triton==2.1.0+440fd1bf20<br />
[pip3] torch==2.1.0a0+git1dba81f<br />
[pip3] torchvision==0.16.0a0+e5bf7cf<br />
[conda] blas                      1.0                         mkl<br />
[conda] mkl                       2021.4.0           h06a4308_640<br />
[conda] mkl-include               2023.0.0         h06a4308_25399<br />
[conda] mkl-service               2.4.0            py39h7f8727e_0<br />
[conda] mkl_fft                   1.3.1            py39hd3c417c_0<br />
[conda] mkl_random                1.2.2            py39h51133e4_0<br />
[conda] numpy                     1.23.1                   pypi_0    pypi<br />
[conda] pytorch-triton            2.1.0+440fd1bf20          pypi_0    pypi<br />
[conda] torch                     2.1.0a0+git1dba81f           dev_0    <develop><br />
[conda] torchvision               0.16.0a0+e5bf7cf           dev_0    <develop></p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305</p>]]></description>
      <pubDate>Thu, 29 Jun 2023 11:35:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/104435</guid>
    </item>
  </channel>
</rss>
