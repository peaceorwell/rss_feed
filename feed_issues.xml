<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[RFC] Use CUDA graphs by default on torch.compile</title>
      <link>https://github.com/pytorch/pytorch/issues/121968</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I keep seeing comparisons between JAX / TF / Keras vs <code>torch.compile</code> where they benchmark the "default XLA settings vs. the default <code>torch.compile</code>" to find that XLA frontends are x3-x4 faster than <code>torch.compile</code>.<br />
The last one I found is the newly posted Keras 3 benchmarks<br />
https://keras.io/getting_started/benchmarks/ <br />
but I have seen these skewed comparisons over and over again.</p>
<p>The only thing stopping us from compiling with CUDA graphs on is that these sometimes have a higher mem footprint. Would it be reasonable to turn these on by default so that people get speed by default, and then catch a potential OOM and re-raise it indicating that using CUDA-graphs off to potentialy mitigate the OOM?</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang @ezyang @jansel @albanD @eellison @Chillee </p>
<h3>Versions</h3>
<p>master</p>]]></description>
      <pubDate>Fri, 15 Mar 2024 05:32:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121968</guid>
    </item>
    <item>
      <title>[PT2.2] PyTorch dispacthing non core ATen ops in .compile mode for torch ops.</title>
      <link>https://github.com/pytorch/pytorch/issues/121967</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>PyTorch dispacthing non core ATen ops in .compile mode for torch ops. <br />
Same non core ATen ops are observed as part of our backend.</p>
<p><strong>List of non core ATen ops observed are:</strong><br />
<code>addmv.default
addbmm.default
cumprod.default
median.default
min.default</code></p>
<h3>Error logs</h3>
<p>aten ops getting involved for addmv ['addmv.default']<br />
aten ops getting involved for addbmm ['addbmm.default']<br />
aten ops getting involved for cumprod ['cumprod.default']<br />
aten ops getting involved for median ['median.default']<br />
aten ops getting involved for min ['min.default']</p>
<h3>Minified repro</h3>
<p>```<br />
import torch<br />
from torch._decomp import core_aten_decompositions<br />
from torch._dynamo.backends.common import aot_autograd</p>
<p>fx_info = {"fx_aten_ops": []}</p>
<p>def inner_compiler(fx_module: torch.fx.GraphModule, inputs):<br />
    for node in fx_module.graph.nodes:<br />
        if node.op == "call_function":<br />
            fx_info["fx_aten_ops"].append(node.target.<strong>name</strong>)<br />
    return fx_module</p>
<h1>UT for addmv ATen op</h1>
<p>def addmv_model(M, mat, vec):<br />
  return torch.addmv(M, mat, vec)</p>
<p>M = torch.randn(2)<br />
mat = torch.randn(2, 3)<br />
vec = torch.randn(3)</p>
<p>fx_trace_backend = aot_autograd(<br />
    fw_compiler=inner_compiler, decompositions=core_aten_decompositions()<br />
)<br />
model_compiled = torch.compile(addmv_model, backend=fx_trace_backend)<br />
result_fwd = model_compiled(M, mat, vec)</p>
<p>print(f"aten ops getting involved for addmv {fx_info['fx_aten_ops']}")</p>
<h1>UT for addbmm ATen op</h1>
<p>def addbmm_model(M, batch1, batch2):<br />
  return torch.addbmm(M, batch1, batch2)</p>
<p>M = torch.randn(3, 5)<br />
batch1 = torch.randn(10, 3, 4)<br />
batch2 = torch.randn(10, 4, 5)</p>
<p>fx_info = {"fx_aten_ops": []}<br />
fx_trace_backend = aot_autograd(<br />
    fw_compiler=inner_compiler, decompositions=core_aten_decompositions()<br />
)<br />
model_compiled = torch.compile(addbmm_model, backend=fx_trace_backend)<br />
result_fwd = model_compiled(M, batch1, batch2)<br />
print(f"aten ops getting involved for addbmm {fx_info['fx_aten_ops']}")</p>
<h1>UT for cumprod ATen op</h1>
<p>def cumprod_model(ifm):<br />
  return torch.cumprod(ifm, dim=0)</p>
<p>ifm = torch.randn(10)</p>
<p>fx_info = {"fx_aten_ops": []}<br />
fx_trace_backend = aot_autograd(<br />
    fw_compiler=inner_compiler, decompositions=core_aten_decompositions()<br />
)<br />
model_compiled = torch.compile(cumprod_model, backend=fx_trace_backend)<br />
result_fwd = model_compiled(ifm)<br />
print(f"aten ops getting involved for cumprod {fx_info['fx_aten_ops']}")</p>
<h1>UT for median ATen op</h1>
<p>def median_model(ifm):<br />
  return torch.median(ifm)</p>
<p>ifm = torch.randn(10)</p>
<p>fx_info = {"fx_aten_ops": []}<br />
fx_trace_backend = aot_autograd(<br />
    fw_compiler=inner_compiler, decompositions=core_aten_decompositions()<br />
)<br />
model_compiled = torch.compile(median_model, backend=fx_trace_backend)<br />
result_fwd = model_compiled(ifm)<br />
print(f"aten ops getting involved for median {fx_info['fx_aten_ops']}")</p>
<h1>UT for min ATen op</h1>
<p>def min_model(ifm):<br />
  return torch.min(ifm)</p>
<p>ifm = torch.randn(10)</p>
<p>fx_info = {"fx_aten_ops": []}<br />
fx_trace_backend = aot_autograd(<br />
    fw_compiler=inner_compiler, decompositions=core_aten_decompositions()<br />
)<br />
model_compiled = torch.compile(min_model, backend=fx_trace_backend)<br />
result_fwd = model_compiled(ifm)<br />
print(f"aten ops getting involved for min {fx_info['fx_aten_ops']}")</p>
<p>```</p>
<h3>Versions</h3>
<p>[pip3] numpy==1.26.4<br />
[pip3] torch==2.2.0a0<br />
[pip3] torchaudio==2.2.0<br />
[pip3] torchdata==0.7.1<br />
[pip3] torchtext==0.17.0<br />
[pip3] torchvision==0.17.0</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Fri, 15 Mar 2024 03:19:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121967</guid>
    </item>
    <item>
      <title>torch.compile fails when used together with activation checkpointing</title>
      <link>https://github.com/pytorch/pytorch/issues/121966</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>```python<br />
import time<br />
import torch<br />
import torch.utils.checkpoint</p>
<p>def add_and_drop(x):<br />
    return torch.nn.functional.dropout(x * 5, 0.5)</p>
<p>x = torch.rand((50001, 3072), device='cuda', dtype=torch.bfloat16)</p>
<p>x.requires_grad_(True)<br />
x.retain_grad()</p>
<p>with torch.cuda.amp.autocast(dtype=torch.bfloat16):<br />
    f = torch.compile(add_and_drop)<br />
    out = torch.utils.checkpoint.checkpoint(f, x, use_reentrant=False)<br />
    out.backward(torch.rand_like(out))<br />
    print(x.grad)<br />
```</p>
<p>fails with</p>
<p>```text</p>
<h1>python3 ./repro.py</h1>
<p>/usr/local/lib/python3.9/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils.<em>pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.<br />
  _torch_pytree._register_pytree_node(<br />
/usr/local/lib/python3.9/dist-packages/torch/autograd/<strong>init</strong>.py:411: UserWarning: Error detected in NativeDropoutBackward0. Traceback of forward call that caused the error:<br />
  File "//./repro.py", line 6, in add_and_drop<br />
    return torch.nn.functional.dropout(x * 5, 0.5)<br />
 (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:113.)<br />
  result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass<br />
Traceback (most recent call last):<br />
  File "//./repro.py", line 15, in <module><br />
    out = torch.utils.checkpoint.checkpoint(f, x, use_reentrant=False)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_compile.py", line 24, in inner<br />
    return torch._dynamo.disable(fn, recursive)(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py", line 489, in checkpoint<br />
    ret = function(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/eval_frame.py", line 655, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 727, in _convert_frame<br />
    result = inner_convert(frame, cache_entry, hooks, frame_state)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 383, in _convert_frame_assert<br />
    compiled_product = _compile(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 646, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 562, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 151, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 527, in transform<br />
    tracer.run()<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/symbolic_convert.py", line 2128, in run<br />
    super().run()<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/symbolic_convert.py", line 818, in run<br />
    and self.step()<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/symbolic_convert.py", line 781, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/symbolic_convert.py", line 2243, in RETURN_VALUE<br />
    self.output.compile_subgraph(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/output_graph.py", line 919, in compile_subgraph<br />
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)<br />
  File "/usr/lib/python3.9/contextlib.py", line 79, in inner<br />
    return func(</em>args, <strong>kwds)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/output_graph.py", line 1087, in compile_and_call_fx_graph<br />
    compiled_fn = self.call_user_compiler(gm)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/output_graph.py", line 1159, in call_user_compiler<br />
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/output_graph.py", line 1140, in call_user_compiler<br />
    compiled_fn = compiler_fn(gm, self.example_inputs())<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper<br />
    compiled_gm = compiler_fn(gm, example_inputs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/<strong>init</strong>.py", line 1668, in <strong>call</strong><br />
    return compile_fx(model</em>, inputs_, config_patches=self.config)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_inductor/compile_fx.py", line 1168, in compile_fx<br />
    return aot_autograd(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/backends/common.py", line 55, in compiler_fn<br />
    cg = aot_module_simplified(gm, example_inputs, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py", line 887, in aot_module_simplified<br />
    compiled_fn = create_aot_dispatcher_function(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py", line 600, in create_aot_dispatcher_function<br />
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 425, in aot_wrapper_dedupe<br />
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 630, in aot_wrapper_synthetic_base<br />
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 151, in aot_dispatch_autograd<br />
    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(  # type: ignore[misc]<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py", line 159, in aot_dispatch_autograd_graph<br />
    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py", line 32, in _create_graph<br />
    fx_g = make_fx(f, decomposition_table=aot_config.decompositions)(<em>args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 871, in wrapped<br />
    t = dispatch_trace(wrap_key(func, args, fx_tracer, pre_dispatch), tracer=fx_tracer, concrete_args=tuple(phs))<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_compile.py", line 24, in inner<br />
    return torch._dynamo.disable(fn, recursive)(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 483, in dispatch_trace<br />
    graph = tracer.trace(root, concrete_args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/_symbolic_trace.py", line 821, in trace<br />
    (self.create_arg(fn(<em>args)),),<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/_symbolic_trace.py", line 688, in flatten_fn<br />
    tree_out = root_fn(</em>tree_args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 519, in wrapped<br />
    out = f(<em>tensors)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 463, in joint_helper<br />
    return _functionalized_f_helper(primals, tangents)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 355, in _functionalized_f_helper<br />
    f_outs = fn(</em>f_args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 250, in inner_fn_with_anomaly<br />
    return inner_fn(<em>args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 235, in inner_fn<br />
    backward_out = torch.autograd.grad(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/autograd/<strong>init</strong>.py", line 411, in grad<br />
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py", line 1112, in unpack_hook<br />
    frame.recompute_fn(</em>args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py", line 1401, in recompute_fn<br />
    fn(<em>args, </em><em>kwargs)<br />
  File "/usr/lib/python3.9/contextlib.py", line 135, in <strong>exit</strong><br />
    self.gen.throw(type, value, traceback)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/random.py", line 175, in fork_rng<br />
    device_mod.set_rng_state(device_rng_state, device)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/cuda/random.py", line 61, in set_rng_state<br />
    new_state_copy = new_state.clone(memory_format=torch.contiguous_format)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/functional_tensor.py", line 309, in <strong>torch_dispatch</strong><br />
    outs_unwrapped = func(</em>args_unwrapped, <strong>kwargs_unwrapped)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_ops.py", line 513, in <strong>call</strong><br />
    return self._op(*args, </strong>(kwargs or {}))<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/_stats.py", line 20, in wrapper<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 596, in <strong>torch_dispatch</strong><br />
    return self.inner_torch_dispatch(func, types, args, kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 631, in inner_torch_dispatch<br />
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 376, in proxy_call<br />
    out = func(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_ops.py", line 513, in <strong>call</strong><br />
    return self._op(*args, </strong>(kwargs or {}))<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/_stats.py", line 20, in wrapper<br />
    return fn(<em>args, </em>*kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/fake_tensor.py", line 1392, in <strong>torch_dispatch</strong><br />
    return self.dispatch(func, types, args, kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/fake_tensor.py", line 1529, in dispatch<br />
    (flat_args, flat_arg_fake_tensors) = self.validate_and_convert_non_fake_tensors(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/fake_tensor.py", line 1776, in validate_and_convert_non_fake_tensors<br />
    validated_args = [validate(a) for a in flat_args]<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/fake_tensor.py", line 1776, in <listcomp><br />
    validated_args = [validate(a) for a in flat_args]<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/fake_tensor.py", line 1766, in validate<br />
    raise Exception(<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.clone.default(tensor([...], size=(16,), dtype=torch.uint8), memory_format=torch.contiguous_format)</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>
<p>This error only appears if <code>use_reentrant</code> is set to <code>False</code>.</p>
<h3>Versions</h3>
<p>```text<br />
PyTorch version: 2.2.1+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Debian GNU/Linux 11 (bullseye) (x86_64)<br />
GCC version: (Debian 10.2.1-6) 10.2.1 20210110<br />
Clang version: Could not collect<br />
CMake version: version 3.26.3<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.9.2 (default, Feb 28 2021, 17:03:44)  [GCC 10.2.1 20210110] (64-bit runtime)<br />
Is CUDA available: True<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Fri, 15 Mar 2024 02:41:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121966</guid>
    </item>
    <item>
      <title>[Inductor] fix addmm fusion check</title>
      <link>https://github.com/pytorch/pytorch/pull/121953</link>
      <description><![CDATA[<p>Fixes #121253.</p>
<p>To avoid functional issue, disable pattern match for <code>addmm</code> when <code>beta!=1</code> or <code>alpha!=1</code>, as either <code>mkl_linear</code> or <code>mkldnn_linear</code> doesn't accept <code>beta</code> or <code>alpha</code> as parameters.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 22:07:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121953</guid>
    </item>
    <item>
      <title>Compiled model raises error "attn_bias is not correctly aligned" in pytorch 2.2</title>
      <link>https://github.com/pytorch/pytorch/issues/121943</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When running the following code, errors may occur in pytorch 2.2.0 or 2.2.1, but not in 2.1.0.</p>
<p>```</p>
<p>from einops import rearrange<br />
import torch,torch.nn as nn<br />
def rotate_half(x):<br />
    x = rearrange(x, '... (d r) -&gt; ... d r', r = 2)<br />
    x1, x2 = x.unbind(dim = -1)<br />
    x = torch.stack((-x2, x1), dim = -1)<br />
    return rearrange(x, '... d r -&gt; ... (d r)')</p>
<p>def random_masking_v4(mask_kernel, percent,loss_kernel, B, H, W, device='cpu', loss_weight_factor = 1.0):<br />
    """<br />
    Perform per-sample random masking by per-sample shuffling.<br />
    Per-sample shuffling is done by argsort random noise.<br />
    x: [N, L, D], sequence<br />
    """<br />
    k1, k2 = mask_kernel<br />
    pad = (loss_kernel -1) // 2<br />
    with torch.no_grad():<br />
        noise1 = torch.rand(B, 1, H + k1 - 1, W + k2 - 1, device=device) * 800<br />
        noise1 = torch.nn.functional.max_pool2d(noise1, kernel_size=(k1, k2), stride=1, padding=0, )<br />
        noise2 = torch.rand(B, 1, H + k2 - 1, W + k1 - 1, device=device) * 800<br />
        noise2 = torch.nn.functional.max_pool2d(noise2, kernel_size=(k2, k1), stride=1, padding=0, )</p>
<pre><code>    noise = (torch.maximum(noise1, noise2)).view(B, 1, H, W)
    noise = (torch.rand(B, 1, H, W, device=device) - noise).view(B, -1)

    ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove, shape:B,N
    ids_restore = torch.argsort(ids_shuffle, dim=1) # represents the order of each id
    ids_mask = ids_restore &lt; int(H*W*percent)

    rand_center = torch.cat([ids_shuffle[:, 0:1] // W, ids_shuffle[:, 0:1] % W], 1).unsqueeze(-1)

    cy, cx = torch.meshgrid(torch.arange(H, device=device),
                            torch.arange(W, device=device), indexing='ij')
    coords = torch.stack([cy, cx]).view(1, 2, H * W)
    distance = (((coords - rand_center + torch.rand(B, 2, 1, device=device)) ** 2).sum(1)) ** 0.5  + 1
    ids_order = (distance * 3).int() * ~ids_mask + -100 * ids_mask
    can_see_p1 = ids_order[:,:,None] &gt;= ids_order[:,None,:]
    attn_mask = can_see_p1.unsqueeze(1)

    patch_order = ids_order.view(B,1,H,W).float()
    loss_order = torch.nn.functional.unfold(patch_order,loss_kernel,dilation=1, padding=pad)

    if loss_kernel == 3:
        loss_weight = torch.as_tensor((2,1,2,1,1,1,2,1,2),dtype=torch.float32,device=device)
    elif loss_kernel == 5:
        loss_weight = torch.as_tensor(((8,5,2,5,8),(5,2,1,2,5),(2,1,1,1,2),
                                            (5,2,1,2,5),(8,5,2,5,8)), dtype=torch.float32, device=device)
    else:
        raise NotImplementedError

    loss_weight = 1.0 / loss_weight.view(1,-1,1) ** loss_weight_factor
    loss_mask = ((loss_order-1e-5) &gt; patch_order.view(B,1,H*W)).float()

return torch.where(attn_mask,0,-9999.0), loss_mask * loss_weight
</code></pre>
<p>class Attention(nn.Module):</p>
<pre><code>def __init__(
        self,
        dim,
        num_heads=8,
        qkv_bias=True,
        qk_norm=False,
        attn_drop=0.,
        proj_drop=0.,
        norm_layer=nn.LayerNorm,
):
    super().__init__()
    assert dim % num_heads == 0, 'dim should be divisible by num_heads'
    self.num_heads = num_heads
    self.head_dim = dim // num_heads
    self.scale = self.head_dim ** -0.5
    self.fused_attn = True

    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
    self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
    self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
    self.attn_drop = nn.Dropout(attn_drop)
    self.proj = nn.Linear(dim, dim)
    self.proj_drop = nn.Dropout(proj_drop)

#@torch.compile
def forward(self, x,mask=None):
    B, N, C = x.shape
    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
    q, k, v = qkv.unbind(0)

    q, k = self.q_norm(q), self.k_norm(k)

    if self.fused_attn:
        x = torch.nn.functional.scaled_dot_product_attention(
            q, k, v,attn_mask=mask,
            dropout_p=self.attn_drop.p,
        )
    else:
        q = q * self.scale
        attn = q @ k.transpose(-2, -1)
        attn = attn + mask
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        x = attn @ v

    x = x.transpose(1, 2).reshape(B, N, C)
    x = self.proj(x)
    x = self.proj_drop(x)
    return x
</code></pre>
<p>class CustomModel(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.attn = Attention(768,12,True)</p>
<pre><code>def forward(self,x):
    mask,_ = random_masking_v4((2,5),0.15,3,x.shape[0],
                               int(x.shape[1]**0.5),int(x.shape[1]**0.5),device=x.device,
                               )
    return self.attn(x,mask)
</code></pre>
<p>model = CustomModel().cuda()<br />
model_without_ddp = model<br />
x =torch.zeros(256,196,768).cuda()</p>
<p>optimizer = torch.optim.AdamW(model_without_ddp.parameters())<br />
model = torch.compile(model)</p>
<p>with torch.cuda.amp.autocast():<br />
    out = model(x)<br />
loss = out.sum()<br />
loss.backward()<br />
optimizer.step()<br />
optimizer.zero_grad(True)</p>
<p>with torch.cuda.amp.autocast():<br />
    out = model(x)<br />
loss = out.sum()<br />
loss.backward()<br />
optimizer.step()<br />
optimizer.zero_grad(True)</p>
<p>```</p>
<p>error messages:<br />
```</p>
<p>File "//test.py", line 130, in <module><br />
    out = model(x)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "//test.py", line 116, in forward<br />
    def forward(self,x):<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 901, in forward<br />
    return compiled_fn(full_args)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 81, in g<br />
    return f(</em>args)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 83, in runtime_wrapper<br />
    all_outs = call_func_at_runtime_with_args(<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 105, in call_func_at_runtime_with_args<br />
    out = normalize_as_list(f(args))<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 81, in g<br />
    return f(<em>args)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/autograd/function.py", line 553, in apply<br />
    return super().apply(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 408, in forward<br />
    fw_outs = call_func_at_runtime_with_args(<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 105, in call_func_at_runtime_with_args<br />
    out = normalize_as_list(f(args))<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 864, in <strong>call</strong><br />
    return self.get_current_callable()(inputs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 611, in run<br />
    return model(new_inputs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 892, in _run_from_cache<br />
    return compiled_graph.compiled_artifact(inputs)<br />
  File "/tmp/torchinductor_root/hz/chzdqrbr5gisewqim47noe7zsijncibkeouw2ryw2no4ecybmvnj.py", line 641, in call<br />
    buf21 = aten._scaled_dot_product_efficient_attention(reinterpret_tensor(buf18, (256, 12, 196, 64), (451584, 64, 2304, 1), 0), reinterpret_tensor(buf18, (256, 12, 196, 64), (451584, 64, 2304, 1), 768), reinterpret_tensor(buf18, (256, 12, 196, 64), (451584, 64, 2304, 1), 1536), buf20, True)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_ops.py", line 755, in <strong>call</strong><br />
    return self._op(*args, </strong>(kwargs or {}))<br />
RuntimeError: attn_bias is not correctly aligned (strideM). attn_bias.stride(2) = 196, and should be a multiple of 8.</p>
<p>```</p>
<h3>Versions</h3>
<p>/usr/local/lib/python3.10/runpy.py:126: RuntimeWarning: 'torch.utils.collect_env' found in sys.modules after import of package 'torch.utils', but prior to execution of 'torch.utils.collect_env'; this may result in unpredictable behaviour<br />
  warn(RuntimeWarning(msg))<br />
Collecting environment information...<br />
PyTorch version: 2.2.0+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.16.3<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.10.13 (main, Dec 19 2023, 08:15:18) [GCC 9.4.0] (64-bit runtime)<br />
Python platform: Linux-4.15.0-189-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: Quadro RTX 6000<br />
GPU 1: Quadro RTX 6000<br />
GPU 2: Quadro RTX 6000<br />
GPU 3: Quadro RTX 6000<br />
GPU 4: Quadro RTX 6000<br />
GPU 5: Quadro RTX 6000<br />
GPU 6: Quadro RTX 6000<br />
GPU 7: Quadro RTX 6000</p>
<p>Nvidia driver version: 535.54.03<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
CPU(s):                          64<br />
On-line CPU(s) list:             0-63<br />
Thread(s) per core:              2<br />
Core(s) per socket:              16<br />
Socket(s):                       2<br />
NUMA node(s):                    2<br />
Vendor ID:                       GenuineIntel<br />
CPU family:                      6<br />
Model:                           85<br />
Model name:                      Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz<br />
Stepping:                        7<br />
CPU MHz:                         2651.207<br />
CPU max MHz:                     3900.0000<br />
CPU min MHz:                     1000.0000<br />
BogoMIPS:                        4600.00<br />
Virtualization:                  VT-x<br />
L1d cache:                       1 MiB<br />
L1i cache:                       1 MiB<br />
L2 cache:                        32 MiB<br />
L3 cache:                        44 MiB<br />
NUMA node0 CPU(s):               0-15,32-47<br />
NUMA node1 CPU(s):               16-31,48-63<br />
Vulnerability Itlb multihit:     KVM: Mitigation: Split huge pages<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Mitigation; TSX disabled<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.3<br />
[pip3] torch==2.2.0+cu118<br />
[pip3] torchlaunch==1.0<br />
[pip3] torchvision==0.17.0+cu118<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 18:01:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121943</guid>
    </item>
    <item>
      <title>[inductor] FX graph cache: Fix bug handling constants</title>
      <link>https://github.com/pytorch/pytorch/pull/121925</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121925</p>
<p>Summary: During key calculation for FX graph caching: Rather than specialize on "small" vs. "large" tensor constants (i.e., inlined vs. not inlined), always hash on the tensor value. Doing so avoids the complication of trying to later attach the constant values as attributes to an already-compiled module. Instead, different constants will cause an FX graph cache miss and we'll just compile.</p>
<p>Test Plan: New unit test</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 11:44:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121925</guid>
    </item>
    <item>
      <title>Cannot `torch.compile` if `weight_norm` is applied to a Module</title>
      <link>https://github.com/pytorch/pytorch/issues/121902</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>MWE:<br />
```python<br />
import torch<br />
from torch.nn.utils.parametrizations import weight_norm</p>
<p>c1 = torch.nn.Conv1d(100, 10, 10)<br />
c1 = weight_norm(c1, dim=0)</p>
<p>c1 = torch.compile(c1, fullgraph=True)<br />
x = torch.zeros(1, 100, 20)<br />
c1(x)<br />
```</p>
<p>Fails with <br />
<code>torch._dynamo.exc.TorchRuntimeError: Failed running call_method forward(*(ParametrizedLinear(
  in_features=20, out_features=10, bias=True
  (parametrizations): ModuleDict(
    (weight): ParametrizationList(
      (0): _WeightNorm()
    )
  )
), FakeTensor(..., size=(1, 100, 20))), **{}):
_weight_norm_interface() missing 1 required positional argument: 'dim'</code></p>
<p>If I omit the torch.compile, nothing goes wrong.</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.2.1<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-97-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.4.99<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA GeForce RTX 3090<br />
GPU 1: NVIDIA GeForce RTX 3090</p>
<p>Nvidia driver version: 535.161.07<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             96<br />
On-line CPU(s) list:                0-95<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz<br />
CPU family:                         6<br />
Model:                              85<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 24<br />
Socket(s):                          2<br />
Stepping:                           7<br />
CPU max MHz:                        3900,0000<br />
CPU min MHz:                        1000,0000<br />
BogoMIPS:                           4800.00<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          1,5 MiB (48 instances)<br />
L1i cache:                          1,5 MiB (48 instances)<br />
L2 cache:                           48 MiB (48 instances)<br />
L3 cache:                           71,5 MiB (2 instances)<br />
NUMA node(s):                       2<br />
NUMA node0 CPU(s):                  0-23,48-71<br />
NUMA node1 CPU(s):                  24-47,72-95<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Mitigation; Enhanced IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Mitigation; TSX disabled</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-lightning==2.2.1<br />
[pip3] torch==2.2.1<br />
[pip3] torchaudio==2.2.1<br />
[pip3] torchmetrics==0.11.4<br />
[pip3] triton==2.2.0<br />
[conda] blas                      1.0                         mkl    conda-forge<br />
[conda] libblas                   3.9.0            16_linux64_mkl    conda-forge<br />
[conda] libcblas                  3.9.0            16_linux64_mkl    conda-forge<br />
[conda] liblapack                 3.9.0            16_linux64_mkl    conda-forge<br />
[conda] mkl                       2022.1.0           hc2b9512_224<br />
[conda] numpy                     1.26.4          py310hb13e2d6_0    conda-forge<br />
[conda] pytorch                   2.2.1           py3.10_cuda12.1_cudnn8.9.2_0    pytorch<br />
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch<br />
[conda] pytorch-lightning         2.2.1              pyhd8ed1ab_0    conda-forge<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch<br />
[conda] torchaudio                2.2.1               py310_cu121    pytorch<br />
[conda] torchmetrics              0.11.4             pyhd8ed1ab_0    conda-forge<br />
[conda] torchtriton               2.2.0                     py310    pytorch<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 06:33:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121902</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Add Inductor Intel GPU backend.</title>
      <link>https://github.com/pytorch/pytorch/pull/121895</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121895<br />
* #121883</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 00:00:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121895</guid>
    </item>
    <item>
      <title>TORCH_USE_CUDA_DSA compiler flag not working</title>
      <link>https://github.com/pytorch/pytorch/issues/121894</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I did <code>export TORCH_USE_CUDA_DSA=1</code> before building using <code>python3 setup.py develop</code>, but when cuda error happens, it still ask me to compile with this option.<br />
Also tried to replace all instances of <code>#ifdef TORCH_USE_CUDA_DSA</code> in the code base with <code>#if 1</code> to force it, also no use.<br />
What am I missing here or is there something that resets this option during build?</p>
<p>I'm building on top of the tagged commit for 2.2.1 stable release (also not sure why is the version <code>2.2.0a0+git6c8c5ad</code> instead of <code>2.2.1</code> but that's not an issue for me)</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.2.0a0+git6c8c5ad<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.3<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.4 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.28.3<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.18 (main, Aug 25 2023, 13:20:14)  [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.19.0-051900rc6-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.3.107<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration:<br />
GPU 0: Tesla V100-PCIE-16GB<br />
GPU 1: Tesla V100-PCIE-16GB</p>
<p>Nvidia driver version: 545.23.08<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
Byte Order:                      Little Endian<br />
CPU(s):                          40<br />
On-line CPU(s) list:             0-39<br />
Vendor ID:                       GenuineIntel<br />
Model name:                      Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz<br />
CPU family:                      6<br />
Model:                           85<br />
Thread(s) per core:              1<br />
Core(s) per socket:              20<br />
Socket(s):                       2<br />
Stepping:                        4<br />
CPU max MHz:                     3700.0000<br />
CPU min MHz:                     1000.0000<br />
BogoMIPS:                        4800.00<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d arch_capabilities<br />
L1d cache:                       1.3 MiB (40 instances)<br />
L1i cache:                       1.3 MiB (40 instances)<br />
L2 cache:                        40 MiB (40 instances)<br />
L3 cache:                        55 MiB (2 instances)<br />
NUMA node(s):                    2<br />
NUMA node0 CPU(s):               0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38<br />
NUMA node1 CPU(s):               1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39<br />
Vulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported<br />
Vulnerability L1tf:              Mitigation; PTE Inversion<br />
Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT disabled<br />
Vulnerability Meltdown:          Mitigation; PTI<br />
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT disabled<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT disabled</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==7.0.0<br />
[pip3] numpy==1.26.4<br />
[pip3] optree==0.10.0<br />
[pip3] pytest-flake8==1.1.1<br />
[pip3] torch==2.2.0a0+git6c8c5ad<br />
[pip3] torchaudio==2.2.1<br />
[pip3] torcheval==0.0.7<br />
[pip3] torchvision==0.17.1<br />
[pip3] triton-nightly==2.1.0.post20240108192258<br />
[conda] Could not collect<br />
```</p>
<p>cc @ptrblck</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 23:54:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121894</guid>
    </item>
    <item>
      <title>[inductor][cpu]llama_v2_7b_16h AMP dynamic shape single thread performance test will be killed </title>
      <link>https://github.com/pytorch/pytorch/issues/121887</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c<br />
/workspace/pytorch# bash inductor_single_run.sh single inference performance torchbench llama_v2_7b_16h amp first dynamic<br />
Testing with dynamic shapes.<br />
Testing with freezing on.<br />
single-thread testing....<br />
/opt/conda/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.<br />
  _torch_pytree._register_pytree_node(<br />
loading model: 0it [00:40, ?it/s]<br />
cpu  eval  llama_v2_7b_16h<br />
skipping cudagraphs due to skipping cudagraphs due to cpu device. Found from :<br />
   File "benchmarks/dynamo/torchbench.py", line 354, in forward_pass<br />
    return mod(<em>inputs)<br />
  File "/workspace/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1174, in forward<br />
    outputs = self.model(<br />
  File "/workspace/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1011, in forward<br />
    inputs_embeds = self.embed_tokens(input_ids)</p>
<p>running benchmark:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 39/50 [08:13&lt;02:21, 12.89s/it]inductor_single_run.sh: line 64: 142610 Killed                  numactl -C 0-0 --membind=0 python benchmarks/dynamo/${SUITE}.py --${SCENARIO} --${DT} -dcpu -n50 --no-skip --dashboard --batch-size 1 --threads 1 --only "${MODEL}" ${Channels_extra} ${Shape_extra} ${Mode_extra} ${Flag_extra} --timeout 9000 --backend=${BACKEND} --output=/tmp/inductor_single_test_st.csv</p>
<p>https://github.com/pytorch/pytorch/commit/26740f853e10e34a7e8d2b72b950875796350418<br />
/workspace/pytorch# bash inductor_single_run.sh single inference performance torchbench llama_v2_7b_16h amp first dynamic<br />
Testing with dynamic shapes.<br />
Testing with freezing on.<br />
single-thread testing....<br />
/opt/conda/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.<br />
  _torch_pytree._register_pytree_node(<br />
loading model: 0it [00:40, ?it/s]<br />
cpu  eval  llama_v2_7b_16h<br />
skipping cudagraphs due to skipping cudagraphs due to cpu device. Found from :<br />
   File "benchmarks/dynamo/torchbench.py", line 354, in forward_pass<br />
    return mod(<em>inputs)<br />
  File "/workspace/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1174, in forward<br />
    outputs = self.model(<br />
  File "/workspace/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1011, in forward<br />
    inputs_embeds = self.embed_tokens(input_ids)</p>
<p>running benchmark: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [09:52&lt;00:00, 11.85s/it]<br />
1.529x<br />
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles<br />
cpu,llama_v2_7b_16h,1,1.529177,4678.804700,4.166590,0.871345,4347.434189,4989.334733,959,1,0,0,0,0</p>
<h3>Versions</h3>
<p></table><p>SW info</p><table border="1" class="dataframe table"><br />
<br />
<thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead></p>
<tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>1ef0a39e</td>
      <td>main</td>
      <td>ff42d907</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>41286f1505ffb214d386d72e4b72ebd680a4a475</td>
      <td>main</td>
      <td>581fe26792849a80f04feaa7f8bdec69b7f41dd8</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>

</table>
<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh single inference performance torchbench llama_v2_7b_16h amp first dynamic<br />
<a href="https://github.com/pytorch/pytorch/files/14597204/torchbench-llama_v2_7b_16h-inference-amp-dynamic-default-single-performance-crash_guilty_commit.log">torchbench-llama_v2_7b_16h-inference-amp-dynamic-default-single-performance-crash_guilty_commit.log</a><br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c<br />
cc @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 21:57:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121887</guid>
    </item>
    <item>
      <title>Training fails on torch.compile + device_map.</title>
      <link>https://github.com/pytorch/pytorch/issues/121873</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I'm trying to train Mixtral MoE Model using transformers, and I'm using the device_map flag which distributes the model layers across the GPUs.</p>
<p>```<br />
import torch<br />
import transformers<br />
from datasets import load_dataset<br />
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments</p>
<p>MODEL_ID = "mistralai/Mixtral-8x7B-v0.1"<br />
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)<br />
model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map="auto", torch_dtype=torch.float16)</p>
<p>tokenizer.pad_token = "!" #Not EOS, will explain another time.\</p>
<p>CUTOFF_LEN = 256  #Our dataset has shot text</p>
<p>dataset = load_dataset("harpreetsahota/modern-to-shakesperean-translation") #Found a good small dataset for a quick test run! Thanks to the uploader!<br />
train_data = dataset["train"] # Not using evaluation data</p>
<p>def generate_prompt(user_query):<br />
    sys_msg= "Translate the given text to Shakespearean style."<br />
    p =  "<s> [INST]" + sys_msg +"\n"+ user_query["modern"] + "[/INST]" +  user_query["shakespearean"] + "</s>"<br />
    return p </p>
<p>def tokenize(prompt):<br />
    return tokenizer(<br />
        prompt + tokenizer.eos_token,<br />
        truncation=True,<br />
        max_length=CUTOFF_LEN ,<br />
        padding="max_length"<br />
    )</p>
<p>train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)), remove_columns=["modern" , "shakespearean"])<br />
opt_model = torch.compile(model)<br />
trainer = Trainer(<br />
    model=opt_model,<br />
    train_dataset=train_data,<br />
    args=TrainingArguments(<br />
        per_device_train_batch_size=1,<br />
        gradient_accumulation_steps=1,<br />
        num_train_epochs=100,<br />
        learning_rate=1e-4,<br />
        logging_steps=1,<br />
        optim="adamw_torch",<br />
        save_strategy="epoch",<br />
        output_dir="mixtral-moe-lora-instruct-shapeskeare",<br />
        remove_unused_columns=False<br />
    ),<br />
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)<br />
)<br />
model.config.use_cache = False</p>
<p>trainer.train()</p>
<p>```</p>
<p>I get this error:</p>
<p><code>File "/home/aiscuser/transformers/src/transformers/models/mixtral/modeling_mixtral.py", line 1171, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1536, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/torch/nn/functional.py", line 2264, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)</code></p>
<h3>Versions</h3>
<p>nightly pyotch<br />
nightly transformers</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 16:35:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121873</guid>
    </item>
    <item>
      <title>Log restart reasons and extra compile time in CompilationMetrics</title>
      <link>https://github.com/pytorch/pytorch/pull/121827</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121827</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 08:52:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121827</guid>
    </item>
    <item>
      <title>Workaround for nvrtcCompileProgram changing locale</title>
      <link>https://github.com/pytorch/pytorch/pull/121822</link>
      <description><![CDATA[<p>There is a bug in CUDA 11.7 through at least CUDA 12.4 which changes the current thread locale when calling nvrtcCompileProgram.<br />
See e.g. https://stackoverflow.com/questions/74044994 This also includes the encoding used by Python by default for e.g. subsequent invocations of <code>subprocess</code> calls. When the user environment is now set to e.g. UTF-8 and changed by CUDA (to ASCII/ANSI_X3.4-1968) Python will fail to decode UTF-8 output from programs invoked.<br />
This happens e.g. in <code>test_torch</code> which calls <code>from scipy import stats</code> which runs <code>lscpu</code> and errors with something like</p>
<blockquote>
<p>UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 96: ordinal not in range(128)</p>
</blockquote>
<p>Fix by wrapping the nvrtcCompileProgram saving and restoring the thread locale.</p>
<p>cc @ptrblck</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 07:47:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121822</guid>
    </item>
    <item>
      <title>[inductor] disable linear weight prepacking pass on double</title>
      <link>https://github.com/pytorch/pytorch/pull/121478</link>
      <description><![CDATA[<p>Fix #121175 </p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121478</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 07 Mar 2024 23:05:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121478</guid>
    </item>
    <item>
      <title>Inductor: fix Conv output stride for dynamic shapes</title>
      <link>https://github.com/pytorch/pytorch/pull/121400</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121400</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/120873.<br />
Fixes the output stride of Conv in the case of dynamic shapes. The previous logic in inductor assumed that the output stride of Conv is always channels last while it is actually contiguous if <code>dynamic_shapes and is_contiguous_storage_and_layout(x)</code>.</p>
<h3>Static shape</h3>
<p>In static shape cases, since weight is prepacked (<code>weight_t.is_mkldnn()</code> will be <code>true</code>), we'll always force output to be channels last in the Conv kernel, thus it's fine to have the assumption in Inductor that the output stride of Conv is always channels last.<br />
https://github.com/pytorch/pytorch/blob/96ed37ac13366cc9a7e6645b8955061d0a14f80b/aten/src/ATen/native/mkldnn/Conv.cpp#L357-L358</p>
<h3>Dynamic shape</h3>
<p>In dynamic shape cases, we won't do weight prepack for Conv, in this case, the Conv kernel decides the output layout based on the input and weight layout.<br />
https://github.com/pytorch/pytorch/blob/96ed37ac13366cc9a7e6645b8955061d0a14f80b/torch/_inductor/fx_passes/mkldnn_fusion.py#L1024-L1025</p>
<p>For input with <code>channels = 1</code>, like tensor of size <code>(s0, 1, 28, 28)</code> and stride <code>(784, 784, 28, 1)</code>, in Inductor, with <code>req_stride_order</code> in channels last order, the <code>require_stride_order</code> on <code>x</code> of such size and stride won't change the stride of the tensor since stride for dimensions of size 1 is ignored<br />
https://github.com/pytorch/pytorch/blob/96ed37ac13366cc9a7e6645b8955061d0a14f80b/torch/_inductor/ir.py#L5451</p>
<p>While in Conv kernel, such tensor is consider it as <strong>contiguous</strong> tensor instead of channels last tensor thus the output of the Conv kernel will be in contiguous format.<br />
https://github.com/pytorch/pytorch/blob/96ed37ac13366cc9a7e6645b8955061d0a14f80b/aten/src/ATen/native/ConvUtils.h#L396-L404</p>
<p>To align the behavior of the Conv kernel, we set the output_stride in such case to be contiguous instead of channels last.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 07 Mar 2024 01:42:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121400</guid>
    </item>
    <item>
      <title>[torch.compile] `fuse_attention` returns inconsistent value for the model</title>
      <link>https://github.com/pytorch/pytorch/issues/121174</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>With <code>torch.compile</code>, <code>fuse_attention</code> returns inconsistent value for the model</p>
<p>```py<br />
import torch<br />
import math</p>
<p>torch.manual_seed(0)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()
    self.q_conv = torch.nn.Conv2d(4, 4, 1)
    self.k_conv = torch.nn.Conv2d(4, 4, 1)
    self.v_conv = torch.nn.Conv2d(4, 4, 1)

def forward(self, x):
    q = self.q_conv(x)
    k = self.k_conv(x)
    v = self.v_conv(x)
    q = q.permute(0, 2, 1, 3)
    k = k.permute(0, 2, 1, 3)
    v = v.permute(0, 2, 1, 3)
    div = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))
    attn_weight = torch.nn.functional.softmax(div, dim=-1)
    output = torch.matmul(attn_weight, v)
    return output
</code></pre>
<p>func = Model()</p>
<p>x = torch.randn(1, 4, 2, 2)</p>
<p>func = Model()</p>
<p>with torch.no_grad():<br />
    print(func(x.clone()))<br />
    # tensor([[[[ 0.1035, -0.2616],<br />
    #         [ 0.0461, -0.3923],<br />
    #         [ 0.0852, -0.4379],<br />
    #         [ 0.0538, -0.5618]],<br />
    #         [[-0.3332, -0.2985],<br />
    #         [-0.1777, -0.2978],<br />
    #         [-0.4235, -0.3401],<br />
    #         [-0.4361, -0.3477]]]])</p>
<pre><code>func1 = torch.compile(func)
print(func1(x.clone()))
# tensor([[[[ 0.1624,  0.2315],
#         [ 0.0282,  0.0822],
#         [ 0.2613,  0.0440],
#         [-0.0161, -0.0560]],

#         [[-0.3225, -0.2300],
#         [-0.2753, -0.3049],
#         [-0.3932, -0.3184],
#         [-0.3870, -0.2880]]]])

print(torch.equal(func(x.clone()), func1(x.clone()))) # False
print(torch.equal(func.k_conv.weight, func1.k_conv.weight)) # True
print(torch.equal(func.q_conv.weight, func1.q_conv.weight)) # True
print(torch.equal(func.v_conv.weight, func1.v_conv.weight)) # True
</code></pre>
<p>```</p>
<p>If I disable the <code>fuse_attention</code>, the compiled model will return the same value without optimization.</p>
<h3>Versions</h3>
<p>PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi</p>
<p>cc @jbschlosser @bhosmer @cpuhrsch @erichan1 @drisspg @mikaylagawarecki @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 04 Mar 2024 15:46:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121174</guid>
    </item>
    <item>
      <title>Tensors with multiple unbacked SymInt dims don't work in Inductor</title>
      <link>https://github.com/pytorch/pytorch/issues/120548</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Internal xref: https://fb.workplace.com/groups/6829516587176185/posts/6892427944218382/</p>
<p>The use case here is modeling 2D/3D variable size data (e.g., images/videos of varying input dimensionality)</p>
<p>This doesn't work:</p>
<p>```<br />
import torch<br />
import torch._dynamo.config</p>
<p>torch._dynamo.config.capture_scalar_outputs = True</p>
<p>@torch.compile(fullgraph=True)<br />
def f(x):<br />
    s = x.tolist()<br />
    return torch.empty(*s)</p>
<p>f(torch.tensor([3, 4, 5]))<br />
```</p>
<p>Fails with</p>
<p>```<br />
  File "/data/users/ezyang/a/pytorch/torch/<em>inductor/graph.py", line 976, in run_node<br />
    dense = torch._prims_common.is_non_overlapping_and_dense(n.meta["val"])<br />
  File "/data/users/ezyang/a/pytorch/torch/_prims_common/<strong>init</strong>.py", line 356, in is_non_overlapping_and_dense<br />
    if is_contiguous(a) or is_channels_last_contiguous(a):<br />
  File "/data/users/ezyang/a/pytorch/torch/_prims_common/<strong>init</strong>.py", line 244, in is_contiguous<br />
    if y != expected_stride:<br />
  File "/data/users/ezyang/a/pytorch/torch/<strong>init</strong>.py", line 374, in <strong>bool</strong><br />
    return self.node.bool</em>()<br />
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/sym_node.py", line 432, in bool_<br />
    return self.guard_bool("", 0)<br />
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/sym_node.py", line 374, in guard_bool<br />
    r = self.shape_env.evaluate_expr(self.expr, self.hint, fx_node=self.fx_node)<br />
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/recording.py", line 255, in wrapper<br />
    return event.run(self)<br />
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/recording.py", line 156, in run<br />
    return self.f(<em>args, </em>*kwargs)<br />
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/symbolic_shapes.py", line 3914, in evaluate_expr<br />
    raise self._make_data_dependent_error(expr.xreplace(self.var_to_val), expr)<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
GuardOnDataDependentSymNode: Could not guard on data-dependent expression Ne(Max(1, u2), u2) (unhinted: Ne(Max(1, u2), u2)).  (Size-like symbols: u2)</p>
<p>Potential framework code culprit (scroll up for full backtrace):<br />
  File "/data/users/ezyang/a/pytorch/torch/_prims_common/<strong>init</strong>.py", line 244, in is_contiguous<br />
    if y != expected_stride:</p>
<p>For more information, run with TORCH_LOGS="dynamic"<br />
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u2"<br />
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1<br />
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing<br />
```</p>
<p>Notably, graph capture is all fine; we are only choking in Inductor. cc @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>
<p>It is tempting to add guard_size_oblivious to torch/_prims_common/<strong>init</strong>.py but this is a bit questionable: the tensors we create here should be obviously contiguous (since we built them with torch.empty(*s)), and adding that here will cause us to report that it is NOT contiguous, which is not what we want. Here's a better fix:</p>
<p><code>diff --git a/torch/_inductor/graph.py b/torch/_inductor/graph.py
index 51af2a51c49..fb8810a94e3 100644
--- a/torch/_inductor/graph.py
+++ b/torch/_inductor/graph.py
@@ -973,7 +973,7 @@ class GraphLowering(torch.fx.Interpreter):
                 n.meta["val"], torch.Tensor
             ):
                 strides = n.meta["val"].stride()
-                dense = torch._prims_common.is_non_overlapping_and_dense(n.meta["val"])
+                dense = torch.ops.aten.is_non_overlapping_and_dense.default(n.meta["val"])
                 # requiring a stride order for a non-dense output wouldn't
                 # recreate the same strides, and would fail with view, defer for now.
                 if dense and len(strides):</code></p>
<p>We then choke at</p>
<p><code>File "/data/users/ezyang/a/pytorch/torch/_inductor/graph.py", line 980, in run_node                                                        
    stride_order = ir.get_stride_order(strides)                                                                                              
  File "/data/users/ezyang/a/pytorch/torch/_inductor/ir.py", line 198, in get_stride_order                                                   
    sorted_idx: List[int] = argsort(seq)                                                                                                     
  File "/data/users/ezyang/a/pytorch/torch/_inductor/utils.py", line 762, in argsort                                                         
    return list(reversed(sorted(a_r, key=getter, reverse=True)))  # noqa: C413                                                               
  File "/data/users/ezyang/a/pytorch/torch/__init__.py", line 374, in __bool__           
    return self.node.bool_()  
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/sym_node.py", line 432, in bool_  
    return self.guard_bool("", 0)
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/sym_node.py", line 374, in guard_bool
    r = self.shape_env.evaluate_expr(self.expr, self.hint, fx_node=self.fx_node)                                                             
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/recording.py", line 255, in wrapper            
    return event.run(self)                                            
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/recording.py", line 156, in run
    return self.f(*args, **kwargs)
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/symbolic_shapes.py", line 3914, in evaluate_expr        
    raise self._make_data_dependent_error(expr.xreplace(self.var_to_val), expr)            
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:                                                                          
GuardOnDataDependentSymNode: Could not guard on data-dependent expression Max(1, u1)*Max(1, u2) &lt; Max(1, u2) (unhinted: Max(1, u1)*Max(1, u2)
 &lt; Max(1, u2)).  (Size-like symbols: u1, u2)</code></p>
<p>So it seems we need to thread the needle and avoid attempting to directly compute ordering on strides if we actually know it's contiguous out of band.</p>
<p>Also cc @shunting314 for NHWC striding optimization.</p>
<p>cc @msaroufim @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames </p>
<h3>Versions</h3>
<p>main</p>]]></description>
      <pubDate>Fri, 23 Feb 2024 21:22:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120548</guid>
    </item>
    <item>
      <title>Increased compile time max GPUs to 512. Switched to int16_t DeviceIndex.</title>
      <link>https://github.com/pytorch/pytorch/pull/119639</link>
      <description><![CDATA[<p>Fixes #115331.</p>
<p>This PR increases the number of valid GPU devices to 512 (from 64) in order to future-proof PyTorch for providers that offer <a href="https://www.tensorwave.com/">single nodes with a large device count</a>. Until now, <code>DeviceIndex</code> was an <code>int8_t</code>, thus multiple changes were necessary:</p>
<ul>
<li><code>DeviceIndex</code> changed to <code>int16_t</code>. Updated consumers that assume it to be an <code>int8_t</code>.</li>
<li>Updated bounds checking for <code>torch.device()</code> in the Python frontend. Right now, we allow funny things like <code>torch.device('cpu', 200).index == -56</code>, which is undefined behavior. I inserted some checks to only allow values between 0 and <code>c10::Device::MAX_NUM_DEVICES - 1</code>.</li>
<li>Updated the <code>ArgumentInfo</code> struct as it hardcodes the device index as 8 bit field [^1]. Might be a breaking change, not sure if users rely on this.</li>
<li>Introduced <code>c10::Device::MAX_NUM_DEVICES</code> as a replacement for the old <code>C10_COMPILE_TIME_MAX_GPUS</code></li>
</ul>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225</p>
<p>[^1]: This field was unsigned, so I guess this has also been undef behavior the whole time? Our default device index is -1, so this always wrapped around to 255 when written to the <code>ArgumentInfo</code> struct. When I switched the <code>DeviceIndex</code> to <code>int16_t</code>, it actually stayed 255 after unpacking from <code>ArgumentInfo</code> again, as the <code>DeviceIndex</code> was now wide enough that it didn't wrap back to -1.</p>]]></description>
      <pubDate>Sat, 10 Feb 2024 05:14:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119639</guid>
    </item>
  </channel>
</rss>
