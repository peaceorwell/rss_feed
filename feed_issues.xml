<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[inductor] for UserDefinedTritonKernels don't mark all inputs as mutating</title>
      <link>https://github.com/pytorch/pytorch/pull/124425</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124425</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 11:54:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124425</guid>
    </item>
    <item>
      <title>DISABLED test_indexing (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124420</link>
      <description><![CDATA[<p>Platforms: linux, rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_indexing&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23985486255">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_indexing</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 10:39:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124420</guid>
    </item>
    <item>
      <title>Add ability to save TORCH_COMPILE_DEBUG logs for CI failures</title>
      <link>https://github.com/pytorch/pytorch/pull/124408</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124408</p>
<p>Summary: The intent is that we can whitelist certain benchmarks to a) enable TORCH_COMPILE_DEBUG=1, and b) save the generated artifacts in test/debug in case of a failure. Via the rules in action.yml, we can then upload test/debug/ to S3 whenever it exists. I chose to introduce a new directory (test/debug/) rather than using an existing one (e.g., test/test-reports/), because these don't seem like test reports and we can later add other debug-related artifacts if we find it useful. For example, we might want to later explore including the inductor cache artifacts.</p>
<p>Test Plan:<br />
See artifacts generated when I force a failure: https://hud.pytorch.org/pr/124234<br />
Specifically: https://gha-artifacts.s3.amazonaws.com/pytorch/pytorch/8729891826/1/artifact/debug-test-inductor_torchbench-2-2-linux.g5.4xlarge.nvidia.gpu_23953679574.zip</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 08:36:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124408</guid>
    </item>
    <item>
      <title>Make torch._inductor.dependencies.Dep a proper class</title>
      <link>https://github.com/pytorch/pytorch/pull/124407</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124407</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 07:58:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124407</guid>
    </item>
    <item>
      <title>DISABLED test_hooks_cpp (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124406</link>
      <description><![CDATA[<p>Platforms: linux, rocm, asan</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_hooks_cpp&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23978895294">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_hooks_cpp</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 07:39:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124406</guid>
    </item>
    <item>
      <title>DISABLED test_leaf_assignment (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124405</link>
      <description><![CDATA[<p>Platforms: linux</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_leaf_assignment&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23967465080">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_leaf_assignment</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 07:39:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124405</guid>
    </item>
    <item>
      <title>DISABLED test_hook_with_no_name (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124392</link>
      <description><![CDATA[<p>Platforms: linux, rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_hook_with_no_name&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23960894990">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_hook_with_no_name</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 04:44:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124392</guid>
    </item>
    <item>
      <title>torch.compile: could not reconstruct view by re-applying a ViewMeta sequence</title>
      <link>https://github.com/pytorch/pytorch/issues/124382</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I got the following error when running a <code>torch.compile</code>'ed function:<br />
<code>W0418 04:04:09.562000 140622378160704 torch/_functorch/_aot_autograd/functional_utils.py:240] [__aot_joint_graph] could not reconstruct view by re-applying a ViewMeta sequence. This error is possibly caused by dynamic shapes. Fallbacking to reconstruction using as_strided. Error message: /home/weiwen/pytorch-fork/build/aten/src/ATen/RegisterCPU.cpp:13111: SymIntArrayRef expected to contain only concrete integers</code></p>
<p>This error is caused by commit https://github.com/pytorch/pytorch/commit/e4c887fbf6fcc9b1864d10b3ab18294e5edebdfc. Before this commit, there is no error messages. With such errors, the program is still able to run but becomes slower than before.</p>
<p>Expected behavior: No error messages and it has the same behavior as before this commit.</p>
<p>Reproducer:<br />
```python<br />
import torch</p>
<p>@torch.compile(dynamic=True)<br />
def func(A: torch.Tensor, threshold=0.0):<br />
    cols = A.shape[-1]<br />
    if len(A.shape) == 3:<br />
        rows = A.shape[0] * A.shape[1]<br />
    else:<br />
        assert A.dim() == 2, f"Input tensor should be 2d or 3d but got {A.dim()}d"<br />
        rows = A.shape[0]<br />
    A = A.reshape(rows, cols)</p>
<pre><code>if threshold == 0.0:
    outlier_indices = None
    outlier_coord = None
    outlier_rows = None
    outlier_cols = None
    outlier_values = None
else:
    outlier_indices = torch.abs(A) &gt;= threshold
    outlier_coord = outlier_indices.nonzero()
    outlier_rows = outlier_coord[:, 0]
    outlier_cols = outlier_coord[:, 1]
    outlier_values = A[outlier_indices]

return outlier_indices, outlier_coord, outlier_rows, outlier_cols, outlier_values
</code></pre>
<p>print('1')<br />
A = torch.randn(2048, 2048) * 3<br />
func(A)<br />
print('2')<br />
A = torch.randn(8192, 2048) * 3<br />
func(A)<br />
print('3')<br />
A = torch.randn(2048, 8192) * 3<br />
func(A)</p>
<p>print('4')<br />
A = torch.randn(2048, 2048) * 3<br />
func(A, threshold=3)<br />
print('5')<br />
A = torch.randn(8192, 2048) * 3<br />
func(A, threshold=3)<br />
print('6')<br />
A = torch.randn(2048, 8192) * 3<br />
func(A, threshold=3)</p>
<p>for i in range(10):<br />
    print('run', i)<br />
    bs = pow(2, i)<br />
    A = torch.randn(bs, 2, 2048) * 3<br />
    func(A, threshold=3)<br />
print('ok')<br />
```</p>
<h3>Versions</h3>
<p>PyTorch version: 2.4.0a0+git236b0d1<br />
Is debug build: False<br />
CUDA used to build PyTorch: None<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: CentOS Stream 8 (x86_64)<br />
GCC version: (conda-forge gcc 12.3.0-3) 12.3.0<br />
Clang version: Could not collect<br />
CMake version: version 3.28.1<br />
Libc version: glibc-2.28</p>
<p>Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.16.0-x86_64-with-glibc2.28<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:        x86_64<br />
CPU op-mode(s):      32-bit, 64-bit<br />
Byte Order:          Little Endian<br />
CPU(s):              240<br />
On-line CPU(s) list: 0-239<br />
Thread(s) per core:  2<br />
Core(s) per socket:  60<br />
Socket(s):           2<br />
NUMA node(s):        2<br />
Vendor ID:           GenuineIntel<br />
CPU family:          6<br />
Model:               143<br />
Model name:          Intel(R) Xeon(R) Platinum 8490H<br />
Stepping:            8<br />
CPU MHz:             1900.000<br />
CPU max MHz:         3500.0000<br />
CPU min MHz:         800.0000<br />
BogoMIPS:            3800.00<br />
Virtualization:      VT-x<br />
L1d cache:           48K<br />
L1i cache:           32K<br />
L2 cache:            2048K<br />
L3 cache:            115200K<br />
NUMA node0 CPU(s):   0-59,120-179<br />
NUMA node1 CPU(s):   60-119,180-239<br />
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr avx512_fp16 amx_tile flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==3.8.2<br />
[pip3] flake8-bugbear==20.1.4<br />
[pip3] flake8-coding==1.3.3<br />
[pip3] flake8-comprehensions==3.3.0<br />
[pip3] flake8-executable==2.0.4<br />
[pip3] flake8-pyi==20.5.0<br />
[pip3] lion-pytorch==0.1.4<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.26.2<br />
[pip3] optree==0.10.0<br />
[pip3] torch==2.4.0a0+gite4c887f<br />
[pip3] torchao==0.1<br />
[pip3] triton==2.3.0<br />
[conda] lion-pytorch              0.1.4                    pypi_0    pypi<br />
[conda] mkl                       2023.1.0         h213fc3f_46344<br />
[conda] mkl-include               2023.2.0                 pypi_0    pypi<br />
[conda] mkl-static                2023.2.0                 pypi_0    pypi<br />
[conda] numpy                     1.26.2                   pypi_0    pypi<br />
[conda] optree                    0.10.0                   pypi_0    pypi<br />
[conda] torch                     2.4.0a0+gite4c887f           dev_0    <develop><br />
[conda] torchao                   0.1                       dev_0    <develop><br />
[conda] triton                    2.3.0                    pypi_0    pypi</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 03:15:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124382</guid>
    </item>
    <item>
      <title>DISABLED test_hook_closure_cycle_use_custom_function_False_use_tensor_hook_True (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124377</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_hook_closure_cycle_use_custom_function_False_use_tensor_hook_True&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23962914651">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_hook_closure_cycle_use_custom_function_False_use_tensor_hook_True</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 01:41:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124377</guid>
    </item>
    <item>
      <title>DISABLED test_free_activation_memory_subclass (__main__.TestCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124376</link>
      <description><![CDATA[<p>Platforms: linux, rocm, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_free_activation_memory_subclass&amp;suite=TestCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23959106877">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 11 workflow(s) with 33 failures and 11 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_free_activation_memory_subclass</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 01:41:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124376</guid>
    </item>
    <item>
      <title>DISABLED test_hook_closure_cycle_use_custom_function_False_use_tensor_hook_False (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124365</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_hook_closure_cycle_use_custom_function_False_use_tensor_hook_False&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23956030827">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 136 workflow(s) with 408 failures and 136 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_hook_closure_cycle_use_custom_function_False_use_tensor_hook_False</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 22:40:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124365</guid>
    </item>
    <item>
      <title>inductor: Add Conv3d support</title>
      <link>https://github.com/pytorch/pytorch/pull/124361</link>
      <description><![CDATA[<p>This PR is to add Conv3d support in inductor. Basicly reuse and expand Conv2d logic and unit tests to Conv3d.</p>
<p>Conv3d inductor support will improve the performance of C2D_R50, I3D_R50, I3D_R101, Slow and SlowFast-R50 from OOB models.</p>
<p>| C2D_R50 | I3D_R50 | I3D_R101 | Slow | SlowFast-R50<br />
-- | -- | -- | -- | -- | --<br />
eager | 15.805 | 13.909 | 11.639 | 12.101 | 6.606<br />
Compile w/o conv3d | 17.244 | 14.893 | 12.109 | 13.015 | 6.603<br />
Compile w/ conv3d | 21.212 | 17.707 | 14.974 | 16.130 | 8.537</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 21:37:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124361</guid>
    </item>
    <item>
      <title>torch.compile produces wrong result for changing attributes in python class instances</title>
      <link>https://github.com/pytorch/pytorch/issues/124357</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>The following toy example demonstrates the issue, it has a python class with instance attribute "self.value". When torch.compile captures the graph, the value is captured as a constant. However, if the user changes the "self.value" before running the compiled method again, the updated "self.value" is not used and the results are wrong.</p>
<p>```<br />
import torch<br />
import torch._dynamo as dynamo<br />
import torch._inductor as inductor<br />
import torch.nn as nn</p>
<p>dynamo.reset()</p>
<p>class ToyModel(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super(ToyModel, self).<strong>init</strong>()<br />
        self.value = -1<br />
        self.cache = torch.ones([2, 3])</p>
<pre><code>def change_value(self, value):
    self.value = value

def forward(self, value):
    return self.cache + self.value

def print_value(self):
    print(f"model value = {self.value}")
</code></pre>
<p>model = ToyModel()<br />
compiled_function = torch.compile(model, backend="inductor")</p>
<p>values = [6, 8, 10]</p>
<p>for value in values:<br />
    model.change_value(value)<br />
    output = compiled_function(value)<br />
    model.print_value()<br />
    print(f"output = {output}")<br />
<code>The result from this is -</code><br />
model value = 6<br />
output = tensor([[7., 7., 7.],<br />
        [7., 7., 7.]])<br />
model value = 8<br />
output = tensor([[7., 7., 7.],<br />
        [7., 7., 7.]])<br />
model value = 10<br />
output = tensor([[7., 7., 7.],<br />
        [7., 7., 7.]])<br />
<code>The expected result (when the model is directly invoked instead of the torch.compiled mode) -</code><br />
model value = 6<br />
output = tensor([[7., 7., 7.],<br />
        [7., 7., 7.]])<br />
model value = 8<br />
output = tensor([[9., 9., 9.],<br />
        [9., 9., 9.]])<br />
model value = 10<br />
output = tensor([[11., 11., 11.],<br />
        [11., 11., 11.]])<br />
```</p>
<p>The FX graph that is captured has the first value (6) captured as a constant and there is no recompilation triggered when the attribute value changes -<br />
<code>def forward(self, arg0_1: "f32[2, 3]"):
         # File: &lt;ipython-input-10-2a476b979374&gt;:18, code: return self.cache + self.value
         add: "f32[2, 3]" = torch.ops.aten.add.Tensor(arg0_1, 6);  arg0_1 = None
        return (add,)</code> </p>
<p>The is no guard for the attribute -<br />
<code>[2024-04-18 04:28:53,734] [9/0] torch._dynamo.guards.__guards: [DEBUG] GUARDS:
[2024-04-18 04:28:53,736] [9/0] torch._dynamo.guards.__guards: [DEBUG] ___check_obj_id(L['self'], 140611742430160)                   # return self.cache + self.value  # &lt;ipython-input-12-27b4b650d0f1&gt;:17 in forward
[2024-04-18 04:28:53,740] [9/0] torch._dynamo.guards.__guards: [DEBUG] L['self'].training == True                                    # return self.cache + self.value  # &lt;ipython-input-12-27b4b650d0f1&gt;:17 in forward
[2024-04-18 04:28:53,743] [9/0] torch._dynamo.guards.__guards: [DEBUG] utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:379 in init_ambient_guards
[2024-04-18 04:28:53,747] [9/0] torch._dynamo.guards.__guards: [DEBUG] (___skip_backend_check() or ___current_backend() == ___lookup_backend(140616675298176))  # _dynamo/output_graph.py:385 in init_ambient_guards
[2024-04-18 04:28:53,751] [9/0] torch._dynamo.guards.__guards: [DEBUG] ___compile_config_hash() == '7c8ad0067f1cee33caeae883101ebbde'  # _dynamo/output_graph.py:387 in init_ambient_guards</code></p>
<h3>Versions</h3>
<p>PyTorch version: 2.2.2+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.27.9<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-6.1.58+-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.2.140<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: Tesla T4<br />
Nvidia driver version: 535.104.05<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             2<br />
On-line CPU(s) list:                0,1<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Xeon(R) CPU @ 2.30GHz<br />
CPU family:                         6<br />
Model:                              63<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 1<br />
Socket(s):                          1<br />
Stepping:                           0<br />
BogoMIPS:                           4599.99<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities<br />
Hypervisor vendor:                  KVM<br />
Virtualization type:                full<br />
L1d cache:                          32 KiB (1 instance)<br />
L1i cache:                          32 KiB (1 instance)<br />
L2 cache:                           256 KiB (1 instance)<br />
L3 cache:                           45 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0,1<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Mitigation; PTE Inversion<br />
Vulnerability Mds:                  Vulnerable; SMT Host state unknown<br />
Vulnerability Meltdown:             Vulnerable<br />
Vulnerability Mmio stale data:      Vulnerable<br />
Vulnerability Retbleed:             Vulnerable<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Vulnerable<br />
Vulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers<br />
Vulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.3<br />
[pip3] torch==2.2.2+cu118<br />
[pip3] torchaudio==2.2.2+cu118<br />
[pip3] torchdata==0.7.1<br />
[pip3] torchsummary==1.5.1<br />
[pip3] torchtext==0.17.1<br />
[pip3] torchvision==0.17.2+cu118<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 20:33:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124357</guid>
    </item>
    <item>
      <title>[inductor] switch assume_aligned_inputs to False</title>
      <link>https://github.com/pytorch/pytorch/pull/124336</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124336<br />
* #123319</p>
<p>In #123319, we guard some behavior behind the <code>assume_aligned_inputs</code> config option. If we set this to <code>False</code>, then the behavior added in #123319 becomes the default behavior. See the referenced PR for more details about the behavior affected.</p>
<p>Side effects:<br />
* It's possible that this will hurt performance in some scenarios. For example, if an unaligned input is used in a matmul, it might be better to perform the clone to align it first.<br />
* This will occasionally cause recompiles. Specifically: the check we perform (<code>(storage_offset * get_dtype_size(dtype)) % ALIGNMENT == 0</code>) can be guarded on if the storage_offset becomes dynamic. storage_offset becomes dynamic during automatic_dynamic_shapes after a shape or stride changes. This is seen in the changes to cpu_inductor_torchbench_inference.csv.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 15:14:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124336</guid>
    </item>
    <item>
      <title>SDPA + torch.compile: (*bias): last dimension must be contiguous</title>
      <link>https://github.com/pytorch/pytorch/issues/124289</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Hi,</p>
<p>I noticed a bug with SDPA when using torch.compile, both on 2.2.2 and 2.3 RC.</p>
<p>Reproduction:<br />
<code>git clone https://github.com/huggingface/transformers.git
cd transformers
git checkout 40eb6d6c5fcdaf99b499c249640d41f28659565b (main branch)</code><br />
and<br />
```python<br />
from transformers import LlamaForCausalLM<br />
import torch</p>
<p>llm_name = "meta-llama/Llama-2-7b-hf"<br />
llm = LlamaForCausalLM.from_pretrained(llm_name).cuda()</p>
<p>llm.forward = torch.compile(llm.forward, fullgraph=True)</p>
<p>attn_mask = torch.ones(1, 450, dtype = torch.long).cuda()<br />
outputs = llm(torch.ones(1, 450, dtype = torch.long).cuda(), attention_mask=attn_mask)</p>
<p>print("------")</p>
<p>attn_mask = torch.ones(1, 1, dtype = torch.long).cuda()<br />
outputs = llm(torch.ones(1, 1, dtype = torch.long).cuda(), attention_mask=attn_mask)<br />
```</p>
<p>Interestingly, adding<br />
<code>query_states = query_states.contiguous()
key_states = key_states.contiguous()
value_states = value_states.contiguous()
causal_mask = causal_mask.contiguous()</code><br />
before SDPA call does not help. So the error below is not very helpful.</p>
<p>We always have:<br />
<code>Traceback (most recent call last):
  File "/home/felix/test_compile.py", line 16, in &lt;module&gt;
    outputs = llm(torch.ones(1, 1, dtype = torch.long).cuda(), attention_mask=attn_mask)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 451, in _fn
    return fn(*args, **kwargs)
  File "/home/felix/transformers/src/transformers/models/llama/modeling_llama.py", line 1144, in forward
    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 451, in _fn
    return fn(*args, **kwargs)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 917, in forward
    return compiled_fn(full_args)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/utils.py", line 89, in g
    return f(*args)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 88, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/utils.py", line 113, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/utils.py", line 89, in g
    return f(*args)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 505, in forward
    fw_outs = call_func_at_runtime_with_args(
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/utils.py", line 113, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 906, in __call__
    return self.get_current_callable()(inputs)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 784, in run
    return model(new_inputs)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 934, in _run_from_cache
    return compiled_graph.compiled_artifact(inputs)
  File "/tmp/torchinductor_felix/ea/ceaid5aqesabg6e5wow7dfa7mruu6carauiqwmtu6mwv6thztgup.py", line 1105, in call
    buf13 = aten._scaled_dot_product_efficient_attention.default(buf10, buf11, reinterpret_tensor(buf6, (1, 32, 1, 128), (4096, 128, 4096, 1), 0), reinterpret_tensor(buf12, (1, 32, 1, 1), (0, 0, 0, 0), 0), True)
  File "/home/felix/miniconda3/envs/fx/lib/python3.9/site-packages/torch/_ops.py", line 594, in __call__
    return self_._op(*args, **kwargs)
RuntimeError: (*bias): last dimension must be contiguous</code></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @drisspg @ArthurZucker this seem to impact a relatively hot SDPA path</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.3.0+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: 10.0.0-4ubuntu1 <br />
CMake version: version 3.24.3<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.4.0-166-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 12.3.52<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A100-SXM4-80GB<br />
GPU 1: NVIDIA A100-SXM4-80GB<br />
GPU 2: NVIDIA A100-SXM4-80GB<br />
GPU 3: NVIDIA DGX Display<br />
GPU 4: NVIDIA A100-SXM4-80GB</p>
<p>Nvidia driver version: 535.129.03<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Byte Order:                         Little Endian<br />
Address sizes:                      43 bits physical, 48 bits virtual<br />
CPU(s):                             128<br />
On-line CPU(s) list:                0-127<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 64<br />
Socket(s):                          1<br />
NUMA node(s):                       1<br />
Vendor ID:                          AuthenticAMD<br />
CPU family:                         23<br />
Model:                              49<br />
Model name:                         AMD EPYC 7742 64-Core Processor<br />
Stepping:                           0<br />
Frequency boost:                    enabled<br />
CPU MHz:                            1879.127<br />
CPU max MHz:                        2250,0000<br />
CPU min MHz:                        1500,0000<br />
BogoMIPS:                           4491.21<br />
Virtualization:                     AMD-V<br />
L1d cache:                          2 MiB<br />
L1i cache:                          2 MiB<br />
L2 cache:                           32 MiB<br />
L3 cache:                           256 MiB<br />
NUMA node0 CPU(s):                  0-127<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Vulnerable<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca sme sev sev_es</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==6.0.0<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.23.4<br />
[pip3] onnx==1.16.0<br />
[pip3] onnx-graphsurgeon==0.3.27<br />
[pip3] onnx-tool==0.9.0<br />
[pip3] onnxruntime==1.17.3<br />
[pip3] torch==2.3.0+cu121<br />
[pip3] torch-tb-profiler==0.4.0<br />
[pip3] torchaudio==2.3.0+cu121<br />
[pip3] torchvision==0.18.0+cu121<br />
[pip3] triton==2.3.0<br />
[conda] numpy                     1.23.4                   pypi_0    pypi<br />
[conda] torch                     2.3.0+cu121              pypi_0    pypi<br />
[conda] torch-tb-profiler         0.4.0                    pypi_0    pypi<br />
[conda] torchaudio                2.3.0+cu121              pypi_0    pypi<br />
[conda] torchvision               0.18.0+cu121             pypi_0    pypi<br />
[conda] triton                    2.3.0                    pypi_0    pypi<br />
```</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 07:00:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124289</guid>
    </item>
    <item>
      <title>DISABLED test_gc_in_destructor (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124266</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_gc_in_destructor&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23907236167">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 21 failures and 7 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_gc_in_destructor</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 22:40:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124266</guid>
    </item>
    <item>
      <title>DISABLED test_function_returns_input (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124243</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_function_returns_input&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23900542662">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_function_returns_input</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 16:57:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124243</guid>
    </item>
    <item>
      <title>[don't land] Add ability to save TORCH_COMPILE_DEBUG logs for CI failures</title>
      <link>https://github.com/pytorch/pytorch/pull/124234</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124234</p>
<p>Summary: The idea is that we can enroll certain benchmarks to a) enable TORCH_COMPILE_DEBUG=1, and b) save those logs in test/debug in case of a failure. We can then update action.yml to upload test/debug/ to S3 whenever it exists.</p>
<p>Test Plan:<br />
See artifacts generated when I force a benchmark failure:<br />
https://hud.pytorch.org/pr/124234 (e.g., debug-test-inductor_torchbench-2-2-linux.g5.4xlarge.nvidia.gpu_23904773386.zip)</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 15:59:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124234</guid>
    </item>
    <item>
      <title>[NestedTensor] torch.compile silent specialization on _max_seqlen</title>
      <link>https://github.com/pytorch/pytorch/issues/124205</link>
      <description><![CDATA[<p>As illustrated in code snippet below, NestedTensor._max_seqlen property is being silently specialized when compiled - problematic behavior (silent hardcoding of value used for tracing) can be reproduced in both <code>inductor</code> and <code>eager</code> compilation modes.</p>
<p>Expected behavior:<br />
* compiler generated dynamic code that doesn't specialize into specific value of NestedTensor<br />
OR at least fails prominently if specialization occurs - and guards kick in.</p>
<p>Observed behavior:<br />
* success that leads to wrong outputs, as torch.compile produces specialized code without adding guards.</p>
<p>cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer</p>
<p>```<br />
from torch.nested._internal.nested_tensor import NestedTensor<br />
import torch</p>
<p>@torch.compile<br />
@torch._dynamo.config.patch(error_on_recompile=True)<br />
def sample_fun(njt: NestedTensor) -&gt; NestedTensor:<br />
    njt = njt.clamp(0.1, 0.5)<br />
    # if this is NOT specialized - this should lead to second printed output be larger than first<br />
    njt *= njt._max_seqlen<br />
    return njt</p>
<p>def create_njt(el_per_row):<br />
    torch.manual_seed(0)<br />
    njt = NestedTensor(<br />
        values=torch.randn(10 * el_per_row, device="cuda"),<br />
        offsets=torch.arange(11, device="cuda") * el_per_row,<br />
    )<br />
    # This sets _max_seqlen in cache<br />
    print(njt._max_seqlen)<br />
    return njt</p>
<p>with torch.inference_mode():<br />
    # This works<br />
    print(sample_fun(create_njt(el_per_row=1)).values())<br />
    # But this printed output should be 2x as big - and it is NOT as max_seqlen is specialized in generated code.<br />
    print(sample_fun(create_njt(el_per_row=2)).values())<br />
```</p>
<p>When run with TORCH_LOGS=output_code this produces code that hardcodes _max_seqlen=1</p>
<p>https://gist.github.com/tgalkovskyi/691ccb841c318a795873bfa30c990b06</p>
<h3>Versions</h3>
<p>ANP kernel pytorch (meta internal)</p>
<p>Version 2266<br />
Built April 16, 2024, 5:22:27‚ÄØAM</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 11:31:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124205</guid>
    </item>
    <item>
      <title>[1/N] Scalar Support: Enable aot compile to support aten operations with scalar input like alpha</title>
      <link>https://github.com/pytorch/pytorch/pull/124177</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124070<br />
* <strong>-&gt;</strong> #124177<br />
* #116368<br />
* #121387<br />
* #123545</p>
<p>Some operations have a scalar input parameter, like <code>torch.add(a, b, alpha=2.0)</code>.  Currently, the aot compile does not support such a case because it requires the signature of the captured graph to align with the operation's signature. This means that some inputs in the captured graph may be scalar(float, int, bool, etc.). It breaks the assumption of <code>compile_fx_aot</code> as it assumes all the example inputs are tensor - https://github.com/pytorch/pytorch/blob/0f6ce45bcbd7026c00da43db0317ede10830378b/torch/_inductor/compile_fx.py#L1048</p>
<p>This PR intends to support such cases by allowing not-aligned signature and filtering out the non-Tensor parameters. </p>
<p>Captured graph for <code>torch.add(a, b, alpha=2.0)</code></p>
<p>```<br />
opcode         name      target           args              kwargs</p>
<hr />
<p>placeholder    arg0_1    arg0_1           ()                {}<br />
placeholder    arg1_1    arg1_1           ()                {}<br />
call_function  add       aten.add.Tensor  (arg0_1, arg1_1)  {'alpha': 2.0}<br />
output         output_1  output           ((add,),)         {}<br />
```</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 07:05:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124177</guid>
    </item>
    <item>
      <title>[WIP] [Inductor Intel GPU backend Upstream] Reuse inductor test for Intel GPU (PART 2)</title>
      <link>https://github.com/pytorch/pytorch/pull/124147</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124147<br />
* #122866</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 21:37:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124147</guid>
    </item>
    <item>
      <title>[inductor] consider pointwise nodes when deciding reduction hint</title>
      <link>https://github.com/pytorch/pytorch/pull/124131</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124131</p>
<p>In certain <strong>rare</strong> scenarios, inductor can generate a reduction kernel with really bad perf. E.g., if <br />
- the reduction kernel contains a reduction node followed by a pointwise node<br />
- And the pointwise node use a transposed layout. <br />
- the reduction node is an inner reduction<br />
- and rnumel &lt;= 1024 , </p>
<p>then inductor will generate a persistent reduction kernel and it causes really bad perf when doing tl.store for the pointwise node since we use a very skinny tile <code>(XBLOCK=1, RBLOCK=next_power_of_2(rnumel))</code> .</p>
<p>I've tried a few version of fix.<br />
- The first version is, if I found any pointwise node in a reduction kernel uses a non-contiguous dependency, we use ReductionHint.DEFAULT. This cause 8s compilation time increase for huggingface with no perf wins... The reason is ReductionHint.DEFAULT does more autotunings.<br />
- Then I changed the code to be more specific. We change the hint from INNER to DEFAULT if we are sure that the pointwise kernel can use a &gt;1 stride for the lowest dimension. Kernels meet this condition should mostly have really bad perf anyways.</p>
<p>The situation mentioned above is rare. But it's reported by internal users. I'll also run one more perf test.</p>
<p>Testing script: https://gist.github.com/shunting314/9d3389891fa43633b49b8b7564ad6d8b . Something equivalent is also added as a unit test.</p>
<p>For this specific test from user reports, we improve the mentioned reduction kernels perf by <strong>4.14x</strong> (451us -&gt; 109us)</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 17:21:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124131</guid>
    </item>
    <item>
      <title>[inductor] Improve stability of scaled softmax</title>
      <link>https://github.com/pytorch/pytorch/pull/124119</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124119</p>
<p>This adds a pattern which replaces:<br />
<code>python
   scale(x) - scale(x).amax(dim, keepdim=True)</code><br />
with<br />
<code>python
   scale(x - x.amax(dim, keepdim=True))</code><br />
where <code>scale</code> can be either multiplication or division by a scalar,<br />
or a tensor that is broadcast in the <code>dim</code> dimension.</p>
<p>We can find this pattern inside of the decomposed graph of:<br />
<code>python
F.softmax(scale(x), dim=dim)</code></p>
<p>This has the effect of both reducing the chance of hitting the <code>fma</code><br />
issue and also means we avoid recomputing <code>scale(x)</code> inside and outside<br />
the reduction which may be significant if we can remove an extra division.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 15:43:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124119</guid>
    </item>
    <item>
      <title>[2/N] Scalar Support: Add scalar to the cache for eager-through-torch.compile</title>
      <link>https://github.com/pytorch/pytorch/pull/124070</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124070<br />
* #124177<br />
* #116368<br />
* #121387<br />
* #123545</p>
<p>Add scalar information to the kernel configuration.</p>
<h4>Additional Context</h4>
<p>Currently, the input parameters are orchestrated by input order in the kernel configuration and loaded/mapped to the kernel at runtime. For example, the cache order of the input parameters of <code>torch.add(a, b, alpha=2.0)</code> is <code>a' first, followed by</code>b<code>and then</code>alpha`. The same order is for cache loading.</p>
<p>However, the orchestration mechanism does not support kwargs because the order of kwargs is useless. For example, the <code>out</code> of <code>aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -&gt; Tensor(a!)</code> may be before <code>approximate</code>. We will support it with subsequent PRs. </p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 07:04:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124070</guid>
    </item>
    <item>
      <title>[inductor] add cpp builder code.</title>
      <link>https://github.com/pytorch/pytorch/pull/124045</link>
      <description><![CDATA[<p>Previous full PR https://github.com/pytorch/pytorch/pull/115248 is failed to merge due to fb_code is hard to debug.<br />
I also tried to submit them as two pieces, https://github.com/pytorch/pytorch/pull/118514 https://github.com/pytorch/pytorch/pull/118515. And they have passed PreCI at that time.</p>
<p>Now I tried to split https://github.com/pytorch/pytorch/pull/115248 into smaller piece, and it is the first step of RFC https://github.com/pytorch/pytorch/issues/124245.<br />
Changes:<br />
1. Add cpp builder code, the new cpp_builder support Windows OS.<br />
2. Add CPU ISA checker which is cross OS and exported from backend cpuinfo.<br />
3. Switch compiler ISA checker to new cpp builder.<br />
4. CppCodeCache use the new ISA checker.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 14 Apr 2024 23:55:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124045</guid>
    </item>
    <item>
      <title>Re-land precompile triton templates</title>
      <link>https://github.com/pytorch/pytorch/pull/124030</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124031<br />
* #122825<br />
* #123229<br />
* #122642<br />
* <strong>-&gt;</strong> #124030</p>
<p>Re-land precompile triton templates. This got reverted because we were precompiling templates without checking the cache. I have since added logic and a test to ensure we do not precompile if there is a cache hit.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 14 Apr 2024 18:27:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124030</guid>
    </item>
    <item>
      <title>[inductor][cpp] GEMM template</title>
      <link>https://github.com/pytorch/pytorch/pull/124021</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124021</p>
<p>cc @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 14 Apr 2024 06:21:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124021</guid>
    </item>
    <item>
      <title>Cannot process/load 2.3 produced aot-inductor shared library file with 2.4-dev</title>
      <link>https://github.com/pytorch/pytorch/issues/123745</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>We are compiling with the compatible option, in 2.3.</p>
<p><code>os.environ["TORCHINDUCTOR_ABI_COMPATIBLE"] = "1"</code></p>
<p>However when we load that with a dev snapshot of 2.4, we get this error. To me this is not expected, if I understand the design of aot inductor (compile once, and rely on the ABI compatibility in future versions of pytorch, to make sure they can load 'old' files. Or did something break/changed drastically in 2.4 ? </p>
<p><code>Error in dlopen: foo.so: undefined symbol: _ZN5torch12aot_inductor31tensor_handle_to_tensor_pointerEP16AtenTensorOpaque
Exception raised from DynamicLibrary at ../aten/src/ATen/DynamicLibrary.cpp:36 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x72924e086177 in /path/_deps/pytorch-src/torch/lib/libc10.so)
frame #1: &lt;unknown function&gt; + 0x10d8324 (0x7292376d8324 in /path/_deps/pytorch-src/torch/lib/libtorch_cpu.so)
frame #2: torch::inductor::AOTIModelContainerRunner::AOTIModelContainerRunner(std::string const&amp;, unsigned long, std::string const&amp;, std::string const&amp;) + 0xbd (0x72923b7d39cd in /path/build/_deps/pytorch-src/torch/lib/libtorch_cpu.so)</code></p>
<p>This is what this symbol is:<br />
<code>$ c++filt 
_ZN5torch12aot_inductor31tensor_handle_to_tensor_pointerEP16AtenTensorOpaque
torch::aot_inductor::tensor_handle_to_tensor_pointer(AtenTensorOpaque*)</code></p>
<h3>Versions</h3>
<p>2.3 + 2.4 ... sorry a bit complicated but everything is in the title.</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @desertfire @chenyang78</p>]]></description>
      <pubDate>Wed, 10 Apr 2024 10:32:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123745</guid>
    </item>
    <item>
      <title>Utilize aot compile</title>
      <link>https://github.com/pytorch/pytorch/pull/123545</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124070<br />
* #124177<br />
* #116368<br />
* #121387<br />
* <strong>-&gt;</strong> #123545</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 07 Apr 2024 22:12:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123545</guid>
    </item>
    <item>
      <title>[aot_inductor] Enable test_aot_inductor tests for ROCm CPU</title>
      <link>https://github.com/pytorch/pytorch/pull/123393</link>
      <description><![CDATA[<p>Fixes #ISSUE_NUMBER</p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 04 Apr 2024 14:40:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123393</guid>
    </item>
    <item>
      <title>[inductor] Specialize on unguarded alignment of example inputs</title>
      <link>https://github.com/pytorch/pytorch/pull/123319</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124336<br />
* <strong>-&gt;</strong> #123319</p>
<p>When inductor generates triton code, the triton code can either assume that the inputs given to it are aligned or unaligned. If they are aligned, triton can use more efficient instructions (like vectorized loads or tensor cores). However, if we generate "aligned" code and pass in unaligned inputs, the triton code will error out; to fix this, we clone unaligned inputs that are passed to triton kernels that expect aligned inputs. This can lead to excessive clones if we have inputs that are not expected to be aligned.</p>
<p>In this PR, we use the example input to decide whether the generated triton code should assume alignment or not. If the example input is aligned, then we will generate triton code that assumes alignment; if at runtime we receive an unaligned input, we'll make a clone. Meanwhile, if the example input is not aligned, the generated triton code will not assume inputs are aligned and we won't ever need to clone.</p>
<p>Note that the alignment of the inputs is not guarded on; we found that adding guards on tensor offsets (a) was slow in cases where we do a lot of comparisons on tensor offsets, and (b) led to a lot of recompilations.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 18:06:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123319</guid>
    </item>
    <item>
      <title>Add registration API for torch.compile-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/121387</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we intend to provide a registration API dedicated to eager-through-torch.compile. The major workflow of this API will be as follows.</p>
<ul>
<li>Load cache</li>
<li>Check cache according to the input tensors</li>
<li>Cache Hit: Run the cached kernel directly</li>
<li>Cache Miss: Run the AOTI to produce kernel and run the produced kernel. If AOTI fails to produce the kernel, invoke the python fallback function.</li>
</ul>
<p>Currently, this PR always fallback to python kernel now and cache mechanism will be implemented in another PR - https://github.com/pytorch/pytorch/pull/116368</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124070<br />
* #124177<br />
* #116368<br />
* <strong>-&gt;</strong> #121387<br />
* #123545</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121387</guid>
    </item>
    <item>
      <title>[PT2D] Make the speedup benchmark works with DDP + CompiledAutograd</title>
      <link>https://github.com/pytorch/pytorch/pull/121315</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121315<br />
* #123424<br />
* #124422<br />
* #124421</p>
<p>https://github.com/pytorch/pytorch/pull/120454 is the original PR. It does not cover the optimizer part, which may be the root cause of breaking the dashboard. This PR save a new copy of optimizer as well.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54591562/">D54591562</a></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 09:35:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121315</guid>
    </item>
    <item>
      <title>DISABLED test_item_to_inputs_kernel_nobreak_cuda (__main__.TestInductorDynamicCUDA)</title>
      <link>https://github.com/pytorch/pytorch/issues/119538</link>
      <description><![CDATA[<p>Platforms: rocm, linux</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_item_to_inputs_kernel_nobreak_cuda&amp;suite=TestInductorDynamicCUDA&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/21391792477">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_item_to_inputs_kernel_nobreak_cuda</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_torchinductor_dynamic_shapes.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Fri, 09 Feb 2024 01:39:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/119538</guid>
    </item>
    <item>
      <title>Add a cache mechanism to accelerate torch.compile-for-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/116368</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124070<br />
* #124177<br />
* <strong>-&gt;</strong> #116368<br />
* #121387<br />
* #123545</p>
<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to enable a cache mechanism to accelerate <strong>eager-through-torch.compile</strong>. When <strong>eager-through-torch.compile</strong> is enabled, we will store a persistent config to cache the kernel information for the aten operation. </p>
<p>The persistent config consists of two parts - meta_info and kernel_path.</p>
<ul>
<li>meta_info: The input tensors' shape, stride, device type, data type, and symbolic flag.</li>
<li>kernel_path: The path of the kernel produced by Inductor.</li>
</ul>
<p>When an aten operation is registered, the <code>kernel_holder</code> will load the persistent config and parse it to build the cache map;  the meta_info is key, and the kernel library is the value.</p>
<p>Currently, this PR only supports static shape to guard the kernel.</p>
<p>Take a <code>mul</code> as an example.<br />
```python<br />
class MulKernel:<br />
  def <strong>init</strong>(self) -&gt; None:<br />
    pass</p>
<p>def <strong>call</strong>(self, <em>args: Any, </em><em>kwargs: Any) -&gt; Any:<br />
    with torch._C._SetExcludeDispatchKeyGuard(torch._C.DispatchKey.Python, False):<br />
      opt_fn = torch.compile(torch.ops.aten.mul, dynamic=False, options={<br />
          "aot_inductor.eager_mode": True,<br />
          "aot_inductor.eager_op_name": "mul_Tensor"<br />
        }<br />
      )<br />
      return opt_fn(</em>args, **kwargs)</p>
<p>torch_compile_op_lib_impl = torch.library.Library("aten", "IMPL")</p>
<p>_, overload_names = torch._C._jit_get_operation("aten::mul")<br />
schema = torch._C._get_schema("aten::mul", overload_name)<br />
reg_name = schema.name<br />
if schema.overload_name:<br />
  reg_name = f"{reg_name}.{schema.overload_name}"<br />
  torch_compile_op_lib_impl.impl(<br />
    reg_name,<br />
    MulKernel(),<br />
    "CUDA",<br />
    compile_mode=True)</p>
<p>a = torch.randn(1024, 1024, device=device)<br />
b = torch.randn(1024, 1024, device=device)<br />
warm_up_iter = 1000<br />
iter = 10000<br />
fn = torch.mul</p>
<h1>Warm up</h1>
<p>for _ in range(warm_up_iter):<br />
    fn(a, b)</p>
<h1>Collect performance</h1>
<p>beg = time.time()<br />
for _ in range(iter):<br />
    fn(a, b)<br />
end = time.time()<br />
print(f"E2E run: {end - beg}")<br />
```<br />
It will produce the config as follows.</p>
<p><code>json
[
    {
        "meta_info": [
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            },
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            }
        ],
        "kernel_path": "/tmp/torchinductor_eikan/e4/ce4jw46i5l2e7v3tvr2pyglpjmahnp7x3hxaqotrvxwoeh5t6qzc.so"
    }
]</code></p>
<p>Performance-wise, we collected mul.Tensor through torch.compile w/ 10000 runs(e2e). The data is as follows. And we will collect data when we support dynamic shape.</p>
<ul>
<li>Eager: ~266.11ms</li>
<li>W/O Cache: ~3455.54ms</li>
<li>W/ Cache and Cache Miss: ~3555.3ms</li>
<li>W/ Cache and Cache Hit: ~267.12ms</li>
</ul>
<p>Hardware:</p>
<ul>
<li>CPU: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz</li>
<li>GPU: CUDA A10</li>
</ul>
<p>Software:</p>
<ul>
<li>PyTorch Version: 39df084001c54cca5fe3174176f9b0206ddb7dcf</li>
<li>GPU Driver Version: 525.147.05</li>
<li>CUDA Version: 12.0</li>
</ul>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 03:21:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116368</guid>
    </item>
    <item>
      <title>torch.compile mode="max-autotune" precision appears to be lower</title>
      <link>https://github.com/pytorch/pytorch/issues/96693</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I'm sorry, this is going to be a terrible bug report, but I wasn't able to simplify the problem. </p>
<p>I'm running a training run with a model that is compiled with <code>torch.compile</code>. The blue line below is loss with the <code>default</code> mode, the red line below is loss with <code>max-autotune</code>, the training runs are otherwise identical:</p>
<p><img alt="spiky_comparison" src="https://user-images.githubusercontent.com/22680696/224837823-788ccd48-6fb0-4099-9c87-46926dca134f.png" /></p>
<p>It appears as if precision/stability is noticeably lower on the max-autotune training run. This is running on an RTXA600 GPU. The forward pass is wrapped in <br />
<code>torch.autocast</code>, there are no graph breaks or warning messages. Max-autotune code is also noticeably a bit faster, so it is definitely doing something that the default mode is not.</p>
<p>Feel free to close this issue if this is not enough info. I could also test more specific things or try to make a mini version to get to the bottom of this, but I have no idea what to look for, or which part would be relevant.</p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.1.0.dev20230307<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Pop!_OS 22.04 LTS (x86_64)<br />
GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0<br />
Clang version: Could not collect<br />
CMake version: Could not collect<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-6.2.0-76060200-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU<br />
Nvidia driver version: 525.85.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>Versions of relevant libraries:<br />
[pip3] lion-pytorch==0.0.7<br />
[pip3] numpy==1.23.5<br />
[pip3] torch==2.1.0.dev20230307<br />
[pip3] torchaudio==2.0.0.dev20230307<br />
[pip3] torchvision==0.15.0.dev20230307<br />
[conda] blas                      1.0                         mkl<br />
[conda] lion-pytorch              0.0.7                    pypi_0    pypi<br />
[conda] mkl                       2021.4.0           h06a4308_640<br />
[conda] mkl-service               2.4.0           py310h7f8727e_0<br />
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0<br />
[conda] mkl_random                1.2.2           py310h00e6091_0<br />
[conda] numpy                     1.23.5          py310hd5efca6_0<br />
[conda] numpy-base                1.23.5          py310h8e6c178_0<br />
[conda] pytorch                   2.1.0.dev20230307 py3.10_cuda11.8_cudnn8.7.0_0    pytorch-nightly<br />
[conda] pytorch-cuda              11.8                 h7e8668a_3    pytorch-nightly<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly<br />
[conda] torchaudio                2.0.0.dev20230307     py310_cu118    pytorch-nightly<br />
[conda] torchtriton               2.0.0+b8b470bc59           py310    pytorch-nightly<br />
[conda] torchvision               0.15.0.dev20230307     py310_cu118    pytorch-nightly</p>
<p>```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @wconstab @soumith @ngimel</p>]]></description>
      <pubDate>Mon, 13 Mar 2023 13:46:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/96693</guid>
    </item>
  </channel>
</rss>
