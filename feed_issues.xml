<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[inductor] FX graph cache: Fix bug handling constants</title>
      <link>https://github.com/pytorch/pytorch/pull/121925</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121925</p>
<p>Summary: During key calculation for FX graph caching: Rather than specialize on "small" vs. "large" tensor constants (i.e., inlined vs. not inlined), always hash on the tensor value. Doing so avoids the complication of trying to later attach the constant values as attributes to an already-compiled module. Instead, different constants will cause an FX graph cache miss and we'll just compile.</p>
<p>Test Plan: New unit test</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 11:44:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121925</guid>
    </item>
    <item>
      <title>[inductor] Lower divide by constant as multiplication by reciprocal</title>
      <link>https://github.com/pytorch/pytorch/pull/121924</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121924</p>
<p>Fixes #101039</p>
<p>This lowers division by a constant value to be multipication by reciprocal.<br />
The same optimization is applied in eager mode on CUDA:</p>
<p>https://github.com/pytorch/pytorch/blob/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu#L36-L38</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 11:41:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121924</guid>
    </item>
    <item>
      <title>[WIP][dynamo][compile-time] Preserve rng state once instead of for each op</title>
      <link>https://github.com/pytorch/pytorch/pull/121923</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121923</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 11:39:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121923</guid>
    </item>
    <item>
      <title>[PT2][Inductor] Customize Split Cat</title>
      <link>https://github.com/pytorch/pytorch/pull/121915</link>
      <description><![CDATA[<p>Summary: Currently, we only enabled the group batch fusion customization, we also enable the split cat customization.</p>
<p>Test Plan:<br />
<code>buck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode batch-split --model_type "cmf" --flow_id 524546542</code><br />
P1196013839</p>
<p>Differential Revision: D54861682</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 09:48:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121915</guid>
    </item>
    <item>
      <title>Disable inductor (default) and inductor (dynamic) by default in the perf run launcher</title>
      <link>https://github.com/pytorch/pytorch/pull/121914</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121914</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 09:46:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121914</guid>
    </item>
    <item>
      <title>[inductor] FX graph cache: simplify "current callable" logic</title>
      <link>https://github.com/pytorch/pytorch/pull/121903</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121903</p>
<p>Summary: The handling of the current_callable and compiled_artifact fields in the CompiledFxGraph object is unnecessarily complicated and confusing. We can simplify by storing only the callable. That field is not serializable, so the caching approach is to store a path to the generated artifact and reload from disk on a cache hit. We can just reload inline in the FX cache hit path. This change has the added benefit that it makes it easier to fallback to a "cache miss" if the path somehow doesn't exist.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 07:27:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121903</guid>
    </item>
    <item>
      <title>Cannot `torch.compile` if `weight_norm` is applied to a Module</title>
      <link>https://github.com/pytorch/pytorch/issues/121902</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>MWE:<br />
```python<br />
import torch<br />
from torch.nn.utils.parametrizations import weight_norm</p>
<p>c1 = torch.nn.Conv1d(100, 10, 10)<br />
c1 = weight_norm(c1, dim=0)</p>
<p>c1 = torch.compile(c1, fullgraph=True)<br />
x = torch.zeros(1, 100, 20)<br />
c1(x)<br />
```</p>
<p>Fails with <br />
<code>torch._dynamo.exc.TorchRuntimeError: Failed running call_method forward(*(ParametrizedLinear(
  in_features=20, out_features=10, bias=True
  (parametrizations): ModuleDict(
    (weight): ParametrizationList(
      (0): _WeightNorm()
    )
  )
), FakeTensor(..., size=(1, 100, 20))), **{}):
_weight_norm_interface() missing 1 required positional argument: 'dim'</code></p>
<p>If omit the torch.compile, nothing goes wrong.</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.2.1<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-97-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.4.99<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA GeForce RTX 3090<br />
GPU 1: NVIDIA GeForce RTX 3090</p>
<p>Nvidia driver version: 535.161.07<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             96<br />
On-line CPU(s) list:                0-95<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz<br />
CPU family:                         6<br />
Model:                              85<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 24<br />
Socket(s):                          2<br />
Stepping:                           7<br />
CPU max MHz:                        3900,0000<br />
CPU min MHz:                        1000,0000<br />
BogoMIPS:                           4800.00<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          1,5 MiB (48 instances)<br />
L1i cache:                          1,5 MiB (48 instances)<br />
L2 cache:                           48 MiB (48 instances)<br />
L3 cache:                           71,5 MiB (2 instances)<br />
NUMA node(s):                       2<br />
NUMA node0 CPU(s):                  0-23,48-71<br />
NUMA node1 CPU(s):                  24-47,72-95<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Mitigation; Enhanced IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Mitigation; TSX disabled</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-lightning==2.2.1<br />
[pip3] torch==2.2.1<br />
[pip3] torchaudio==2.2.1<br />
[pip3] torchmetrics==0.11.4<br />
[pip3] triton==2.2.0<br />
[conda] blas                      1.0                         mkl    conda-forge<br />
[conda] libblas                   3.9.0            16_linux64_mkl    conda-forge<br />
[conda] libcblas                  3.9.0            16_linux64_mkl    conda-forge<br />
[conda] liblapack                 3.9.0            16_linux64_mkl    conda-forge<br />
[conda] mkl                       2022.1.0           hc2b9512_224<br />
[conda] numpy                     1.26.4          py310hb13e2d6_0    conda-forge<br />
[conda] pytorch                   2.2.1           py3.10_cuda12.1_cudnn8.9.2_0    pytorch<br />
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch<br />
[conda] pytorch-lightning         2.2.1              pyhd8ed1ab_0    conda-forge<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch<br />
[conda] torchaudio                2.2.1               py310_cu121    pytorch<br />
[conda] torchmetrics              0.11.4             pyhd8ed1ab_0    conda-forge<br />
[conda] torchtriton               2.2.0                     py310    pytorch<br />
```</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 06:33:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121902</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Add Inductor Intel GPU backend.</title>
      <link>https://github.com/pytorch/pytorch/pull/121895</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121895<br />
* #121883</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 00:00:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121895</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Register general runtime device for Intel GPU</title>
      <link>https://github.com/pytorch/pytorch/pull/121883</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121895<br />
* <strong>-&gt;</strong> #121883</p>
<p>interface for Intel GPU.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 18:20:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121883</guid>
    </item>
    <item>
      <title>[compiled autograd] Dynamo segfaults on exit</title>
      <link>https://github.com/pytorch/pytorch/issues/121862</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Likely a double-free from gdb stacktrace. The repro below always fails for me locally. This also affects all compiled autograd tests, but sometimes flaky on the first run.</p>
<details>
  <summary>Segfault logs</summary>

```
> python t.py
/home/xmfan/.conda/envs/benchmarks/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
tensor([[ -4.8995,  -5.9998, -10.6205,  ...,   6.6345,  35.8347,   2.6884],
        [ -4.8995,  -5.9998, -10.6205,  ...,   6.6345,  35.8347,   2.6884],
        [ -4.8995,  -5.9998, -10.6205,  ...,   6.6345,  35.8347,   2.6884],
        ...,
        [ -4.8995,  -5.9998, -10.6205,  ...,   6.6345,  35.8347,   2.6884],
        [ -4.8995,  -5.9998, -10.6205,  ...,   6.6345,  35.8347,   2.6884],
        [ -4.8995,  -5.9998, -10.6205,  ...,   6.6345,  35.8347,   2.6884]],
       device='cuda:0')
resetting dynamo
Segmentation fault (core dumped)
```

</details>

<p>```python</p>
<h1>t.py</h1>
<p>import torch</p>
<p>TRIGGER_SEGFAULT = True</p>
<p>def no_op(<em>args, </em>*kwargs):<br />
    return []</p>
<p>def compiler_fn(gm):<br />
    if TRIGGER_SEGFAULT:<br />
        return torch.compile(gm, backend="eager")<br />
    else:<br />
        return no_op</p>
<p>def main():<br />
    def inner():<br />
        x = torch.randn(1000, 3000)#, device="cuda")<br />
        w = torch.randn(1000, 3000, requires_grad=True)#, device="cuda")</p>
<pre><code>    def model(i):
        return torch.nn.functional.linear(i, w)

    out = model(x)
    loss = out.sum()
    with torch._dynamo.compiled_autograd.enable(compiler_fn):
        loss.backward()
    print(w.grad)

inner()
print("resetting dynamo")
torch._dynamo.reset()
inner()
</code></pre>
<p>if <strong>name</strong> == "<strong>main</strong>":<br />
    main()<br />
```</p>
<h3>Versions</h3>
<p>main: f79055b176b3e80478e65e6524f391d4f0241f0f<br />
nightly: 2.3.0.dev20240313</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 14:58:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121862</guid>
    </item>
    <item>
      <title>[dynamo][compile-time] Make output_graph new_var linear</title>
      <link>https://github.com/pytorch/pytorch/pull/121858</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120726<br />
* <strong>-&gt;</strong> #121858</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/121679</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 14:18:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121858</guid>
    </item>
    <item>
      <title>Log graph break reasons and wasted compile time in CompilationMetrics</title>
      <link>https://github.com/pytorch/pytorch/pull/121827</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121827</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 08:52:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121827</guid>
    </item>
    <item>
      <title>[FSDP2] Disabled compile for  `pre_forward`</title>
      <link>https://github.com/pytorch/pytorch/pull/121816</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120952<br />
* #121680<br />
* <strong>-&gt;</strong> #121816<br />
* #121747</p>
<p>Without this, we can see some tracing errors when tracing <code>pre_forward</code> for FSDP2 + SP.</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 07:18:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121816</guid>
    </item>
    <item>
      <title>[Inductor] Fix for WrapperCodeGen.statically_known_int_or_none</title>
      <link>https://github.com/pytorch/pytorch/pull/121808</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121808</p>
<p>There's obviously a small typo in WrapperCodeGen.statically_known_int_or_none,<br />
where the return value of a call to V.graph._shape_env._maybe_evaluate_static<br />
is being discarded.</p>
<p>This fix changes that to work how it was likely intended to.</p>
<p>Test Plan:<br />
CI</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 05:33:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121808</guid>
    </item>
    <item>
      <title>[dynamo] Compile time optimizations in tx.step()</title>
      <link>https://github.com/pytorch/pytorch/pull/121790</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121810<br />
* <strong>-&gt;</strong> #121790</p>
<p><code>python benchmarks/dynamo/microbenchmarks/dynamo_microbenchmarks.py</code><br />
- Before: <code>symbolic_convert_overhead_stress_test: 10.7s</code><br />
- After: <code>symbolic_convert_overhead_stress_test: 8.6s</code></p>
<p><code>tx.step()</code> is a small part of that benchmark, so likely the speedup in that isolated function is larger than the top line.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 21:22:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121790</guid>
    </item>
    <item>
      <title>[Inductor] Update the cpp_wrapper entry function signature</title>
      <link>https://github.com/pytorch/pytorch/pull/121745</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121745<br />
* #121744<br />
* #121743<br />
* #121523</p>
<p>Summary: Update the entry function to use AtenTensorHandle instead of at::Tensor. This makes the compilation of the generated cpp wrapper code much faster: test_cpu_cpp_wrapper.py from 35 min to 21 min, and test_cuda_cpp_wrapper.py from 21 min to 14 min.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54818715">D54818715</a></p>]]></description>
      <pubDate>Tue, 12 Mar 2024 11:20:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121745</guid>
    </item>
    <item>
      <title>Don't record autograd state ops while torch.compile in pre-dispatch export</title>
      <link>https://github.com/pytorch/pytorch/pull/121736</link>
      <description><![CDATA[<p>Summary: Refer to OSS PR for details</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D54812833</p>
<p>In pre-dispatch export, we have a special proxy torch mode where we intercept torch._C._set_grad_enabled op to correctly capture user's intention on train/eval. However, this is bit problematic when we are tracing torch.cond during export as it calls torch.compile internally. As a result, we end up capturing unwanted autograd context manager  calls that are happening inside dynamo framework code because the top level tracer is still active. We fix it by turning off this proxy torch mode. We can still capture autograd ops inside cond branches because dynamo will translate them into HOP for us, so we don't have to intercept with special proxy mode. </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 09:57:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121736</guid>
    </item>
    <item>
      <title>[compiled autograd] free stack objects before calling compiled graph</title>
      <link>https://github.com/pytorch/pytorch/pull/121707</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121698<br />
* <strong>-&gt;</strong> #121707</p>
<p>Moved compilation code into _compiled_autograd_impl, frees stack allocated objects e.g. AutogradCompilerCall</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 20:45:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121707</guid>
    </item>
    <item>
      <title>[Inductor] [ReImplement] Outer Loop Fusion for CPP Backend</title>
      <link>https://github.com/pytorch/pytorch/pull/121625</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121625</p>
<p><strong>Summary</strong><br />
Re-implement of https://github.com/pytorch/pytorch/pull/121064</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_outer_loop_fusion</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 10 Mar 2024 23:44:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121625</guid>
    </item>
    <item>
      <title>[inductor] disable linear weight prepacking pass on double</title>
      <link>https://github.com/pytorch/pytorch/pull/121478</link>
      <description><![CDATA[<p>Fix #121175 </p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121478</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 07 Mar 2024 23:05:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121478</guid>
    </item>
    <item>
      <title>Torch 2.1 compile + FSDP (mixed precision) + LlamaForCausalLM: `RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'.`</title>
      <link>https://github.com/pytorch/pytorch/issues/111317</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I am getting the following error when training LlamaForCausalLM with torch 2.1 and FSDP (mixed precision) and torch.compile. Same exact code works when torch.compile disabled or when torch 2.0.1 is used. I also tried enabling and disabling amp autocast, it doesn't matter and the same error happens.</p>
<p><code>RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'.
Please ensure that the gradient and the tensor have the same dtype</code></p>
<p>I am using a docker image, error happens in <strong>Environment 2</strong> which is provided in the <strong>Versions</strong> section.</p>
<h3>Error logs</h3>
<p>```<br />
  0%|          | 0/5 [00:00&lt;?, ?it/s]Traceback (most recent call last):<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
Traceback (most recent call last):<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
    main()<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    outputs = model(<strong>batch)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    main()<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    outputs = model(</strong>batch)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
Traceback (most recent call last):<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
Traceback (most recent call last):<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
    Traceback (most recent call last):<br />
return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
Traceback (most recent call last):<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
    main()<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        return forward_call(*args, </strong>kwargs)outputs = model(**batch)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    main()  <br />
main()<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    outputs = model(</strong>batch)<br />
Traceback (most recent call last):<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        outputs = model(<strong>batch)<br />
return fn(*args, </strong>kwargs)  <br />
main()  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl</p>
<p>File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    outputs = model(</em><em>batch)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
        return self._call_impl(<em>args, </em><em>kwargs)return self._call_impl(</em>args, **kwargs)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    main()<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
    output = self._fsdp_wrapped_module(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return self._call_impl(*args, </strong>kwargs)<br />
      File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
outputs = model(<strong>batch)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
        return forward_call(<em>args, </em><em>kwargs)<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
return forward_call(*args, </strong>kwargs)  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        return fn(</em>args, <strong>kwargs)  <br />
return fn(*args, </strong>kwargs)return self._call_impl(<em>args, </em>*kwargs)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
        return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
return fn(</em>args, <strong>kwargs)  <br />
output = self._fsdp_wrapped_module(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return self._call_impl(*args, </strong>kwargs)  <br />
return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
        return forward_call(</em>args, <strong>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        return self._call_impl(<em>args, </em><em>kwargs)<br />
      File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
return self._call_impl(</em>args, <strong>kwargs)return self._call_impl(*args, </strong>kwargs)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
    return model_forward(</em>args, <strong>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
        return self._call_impl(</em>args, <strong>kwargs)<br />
return fn(*args, </strong>kwargs)  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return convert_to_fp32(self.model_forward(<em>args, </em><em>kwargs))<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    output = self._fsdp_wrapped_module(</em>args, <strong>kwargs)<br />
      File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
return func(*args, </strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
    return model_forward(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
    output = self._fsdp_wrapped_module(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    output = self._fsdp_wrapped_module(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    outputs = self.model(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return convert_to_fp32(self.model_forward(*args, </strong>kwargs))<br />
    return self._call_impl(<em>args, </em>*kwargs)  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return func(<em>args, </em><em>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
        return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
output = self._fsdp_wrapped_module(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    outputs = self.model(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        return forward_call(</em>args, <strong>kwargs)output = self._fsdp_wrapped_module(*args, </strong>kwargs)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return model_forward(*args, </strong>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
        return self._call_impl(</em>args, <strong>kwargs)    return model_forward(*args, </strong>kwargs)<br />
return model_forward(<em>args, </em>*kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong></p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
    return convert_to_fp32(self.model_forward(<em>args, </em><em>kwargs))<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast<br />
    return func(</em>args, <strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    layer_outputs = decoder_layer(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return convert_to_fp32(self.model_forward(</em>args, <strong>kwargs))<br />
    return convert_to_fp32(self.model_forward(*args, </strong>kwargs))  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast<br />
        return func(<em>args, </em><em>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
return func(</em>args, <strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
    return model_forward(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
    outputs = self.model(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
        outputs = self.model(<br />
outputs = self.model(  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        return convert_to_fp32(self.model_forward(<em>args, </em>*kwargs))layer_outputs = decoder_layer(</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast<br />
        return model_forward(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
return func(</em>args, <strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
    return convert_to_fp32(self.model_forward(</em>args, <strong>kwargs))<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast<br />
        return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return func(*args, </strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
    outputs = self.model(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    args, kwargs = _pre_forward(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
    unshard_fn(state, handle)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
        return forward_call(</em>args, <strong>kwargs)<br />
return forward_call(*args, </strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
      File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
outputs = self.model(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        layer_outputs = decoder_layer(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
return self._call_impl(</em>args, **kwargs)<br />
      File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
_unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard<br />
    args, kwargs = _pre_forward(<br />
        layer_outputs = decoder_layer(layer_outputs = decoder_layer(  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    ran_pre_unshard = handle.pre_unshard()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    unshard_fn(state, handle)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
    _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
    ret = self._writeback_orig_params()  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard</p>
<pre><code>    return self._call_impl(*args, **kwargs)return self._call_impl(*args, **kwargs)
</code></pre>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
        return func(<em>args, </em>*kwargs)ran_pre_unshard = handle.pre_unshard()</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 2202, in _writeback_orig_params<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
    layer_outputs = decoder_layer(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
        return forward_call(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
    layer_outputs = decoder_layer(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    ret = self._writeback_orig_params()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context<br />
    args, kwargs = _pre_forward(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward<br />
        return self._call_impl(</em>args, <strong>kwargs)return func(*args, </strong>kwargs)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 2202, in _writeback_orig_params<br />
    args, kwargs = _pre_forward(<br />
    args, kwargs = _pre_forward(  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward<br />
    unshard_fn(state, handle)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
    flat_param.grad = flat_param_grad<br />
RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'. Please ensure that the gradient and the tensor have the same dtype<br />
Traceback (most recent call last):<br />
        unshard_fn(state, handle)unshard_fn(state, handle)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
    _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard<br />
    return self._call_impl(<em>args, </em>*kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
        _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
    ran_pre_unshard = handle.pre_unshard()  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard</p>
<p>_unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
    ran_pre_unshard = handle.pre_unshard()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    ran_pre_unshard = handle.pre_unshard()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    flat_param.grad = flat_param_grad<br />
RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'. Please ensure that the gradient and the tensor have the same dtype<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
    args, kwargs = _pre_forward(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward<br />
    ret = self._writeback_orig_params()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context<br />
    return func(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 2202, in _writeback_orig_params<br />
    unshard_fn(state, handle)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
        ret = self._writeback_orig_params()ret = self._writeback_orig_params()</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context<br />
    args, kwargs = _pre_forward(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward<br />
        return func(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 2202, in _writeback_orig_params<br />
return func(</em>args, **kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 2202, in _writeback_orig_params<br />
    _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard<br />
    unshard_fn(state, handle)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
    ran_pre_unshard = handle.pre_unshard()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard<br />
    ran_pre_unshard = handle.pre_unshard()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    flat_param.grad = flat_param_grad<br />
RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'. Please ensure that the gradient and the tensor have the same dtype<br />
        flat_param.grad = flat_param_grad<br />
flat_param.grad = flat_param_grad<br />
    ret = self._writeback_orig_params()<br />
RuntimeError  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context<br />
RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'. Please ensure that the gradient and the tensor have the same dtype: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'. Please ensure that the gradient and the tensor have the same dtype<br />
============================================================<br />
pretrain_fsdp_torch2.1_minimal.py FAILED</p>
<hr />
<p>Failures:<br />
[1]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 2 (local_rank: 2)<br />
  exitcode  : 1 (pid: 348906)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
[2]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 3 (local_rank: 3)<br />
  exitcode  : 1 (pid: 348907)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
[3]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 4 (local_rank: 4)<br />
  exitcode  : 1 (pid: 348908)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
[4]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 5 (local_rank: 5)<br />
  exitcode  : 1 (pid: 348909)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
[5]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 6 (local_rank: 6)<br />
  exitcode  : 1 (pid: 348910)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
[6]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 7 (local_rank: 7)<br />
  exitcode  : 1 (pid: 348911)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html</p>
<hr />
<p>Root Cause (first observed failure):<br />
[0]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 1 (local_rank: 1)<br />
  exitcode  : 1 (pid: 348905)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
============================================================<br />
```</p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p><strong>Environment 1</strong></p>
<p>```<br />
PyTorch version: 2.0.1+cu117<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.7<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.27.4<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.2.128<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A100-SXM4-80GB<br />
GPU 1: NVIDIA A100-SXM4-80GB<br />
GPU 2: NVIDIA A100-SXM4-80GB<br />
GPU 3: NVIDIA A100-SXM4-80GB<br />
GPU 4: NVIDIA A100-SXM4-80GB<br />
GPU 5: NVIDIA A100-SXM4-80GB<br />
GPU 6: NVIDIA A100-SXM4-80GB<br />
GPU 7: NVIDIA A100-SXM4-80GB</p>
<p>Nvidia driver version: 535.104.12<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      48 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             224<br />
On-line CPU(s) list:                0-223<br />
Vendor ID:                          AuthenticAMD<br />
Model name:                         AMD EPYC 7713 64-Core Processor<br />
CPU family:                         25<br />
Model:                              1<br />
Thread(s) per core:                 1<br />
Core(s) per socket:                 224<br />
Socket(s):                          1<br />
Stepping:                           1<br />
BogoMIPS:                           3999.99<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr wbnoinvd arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm arch_capabilities<br />
Virtualization:                     AMD-V<br />
Hypervisor vendor:                  KVM<br />
Virtualization type:                full<br />
L1d cache:                          14 MiB (224 instances)<br />
L1i cache:                          14 MiB (224 instances)<br />
L2 cache:                           112 MiB (224 instances)<br />
L3 cache:                           3.5 GiB (224 instances)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-223<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Mitigation; safe RET<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.23.1<br />
[pip3] onnx==1.14.0<br />
[pip3] pytorch-quantization==2.1.2<br />
[pip3] torch==2.0.1<br />
[pip3] torch-tensorrt==2.0.0.dev0<br />
[pip3] torchdata==0.7.0a0<br />
[pip3] torchtext==0.16.0a0<br />
[pip3] torchvision==0.16.0<br />
[pip3] triton==2.0.0<br />
[conda] Could not collect<br />
```</p>
<p><strong>Environment 2</strong></p>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.1.0+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.27.4<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.2.128<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A100-SXM4-80GB<br />
GPU 1: NVIDIA A100-SXM4-80GB<br />
GPU 2: NVIDIA A100-SXM4-80GB<br />
GPU 3: NVIDIA A100-SXM4-80GB<br />
GPU 4: NVIDIA A100-SXM4-80GB<br />
GPU 5: NVIDIA A100-SXM4-80GB<br />
GPU 6: NVIDIA A100-SXM4-80GB<br />
GPU 7: NVIDIA A100-SXM4-80GB</p>
<p>Nvidia driver version: 535.104.12<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      48 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             224<br />
On-line CPU(s) list:                0-223<br />
Vendor ID:                          AuthenticAMD<br />
Model name:                         AMD EPYC 7713 64-Core Processor<br />
CPU family:                         25<br />
Model:                              1<br />
Thread(s) per core:                 1<br />
Core(s) per socket:                 224<br />
Socket(s):                          1<br />
Stepping:                           1<br />
BogoMIPS:                           3999.99<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr wbnoinvd arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm arch_capabilities<br />
Virtualization:                     AMD-V<br />
Hypervisor vendor:                  KVM<br />
Virtualization type:                full<br />
L1d cache:                          14 MiB (224 instances)<br />
L1i cache:                          14 MiB (224 instances)<br />
L2 cache:                           112 MiB (224 instances)<br />
L3 cache:                           3.5 GiB (224 instances)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-223<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Mitigation; safe RET<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.23.1<br />
[pip3] onnx==1.14.0<br />
[pip3] pytorch-quantization==2.1.2<br />
[pip3] torch==2.1.0<br />
[pip3] torch-tensorrt==2.0.0.dev0<br />
[pip3] torchdata==0.7.0a0<br />
[pip3] torchtext==0.16.0a0<br />
[pip3] torchvision==0.16.0<br />
[pip3] triton==2.1.0+e621604<br />
[conda] Could not collect<br />
```</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @wanchaol @fduwjj @wz337 @kiukchung @d4l3k @lucasllc @tianyu-l @gchanan @kadeng</p>]]></description>
      <pubDate>Sat, 14 Oct 2023 18:20:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/111317</guid>
    </item>
    <item>
      <title>Compile dynamic does not support GroupNorm in module</title>
      <link>https://github.com/pytorch/pytorch/issues/97623</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>If my module simply wraps group norm, it does not compile with dynamic=True.</p>
<p>```python<br />
import torch<br />
from torch import nn</p>
<p>class MyModule(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.norm = nn.GroupNorm(32, 512)</p>
<pre><code>def forward(self, x):
    return self.norm(x)
</code></pre>
<p>model = MyModule().cuda().eval()<br />
model = torch.compile(model, dynamic=True)</p>
<p>x = torch.randn([1, 512, 32, 32], device="cuda")<br />
y = model(x)<br />
```</p>
<p>Error:<br />
```<br />
Traceback (most recent call last):<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/<em>dynamo/output_graph.py", line 670, in call_user_compiler<br />
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py", line 1055, in debug_wrapper<br />
    compiled_gm = compiler_fn(gm, example_inputs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/<strong>init</strong>.py", line 1390, in <strong>call</strong><br />
    return compile_fx(model</em>, inputs_, config_patches=self.config)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 401, in compile_fx<br />
    return compile_fx(<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 455, in compile_fx<br />
    return aot_autograd(<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/backends/common.py", line 48, in compiler_fn<br />
    cg = aot_module_simplified(gm, example_inputs, <strong>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 2805, in aot_module_simplified<br />
    compiled_fn = create_aot_dispatcher_function(<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 2498, in create_aot_dispatcher_function<br />
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1713, in aot_wrapper_dedupe<br />
    return compiler_fn(flat_fn, leaf_flat_args, aot_config)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 2087, in aot_dispatch_autograd<br />
    fx_g = make_fx(joint_forward_backward, aot_config.decompositions)(<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 714, in wrapped<br />
    t = dispatch_trace(wrap_key(func, args, fx_tracer), tracer=fx_tracer, concrete_args=tuple(phs))<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 443, in dispatch_trace<br />
    graph = tracer.trace(root, concrete_args)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py", line 778, in trace<br />
    (self.create_arg(fn(<em>args)),),<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py", line 652, in flatten_fn<br />
    tree_out = root_fn(</em>tree_args)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 459, in wrapped<br />
    out = f(<em>tensors)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1156, in traced_joint<br />
    return functionalized_f_helper(primals, tangents)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1108, in functionalized_f_helper<br />
    f_outs = flat_fn_no_input_mutations(fn, f_primals, f_tangents, meta, keep_input_mutations)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1076, in flat_fn_no_input_mutations<br />
    outs = flat_fn_with_synthetic_bases_expanded(fn, primals, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1048, in flat_fn_with_synthetic_bases_expanded<br />
    outs = forward_or_joint(fn, primals_before_cloning, primals, maybe_tangents, meta, keep_input_mutations)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1017, in forward_or_joint<br />
    backward_out = torch.autograd.grad(<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/autograd/<strong>init</strong>.py", line 269, in grad<br />
    return handle_torch_function(<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/overrides.py", line 1534, in handle_torch_function<br />
    result = mode.<strong>torch_function</strong>(public_api, types, args, kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_inductor/overrides.py", line 38, in <strong>torch_function</strong><br />
    return func(</em>args, </strong>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/autograd/<strong>init</strong>.py", line 303, in grad<br />
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/utils/_stats.py", line 20, in wrapper<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 487, in <strong>torch_dispatch</strong><br />
    return self.inner_torch_dispatch(func, types, args, kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 512, in inner_torch_dispatch<br />
    out = proxy_call(self, func, args, kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py", line 248, in proxy_call<br />
    r = CURRENT_DECOMPOSITION_TABLE<a href="*args, **kwargs">func</a><br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_decomp/decompositions.py", line 70, in inner<br />
    r = f(</em>tree_map(increase_prec, args), **tree_map(increase_prec, kwargs))<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_decomp/decompositions.py", line 1171, in native_group_norm_backward<br />
    cpg, _rem = divmod(C, group)<br />
TypeError: unsupported operand type(s) for divmod(): 'SymInt' and 'int'</p>
<p>The above exception was the direct cause of the following exception:</p>
<p>Traceback (most recent call last):<br />
  File "/opt/tiger/code/test.py", line 16, in <module><br />
    y = model(x)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 82, in forward<br />
    return self.dynamo_ctx(self._orig_mod.forward)(</em>args, <strong>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 337, in catch_errors<br />
    return callback(frame, cache_size, hooks)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 404, in _convert_frame<br />
    result = inner_convert(frame, cache_size, hooks)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 104, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 262, in _convert_frame_assert<br />
    return _compile(<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 324, in _compile<br />
    out_code = transform_code_object(code, transform)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py", line 445, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 311, in transform<br />
    tracer.run()<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 1726, in run<br />
    super().run()<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 576, in run<br />
    and self.step()<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 540, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 1792, in RETURN_VALUE<br />
    self.output.compile_subgraph(<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 517, in compile_subgraph<br />
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 588, in compile_and_call_fx_graph<br />
    compiled_fn = self.call_user_compiler(gm)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/home/tiger/.local/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 675, in call_user_compiler<br />
    raise BackendCompilerFailed(self.compiler_fn, e) from e<br />
torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised TypeError: unsupported operand type(s) for divmod(): 'SymInt' and 'int'</p>
<p>Set torch._dynamo.config.verbose=True for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>
<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.0.0+cu117<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.7<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Debian GNU/Linux 11 (bullseye) (x86_64)<br />
GCC version: (Debian 10.2.1-6) 10.2.1 20210110<br />
Clang version: Could not collect<br />
CMake version: version 3.25.0<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.9.2 (default, Feb 28 2021, 17:03:44)  [GCC 10.2.1 20210110] (64-bit runtime)<br />
Python platform: Linux-5.4.143.bsk.7-amd64-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.7.99<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: A100-SXM-80GB<br />
Nvidia driver version: 450.191.01<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.5.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   46 bits physical, 57 bits virtual<br />
CPU(s):                          128<br />
On-line CPU(s) list:             0-127<br />
Thread(s) per core:              2<br />
Core(s) per socket:              32<br />
Socket(s):                       2<br />
NUMA node(s):                    2<br />
Vendor ID:                       GenuineIntel<br />
CPU family:                      6<br />
Model:                           106<br />
Model name:                      Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz<br />
Stepping:                        6<br />
CPU MHz:                         3000.000<br />
CPU max MHz:                     3500.0000<br />
CPU min MHz:                     800.0000<br />
BogoMIPS:                        4600.00<br />
Virtualization:                  VT-x<br />
L1d cache:                       3 MiB<br />
L1i cache:                       2 MiB<br />
L2 cache:                        80 MiB<br />
L3 cache:                        108 MiB<br />
NUMA node0 CPU(s):               0-31,64-95<br />
NUMA node1 CPU(s):               32-63,96-127<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] byted-torch==2.0.0.post0<br />
[pip3] byted-torch-monitor==0.0.1<br />
[pip3] numpy==1.24.2<br />
[pip3] pytorch-triton==2.0.0+0d7e753227<br />
[pip3] torch==2.0.0<br />
[pip3] torchaudio==2.0.1<br />
[pip3] torchvision==0.15.1<br />
[pip3] triton==2.0.0<br />
[conda] Could not collect<br />
```</p>
<p>cc @ezyang @soumith @msaroufim @wconstab @ngimel @bdhirsh</p>]]></description>
      <pubDate>Sun, 26 Mar 2023 00:17:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/97623</guid>
    </item>
  </channel>
</rss>
