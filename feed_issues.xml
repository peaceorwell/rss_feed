<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>DISABLED test_comprehensive_fft_fft_cuda_float64 (__main__.TestInductorOpInfoCUDA)</title>
      <link>https://github.com/pytorch/pytorch/issues/122715</link>
      <description><![CDATA[<p>Platforms: inductor</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_comprehensive_fft_fft_cuda_float64&amp;suite=TestInductorOpInfoCUDA&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23110460161">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_comprehensive_fft_fft_cuda_float64</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_torchinductor_opinfo.py</code></p>
<p>cc @clee2000 @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 10:39:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122715</guid>
    </item>
    <item>
      <title>[Inductor]Fix a couple of broken unit tests</title>
      <link>https://github.com/pytorch/pytorch/pull/122714</link>
      <description><![CDATA[<p>Summary: Titled</p>
<p>Test Plan:<br />
<code>buck2 test mode/dev-nosan //caffe2/test/inductor:group_batch_fusion</code><br />
Buck UI: https://www.internalfb.com/buck2/ad05a43c-cb4a-443e-8904-b4d53e4f4b1e<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/13510798909218388<br />
Network: Up: 107KiB  Down: 28KiB  (reSessionID-d7146e4f-773a-46ea-9852-f10f59302479)<br />
Jobs completed: 24. Time elapsed: 1:49.3s.<br />
Cache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)<br />
Tests finished: Pass 9. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<p><code>buck2 test 'fbcode//mode/opt' fbcode//caffe2/test/inductor/fb:split_cat_fx_passes_fb</code></p>
<p>Buck UI: https://www.internalfb.com/buck2/82dbf3b0-c747-4c07-98b8-53b69afa3157<br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/1125900267699118<br />
Network: Up: 1.4GiB  Down: 2.3GiB  (reSessionID-0bd22c6d-5dfe-4b4a-bc24-705eadac884b)<br />
Jobs completed: 252570. Time elapsed: 7:25.2s.<br />
Cache hits: 95%. Commands: 123778 (cached: 117999, remote: 2779, local: 3000)<br />
Tests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<p>Differential Revision: D55378009</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 10:20:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122714</guid>
    </item>
    <item>
      <title>`torch.compile` is not usable on MacOS out of box</title>
      <link>https://github.com/pytorch/pytorch/issues/122705</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Attempts to run any <code>torch.compile</code>d code fails out-of-box with 2.3.0 wheels, as it attempts to link against wrong copy of <code>libomp.dylib</code>:<br />
<code>OMP: Error #15: Initializing libomp.dylib, but found libomp.dylib already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/</code></p>
<p>This happens, because <code>delocate</code> embeds libomp.dylib into <code>torch/.libs</code> folder, but another instance is shipped in <code>torch/libs</code></p>
<h3>Versions</h3>
<p>2.3.0</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @seemethere @osalpekar @atalman @albanD @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 07:38:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122705</guid>
    </item>
    <item>
      <title>[AOTInductor] Add tensor_constantX to pass constant buffer update's check (#122562)</title>
      <link>https://github.com/pytorch/pytorch/pull/122690</link>
      <description><![CDATA[<p>Summary:</p>
<p>During tracing, some constants (tensor_constant{idx}) are being generated internally.<br />
Those constants are neither parameters or buffers, and users have zero control on them.</p>
<p>To accomodate this, we should allow users not passing in those constants generated internally but still be able the constants in the model.</p>
<p>Test Plan:<br />
Included in commit.<br />
<code>build/bin/test_aot_inductor</code></p>
<p>Reviewed By: zoranzhao</p>
<p>Differential Revision: D55354548</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 21:59:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122690</guid>
    </item>
    <item>
      <title>Stopped TORCH_COMPILE_DEBUG from printing out a bunch of logs</title>
      <link>https://github.com/pytorch/pytorch/pull/122688</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122689<br />
* #122581<br />
* #121692<br />
* <strong>-&gt;</strong> #122688<br />
* #122686</p>
<p>@ezyang suggests using TORCH_TRACE for dumping out all intermediate logs.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 21:19:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122688</guid>
    </item>
    <item>
      <title>[inductor] Add FileLock around V.debug.copy</title>
      <link>https://github.com/pytorch/pytorch/pull/122665</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122665</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 17:24:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122665</guid>
    </item>
    <item>
      <title>[inductor] Fix fresh_inductor_cache()</title>
      <link>https://github.com/pytorch/pytorch/pull/122661</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122661</p>
<p>Summary: Modify fresh_inductor_cache() to clear cached state before mocking the toplevel cache_dir directory. Any lru_caches (or otherwise) can use the @clear_on_fresh_inductor_cache decorator to register the cache for clearing. Also change the base inductor TestCase class to use fresh_inductor_cache(). Previously that TestCase was only mocking the subdirectory within the toplevel cache dir designated for the FX graph cache artifacts.</p>
<p>Test Plan:<br />
- New unit test<br />
- All existing inductor tests will exercise fresh_inductor_cache()</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 16:12:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122661</guid>
    </item>
    <item>
      <title>[Inductor] Enable _sfdp_pattern_18 pattern for CUDA</title>
      <link>https://github.com/pytorch/pytorch/pull/122627</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122627</p>
<p>Summary: https://github.com/pytorch/pytorch/pull/121866 added a SDPA pattern matching, but it was hitting some issue on AOTI CI, so CUDA was diabled in that PR.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 10:00:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122627</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Enable lowering of qlinear-binary(-unary) fusion for X86Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/122593</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122667<br />
* <strong>-&gt;</strong> #122593<br />
* #122387<br />
* #122288</p>
<p><strong>Test plan</strong><br />
python test/inductor/test_mkldnn_pattern_matcher.py -k test_qlinear_add</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Mar 2024 23:49:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122593</guid>
    </item>
    <item>
      <title>AOT Compiled SAM Model produces different result than Eager Model</title>
      <link>https://github.com/pytorch/pytorch/issues/122553</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>An AOT-compiled SAM Model produces a different result than the Eager Version. </p>
<p>The change needed to make both models produce the same result is <a href="https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/sam.py#L167">avoiding</a> standard-normalizing the image input:<br />
<code>x = (x - self.pixel_mean) / self.pixel_std</code><br />
I could not reproduce this error in a stand-alone example containing a module whose forward pass only does such normalization. </p>
<p>I can provide a reproducible repo that permits AOT compiling SAM, but to understand AOT Compiled models better, I wonder whether I can also infer which precise kernels were called by the compiled model and the eager model by introspection? Or would there be another way to gain more information on why the models reproduce different results?  </p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.2.0+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA RTX A5000<br />
Nvidia driver version: 535.146.02<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             40<br />
On-line CPU(s) list:                0-39<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz<br />
CPU family:                         6<br />
Model:                              85<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 10<br />
Socket(s):                          2<br />
Stepping:                           4<br />
CPU max MHz:                        3000.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           4400.00<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          640 KiB (20 instances)<br />
L1i cache:                          640 KiB (20 instances)<br />
L2 cache:                           20 MiB (20 instances)<br />
L3 cache:                           27.5 MiB (2 instances)<br />
NUMA node(s):                       2<br />
NUMA node0 CPU(s):                  0-9,20-29<br />
NUMA node1 CPU(s):                  10-19,30-39<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable<br />
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Meltdown:             Mitigation; PTI<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Mitigation; IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT vulnerable</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==7.0.0<br />
[pip3] mypy==1.9.0<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.24.1<br />
[pip3] torch==2.2.0<br />
[pip3] torchaudio==2.1.0+cu118<br />
[pip3] torchdata==0.7.1<br />
[pip3] torchtext==0.17.0<br />
[pip3] torchvision==0.17.0<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @desertfire @chenyang78</p>]]></description>
      <pubDate>Sat, 23 Mar 2024 03:19:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122553</guid>
    </item>
    <item>
      <title>[WIP][compiled autograd][aot autograd] Boxed inputs at runtime</title>
      <link>https://github.com/pytorch/pytorch/pull/122535</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122535<br />
* #122353<br />
* #122691</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 22 Mar 2024 16:04:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122535</guid>
    </item>
    <item>
      <title>Higher peak memory with torch.compile</title>
      <link>https://github.com/pytorch/pytorch/issues/122512</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>If I fuse the backward with compiled forward+loss, there's a higher peak memory than if I separate the backward from compiled forward+loss. It looks like the logits aren't being cleared.</p>
<p>Fused forward+loss+backward:<br />
```<br />
@torch.compile<br />
def fused_forward_and_loss_and_backward(input_ids, labels):<br />
    logits = model.forward(input_ids)<br />
    loss = F.cross_entropy(logits.view(-1, logits.shape[-1]).float(), labels.view(-1))<br />
    # del logits # doesn't change peak memory<br />
    loss.backward()<br />
    return loss<br />
...</p>
<h1>usage:</h1>
<p>loss = fused_forward_and_loss_and_backward(batch_input_ids, gas_labels)<br />
print("peak memory usage", torch.cuda.max_memory_allocated())<br />
<code>Results:</code><br />
peak memory usage 22139266048 (first step)<br />
peak memory usage 23943063552 (second step)<br />
peak memory usage 23943063552 (third step)<br />
```</p>
<p>Fused forward+loss, separated backward:</p>
<p>```<br />
@torch.compile<br />
def fused_forward_and_loss(input_ids, labels):<br />
    logits = model.forward(input_ids)<br />
    loss = F.cross_entropy(logits.view(-1, logits.shape[-1]).float(), labels.view(-1))<br />
    # loss.backward() # no backward here!<br />
    return loss<br />
...</p>
<h1>usage:</h1>
<p>loss = fused_forward_and_loss(batch_input_ids, gas_labels)<br />
loss.backward()<br />
print("peak memory usage", torch.cuda.max_memory_allocated())<br />
```</p>
<p>Results:<br />
<code>peak memory usage 19991782400
peak memory usage 21795579392
peak memory usage 21795579392</code></p>
<p>Memory traces:<br />
https://drive.google.com/file/d/18UywAfWmBDNJMzbCy44KVUy6qCug3cxX/view?usp=sharing<br />
https://drive.google.com/file/d/1A0Cu9fAbJS1dbzBJvRmm-0U1ygsmtT9W/view?usp=sharing</p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.2.0<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.2<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.4 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.28.3<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.19.17-coreweave-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.2.140<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA A40<br />
Nvidia driver version: 535.161.07<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Address sizes:                   48 bits physical, 48 bits virtual<br />
Byte Order:                      Little Endian<br />
CPU(s):                          96<br />
On-line CPU(s) list:             0-95<br />
Vendor ID:                       AuthenticAMD<br />
Model name:                      AMD EPYC 7413 24-Core Processor<br />
CPU family:                      25<br />
Model:                           1<br />
Thread(s) per core:              2<br />
Core(s) per socket:              24<br />
Socket(s):                       2<br />
Stepping:                        1<br />
Frequency boost:                 enabled<br />
CPU max MHz:                     3630.8101<br />
CPU min MHz:                     1500.0000<br />
BogoMIPS:                        5299.98<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm<br />
Virtualization:                  AMD-V<br />
L1d cache:                       1.5 MiB (48 instances)<br />
L1i cache:                       1.5 MiB (48 instances)<br />
L2 cache:                        24 MiB (48 instances)<br />
L3 cache:                        256 MiB (8 instances)<br />
NUMA node(s):                    2<br />
NUMA node0 CPU(s):               0-23,48-71<br />
NUMA node1 CPU(s):               24-47,72-95<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Not affected<br />
Vulnerability Retbleed:          Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] torch==2.2.0<br />
[pip3] torchaudio==2.2.0<br />
[pip3] torchvision==0.17.0<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Fri, 22 Mar 2024 10:41:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122512</guid>
    </item>
    <item>
      <title>compile: mutations under no_grad might not be included in the graph when they are on aliases</title>
      <link>https://github.com/pytorch/pytorch/issues/122436</link>
      <description><![CDATA[<p>Simple repro (came from tracing ppFSDP work):<br />
<code>def f(a):
            a_alias = a.view(-1)
            with torch.no_grad():
                a_alias.mul_(2)
            return a + 1
        inp = torch.ones(4, requires_grad=True)
        out = torch.compile(f)(inp)</code></p>
<p>Running this with <code>TORCH_LOGS="aot", you will se that there is not a</code>copy_()` in the graph - this means that it is hidden outside of the graph, and run in an opaque runtime epilogue.</p>
<p>This mutations should be safe to include in the graph - since it is under no_grad, no autograd metadata needs to be updated (except for the version counter of a, which AOTAutograd handles today directly).</p>
<p>cc @ezyang @msaroufim @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 12:58:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122436</guid>
    </item>
    <item>
      <title>GPT2ForSequenceClassification accuracy test fails for CUDA AOT Inductor export when its SDPA pattern is mapped to the SDPA op</title>
      <link>https://github.com/pytorch/pytorch/issues/122429</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>In #121866, GPT2 SDPA pattern is being mapped to <code>torch.nn.functional.scaled_dot_product_attention</code>.<br />
With CUDA, <code>GPT2ForSequenceClassification</code> <a href="https://github.com/pytorch/pytorch/actions/runs/8319111885/job/22762567770">fails on CI for Inductor AOT export path</a>, but not in other trunk CI checks.</p>
<p>Disabling this pattern's replacement for CUDA, until this issue would be resolved.<br />
For attaining better performance, this issue should be fixed.</p>
<p>Thanks!</p>
<h3>Versions</h3>
<p>PyTorch main branch</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @desertfire @chenyang78 @drisspg @eellison </p>]]></description>
      <pubDate>Thu, 21 Mar 2024 11:27:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122429</guid>
    </item>
    <item>
      <title>`torch.compile` should result in an optimized module where `module.training` is the same as in the unoptimized module</title>
      <link>https://github.com/pytorch/pytorch/issues/122414</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>Hi, basically what the title says.<br />
The current behavior of <code>torch.compile</code> is imo quite unexpected and can lead users to the false belief that a model is in eval mode.</p>
<h3>Alternatives</h3>
<p>Alternatively, it would be a good idea to add to the documentation of <code>torch.compile</code> that the resulting optimized module always is in train mode.</p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 07:45:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122414</guid>
    </item>
    <item>
      <title>[Quant][PT2E] Enable linear-binary(-unary) post-op recipe for X86Inductor quantizer</title>
      <link>https://github.com/pytorch/pytorch/pull/122387</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122667<br />
* #122593<br />
* <strong>-&gt;</strong> #122387<br />
* #122288</p>
<p>As the title<br />
<strong>Test plan</strong><br />
python test/test_quantization.py -k test_linear_binary</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 22:29:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122387</guid>
    </item>
    <item>
      <title>[torch.compile] A potential OOB access in CUDA for attention model</title>
      <link>https://github.com/pytorch/pytorch/issues/122381</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>For the following model, the result optimized by <code>torch.compile</code> on CUDA is totally wrong, which may out-of-bound access some data.</p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(420)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self, d_model, dropout_p=0.1, inv_scale_factor=1e-08):
    super().__init__()
    self.d_model = d_model
    self.dropout_p = dropout_p
    self.inv_scale_factor = inv_scale_factor
    self.dropout = torch.nn.Dropout(dropout_p)

def forward(self, query, key, value):
    qk = torch.matmul(query, key.transpose(-2, -1))
    scaled_qk = qk.div(self.inv_scale_factor)
    softmax_qk = scaled_qk.softmax(dim=-1)
    dropout_qk = self.dropout(softmax_qk)
    return dropout_qk
</code></pre>
<p>device = "cuda"<br />
d_model = 1<br />
func = Model(10).to(device)</p>
<p>query = torch.randn(1, 5, 10).to(device)</p>
<p>key = torch.randn(1, 5, 10).to(device)</p>
<p>value = torch.randn(1, 5, 10).to(device)</p>
<p>with torch.no_grad():<br />
    naive_result = func(query.clone(), key.clone(), value.clone())</p>
<pre><code>func1 = torch.compile(func)
jit_result = func1(query.clone(), key.clone(), value.clone())

print(naive_result)
print(jit_result)
print(torch._dynamo.utils.counters["inductor"])
</code></pre>
<p>"""<br />
tensor([[[0.0000, 0.0000, 0.0000, 1.1111, 0.0000],<br />
         [0.0000, 0.0000, 1.1111, 0.0000, 0.0000],<br />
         [0.0000, 0.0000, 0.0000, 0.0000, 1.1111],<br />
         [0.0000, 0.0000, 0.0000, 1.1111, 0.0000],<br />
         [0.0000, 0.0000, 1.1111, 0.0000, 0.0000]]], device='cuda:0')<br />
tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],<br />
         [0.0000e+00, 0.0000e+00, 1.6732e+03, 0.0000e+00, 0.0000e+00],<br />
         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0731e-10],<br />
         [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4720e-03, 0.0000e+00],<br />
         [0.0000e+00, 0.0000e+00, 4.0205e+06, 0.0000e+00, 0.0000e+00]]],<br />
       device='cuda:0')<br />
Counter({'pattern_matcher_count': 4, 'pattern_matcher_nodes': 4})<br />
"""<br />
```</p>
<p>This model has been minimized; therefore, if any operator is removed, it won't trigger the bug. Additionally, I've printed the patterns optimized by the Inductor to assist with debugging.</p>
<p><code>view (sum_1, [4]) Match(..., [], {'arg': sum_1, 'size': [4]}) CallFunction(aten.view.default, KeywordArg('arg'), KeywordArg('size'))
view_2 (None, [1, 5, 5]) Match(..., [], {'arg': bmm, 'size': [1, 5, 5]}) CallFunction(aten.view.default, KeywordArg('arg'), KeywordArg('size'))
view_1 (None, [1, 10, 5]) Match(..., [], {'arg': expand_1, 'size': [1, 10, 5]}) CallFunction(aten.view.default, KeywordArg('arg'), KeywordArg('size'))
view (None, [1, 5, 10]) Match(..., [], {'arg': expand, 'size': [1, 5, 10]}) CallFunction(aten.view.default, KeywordArg('arg'), KeywordArg('size'))
rand ([1, 5, 5],) Match(..., [[1, 5, 5]], {'dtype': torch.float32, 'device': device(type='cuda', index=0), 'pin_memory': False}) CallFunctionVarArgs(aten.rand.default)</code></p>
<p>It seems that this is <strong>not</strong> related to the <code>fuse_attention</code> optimization</p>
<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.3.0.dev20240301+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0<br />
Clang version: 11.0.0 (https://github.com/aflgo/aflgo.git fa125da5d70621daf7141c6279877c97708c8c1f)<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56)  [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-26-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060<br />
Nvidia driver version: 525.147.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             24<br />
On-line CPU(s) list:                0-23<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family:                         6<br />
Model:                              151<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 16<br />
Socket(s):                          1<br />
Stepping:                           2<br />
CPU max MHz:                        5200.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           6374.40<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          640 KiB (16 instances)<br />
L1i cache:                          768 KiB (16 instances)<br />
L2 cache:                           14 MiB (10 instances)<br />
L3 cache:                           30 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu121<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[pip3] triton==2.2.0<br />
[conda] numpy                     1.26.4                   pypi_0    pypi<br />
[conda] pytorch-triton            3.0.0+901819d2b6          pypi_0    pypi<br />
[conda] torch                     2.3.0.dev20240301+cu121          pypi_0    pypi<br />
[conda] torchaudio                2.2.0.dev20240301+cu118          pypi_0    pypi<br />
[conda] torchvision               0.18.0.dev20240301+cu118          pypi_0    pypi<br />
[conda] triton                    2.2.0                    pypi_0    pypi</p>
<p>```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 20:09:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122381</guid>
    </item>
    <item>
      <title>[torch.compile] `split_cat_norm` and `cat_mutated` cause the optimized model to return a tensor with wrong shape</title>
      <link>https://github.com/pytorch/pytorch/issues/122379</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>The following model should return a tensor with shape <code>[10, 3, 64, 64]</code>, however, it seems that <code>split_cat_norm</code> and <code>cat_mutated</code> cause the optimized model to return a tensor with wrong shape, which is <code>[2, 3, 64, 64]</code></p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(0)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()
    self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)
    self.indices = [i for i in range(0, 10)]

def forward(self, x):
    split_tensors = torch.split(x, 1, 0) # len(split_tensors) == 10
    chosen_tensors = [split_tensors[i] for i in self.indices if i in range(0, 10)]
    result = torch.cat(chosen_tensors, 0)
    return result
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(10, 3, 64, 64)</p>
<p>with torch.no_grad():<br />
    func1 = torch.compile(func)<br />
    print(func1(x.clone()).shape)<br />
    # torch.Size([2, 3, 64, 64])<br />
    print(torch._dynamo.utils.counters['inductor'])<br />
    # Counter({'pattern_matcher_nodes': 11, 'pattern_matcher_count': 5, 'split_cat_norm': 2, 'cat_mutated': 1})</p>
<pre><code>print(func(x.clone()).shape)
# torch.Size([10, 3, 64, 64])
</code></pre>
<p>```</p>
<h3>Versions</h3>
<p>```<br />
Versions<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 19:19:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122379</guid>
    </item>
    <item>
      <title>[WIP][compiled autograd][dynamo] Make compiled graph take in boxed inputs</title>
      <link>https://github.com/pytorch/pytorch/pull/122353</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122535<br />
* <strong>-&gt;</strong> #122353<br />
* #122691</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 15:17:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122353</guid>
    </item>
    <item>
      <title>AOIInductor: Dynamic Shapes Specificiaton fails for SAM </title>
      <link>https://github.com/pytorch/pytorch/issues/122294</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I am trying to aot_compile a SAM model specifying dynamic shapes fail with the following error:<br />
```BASH<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR] Error while creating guard:<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR] Name: ''<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Source: shape_env<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Create Function: SHAPE_ENV<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Guard Types: None<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Code List: None<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Object Weakref: None<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Guarded Class Weakref: None<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR] Created at:<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]   File "/home/fabian/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 509, in transform<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     tracer = InstructionTranslator(<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]   File "/home/fabian/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in <strong>init</strong><br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     output=OutputGraph(<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]   File "/home/fabian/.local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 297, in <strong>init</strong><br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     self.init_ambient_guards()<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]   File "/home/fabian/.local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 371, in init_ambient_guards<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     self.guards.add(ShapeEnvSource().make_guard(GuardBuilder.SHAPE_ENV))</p>
<hr />
<p>AssertionError                            Traceback (most recent call last)<br />
File /tmp/model.py:21<br />
     19 n_labels = torch.export.Dim("n_labels", min=1, max=12)<br />
     20 example_inputs = (img, points, labels)<br />
---&gt; 21 so_path = torch._export.aot_compile(<br />
     22     model,<br />
     23     example_inputs,<br />
     24     dynamic_shapes={<br />
     25                     "img":{},<br />
     26                     "points": {1: n_labels},<br />
     27                     "labels": {1: n_labels}},<br />
     28     # Specify the generated shared library path<br />
     29     options={"aot_inductor.output_path": os.path.join(os.getcwd(), "model.so")},<br />
     30 )</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_export/<strong>init</strong>.py:1143, in aot_compile(f, args, kwargs, constraints, dynamic_shapes, options, remove_runtime_assertions, disable_constraint_solver)<br />
   1139     constraints = _process_dynamic_shapes(f, args, kwargs, dynamic_shapes)<br />
   1141 # We want to export to Torch IR here to utilize the pre_grad passes in<br />
   1142 # inductor, which run on Torch IR.<br />
-&gt; 1143 gm = _export_to_torch_ir(<br />
   1144     f,<br />
   1145     args,<br />
   1146     kwargs,<br />
   1147     constraints,<br />
   1148     disable_constraint_solver=disable_constraint_solver<br />
   1149 )<br />
   1150 flat_example_inputs = pytree.arg_tree_leaves(<em>args, </em>*(kwargs or {}))<br />
   1152 with torch.no_grad():</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_export/<strong>init</strong>.py:516, in _export_to_torch_ir(f, args, kwargs, constraints, preserve_module_call_signature, disable_constraint_solver)<br />
    514     module_call_specs: Dict[str, Dict[str, pytree.TreeSpec]] = {}<br />
    515     with _wrap_submodules(f, preserve_module_call_signature, module_call_specs):<br />
--&gt; 516         gm_torch_level, _ = torch._dynamo.export(<br />
    517             f,<br />
    518             constraints=constraints,<br />
    519             assume_static_by_default=True,<br />
    520             tracing_mode="symbolic",<br />
    521             disable_constraint_solver=disable_constraint_solver,<br />
    522         )(<br />
    523             <em>args,<br />
    524             </em>*kwargs,<br />
    525         )<br />
    526 except (ConstraintViolationError, ValueRangeError) as e:<br />
    527     raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: TRY200</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1342, in export.<locals>.inner(<em>args, </em><em>kwargs)<br />
   1340 # TODO(voz): We may have instances of <code>f</code> that mutate inputs, we should track sideeffects and reject.<br />
   1341 try:<br />
-&gt; 1342     result_traced = opt_f(</em>args, **kwargs)<br />
   1343 except ConstraintViolationError as e:<br />
   1344     constraint_violation_error = e</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1509     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1510 else:<br />
-&gt; 1511     return self._call_impl(*args, </strong>kwargs)</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1515 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1516 # this function, and just call forward.<br />
   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1518         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1519         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1520     return forward_call(</em>args, **kwargs)<br />
   1522 try:<br />
   1523     result = None</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:489, in _TorchDynamoContext.<strong>call</strong>.<locals>._fn(<em>args, </em><em>kwargs)<br />
    487     dynamo_config_ctx.<strong>enter</strong>()<br />
    488 try:<br />
--&gt; 489     return fn(</em>args, **kwargs)<br />
    490 finally:<br />
    491     set_eval_frame(prior)</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1509     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1510 else:<br />
-&gt; 1511     return self._call_impl(*args, </strong>kwargs)</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1515 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1516 # this function, and just call forward.<br />
   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1518         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1519         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1520     return forward_call(</em>args, **kwargs)<br />
   1522 try:<br />
   1523     result = None</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:655, in catch_errors_wrapper.<locals>.catch_errors(frame, cache_entry, frame_state)<br />
    652             return hijacked_callback(frame, cache_entry, hooks, frame_state)<br />
    654 with compile_lock, _disable_current_modes():<br />
--&gt; 655     return callback(frame, cache_entry, hooks, frame_state)</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:383, in convert_frame_assert.<locals>._convert_frame_assert(frame, cache_entry, hooks, frame_state)<br />
    370 signpost_event(<br />
    371     "dynamo",<br />
    372     "_convert_frame_assert._compile",<br />
   (...)<br />
    379     },<br />
    380 )<br />
    382 with config.patch(_patch_config_if_changed()):<br />
--&gt; 383     compiled_product = _compile(<br />
    384         frame.f_code,<br />
    385         frame.f_globals,<br />
    386         frame.f_locals,<br />
    387         frame.f_builtins,<br />
    388         compiler_fn,<br />
    389         one_graph,<br />
    390         export,<br />
    391         export_constraints,<br />
    392         hooks,<br />
    393         cache_size,<br />
    394         frame,<br />
    395         frame_state=frame_state,<br />
    396         compile_id=compile_id,<br />
    397     )<br />
    398 return compiled_product</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:646, in _compile(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id)<br />
    644 with compile_context(CompileContext(compile_id)):<br />
    645     try:<br />
--&gt; 646         guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
    647         return guarded_code<br />
    648     except (<br />
    649         Unsupported,<br />
    650         TorchRuntimeError,<br />
   (...)<br />
    657         BisectValidationException,<br />
    658     ) as e:</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:244, in dynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper(<em>args, </em><em>kwargs)<br />
    242 with torch.profiler.record_function(f"{key} (dynamo_timed)"):<br />
    243     t0 = time.time()<br />
--&gt; 244     r = func(</em>args, **kwargs)<br />
    245     time_spent = time.time() - t0<br />
    246 compilation_time_metrics[key].append(time_spent)</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:626, in _compile.<locals>.compile_inner(code, one_graph, hooks, transform)<br />
    624 assert output.guards is not None<br />
    625 CleanupManager.instance[out_code] = output.cleanups<br />
--&gt; 626 check_fn = CheckFunctionManager(<br />
    627     output,<br />
    628     hooks.guard_fail_fn if hooks else None,<br />
    629 )<br />
    631 guarded_code = GuardedCode(out_code, check_fn.check_fn)<br />
    633 if not output.is_empty_graph() and hooks.guard_export_fn is not None:<br />
    634     # We should not run the guard_export_fn when Dynamo does not<br />
    635     # generate any graph. This can happen in export when TorchDynamo<br />
    636     # generated bytecode has some reconstruction logic for mutated<br />
    637     # variables which can trigger TorchDynamo on the children frames but<br />
    638     # they are benign and do not generate any new graphs.</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/guards.py:1011, in CheckFunctionManager.<strong>init</strong>(self, output_graph, guard_fail_fn)<br />
   1000     if (<br />
   1001         not config.guard_nn_modules<br />
   1002         and guard.is_nn_module()<br />
   (...)<br />
   1007         and (config.skip_nnmodule_hook_guards or "hooks" not in guard.name)<br />
   1008     ):<br />
   1009         continue<br />
-&gt; 1011     guard.create(builder)<br />
   1012 self.check_fn = self.compile_check_fn(builder, guards, guard_fail_fn)<br />
   1013 self._weakrefs.clear()</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_guards.py:246, in Guard.create(self, builder)<br />
    244 def create(self, builder: GuardBuilderBase):<br />
    245     try:<br />
--&gt; 246         return self.create_fn(builder, self)<br />
    247     except Exception:<br />
    248         log.error("Error while creating guard:\n%s", str(self).rstrip())</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/guards.py:670, in GuardBuilder.SHAPE_ENV(self, guard)<br />
    668 else:<br />
    669     equalities_inputs = None<br />
--&gt; 670 guards = output_graph.shape_env.produce_guards(<br />
    671     [a.fake for a in fs],<br />
    672     [a.source for a in fs],<br />
    673     constraint_inputs=constraint_inputs,<br />
    674     equalities_inputs=equalities_inputs,<br />
    675     source_ref=self.source_ref,<br />
    676     # Export keeps static.<br />
    677     ignore_static=(not self.check_fn_manager.output_graph.export),<br />
    678 )<br />
    679 output_graph.shape_env.freeze()<br />
    680 for shape_guard in guards:</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:2630, in ShapeEnv.produce_guards(self, placeholders, sources, source_ref, constraint_inputs, equalities_inputs, _simplified, ignore_static)<br />
   2627     return symint.node.expr<br />
   2629 for src1, src2 in equalities_inputs.source_pairs:<br />
-&gt; 2630     s1, s2 = get_symbol(src1), get_symbol(src2)<br />
   2631     concrete_val = self.evaluate_expr(sympy.Eq(s1, s2))<br />
   2632     if not concrete_val:</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:2626, in ShapeEnv.produce_guards.<locals>.get_symbol(tensor_dim_src)<br />
   2624 fake = placeholders[source_index[tensor_dim_src.base.name()]]<br />
   2625 symint = fake.shape[tensor_dim_src.idx]<br />
-&gt; 2626 assert isinstance(symint, torch.SymInt)<br />
   2627 return symint.node.expr</p>
<p>AssertionError: </p>
<p>```</p>
<h2>Reproduce</h2>
<p>Consider the following model:<br />
```Python<br />
import os<br />
import torch</p>
<p>class SAMInterface(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.conv1 = torch.nn.Conv2d(3, 32, 3)</p>
<pre><code>def forward(self, img, points, labels):
    x = self.conv1(img)
    return x
</code></pre>
<p>with torch.no_grad():<br />
    device = "cuda" if torch.cuda.is_available() else "cpu"<br />
    model = SAMInterface().to(device=device)<br />
    img=torch.randn(1, 3, 1024, 1024, device=device)<br />
    points = torch.Tensor([[[500.0, 630.5]]])<br />
    labels = torch.Tensor([[[1]]])<br />
    n_labels = torch.export.Dim("n_labels", min=1, max=12)<br />
    example_inputs = (img, points, labels)<br />
    so_path = torch._export.aot_compile(<br />
        model,<br />
        example_inputs,<br />
        dynamic_shapes={<br />
                        "img": {},<br />
                        "points": {1: n_labels},<br />
                        "labels": {1: n_labels}},<br />
        # Specify the generated shared library path<br />
        options={"aot_inductor.output_path": os.path.join(os.getcwd(), "model.so")},<br />
    )<br />
<code>``
The forward function of</code>SamInterface<code>takes three arguments</code>img<code>,</code>points<code>, and</code>labels<code>. For inference, the batch size is always one, but users can modify the number of labels and points from one to twelve. AOT Compilation fails with the error posted above. I tried several variations of the</code>dynamic_shapes` input, but none succeeded. How can I aot_compile such a model? </p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.2.1+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.4 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.28.3<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.13 (main, Sep  5 2023, 06:03:44) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-6.2.0-35-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: Quadro T1000<br />
Nvidia driver version: 535.161.07<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      39 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             12<br />
On-line CPU(s) list:                0-11<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz<br />
CPU family:                         6<br />
Model:                              158<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 6<br />
Socket(s):                          1<br />
Stepping:                           10<br />
CPU max MHz:                        4500,0000<br />
CPU min MHz:                        800,0000<br />
BogoMIPS:                           5199.98<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust sgx bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp sgx_lc md_clear flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          192 KiB (6 instances)<br />
L1i cache:                          192 KiB (6 instances)<br />
L2 cache:                           1,5 MiB (6 instances)<br />
L3 cache:                           12 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-11<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable<br />
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Meltdown:             Mitigation; PTI<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Mitigation; IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Mitigation; Microcode<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] fast-pytorch-kmeans==0.2.0.1<br />
[pip3] flake8==6.0.0<br />
[pip3] mypy==1.4.1<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==2.1.0+3c400e7818<br />
[pip3] torch==2.2.1<br />
[pip3] torch-tb-profiler==0.4.1<br />
[pip3] torchaudio==2.1.0.dev20230714+cu121<br />
[pip3] torchdata==0.7.0<br />
[pip3] torchprofile==0.0.4<br />
[pip3] torchtext==0.16.0<br />
[pip3] torchvision==0.17.1<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 00:45:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122294</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Add qlinear_pointwise.binary op for X86Inductor backend</title>
      <link>https://github.com/pytorch/pytorch/pull/122288</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122667<br />
* #122593<br />
* #122387<br />
* <strong>-&gt;</strong> #122288</p>
<p><strong>Description</strong><br />
Add qlinear_binary op for X86Inductor backend of quantization PT2E. It only supports <code>add</code> and <code>add_relu</code> now.</p>
<p><strong>Test plan</strong><br />
python test_quantization.py -k test_qlinear_add_pt2e<br />
python test_quantization.py -k test_qlinear_add_relu_pt2e</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 23:40:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122288</guid>
    </item>
    <item>
      <title>fma can cause drastically worse precision in torch.compile/Triton</title>
      <link>https://github.com/pytorch/pytorch/issues/122260</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>```python<br />
import torch<br />
torch.set_default_device('cuda')</p>
<p>scale = torch.tensor(0.180336877703666687)<br />
x = torch.tensor(1134139801600.000000)</p>
<p>def f(x, scale):<br />
    max_scaled = x * scale<br />
    return torch.exp(max_scaled - x * scale)</p>
<p>print(f(x, scale))<br />
print(torch.compile(f)(x, scale))</p>
<blockquote>
<blockquote>
<blockquote>
<p>tensor(1., device='cuda:0')<br />
tensor(inf, device='cuda:0')<br />
```</p>
</blockquote>
</blockquote>
</blockquote>
<p>The root cause here is the same as here: https://github.com/pytorch/pytorch/issues/121558#issuecomment-2008359796</p>
<p>I think the general structure of this can show up in many cases, but in this case, this is a Triton issue. Here, Inductor actually CSE's the <code>x * scale</code> call.</p>
<p>The generated Triton code is<br />
<code>python
@triton_heuristics.pointwise(
    size_hints=[1],
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {3: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=(3,), ids_of_folded_args=(3,), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_exp_mul_sub_0', 'mutated_arg_names': [], 'no_x_dim': False, 'backend_hash': 'cddd2cc4107921f6715d58b181fdb08b23055085471461101752ba6efb772ae1'},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    tmp0 = tl.load(in_ptr0 + (0))
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
    tmp2 = tl.load(in_ptr1 + (0))
    tmp3 = tl.broadcast_to(tmp2, [XBLOCK])
    tmp4 = tmp1 * tmp3
    tmp5 = tmp4 - tmp4
    tmp6 = tl_math.exp(tmp5)
    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp6, None)</code></p>
<p>Here, you would imagine that <code>tmp4 - tmp4</code> is 0. Unfortunately, Triton generates this PTX. <br />
<code>mul.f32         %f5, %f3, %f4;
        .loc    1 30 18
        neg.f32         %f6, %f5;
        fma.rn.f32      %f7, %f3, %f4, %f6;</code><br />
Which duplicates the FMA again!</p>
<h3>Versions</h3>
<p>N/A</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 17:40:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122260</guid>
    </item>
    <item>
      <title>Use graph.find_nodes in inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/122256</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122331<br />
* #122258<br />
* #122257<br />
* <strong>-&gt;</strong> #122256<br />
* #122255<br />
* #121565</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 17:27:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122256</guid>
    </item>
    <item>
      <title>Use graph.find_nodes in inductor/fx_passes</title>
      <link>https://github.com/pytorch/pytorch/pull/122255</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122331<br />
* #122258<br />
* #122257<br />
* #122256<br />
* <strong>-&gt;</strong> #122255<br />
* #121565</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 17:27:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122255</guid>
    </item>
    <item>
      <title>[aoti] Change aot_compile callsites</title>
      <link>https://github.com/pytorch/pytorch/pull/122225</link>
      <description><![CDATA[<p>Summary:<br />
Replacing <code>torch._export.aot_compile</code> callsites with <br />
<code>ep = torch.export._trace._export(.., predispatch=True)   # Traces the given program into predispatch IR
so_path = torch._inductor.aot_compile_ep(ep, ...)  # Takes an exported program and compiles it into a .so</code></p>
<p>This allows us to explicitly split up the export step from AOTInductor. We can later modify tests to do <code>export + serialize + deserialize + inductor</code> to mimic internal production use cases better.</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D54808612</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 12:18:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122225</guid>
    </item>
    <item>
      <title>Need to FUNCTORCH_STACK_MATCH in the vmap(compile(f)) case</title>
      <link>https://github.com/pytorch/pytorch/issues/122201</link>
      <description><![CDATA[<p>When entering torch.compile, if the functorch state is already active, then we need to emit this guard</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @Chillee @samdow @kshitij12345 @janeyx99 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 07:41:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122201</guid>
    </item>
    <item>
      <title>Bug: `torch.compile` fails to trace jvp with raw python numbers.</title>
      <link>https://github.com/pytorch/pytorch/issues/122197</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>torch.compile</code> fails to compile jvp transformation with native python numbers. Many tests ignored on <code>dynamo_expected_fail</code> fails for this same reason.</p>
<p>```python<br />
import torch<br />
torch._dynamo.config.capture_func_transforms = True<br />
jvp = torch.func.jvp</p>
<p>def f(x, y, z):<br />
    a, b = x<br />
    return a + 2 * b + 3 * y + 4 * z</p>
<p>@torch.compile(backend='aot_eager', fullgraph=True)<br />
def fn(x):<br />
    return jvp(f, ((x, x,), x, x), ((x, x), x, x))</p>
<p>x = torch.tensor(1.,)<br />
y = fn(x)<br />
print(y)<br />
```</p>
<p>```python<br />
Traceback (most recent call last):<br />
  File "/home/guilhermeleobas/git/pytorch/a.py", line 24, in <module><br />
    y = fn(x)<br />
        ^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/eval_frame.py", line 450, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/convert_frame.py", line 923, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state, skip=1)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert<br />
    return _compile(<br />
           ^^^^^^^^^<br />
  File "/home/guilhermeleobas/micromamba/envs/pytorch/lib/python3.11/contextlib.py", line 81, in inner<br />
    return func(</em>args, <strong>kwds)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/convert_frame.py", line 676, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
        ^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/convert_frame.py", line 535, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/convert_frame.py", line 165, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/convert_frame.py", line 500, in transform<br />
    tracer.run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 2150, in run<br />
    super().run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
        ^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 489, in wrapper<br />
    return inner_fn(self, inst)<br />
           ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 1802, in CALL<br />
    self.call_function(fn, args, kwargs)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
    self.push(fn.call_function(self, args, kwargs))<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 289, in call_function<br />
    return super().call_function(tx, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 90, in call_function<br />
    return tx.inline_user_function_return(<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 680, in inline_user_function_return<br />
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 2286, in inline_call<br />
    return cls.inline_call</em>(parent, func, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 2400, in inline_call</em><br />
    tracer.run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
        ^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 489, in wrapper<br />
    return inner_fn(self, inst)<br />
           ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 1802, in CALL<br />
    self.call_function(fn, args, kwargs)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
    self.push(fn.call_function(self, args, kwargs))<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 289, in call_function<br />
    return super().call_function(tx, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 90, in call_function<br />
    return tx.inline_user_function_return(<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 680, in inline_user_function_return<br />
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 2286, in inline_call<br />
    return cls.inline_call</em>(parent, func, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 2400, in inline_call</em><br />
    tracer.run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
        ^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 489, in wrapper<br />
    return inner_fn(self, inst)<br />
           ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 1260, in CALL_FUNCTION_EX<br />
    self.call_function(fn, argsvars.items, kwargsvars)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
    self.push(fn.call_function(self, args, kwargs))<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 289, in call_function<br />
    return super().call_function(tx, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 90, in call_function<br />
    return tx.inline_user_function_return(<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 680, in inline_user_function_return<br />
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 2286, in inline_call<br />
    return cls.inline_call</em>(parent, func, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 2400, in inline_call</em><br />
    tracer.run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
        ^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 489, in wrapper<br />
    return inner_fn(self, inst)<br />
           ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 1260, in CALL_FUNCTION_EX<br />
    self.call_function(fn, argsvars.items, kwargsvars)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
    self.push(fn.call_function(self, args, kwargs))<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 289, in call_function<br />
    return super().call_function(tx, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 90, in call_function<br />
    return tx.inline_user_function_return(<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 680, in inline_user_function_return<br />
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 2286, in inline_call<br />
    return cls.inline_call</em>(parent, func, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 2400, in inline_call</em><br />
    tracer.run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
        ^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 1764, in BINARY_OP<br />
    return getattr(self, "BINARY</em>" + opname)(inst)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 242, in impl<br />
    self.push(fn_var.call_function(self, self.popn(nargs), {}))<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/builtin.py", line 641, in call_function<br />
    return wrap_fx_proxy(tx, proxy)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/builder.py", line 1338, in wrap_fx_proxy<br />
    return wrap_fx_proxy_cls(target_cls=TensorVariable, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/builder.py", line 1423, in wrap_fx_proxy_cls<br />
    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)<br />
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1732, in get_fake_value<br />
    raise TorchRuntimeError(str(e)).with_traceback(e.<strong>traceback</strong>) from None<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1667, in get_fake_value<br />
    ret_val = wrap_fake_exception(<br />
              ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1198, in wrap_fake_exception<br />
    return fn()<br />
           ^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1668, in <lambda><br />
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)<br />
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1800, in run_node<br />
    raise RuntimeError(make_error_message(e)).with_traceback(<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1782, in run_node<br />
    return node.target(</em>args, <strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/utils/_stats.py", line 20, in wrapper<br />
    return fn(*args, </strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_subclasses/fake_tensor.py", line 893, in <strong>torch_dispatch</strong><br />
    return self.dispatch(func, types, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_subclasses/fake_tensor.py", line 1238, in dispatch<br />
    return self._cached_dispatch_impl(func, types, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_subclasses/fake_tensor.py", line 963, in _cached_dispatch_impl<br />
    output = self._dispatch_impl(func, types, args, kwargs)<br />
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_subclasses/fake_tensor.py", line 1455, in _dispatch_impl<br />
    r = func(<em>args, </em><em>kwargs)<br />
        ^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>ops.py", line 600, in <strong>call</strong><br />
    return self</em>._op(</em>args, <strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function mul>(*(2, GradTrackingTensor(lvl=1, value=<br />
    FakeTensor(..., size=())<br />
)), </strong>{}):<br />
aten::alias() Expected a value of type 'Tensor' for argument 'self' but instead found type 'int'.<br />
Position: 0<br />
Value: 2<br />
Declaration: aten::alias(Tensor(a) self) -&gt; Tensor(a)<br />
Cast error details: Unable to cast 2 to Tensor</p>
<p>from user code:<br />
   File "/home/guilhermeleobas/git/pytorch/a.py", line 20, in fn<br />
    return jvp(f, ((x, x,), x, x), ((x, x), x, x))<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_functorch/eager_transforms.py", line 985, in jvp<br />
    return _jvp_with_argnums(func, primals, tangents, argnums=None, strict=strict, has_aux=has_aux)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_functorch/vmap.py", line 47, in fn<br />
    return f(<em>args, </em><em>kwargs)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_functorch/eager_transforms.py", line 1031, in _jvp_with_argnums<br />
    result_duals = func(</em>duals)<br />
  File "/home/guilhermeleobas/git/pytorch/a.py", line 16, in f<br />
    return a + 2 * b + 3 * y + 4 * z</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>
<h3>Versions</h3>
<p>main</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 07:09:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122197</guid>
    </item>
    <item>
      <title>Bug in `torch.compile` with standard type checking tools beartype</title>
      <link>https://github.com/pytorch/pytorch/issues/122093</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Reproduction script: </p>
<p>```<br />
import torch <br />
from torch import nn<br />
from beartype import beartype <br />
from jaxtyping import Float</p>
<p>class Net(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super(Net, self).<strong>init</strong>()<br />
        self.conv1 = nn.Conv2d(1, 20, 5)<br />
        self.conv2 = nn.Conv2d(20, 20, 5)</p>
<pre><code>def forward(self, x):
    x = nn.functional.relu(self.conv1(x))
    return nn.functional.relu(self.conv2(x))
</code></pre>
<p>class BearNet(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super(Net, self).<strong>init</strong>()<br />
        self.conv1 = nn.Conv2d(1, 20, 5)<br />
        self.conv2 = nn.Conv2d(20, 20, 5)</p>
<pre><code>@beartype
def forward(self, x):
    x = nn.functional.relu(self.conv1(x))
    return nn.functional.relu(self.conv2(x))
</code></pre>
<p>if <strong>name</strong> == "<strong>main</strong>":<br />
    net = Net()<br />
    x = torch.randn(1, 1, 28, 28)<br />
    y = net(x)<br />
    print(y)</p>
<pre><code>model = torch.compile(net)
y = model(x)
print(y)

net = BearNet()
x = torch.randn(1, 1, 28, 28)
y = net(x)
print('with beartype: ', y)

model = torch.compile(net)
y = model(x)
print('with beartype: ', y)
</code></pre>
<p>```</p>
<p>First this throws an error that asks for installing something (Mac OS 14):</p>
<p><code>OpenMP support not found. Please try one of the following solutions:
(1) Set the `CXX` environment variable to a compiler other than Apple clang++/g++ that has builtin OpenMP support;
(2) install OpenMP via conda: `conda install llvm-openmp`;
(3) install libomp via brew: `brew install libomp`;
(4) manually setup OpenMP and set the `OMP_PREFIX` environment variable to point to a path with `include/omp.h` under it.</code></p>
<p>After installing libomp: <code>brew install libomp</code>, this works. </p>
<p>However, then this next reproduction script breaks: </p>
<p>```<br />
import torch <br />
from torch import nn<br />
from beartype import beartype <br />
from jaxtyping import Float<br />
from torch import Tensor</p>
<p>class Net(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super(Net, self).<strong>init</strong>()<br />
        self.conv1 = nn.Conv2d(1, 20, 5)<br />
        self.conv2 = nn.Conv2d(20, 20, 5)</p>
<pre><code>def forward(self, x):
    x = nn.functional.relu(self.conv1(x))
    return nn.functional.relu(self.conv2(x))
</code></pre>
<p>class BearNet(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super(BearNet, self).<strong>init</strong>()<br />
        self.conv1 = nn.Conv2d(1, 20, 5)<br />
        self.conv2 = nn.Conv2d(20, 20, 5)</p>
<pre><code>@beartype
def forward(self, x):
    x = nn.functional.relu(self.conv1(x))
    return nn.functional.relu(self.conv2(x))
</code></pre>
<p>class BearNetJaxTyped(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super(BearNetJaxTyped, self).<strong>init</strong>()<br />
        self.conv1 = nn.Conv2d(1, 20, 5)<br />
        self.conv2 = nn.Conv2d(20, 20, 5)</p>
<pre><code>@beartype
def forward(self, x: Float[Tensor, "1 1 28 28"]):
    x = nn.functional.relu(self.conv1(x))
    return nn.functional.relu(self.conv2(x))
</code></pre>
<p>if <strong>name</strong> == "<strong>main</strong>":<br />
    net = Net()<br />
    x = torch.randn(1, 1, 28, 28)<br />
    y = net(x)<br />
    print(y)</p>
<pre><code>model = torch.compile(net)
y = model(x)
print(y)

net = BearNet()
x = torch.randn(1, 1, 28, 28)
y = net(x)
print('with beartype: ', y)

model = torch.compile(net)
y = model(x)
print('with beartype: ', y)

net = BearNetJaxTyped()
x = torch.randn(1, 1, 28, 28)
y = net(x)
print('with beartype: ', y)

model = torch.compile(net)
y = model(x)
print('with beartype: ', y)
</code></pre>
<p>```</p>
<p>Results:</p>
<p>```<br />
  File "/Users/me/projects/phare-sandbox/.venv/lib/python3.11/site-packages/torch/_dynamo/guards.py", line 235, in get<br />
    return eval(name, self.scope, CLOSURE_VARS)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "<string>", line 1, in <module><br />
torch._dynamo.exc.InternalTorchDynamoError: '_thread._local' object has no attribute 'value'</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>
<p>This script runs fine when disabling beartype:</p>
<p><code>PYTHONOPTIMIZE=1 python test_compile.py
... prints out tensors</code></p>
<p>This is also an issue on Ubuntu virtual machines (e.g. on AWS), and on Jupyter notebooks; workaround for now: https://github.com/beartype/beartype/issues/341 </p>
<p>Discussion: https://github.com/beartype/beartype/issues/343</p>
<p>From @justinchuby : </p>
<p>```quote<br />
If I have to guess, is it this line?</p>
<p>https://github.com/pytorch/pytorch/blob/6d9588a12b5834f29bd8970936749a9725e7f609/torch/_dynamo/guards.py#L316</p>
<p>t = type(self.get(guard.name))</p>
<p>and somehow in eval(name, self.scope, CLOSURE_VARS) it is trying to access value which does not exist?<br />
```</p>
<h3>Versions</h3>
<p>```<br />
‚ùØ python collect_env.py <br />
Collecting environment information...<br />
PyTorch version: 2.1.2<br />
Is debug build: False<br />
CUDA used to build PyTorch: None<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: macOS 14.4 (arm64)<br />
GCC version: Could not collect<br />
Clang version: 17.0.6<br />
CMake version: Could not collect<br />
Libc version: N/A</p>
<p>Python version: 3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ] (64-bit runtime)<br />
Python platform: macOS-14.4-arm64-arm-64bit<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Apple M3 Pro</p>
<p>Versions of relevant libraries:<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.26.2<br />
[pip3] torch==2.1.2<br />
[pip3] torchaudio==2.1.2<br />
[pip3] torcheval==0.0.7<br />
[pip3] torchvision==0.16.2<br />
[conda] No relevant packages<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Mon, 18 Mar 2024 06:23:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122093</guid>
    </item>
    <item>
      <title>[RFC] Use CUDA graphs by default on torch.compile</title>
      <link>https://github.com/pytorch/pytorch/issues/121968</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I keep seeing comparisons between JAX / TF / Keras vs <code>torch.compile</code> where they benchmark the "default XLA settings vs. the default <code>torch.compile</code>" to find that XLA frontends are x3-x4 faster than <code>torch.compile</code>.<br />
The last one I found is the newly posted Keras 3 benchmarks<br />
https://keras.io/getting_started/benchmarks/ <br />
but I have seen these skewed comparisons over and over again.</p>
<p>The only thing stopping us from compiling with CUDA graphs on is that these sometimes have a higher mem footprint. Would it be reasonable to turn these on by default so that people get speed by default, and then catch a potential OOM and re-raise it indicating that using CUDA-graphs off to potentialy mitigate the OOM?</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @jansel @albanD @eellison @Chillee </p>
<h3>Versions</h3>
<p>master</p>]]></description>
      <pubDate>Fri, 15 Mar 2024 05:32:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121968</guid>
    </item>
    <item>
      <title>torch.compile fails when used together with activation checkpointing</title>
      <link>https://github.com/pytorch/pytorch/issues/121966</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>```python<br />
import time<br />
import torch<br />
import torch.utils.checkpoint</p>
<p>def add_and_drop(x):<br />
    return torch.nn.functional.dropout(x * 5, 0.5)</p>
<p>x = torch.rand((50001, 3072), device='cuda', dtype=torch.bfloat16)</p>
<p>x.requires_grad_(True)<br />
x.retain_grad()</p>
<p>with torch.cuda.amp.autocast(dtype=torch.bfloat16):<br />
    f = torch.compile(add_and_drop)<br />
    out = torch.utils.checkpoint.checkpoint(f, x, use_reentrant=False)<br />
    out.backward(torch.rand_like(out))<br />
    print(x.grad)<br />
```</p>
<p>fails with</p>
<p>```text</p>
<h1>python3 ./repro.py</h1>
<p>/usr/local/lib/python3.9/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils.<em>pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.<br />
  _torch_pytree._register_pytree_node(<br />
/usr/local/lib/python3.9/dist-packages/torch/autograd/<strong>init</strong>.py:411: UserWarning: Error detected in NativeDropoutBackward0. Traceback of forward call that caused the error:<br />
  File "//./repro.py", line 6, in add_and_drop<br />
    return torch.nn.functional.dropout(x * 5, 0.5)<br />
 (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:113.)<br />
  result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass<br />
Traceback (most recent call last):<br />
  File "//./repro.py", line 15, in <module><br />
    out = torch.utils.checkpoint.checkpoint(f, x, use_reentrant=False)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_compile.py", line 24, in inner<br />
    return torch._dynamo.disable(fn, recursive)(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py", line 489, in checkpoint<br />
    ret = function(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/eval_frame.py", line 655, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 727, in _convert_frame<br />
    result = inner_convert(frame, cache_entry, hooks, frame_state)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 383, in _convert_frame_assert<br />
    compiled_product = _compile(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 646, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 562, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 151, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/convert_frame.py", line 527, in transform<br />
    tracer.run()<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/symbolic_convert.py", line 2128, in run<br />
    super().run()<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/symbolic_convert.py", line 818, in run<br />
    and self.step()<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/symbolic_convert.py", line 781, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/symbolic_convert.py", line 2243, in RETURN_VALUE<br />
    self.output.compile_subgraph(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/output_graph.py", line 919, in compile_subgraph<br />
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)<br />
  File "/usr/lib/python3.9/contextlib.py", line 79, in inner<br />
    return func(</em>args, <strong>kwds)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/output_graph.py", line 1087, in compile_and_call_fx_graph<br />
    compiled_fn = self.call_user_compiler(gm)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/output_graph.py", line 1159, in call_user_compiler<br />
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/output_graph.py", line 1140, in call_user_compiler<br />
    compiled_fn = compiler_fn(gm, self.example_inputs())<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper<br />
    compiled_gm = compiler_fn(gm, example_inputs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/<strong>init</strong>.py", line 1668, in <strong>call</strong><br />
    return compile_fx(model</em>, inputs_, config_patches=self.config)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_inductor/compile_fx.py", line 1168, in compile_fx<br />
    return aot_autograd(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/backends/common.py", line 55, in compiler_fn<br />
    cg = aot_module_simplified(gm, example_inputs, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py", line 887, in aot_module_simplified<br />
    compiled_fn = create_aot_dispatcher_function(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py", line 600, in create_aot_dispatcher_function<br />
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 425, in aot_wrapper_dedupe<br />
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 630, in aot_wrapper_synthetic_base<br />
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 151, in aot_dispatch_autograd<br />
    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(  # type: ignore[misc]<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py", line 159, in aot_dispatch_autograd_graph<br />
    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py", line 32, in _create_graph<br />
    fx_g = make_fx(f, decomposition_table=aot_config.decompositions)(<em>args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 871, in wrapped<br />
    t = dispatch_trace(wrap_key(func, args, fx_tracer, pre_dispatch), tracer=fx_tracer, concrete_args=tuple(phs))<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_compile.py", line 24, in inner<br />
    return torch._dynamo.disable(fn, recursive)(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 483, in dispatch_trace<br />
    graph = tracer.trace(root, concrete_args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/_symbolic_trace.py", line 821, in trace<br />
    (self.create_arg(fn(<em>args)),),<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/_symbolic_trace.py", line 688, in flatten_fn<br />
    tree_out = root_fn(</em>tree_args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 519, in wrapped<br />
    out = f(<em>tensors)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 463, in joint_helper<br />
    return _functionalized_f_helper(primals, tangents)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 355, in _functionalized_f_helper<br />
    f_outs = fn(</em>f_args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 250, in inner_fn_with_anomaly<br />
    return inner_fn(<em>args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 235, in inner_fn<br />
    backward_out = torch.autograd.grad(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/autograd/<strong>init</strong>.py", line 411, in grad<br />
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py", line 1112, in unpack_hook<br />
    frame.recompute_fn(</em>args)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py", line 1401, in recompute_fn<br />
    fn(<em>args, </em><em>kwargs)<br />
  File "/usr/lib/python3.9/contextlib.py", line 135, in <strong>exit</strong><br />
    self.gen.throw(type, value, traceback)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/random.py", line 175, in fork_rng<br />
    device_mod.set_rng_state(device_rng_state, device)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/cuda/random.py", line 61, in set_rng_state<br />
    new_state_copy = new_state.clone(memory_format=torch.contiguous_format)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/functional_tensor.py", line 309, in <strong>torch_dispatch</strong><br />
    outs_unwrapped = func(</em>args_unwrapped, <strong>kwargs_unwrapped)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_ops.py", line 513, in <strong>call</strong><br />
    return self._op(*args, </strong>(kwargs or {}))<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/_stats.py", line 20, in wrapper<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 596, in <strong>torch_dispatch</strong><br />
    return self.inner_torch_dispatch(func, types, args, kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 631, in inner_torch_dispatch<br />
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/proxy_tensor.py", line 376, in proxy_call<br />
    out = func(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_ops.py", line 513, in <strong>call</strong><br />
    return self._op(*args, </strong>(kwargs or {}))<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/_stats.py", line 20, in wrapper<br />
    return fn(<em>args, </em>*kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/fake_tensor.py", line 1392, in <strong>torch_dispatch</strong><br />
    return self.dispatch(func, types, args, kwargs)<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/fake_tensor.py", line 1529, in dispatch<br />
    (flat_args, flat_arg_fake_tensors) = self.validate_and_convert_non_fake_tensors(<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/fake_tensor.py", line 1776, in validate_and_convert_non_fake_tensors<br />
    validated_args = [validate(a) for a in flat_args]<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/fake_tensor.py", line 1776, in <listcomp><br />
    validated_args = [validate(a) for a in flat_args]<br />
  File "/usr/local/lib/python3.9/dist-packages/torch/_subclasses/fake_tensor.py", line 1766, in validate<br />
    raise Exception(<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.clone.default(tensor([...], size=(16,), dtype=torch.uint8), memory_format=torch.contiguous_format)</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>
<p>This error only appears if <code>use_reentrant</code> is set to <code>False</code>.</p>
<h3>Versions</h3>
<p>```text<br />
PyTorch version: 2.2.1+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Debian GNU/Linux 11 (bullseye) (x86_64)<br />
GCC version: (Debian 10.2.1-6) 10.2.1 20210110<br />
Clang version: Could not collect<br />
CMake version: version 3.26.3<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.9.2 (default, Feb 28 2021, 17:03:44)  [GCC 10.2.1 20210110] (64-bit runtime)<br />
Is CUDA available: True<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Fri, 15 Mar 2024 02:41:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121966</guid>
    </item>
    <item>
      <title>Compiled model raises error "attn_bias is not correctly aligned" in pytorch 2.2</title>
      <link>https://github.com/pytorch/pytorch/issues/121943</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When running the following code, errors may occur in pytorch 2.2.0 or 2.2.1, but not in 2.1.0.</p>
<p>```</p>
<p>from einops import rearrange<br />
import torch,torch.nn as nn<br />
def rotate_half(x):<br />
    x = rearrange(x, '... (d r) -&gt; ... d r', r = 2)<br />
    x1, x2 = x.unbind(dim = -1)<br />
    x = torch.stack((-x2, x1), dim = -1)<br />
    return rearrange(x, '... d r -&gt; ... (d r)')</p>
<p>def random_masking_v4(mask_kernel, percent,loss_kernel, B, H, W, device='cpu', loss_weight_factor = 1.0):<br />
    """<br />
    Perform per-sample random masking by per-sample shuffling.<br />
    Per-sample shuffling is done by argsort random noise.<br />
    x: [N, L, D], sequence<br />
    """<br />
    k1, k2 = mask_kernel<br />
    pad = (loss_kernel -1) // 2<br />
    with torch.no_grad():<br />
        noise1 = torch.rand(B, 1, H + k1 - 1, W + k2 - 1, device=device) * 800<br />
        noise1 = torch.nn.functional.max_pool2d(noise1, kernel_size=(k1, k2), stride=1, padding=0, )<br />
        noise2 = torch.rand(B, 1, H + k2 - 1, W + k1 - 1, device=device) * 800<br />
        noise2 = torch.nn.functional.max_pool2d(noise2, kernel_size=(k2, k1), stride=1, padding=0, )</p>
<pre><code>    noise = (torch.maximum(noise1, noise2)).view(B, 1, H, W)
    noise = (torch.rand(B, 1, H, W, device=device) - noise).view(B, -1)

    ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove, shape:B,N
    ids_restore = torch.argsort(ids_shuffle, dim=1) # represents the order of each id
    ids_mask = ids_restore &lt; int(H*W*percent)

    rand_center = torch.cat([ids_shuffle[:, 0:1] // W, ids_shuffle[:, 0:1] % W], 1).unsqueeze(-1)

    cy, cx = torch.meshgrid(torch.arange(H, device=device),
                            torch.arange(W, device=device), indexing='ij')
    coords = torch.stack([cy, cx]).view(1, 2, H * W)
    distance = (((coords - rand_center + torch.rand(B, 2, 1, device=device)) ** 2).sum(1)) ** 0.5  + 1
    ids_order = (distance * 3).int() * ~ids_mask + -100 * ids_mask
    can_see_p1 = ids_order[:,:,None] &gt;= ids_order[:,None,:]
    attn_mask = can_see_p1.unsqueeze(1)

    patch_order = ids_order.view(B,1,H,W).float()
    loss_order = torch.nn.functional.unfold(patch_order,loss_kernel,dilation=1, padding=pad)

    if loss_kernel == 3:
        loss_weight = torch.as_tensor((2,1,2,1,1,1,2,1,2),dtype=torch.float32,device=device)
    elif loss_kernel == 5:
        loss_weight = torch.as_tensor(((8,5,2,5,8),(5,2,1,2,5),(2,1,1,1,2),
                                            (5,2,1,2,5),(8,5,2,5,8)), dtype=torch.float32, device=device)
    else:
        raise NotImplementedError

    loss_weight = 1.0 / loss_weight.view(1,-1,1) ** loss_weight_factor
    loss_mask = ((loss_order-1e-5) &gt; patch_order.view(B,1,H*W)).float()

return torch.where(attn_mask,0,-9999.0), loss_mask * loss_weight
</code></pre>
<p>class Attention(nn.Module):</p>
<pre><code>def __init__(
        self,
        dim,
        num_heads=8,
        qkv_bias=True,
        qk_norm=False,
        attn_drop=0.,
        proj_drop=0.,
        norm_layer=nn.LayerNorm,
):
    super().__init__()
    assert dim % num_heads == 0, 'dim should be divisible by num_heads'
    self.num_heads = num_heads
    self.head_dim = dim // num_heads
    self.scale = self.head_dim ** -0.5
    self.fused_attn = True

    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
    self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
    self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
    self.attn_drop = nn.Dropout(attn_drop)
    self.proj = nn.Linear(dim, dim)
    self.proj_drop = nn.Dropout(proj_drop)

#@torch.compile
def forward(self, x,mask=None):
    B, N, C = x.shape
    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
    q, k, v = qkv.unbind(0)

    q, k = self.q_norm(q), self.k_norm(k)

    if self.fused_attn:
        x = torch.nn.functional.scaled_dot_product_attention(
            q, k, v,attn_mask=mask,
            dropout_p=self.attn_drop.p,
        )
    else:
        q = q * self.scale
        attn = q @ k.transpose(-2, -1)
        attn = attn + mask
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        x = attn @ v

    x = x.transpose(1, 2).reshape(B, N, C)
    x = self.proj(x)
    x = self.proj_drop(x)
    return x
</code></pre>
<p>class CustomModel(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.attn = Attention(768,12,True)</p>
<pre><code>def forward(self,x):
    mask,_ = random_masking_v4((2,5),0.15,3,x.shape[0],
                               int(x.shape[1]**0.5),int(x.shape[1]**0.5),device=x.device,
                               )
    return self.attn(x,mask)
</code></pre>
<p>model = CustomModel().cuda()<br />
model_without_ddp = model<br />
x =torch.zeros(256,196,768).cuda()</p>
<p>optimizer = torch.optim.AdamW(model_without_ddp.parameters())<br />
model = torch.compile(model)</p>
<p>with torch.cuda.amp.autocast():<br />
    out = model(x)<br />
loss = out.sum()<br />
loss.backward()<br />
optimizer.step()<br />
optimizer.zero_grad(True)</p>
<p>with torch.cuda.amp.autocast():<br />
    out = model(x)<br />
loss = out.sum()<br />
loss.backward()<br />
optimizer.step()<br />
optimizer.zero_grad(True)</p>
<p>```</p>
<p>error messages:<br />
```</p>
<p>File "//test.py", line 130, in <module><br />
    out = model(x)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "//test.py", line 116, in forward<br />
    def forward(self,x):<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 901, in forward<br />
    return compiled_fn(full_args)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 81, in g<br />
    return f(</em>args)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 83, in runtime_wrapper<br />
    all_outs = call_func_at_runtime_with_args(<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 105, in call_func_at_runtime_with_args<br />
    out = normalize_as_list(f(args))<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 81, in g<br />
    return f(<em>args)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/autograd/function.py", line 553, in apply<br />
    return super().apply(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 408, in forward<br />
    fw_outs = call_func_at_runtime_with_args(<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 105, in call_func_at_runtime_with_args<br />
    out = normalize_as_list(f(args))<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 864, in <strong>call</strong><br />
    return self.get_current_callable()(inputs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 611, in run<br />
    return model(new_inputs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 892, in _run_from_cache<br />
    return compiled_graph.compiled_artifact(inputs)<br />
  File "/tmp/torchinductor_root/hz/chzdqrbr5gisewqim47noe7zsijncibkeouw2ryw2no4ecybmvnj.py", line 641, in call<br />
    buf21 = aten._scaled_dot_product_efficient_attention(reinterpret_tensor(buf18, (256, 12, 196, 64), (451584, 64, 2304, 1), 0), reinterpret_tensor(buf18, (256, 12, 196, 64), (451584, 64, 2304, 1), 768), reinterpret_tensor(buf18, (256, 12, 196, 64), (451584, 64, 2304, 1), 1536), buf20, True)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_ops.py", line 755, in <strong>call</strong><br />
    return self._op(*args, </strong>(kwargs or {}))<br />
RuntimeError: attn_bias is not correctly aligned (strideM). attn_bias.stride(2) = 196, and should be a multiple of 8.</p>
<p>```</p>
<h3>Versions</h3>
<p>/usr/local/lib/python3.10/runpy.py:126: RuntimeWarning: 'torch.utils.collect_env' found in sys.modules after import of package 'torch.utils', but prior to execution of 'torch.utils.collect_env'; this may result in unpredictable behaviour<br />
  warn(RuntimeWarning(msg))<br />
Collecting environment information...<br />
PyTorch version: 2.2.0+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.16.3<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.10.13 (main, Dec 19 2023, 08:15:18) [GCC 9.4.0] (64-bit runtime)<br />
Python platform: Linux-4.15.0-189-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: Quadro RTX 6000<br />
GPU 1: Quadro RTX 6000<br />
GPU 2: Quadro RTX 6000<br />
GPU 3: Quadro RTX 6000<br />
GPU 4: Quadro RTX 6000<br />
GPU 5: Quadro RTX 6000<br />
GPU 6: Quadro RTX 6000<br />
GPU 7: Quadro RTX 6000</p>
<p>Nvidia driver version: 535.54.03<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
CPU(s):                          64<br />
On-line CPU(s) list:             0-63<br />
Thread(s) per core:              2<br />
Core(s) per socket:              16<br />
Socket(s):                       2<br />
NUMA node(s):                    2<br />
Vendor ID:                       GenuineIntel<br />
CPU family:                      6<br />
Model:                           85<br />
Model name:                      Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz<br />
Stepping:                        7<br />
CPU MHz:                         2651.207<br />
CPU max MHz:                     3900.0000<br />
CPU min MHz:                     1000.0000<br />
BogoMIPS:                        4600.00<br />
Virtualization:                  VT-x<br />
L1d cache:                       1 MiB<br />
L1i cache:                       1 MiB<br />
L2 cache:                        32 MiB<br />
L3 cache:                        44 MiB<br />
NUMA node0 CPU(s):               0-15,32-47<br />
NUMA node1 CPU(s):               16-31,48-63<br />
Vulnerability Itlb multihit:     KVM: Mitigation: Split huge pages<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Mitigation; TSX disabled<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.3<br />
[pip3] torch==2.2.0+cu118<br />
[pip3] torchlaunch==1.0<br />
[pip3] torchvision==0.17.0+cu118<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 18:01:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121943</guid>
    </item>
    <item>
      <title>[Inductor] [ReImplement] Outer Loop Fusion for CPP Backend</title>
      <link>https://github.com/pytorch/pytorch/pull/121625</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121625</p>
<p><strong>Summary</strong><br />
Re-implement of https://github.com/pytorch/pytorch/pull/121064</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_outer_loop_fusion</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 10 Mar 2024 23:44:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121625</guid>
    </item>
    <item>
      <title>Add registration API for torch.compile-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/121387</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we intend to provide a registration API dedicated to eager-through-torch.compile. The major workflow of this API will be as follows.</p>
<ul>
<li>Load cache</li>
<li>Check cache according to the input tensors</li>
<li>Cache Hit: Run the cached kernel directly</li>
<li>Cache Miss: Run the registered python kernel</li>
</ul>
<p>Currently, this PR always fallback to python kernel now and cache mechanism will be implemented in another PR.</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120595<br />
* #116449<br />
* #116368<br />
* #121727<br />
* <strong>-&gt;</strong> #121387<br />
* #121296</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121387</guid>
    </item>
    <item>
      <title>[torch.compile] `index_select` out of bound read</title>
      <link>https://github.com/pytorch/pytorch/issues/121251</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When using <code>torch.compile</code>, <code>index_select</code> will out-of-bound read</p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(42)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()

def forward(self, x1):
    v = torch.index_select(x1, 0, torch.tensor([2]))
    return v
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(2, 2, 2, 2)</p>
<p>with torch.no_grad():<br />
    func1 = torch.compile(func)<br />
    print(func1(x.clone()))<br />
    # tensor([[[[1.0743e+23, 3.0618e-41],<br />
    #       [9.1084e-44, 0.0000e+00]],</p>
<pre><code>#      [[3.2230e-44, 0.0000e+00],
#       [3.2230e-44, 0.0000e+00]]]])

print(func(x.clone()))
# IndexError: index out of range in self
</code></pre>
<p>```</p>
<h3>Versions</h3>
<p>```<br />
Versions<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 10:35:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121251</guid>
    </item>
    <item>
      <title>[torch.compile] `merge_stack_tahn_unbind` returns a tensor with wrong values</title>
      <link>https://github.com/pytorch/pytorch/issues/121227</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When using <code>torch.compile</code> and enabling <code>TORCHINDUCTOR_FREEZING=1</code>, <code>merge_stack_tahn_unbind</code> returns a tensor with wrong values. More specifically, the tensor's value is mistakenly placed compared to the tensor computed by naive execution for the model with <code>split</code> + <code>stack</code> + <code>tanh</code></p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(42)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()

def forward(self, x1):
    v1 = torch.split(x1, [3, 3, 3], dim=-1)
    v2 = torch.stack(v1, dim=-1)
    v3 = torch.tanh(v2)
    return v3
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(1, 9)</p>
<p>with torch.no_grad():<br />
    print(func(x.clone()))<br />
    # tensor([[[ 0.3245,  0.2263,  0.9761],<br />
    #      [ 0.1281, -0.8086, -0.5635],<br />
    #      [ 0.2303, -0.1842,  0.4314]]])</p>
<pre><code>func1 = torch.compile(func)
print(func1(x.clone()))
# tensor([[[ 0.3245,  0.1281,  0.2303],
#      [ 0.2263, -0.8086, -0.1842],
#      [ 0.9761, -0.5635,  0.4314]]])
</code></pre>
<p>```</p>
<p>I think the root cause is <code>dim=-1</code>, that <code>merge_stack_tahn_unbind</code> doesn't consider the case when dim is negative. This is because when I replace <code>dim=-1</code> with <code>dim=2</code>, it will return the consistent result</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 06:19:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121227</guid>
    </item>
    <item>
      <title>[`torch.compile`] Inductor gets stuck, unbounded RAM usage</title>
      <link>https://github.com/pytorch/pytorch/issues/121197</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When I run a GNN workload with a specific dataset, the compilation process gets stuck on step 2.</p>
<h3>Error logs</h3>
<p><code>[2024-03-05 04:00:37,435] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env
[2024-03-05 04:00:37,668] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s0 = 341523 for L['x'].size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,669] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s1 = 128 for L['x'].size()[1] [2, 9223372036854775806]
[2024-03-05 04:00:37,853] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s2 = 471405 for L['subgraphs'][0].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,856] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s3 = 95082 for L['subgraphs'][0].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,884] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s3 - 1 &lt;= s0 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:37,951] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s3 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:37,951] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s3 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] set_replacement s1 = 128
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] set_replacement s1 = 128
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s1, 128) [guard added] at torch_geometric/nn/dense/linear.py:147 in forward (_meta_registrations.py:1980 in meta_mm)
[2024-03-05 04:00:38,017] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s4 = 124196 for L['subgraphs'][1].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,020] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s5 = 12758 for L['subgraphs'][1].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,039] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s5 - 1 &lt;= s3 - 1 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:38,098] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s5 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:38,099] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s5 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:38,143] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s6 = 12033 for L['subgraphs'][2].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,147] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s7 = 1025 for L['subgraphs'][2].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,165] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s7 - 1 &lt;= s5 - 1 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:38,229] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s7 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:38,230] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s7 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:38,277] [0/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2024-03-05 04:00:38,283] [0/0] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor</code></p>
<h3>Minified repro</h3>
<p>Running with the environment variable gets stuck too, producing no output.</p>
<p>The code in question is here: https://github.com/dmlc/dgl/pull/7183</p>
<p>You can install DGL nightly build and install torch_geometric via pip.</p>
<p><code>bash
pip install --pre dgl -f https://data.dgl.ai/wheels-test/cu121/repo.html
pip install torch_geometric
python dgl/examples/sampling/graphbolt/pyg/node_classification_advanced.py --dataset=ogbn-papers100M --torch-compile</code></p>
<h3>Versions</h3>
<details><summary>Details</summary>
<p>

Collecting environment information...
PyTorch version: 2.3.0a0+ebedce2
Is debug build: False
CUDA used to build PyTorch: 12.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.28.1
Libc version: glibc-2.35

Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.3.107
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100-SXM4-80GB
GPU 1: NVIDIA A100-SXM4-80GB
GPU 2: NVIDIA A100-SXM4-80GB
GPU 3: NVIDIA A100-SXM4-80GB

Nvidia driver version: 545.23.08
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      43 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             256
On-line CPU(s) list:                0-255
Vendor ID:                          AuthenticAMD
Model name:                         AMD EPYC 7702 64-Core Processor
CPU family:                         23
Model:                              49
Thread(s) per core:                 2
Core(s) per socket:                 64
Socket(s):                          2
Stepping:                           0
Frequency boost:                    enabled
CPU max MHz:                        2183.5930
CPU min MHz:                        1500.0000
BogoMIPS:                           4000.17
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es
Virtualization:                     AMD-V
L1d cache:                          4 MiB (128 instances)
L1i cache:                          4 MiB (128 instances)
L2 cache:                           64 MiB (128 instances)
L3 cache:                           512 MiB (32 instances)
NUMA node(s):                       2
NUMA node0 CPU(s):                  0-63,128-191
NUMA node1 CPU(s):                  64-127,192-255
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow: Mitigation; safe RET
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.24.4
[pip3] onnx==1.15.0rc2
[pip3] optree==0.10.0
[pip3] pytorch-quantization==2.1.2
[pip3] torch==2.3.0a0+ebedce2
[pip3] torch_geometric==2.5.0
[pip3] torch-tensorrt==2.3.0a0
[pip3] torchdata==0.7.1a0
[pip3] torchmetrics==1.3.1
[pip3] torchtext==0.17.0a0
[pip3] torchvision==0.18.0a0
[pip3] triton==2.2.0+e28a256
[conda] Could not collect


</p>
</details>

<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Mon, 04 Mar 2024 20:12:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121197</guid>
    </item>
    <item>
      <title>'torch.compile()' causes 'Accuracy failed: uint8 tensor did not match'</title>
      <link>https://github.com/pytorch/pytorch/issues/121069</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I encountered an accuracy error when calling <code>torch.compile</code> to train moco on V100.  While there is no problem with the model running on normal calls, the model accuracy returned by torch.compile is poor.</p>
<p>I tried:<br />
<code>$ TORCHDYNAMO_REPRO_AFTER="aot"  TORCHDYNAMO_REPRO_LEVEL=4  python script.py
  ...
  torch._dynamo.debug_utils.AccuracyError: Bad accuracy detected</code><br />
```<br />
$ python run_2024_03_02_07_19_53_491950-pid_111736/minifier/minifier_launcher.py<br />
  Loading inputs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:00&lt;00:00, 108.27it/s]</p>
<blockquote>
<blockquote>
<p>[2024-03-02 07:31:19,733] torch._inductor.debug: [WARNING] model<strong><em>0 debug trace: /tmp/torchinductor</em>root/fd/cfdnftfl2xq7j6wlxq6fpsufxfazwh2vaquexbcmnsd67rujg3ph.debug<br />
 [2024-03-02 07:31:25,343] torch._inductor.debug: [WARNING] model</strong>_0 debug trace: /tmp/torchinductor_root/fd/cfdnftfl2xq7j6wlxq6fpsufxfazwh2vaquexbcmnsd67rujg3ph.debug<br />
 [2024-03-02 07:31:26,150] torch._dynamo.utils: [ERROR] Accuracy failed: uint8 tensor did not match<br />
  ...<br />
RuntimeError: Input graph did not fail the tester<br />
```</p>
</blockquote>
</blockquote>
<p>I tried below instead:<br />
<code>$ TORCHDYNAMO_REPRO_AFTER="dynamo" TORCHDYNAMO_REPRO_LEVEL=4 python script.py
 ...
 [rank0]:[2024-03-02 07:51:03,687] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='compile_fn' raised:
[rank0]:[2024-03-02 07:51:03,687] torch._dynamo.convert_frame: [WARNING] AccuracyError: Bad accuracy detected.
...</code></p>
<p><code>python repro.py 
Loading inputs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 104/104 [00:00&lt;00:00, 758.15it/s]
[2024-03-02 08:00:59,878] torch._dynamo.utils: [ERROR] Accuracy failed: uint8 tensor did not match
Traceback (most recent call last):
  File "repro.py", line 756, in &lt;module&gt;
    run_repro(mod, load_args, accuracy=True, command='run', save_dir='/public/moco/torch_compile_debug/run_2024_03_02_07_19_53_491950-pid_111736/minifier/checkpoints', tracing_mode='real', check_str=None)
  File "/usr/local/lib/python3.8/site-packages/torch/_dynamo/repro/after_aot.py", line 927, in run_repro
    COMMAND_FNS[options.command](options, mod, load_args)
  File "/usr/local/lib/python3.8/site-packages/torch/_dynamo/repro/after_aot.py", line 694, in repro_run
    raise AccuracyError("Bad accuracy detected")
torch._dynamo.debug_utils.AccuracyError: Bad accuracy detected</code></p>
<h3>Versions</h3>
<p>PyTorch version: 2.1.0+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.5 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.27.7<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10)  [GCC 10.3.0] (64-bit runtime)<br />
Python platform: Linux-4.18.0-372.9.1.el8.x86_64-x86_64-with-glibc2.10<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A800 80GB PCIe<br />
GPU 1: NVIDIA A800 80GB PCIe<br />
GPU 2: NVIDIA A800 80GB PCIe<br />
GPU 3: Tesla V100S-PCIE-32GB<br />
GPU 4: Tesla V100S-PCIE-32GB<br />
GPU 5: Tesla V100S-PCIE-32GB<br />
GPU 6: NVIDIA A800 80GB PCIe</p>
<p>Nvidia driver version: 535.54.03<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.6.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.6.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   43 bits physical, 48 bits virtual<br />
CPU(s):                          128<br />
On-line CPU(s) list:             0-127<br />
Thread(s) per core:              2<br />
Core(s) per socket:              32<br />
Socket(s):                       2<br />
NUMA node(s):                    8<br />
Vendor ID:                       HygonGenuine<br />
CPU family:                      24<br />
Model:                           2<br />
Model name:                      Hygon C86 7385 32-core Processor<br />
Stepping:                        2<br />
CPU MHz:                         2439.193<br />
BogoMIPS:                        3999.97<br />
Virtualization:                  AMD-V<br />
L1d cache:                       2 MiB<br />
L1i cache:                       4 MiB<br />
L2 cache:                        32 MiB<br />
L3 cache:                        128 MiB<br />
NUMA node0 CPU(s):               0-7,64-71<br />
NUMA node1 CPU(s):               8-15,72-79<br />
NUMA node2 CPU(s):               16-23,80-87<br />
NUMA node3 CPU(s):               24-31,88-95<br />
NUMA node4 CPU(s):               32-39,96-103<br />
NUMA node5 CPU(s):               40-47,104-111<br />
NUMA node6 CPU(s):               48-55,112-119<br />
NUMA node7 CPU(s):               56-63,120-127<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP disabled, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev sev_es</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==3.7.9<br />
[pip3] functorch==0.3.0a0<br />
[pip3] numpy==1.22.2<br />
[pip3] onnx==1.12.0<br />
[pip3] pytorch-quantization==2.1.2<br />
[pip3] torch==2.1.0<br />
[pip3] torch-tensorrt==1.3.0a0<br />
[pip3] torchaudio==2.0.1<br />
[pip3] torchtext==0.11.0a0<br />
[pip3] torchvision==0.15.1<br />
[pip3] triton==2.1.0<br />
[conda] functorch                 0.3.0a0                  pypi_0    pypi<br />
[conda] mkl                       2020.4             h726a3e6_304    conda-forge<br />
[conda] mkl-include               2020.4             h726a3e6_304    conda-forge<br />
[conda] numpy                     1.22.2           py38h6ae9a64_0    conda-forge<br />
[conda] pytorch-quantization      2.1.2                    pypi_0    pypi<br />
[conda] torch                     2.1.0                    pypi_0    pypi<br />
[conda] torch-tensorrt            1.3.0a0                  pypi_0    pypi<br />
[conda] torchaudio                2.0.1                    pypi_0    pypi<br />
[conda] torchtext                 0.11.0a0                 pypi_0    pypi<br />
[conda] torchvision               0.15.1                   pypi_0    pypi<br />
[conda] triton                    2.1.0                    pypi_0    pypi</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @wconstab</p>]]></description>
      <pubDate>Sat, 02 Mar 2024 00:06:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121069</guid>
    </item>
    <item>
      <title>can't compile torchvision RPN with AOTInductor</title>
      <link>https://github.com/pytorch/pytorch/issues/121036</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I try to compile an object detection model that uses torchvison's rpn module. it fails when generating anchors with <code>AssertionError: Mutating module attribute cell_anchors during export.</code> I think this is related to https://github.com/pytorch/pytorch/issues/108805</p>
<h3>Error logs</h3>
<p>```python<br />
I0301 19:15:52.323000 140521716340544 torch/fx/experimental/symbolic_shapes.py:1869] [0/0] create_env<br />
I0301 19:15:52.435000 140521716340544 torch/fx/experimental/symbolic_shapes.py:2620] [0/0] create_symbol s0 = 2 for L['batch_tensor'].size()[0] [2, 13] (_dynamo/variables/builder.py:1791 in <lambda>)<br />
V0301 19:15:52.437000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval True == True [statically known]<br />
V0301 19:15:52.437000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval False == False [statically known]<br />
V0301 19:15:52.537000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(s0, 9223372036854775807) == False [statically known]<br />
V0301 19:15:52.538000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval False == False [statically known]<br />
V0301 19:15:52.539000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval s0 &lt; 9223372036854775807 == True [statically known]<br />
V0301 19:15:52.575000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Ne(s0, 1) == True [statically known]<br />
V0301 19:15:52.592000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(s0, 1) == False [statically known]<br />
V0301 19:15:52.659000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval True == True [statically known]<br />
V0301 19:16:11.974000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(3<em>s0, 3) == False [statically known]<br />
V0301 19:16:11.975000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Ne(3</em>s0, 3) == True [statically known]<br />
I0301 19:16:12.160000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3622] [0/0] set_replacement s0 = 2 (solve_backed) ValueRanges(lower=2, upper=2, is_bool=False)<br />
I0301 19:16:12.162000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3622] [0/0] set_replacement s0 = 2 (find) ValueRanges(lower=2, upper=2, is_bool=False)<br />
I0301 19:16:12.163000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3832] [0/0] eval Eq(s0, 2) [guard added] at torchvision/models/detection/transform.py:243 in batch_images (_dynamo/variables/tensor.py:892 in evaluate_expr)<br />
V0301 19:16:12.171000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3902] [0/0] eval 2 [trivial]</p>
<hr />
<p>AssertionError                            Traceback (most recent call last)<br />
Cell In[2], line 15<br />
     12 width_dim = torch.export.Dim("width")<br />
     14 if not os.path.exists(model_path):<br />
---&gt; 15     so_path = torch._export.aot_compile(<br />
     16         f = model,<br />
     17         args = (x, ),<br />
     18         # Specify the first dimension of the input x as dynamic<br />
     19         dynamic_shapes={"batch_tensor": {0: batch_dim}},<br />
     20         # Specify the generated shared library path<br />
     21         options={<br />
     22             "aot_inductor.output_path": model_path,<br />
     23             "max_autotune": True,<br />
     24         },<br />
     25     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_export/<strong>init</strong>.py:382, in aot_compile(f, args, kwargs, constraints, dynamic_shapes, options, remove_runtime_assertions, disable_constraint_solver)<br />
    378     gm = torch.export._trace._export(f, args, kwargs, constraints, pre_dispatch=True).module()<br />
    379 else:<br />
    380     # We want to export to Torch IR here to utilize the pre_grad passes in<br />
    381     # inductor, which run on Torch IR.<br />
--&gt; 382     gm = _export_to_torch_ir(<br />
    383         f,<br />
    384         args,<br />
    385         kwargs,<br />
    386         constraints,<br />
    387         disable_constraint_solver=disable_constraint_solver,<br />
    388         # Disabling this flag, because instead we can rely on the mapping<br />
    389         # dynamo_flat_name_to_original_fqn which is coming from Dynamo.<br />
    390         restore_fqn=False,<br />
    391     )<br />
    392 flat_example_inputs = pytree.arg_tree_leaves(<em>args, </em>*(kwargs or {}))<br />
    394 with torch.no_grad():</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/export/_trace.py:320, in _export_to_torch_ir(f, args, kwargs, constraints, preserve_module_call_signature, disable_constraint_solver, restore_fqn, _log_export_usage)<br />
    316     module_call_specs: Dict[str, Dict[str, pytree.TreeSpec]] = {}<br />
    317     with _wrap_submodules(<br />
    318         f, preserve_module_call_signature, module_call_specs<br />
    319     ), _ignore_backend_decomps():<br />
--&gt; 320         gm_torch_level, _ = torch._dynamo.export(<br />
    321             f,<br />
    322             constraints=constraints,<br />
    323             assume_static_by_default=True,<br />
    324             tracing_mode="symbolic",<br />
    325             disable_constraint_solver=disable_constraint_solver,<br />
    326             _log_export_usage=_log_export_usage,<br />
    327         )(<br />
    328             <em>args,<br />
    329             </em>*kwargs,<br />
    330         )<br />
    331 except (ConstraintViolationError, ValueRangeError) as e:<br />
    332     raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: TRY200</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1314, in export.<locals>.inner(<em>args, </em><em>kwargs)<br />
   1312 # TODO(voz): We may have instances of <code>f</code> that mutate inputs, we should track sideeffects and reject.<br />
   1313 try:<br />
-&gt; 1314     result_traced = opt_f(</em>args, **kwargs)<br />
   1315 except ConstraintViolationError as e:<br />
   1316     constraint_violation_error = e</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1529, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1527     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1528 else:<br />
-&gt; 1529     return self._call_impl(*args, </strong>kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1538, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1533 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1534 # this function, and just call forward.<br />
   1535 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1536         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1537         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1538     return forward_call(</em>args, **kwargs)<br />
   1540 try:<br />
   1541     result = None</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:455, in _TorchDynamoContext.<strong>call</strong>.<locals>._fn(<em>args, </em><em>kwargs)<br />
    453 prior = set_eval_frame(callback)<br />
    454 try:<br />
--&gt; 455     return fn(</em>args, **kwargs)<br />
    456 finally:<br />
    457     set_eval_frame(prior)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1529, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1527     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1528 else:<br />
-&gt; 1529     return self._call_impl(*args, </strong>kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1538, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1533 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1534 # this function, and just call forward.<br />
   1535 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1536         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1537         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1538     return forward_call(</em>args, **kwargs)<br />
   1540 try:<br />
   1541     result = None</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:912, in catch_errors_wrapper.<locals>.catch_errors(frame, cache_entry, frame_state)<br />
    908             return hijacked_callback(frame, cache_entry, hooks, frame_state)<br />
    910 with compile_lock, _disable_current_modes():<br />
    911     # skip=1: skip this frame<br />
--&gt; 912     return callback(frame, cache_entry, hooks, frame_state, skip=1)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:398, in convert_frame_assert.<locals>._convert_frame_assert(frame, cache_entry, hooks, frame_state, skip)<br />
    384 compile_id = CompileId(frame_id, frame_compile_id)<br />
    386 signpost_event(<br />
    387     "dynamo",<br />
    388     "_convert_frame_assert._compile",<br />
   (...)<br />
    395     },<br />
    396 )<br />
--&gt; 398 return _compile(<br />
    399     frame.f_code,<br />
    400     frame.f_globals,<br />
    401     frame.f_locals,<br />
    402     frame.f_builtins,<br />
    403     compiler_fn,<br />
    404     one_graph,<br />
    405     export,<br />
    406     export_constraints,<br />
    407     hooks,<br />
    408     cache_size,<br />
    409     frame,<br />
    410     frame_state=frame_state,<br />
    411     compile_id=compile_id,<br />
    412     skip=skip + 1,<br />
    413 )</p>
<p>File /opt/conda/lib/python3.10/contextlib.py:79, in ContextDecorator.<strong>call</strong>.<locals>.inner(<em>args, </em><em>kwds)<br />
     76 @wraps(func)<br />
     77 def inner(</em>args, <strong>kwds):<br />
     78     with self._recreate_cm():<br />
---&gt; 79         return func(*args, </strong>kwds)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:669, in _compile(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)<br />
    659 log.debug(<br />
    660     "torchdynamo start compiling %s %s:%s, stack (elided %s frames):\n%s",<br />
    661     code.co_name,<br />
   (...)<br />
    666     "".join(traceback.format_list(traceback.extract_stack()[: -2 - skip])),<br />
    667 )<br />
    668 try:<br />
--&gt; 669     guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
    670     return guarded_code<br />
    671 except (<br />
    672     Unsupported,<br />
    673     TorchRuntimeError,<br />
   (...)<br />
    680     BisectValidationException,<br />
    681 ) as e:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py:256, in dynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper(<em>args, </em><em>kwargs)<br />
    254 with torch.profiler.record_function(f"{key} (dynamo_timed)"):<br />
    255     t0 = time.time()<br />
--&gt; 256     r = func(</em>args, **kwargs)<br />
    257     time_spent = time.time() - t0<br />
    258 compilation_time_metrics[key].append(time_spent)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:542, in _compile.<locals>.compile_inner(code, one_graph, hooks, transform)<br />
    540 CompileContext.get().attempt = attempt<br />
    541 try:<br />
--&gt; 542     out_code = transform_code_object(code, transform)<br />
    543     break<br />
    544 except exc.RestartAnalysis as e:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1033, in transform_code_object(code, transformations, safe)<br />
   1030 instructions = cleaned_instructions(code, safe)<br />
   1031 propagate_line_nums(instructions)<br />
-&gt; 1033 transformations(instructions, code_options)<br />
   1034 return clean_and_assemble_instructions(instructions, keys, code_options)[1]</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:163, in preserve_global_state.<locals>._fn(<em>args, </em><em>kwargs)<br />
    161 cleanup = setup_compile_debug()<br />
    162 try:<br />
--&gt; 163     return fn(</em>args, **kwargs)<br />
    164 finally:<br />
    165     cleanup.close()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:507, in _compile.<locals>.transform(instructions, code_options)<br />
    505 try:<br />
    506     with tracing(tracer.output.tracing_context), tracer.set_current_tx():<br />
--&gt; 507         tracer.run()<br />
    508 except exc.UnspecializeRestartAnalysis:<br />
    509     speculation_log.clear()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2130, in InstructionTranslator.run(self)<br />
   2129 def run(self):<br />
-&gt; 2130     super().run()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1308, in InstructionTranslatorBase.STORE_ATTR(self, inst)<br />
   1303 val, obj = self.popn(2)<br />
   1305 if isinstance(obj, NNModuleVariable):<br />
   1306     # We don't allow side effects during export<br />
   1307     # https://github.com/pytorch/torchdynamo/issues/1475<br />
-&gt; 1308     assert (<br />
   1309         not self.export<br />
   1310     ), f"Mutating module attribute {inst.argval} during export."<br />
   1312 try:<br />
   1313     BuiltinVariable(setattr).call_function(<br />
   1314         self, [obj, ConstantVariable.create(inst.argval), val], {}<br />
   1315     )</p>
<p>AssertionError: Mutating module attribute cell_anchors during export.</p>
<p>from user code:<br />
   File "/opt/workspace/./satlas-src/satlas/model/model.py", line 841, in forward<br />
    cur_outputs, _ = head(batch_tensor, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/opt/workspace/<a href="http://127.0.0.1:8888/lab/tree/satlas-src/satlas/model/model.py#line=530">./satlas-src/satlas/model/model.py", line 531</a>, in forward<br />
    proposals, proposal_losses = self.rpn(images, features, targets)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/rpn.py", line 362, in forward<br />
    anchors = self.anchor_generator(images, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 126, in forward<br />
    self.set_cell_anchors(dtype, device)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 77, in set_cell_anchors<br />
    self.cell_anchors = [cell_anchor.to(dtype=dtype, device=device) for cell_anchor in self.cell_anchors]<br />
```</p>
<h3>Minified repro</h3>
<p>os.environ["TORCH_DYNAMO_REPRO_AFTER"] = "aot" doesn't seem to do anything. after I run the code, the traceback is the same.</p>
<p>a minimal repro is to do</p>
<p>```python<br />
import torch<br />
import torch._dynamo as torchdynamo<br />
from torchvision.models.detection import (<br />
    maskrcnn_resnet50_fpn,<br />
)</p>
<p>import torch._dynamo.config</p>
<h1>torch._dynamo.config.capture_scalar_outputs = True</h1>
<p>net = maskrcnn_resnet50_fpn()<br />
net.eval()</p>
<h1>Single input which is a list of images.</h1>
<p>images = [torch.rand(3, 16, 16)]</p>
<h1>Double-check that the inputs work for the normal net.</h1>
<p>net(images)<br />
torch._dynamo.export(net, images, tracing_mode="symbolic", aten_graph=True)</p>
<p>```</p>
<p>```python<br />
AssertionError: Mutating module attribute cell_anchors during export.</p>
<p>from user code:<br />
   File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py", line 104, in forward<br />
    proposals, proposal_losses = self.rpn(images, features, targets)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/rpn.py", line 362, in forward<br />
    anchors = self.anchor_generator(images, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(</em>args, **kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 126, in forward<br />
    self.set_cell_anchors(dtype, device)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 77, in set_cell_anchors<br />
    self.cell_anchors = [cell_anchor.to(dtype=dtype, device=device) for cell_anchor in self.cell_anchors]</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information<br />
```</p>
<p>from the test script here: https://docs.google.com/document/d/159NTQQhz8ovIBxbQvGQ-fZ10pF9e2RPXm1JZYqdEzt4/edit from this issue https://github.com/pytorch/pytorch/issues/108805</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.3.0.dev20240221<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.26.4<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.1.105<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090<br />
Nvidia driver version: 535.161.07<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      39 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             12<br />
On-line CPU(s) list:                0-11<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz<br />
CPU family:                         6<br />
Model:                              158<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 6<br />
Socket(s):                          1<br />
Stepping:                           10<br />
CPU max MHz:                        4600.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           6399.96<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp vnmi md_clear flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          192 KiB (6 instances)<br />
L1i cache:                          192 KiB (6 instances)<br />
L2 cache:                           1.5 MiB (6 instances)<br />
L3 cache:                           12 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-11<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable<br />
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Meltdown:             Mitigation; PTI<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Mitigation; IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Mitigation; Microcode<br />
Vulnerability Tsx async abort:      Mitigation; TSX disabled</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==5.0.4<br />
[pip3] mypy==1.0.1<br />
[pip3] mypy-extensions==0.4.4<br />
[pip3] numpy==1.26.4<br />
[pip3] optree==0.10.0<br />
[pip3] torch==2.3.0.dev20240221<br />
[pip3] torchaudio==2.2.0.dev20240221<br />
[pip3] torchelastic==0.2.2<br />
[pip3] torchvision==0.18.0.dev20240221<br />
[pip3] triton==2.2.0<br />
[pip3] vit-pytorch==1.6.5<br />
[conda] blas                      1.0                         mkl<br />
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch-nightly<br />
[conda] mkl                       2023.1.0         h213fc3f_46344<br />
[conda] mkl-service               2.4.0           py310h5eee18b_1<br />
[conda] mkl_fft                   1.3.8           py310h5eee18b_0<br />
[conda] mkl_random                1.2.4           py310hdb19cb5_0<br />
[conda] numpy                     1.26.4                   pypi_0    pypi<br />
[conda] optree                    0.10.0                   pypi_0    pypi<br />
[conda] pytorch                   2.3.0.dev20240221 py3.10_cuda12.1_cudnn8.9.2_0    pytorch-nightly<br />
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch-nightly<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly<br />
[conda] torchaudio                2.2.0.dev20240221     py310_cu121    pytorch-nightly<br />
[conda] torchelastic              0.2.2                    pypi_0    pypi<br />
[conda] torchvision               0.18.0.dev20240221     py310_cu121    pytorch-nightly<br />
[conda] triton                    2.2.0                    pypi_0    pypi<br />
[conda] vit-pytorch               1.6.5                    pypi_0    pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4</p>]]></description>
      <pubDate>Fri, 01 Mar 2024 11:34:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121036</guid>
    </item>
    <item>
      <title>[inductor][cpp] unified the vectorized conversion with `at::vec::convert` for all data types</title>
      <link>https://github.com/pytorch/pytorch/pull/119979</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #119979<br />
* #119734<br />
* #119655<br />
* #119654</p>
<p>This PR unified the vectorized conversion with <code>at::vec::convert</code> for all vectorized data types. The intrinsics implementations are implemented as a specialization and moved to their own arch-specific files. The vectorized conversion logic in cpp Inductor is simplified.</p>
<p>cc @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Wed, 14 Feb 2024 21:46:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119979</guid>
    </item>
    <item>
      <title>[inductor][cpp] simplify CppVecKernelChecker (remove bool/int8 load as mask and load as float flags)</title>
      <link>https://github.com/pytorch/pytorch/pull/119734</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #119979<br />
* <strong>-&gt;</strong> #119734<br />
* #119655<br />
* #119654</p>
<p>cc @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Mon, 12 Feb 2024 16:39:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119734</guid>
    </item>
    <item>
      <title>[inductor][cpp] support vectorized indirect indexing</title>
      <link>https://github.com/pytorch/pytorch/pull/119655</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #119979<br />
* #119734<br />
* <strong>-&gt;</strong> #119655<br />
* #119654</p>
<p>This PR adds the vectorized indirect indexing so that we can further simplify the <code>CppVecKernelChecker</code> (done in the later PR #119734) and remove the check that throws <code>CppVecUnsupportedError</code>. A boundary assertion check is added on vectorized indices and via the new <code>indirect_assert</code> method on <code>Kernel</code> - the base implementation is for scalar indices, overridden in <code>CppVecKernel</code> for vectorized indices.</p>
<p>cc @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Sun, 11 Feb 2024 07:08:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119655</guid>
    </item>
    <item>
      <title>[inductor][cpp] generalize vector mask for dtypes</title>
      <link>https://github.com/pytorch/pytorch/pull/119654</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #119979<br />
* #119734<br />
* #119655<br />
* <strong>-&gt;</strong> #119654</p>
<p>Vectorized boolean values in CPU Inductor were modeled with <code>Vectorized&lt;float&gt;</code> which cannot work for operations with other data types. This PR generalizes it with the new <code>VecMask</code> template class that can work for masks on any vectorized data types. The intrinsics implementation in <code>cpp_prefix.h</code> for mask conversion, cast and masked load are now implemented as the specialization for <code>VecMask</code> and moved to corresponding header files.</p>
<p>cc @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 11 Feb 2024 07:08:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119654</guid>
    </item>
    <item>
      <title>Different behaviors in `torch.nn.functional.hinge_embedding_loss` between eagermode and torch.compile</title>
      <link>https://github.com/pytorch/pytorch/issues/118175</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>with a specific input in <a href="https://github.com/pytorch/pytorch/files/14034247/inputs.pkl.zip">inputs.pkl.zip</a>, outputs on eagermode and torch.compile mode are different.</p>
<p>```<br />
import torch<br />
import torch.nn as nn<br />
import os, pickle</p>
<p>with open('inputs.pkl', 'rb') as f:<br />
    inputs = pickle.load(f) # 'p1': ..., 'p2': ...</p>
<p>class Model(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
    def forward(self, target):<br />
        target = torch.nn.functional.hinge_embedding_loss(target=target, input=inputs[0]['input'], margin=7.2, reduction='none')      <br />
        return target</p>
<p>input1 = torch.rand([10, 9, 8, 6, 8], dtype=torch.float32)<br />
input2 = input1.clone()</p>
<p>model = Model().to(torch.device('cpu'))<br />
eag = model(input1)<br />
opt = torch.compile(model.forward)(input2)</p>
<p>same_val = torch.allclose(eag.to('cpu'), <br />
                            opt.to('cpu'), <br />
                            rtol=1e-3, atol=1e-3, <br />
                            equal_nan=True)<br />
if same_val == False : <br />
        raise ValueError('diff value')<br />
<code>error stracktrace</code><br />
    raise ValueError('diff value')<br />
ValueError: diff value<br />
```</p>
<h3>Versions</h3>
<p>PyTorch version: 2.3.0.dev20240110+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.17 (main, Jul 5 2023, 20:41:20) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.1.105<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration:<br />
GPU 0: NVIDIA GeForce RTX 2070<br />
GPU 1: NVIDIA GeForce RTX 2070<br />
GPU 2: NVIDIA GeForce RTX 2070<br />
GPU 3: NVIDIA GeForce RTX 2070</p>
<p>Nvidia driver version: 535.104.12<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 32<br />
On-line CPU(s) list: 0-31<br />
Vendor ID: GenuineIntel<br />
Model name: Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz<br />
CPU family: 6<br />
Model: 63<br />
Thread(s) per core: 2<br />
Core(s) per socket: 8<br />
Socket(s): 2<br />
Stepping: 2<br />
CPU max MHz: 3200.0000<br />
CPU min MHz: 1200.0000<br />
BogoMIPS: 4794.64<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear flush_l1d<br />
Virtualization: VT-x<br />
L1d cache: 512 KiB (16 instances)<br />
L1i cache: 512 KiB (16 instances)<br />
L2 cache: 4 MiB (16 instances)<br />
L3 cache: 40 MiB (2 instances)<br />
NUMA node(s): 2<br />
NUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30<br />
NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf: Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable<br />
Vulnerability Mds: Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Meltdown: Mitigation; PTI<br />
Vulnerability Mmio stale data: Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.3<br />
[pip3] pytorch-triton==2.2.0+e28a256d71<br />
[pip3] torch==2.3.0.dev20240110+cu118<br />
[pip3] torchaudio==2.2.0.dev20240110+cu118<br />
[pip3] torchvision==0.18.0.dev20240110+cu118<br />
[conda] cudatoolkit 11.8.0 h6a678d5_0 defaults<br />
[conda] numpy 1.24.3 pypi_0 pypi<br />
[conda] pytorch-triton 2.2.0+e28a256d71 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240110+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240110+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240110+cu118 pypi_0 pypi</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 23 Jan 2024 23:03:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/118175</guid>
    </item>
    <item>
      <title>PT2 + Mamba - accuracy error on inductor</title>
      <link>https://github.com/pytorch/pytorch/issues/117070</link>
      <description><![CDATA[<p>Repro in this PR:</p>
<p>https://github.com/alxndrTL/mamba.py/pull/1</p>
<p>Accuracy error w/ inductor</p>
<p>cc: @Chillee @ezyang @jansel </p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @wconstab @bdhirsh @anijain2305 @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Tue, 09 Jan 2024 15:19:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/117070</guid>
    </item>
    <item>
      <title>`assert_size_stride` bug in inductor generated code</title>
      <link>https://github.com/pytorch/pytorch/issues/115344</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Hi,</p>
<p>I'm trying to make my <a href="https://github.com/RobertCsordas/moe_layer/blob/master/triton_src/moe_layer/cvmm.py">MoE Triton kernel</a> work with torch.compile(). I know that this is not supported in the current stable version, but it is in the nightly (at least it worked with the simple kernels I tried). However, when I try to use my actual kernel, it fails.</p>
<p>There are two independent issues: one is the "Illegal getattr invocation stride in strict mode" issue (see the log below), which seems to prevent compilation but doesn't seem to be fatal. However the "RuntimeError: CUDA error: an illegal memory access was encountered" problem is fatal. Note that both the example and the full kernel work well without compile, and without running into illegal memory access issues.</p>
<p>Interestingly, if I remove an unused, static IF  (which is never true in this example, and it depends only on external arguments), the code works. Also, if there is only one option in the triton.autotune(), it works as well. I marked both places with a comment starting with "!!!!!!" in the code below.</p>
<p>This is the simplified code I was able to come up with:<br />
```python3<br />
import torch</p>
<p>import triton<br />
import triton.language as tl</p>
<h1>CVMM from: https://github.com/RobertCsordas/moe_layer/blob/master/triton_src/moe_layer/cvmm.py</h1>
<h1>Based on: https://github.com/openai/triton/blob/main/python/tutorials/03-matrix-multiplication.py</h1>
<p>from typing import Union, Optional<br />
from dataclasses import dataclass</p>
<p>@dataclass<br />
class CVMMSel:<br />
    raw_sel: torch.Tensor<br />
    sel: torch.Tensor<br />
    sel_index: torch.Tensor<br />
    out_index: Optional[torch.Tensor] = None</p>
<p>def cvmm_prepare_sel(sel: torch.Tensor, n_experts: int) -&gt; CVMMSel:<br />
    fsel = sel.flatten()<br />
    ssel, sel_index = fsel.sort()<br />
    return CVMMSel(sel, ssel.view_as(sel), sel_index, None)</p>
<h1>!!!!!! Leaving just one autotune config solves the "RuntimeError: CUDA error: an illegal memory access was</h1>
<h1>encountered" problem !!!!!!</h1>
<p>@triton.autotune(<br />
    configs=[<br />
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),<br />
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),<br />
    ],<br />
    key=['M', 'N', 'K']<br />
)<br />
@triton.jit<br />
def cvmm_kernel(<br />
    # Pointers to matrices<br />
    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,<br />
    # Matrix dimensions<br />
    M, N, K,<br />
    stride_cm, stride_cn,<br />
    stride_index, stride_sel, stride_out_index,<br />
    # Meta-parameters<br />
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,<br />
    GROUP_SIZE_M: tl.constexpr<br />
):<br />
    pid = tl.program_id(axis=0)</p>
<pre><code>num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
num_pid_in_group = GROUP_SIZE_M * num_pid_n
group_id = pid // num_pid_in_group
first_pid_m = group_id * GROUP_SIZE_M
group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
pid_n = (pid % num_pid_in_group) // group_size_m

pid_m = first_pid_m + (pid % group_size_m)

offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M

remap_offs_am = tl.load(index_ptr + stride_index * offs_am)

# Create offset pointers
c = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float16)

offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)


# !!!!!! Removing this IF solves the "RuntimeError: CUDA error: an illegal memory access was encountered" problem,
# even though it is always False in this example !!!!!!
# To test it, keep the else branch.
if out_index_ptr is not None:
    remap_offs_cm = tl.load(out_index_ptr + stride_out_index * offs_am)
else:
    remap_offs_cm = remap_offs_am

offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
c_ptrs = c_ptr + stride_cm * remap_offs_cm[:, None] + stride_cn * offs_cn[None, :]
c_mask = (offs_cm[:, None] &lt; M) &amp; (offs_cn[None, :] &lt; N)
tl.store(c_ptrs, c, mask=c_mask)
</code></pre>
<p>def cvmm_triton(x: torch.Tensor, sel_index: torch.Tensor, sel: torch.Tensor, keys: torch.Tensor, out_dtype: torch.dtype, out_index: Optional[torch.Tensor] = None):<br />
    x = x.flatten(end_dim=-2)<br />
    assert x.shape[-1] == keys.shape[1]</p>
<pre><code>sel_shape = sel.shape
sel = sel.flatten()

M = sel.shape[0]
O, K, N = keys.shape
# Allocates output.
out = torch.empty((M, N), device=x.device, dtype=out_dtype)

grid = lambda META: (
    triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
)

cvmm_kernel[grid](
    x, keys, out, sel_index, sel, None,
    M, N, K,
    out.stride(0), out.stride(1),
    sel_index.stride(0), sel.stride(0), 0,
)

return out.view(*sel_shape, N)
</code></pre>
<p>class CVMM(torch.autograd.Function):<br />
    warned = False</p>
<pre><code>@staticmethod
def forward(ctx, x: torch.Tensor, sel_index: torch.Tensor, sel: torch.Tensor, keys: torch.Tensor, out_index: Optional[torch.Tensor] = None):
    ctx.save_for_backward(x, keys, sel, sel_index, out_index)

    out_type = torch.float16 if torch.is_autocast_enabled() else x.dtype
    res = cvmm_triton(x, sel_index, sel, keys, out_type, out_index)
    ctx.op_type = out_type
    ctx.keys_type = keys.dtype
    ctx.is_autocast = torch.is_autocast_enabled()
    return res

@staticmethod
def backward(ctx, grad_output):
    x, keys, sel, sel_index, out_index = ctx.saved_tensors

    keys_dt = keys

    grad_x_full = cvmm_triton(grad_output, sel_index, sel, keys_dt.transpose(1,2), ctx.op_type, None)
    grad_x = grad_x_full.view_as(x)

    return grad_x, None, None, None, None
</code></pre>
<p>def cvmm(x: torch.Tensor, sel: Union[torch.Tensor, CVMMSel], keys: torch.Tensor):<br />
    if not isinstance(sel, CVMMSel):<br />
        sel = cvmm_prepare_sel(sel, keys.shape[0])</p>
<pre><code>return CVMM.apply(x, sel.sel_index, sel.sel, keys, sel.out_index)
</code></pre>
<h1>Compile test</h1>
<p>class Model(torch.nn.Module):<br />
    def forward(self, x, sel, w):<br />
        return cvmm(x, sel, w)</p>
<p>model = torch.compile(Model().cuda())</p>
<h1>model = Model().cuda()</h1>
<p>torch.manual_seed(0)<br />
n_experts = 8<br />
n_channels = 64<br />
expert_size = 64<br />
bs = 64</p>
<p>device = torch.device("cuda")<br />
dtype = torch.float16</p>
<p>keys = torch.nn.Parameter(torch.randn(n_experts, n_channels, expert_size, dtype=dtype, device=device))<br />
testvec = torch.randn(bs, n_channels, dtype=dtype, device=device)<br />
sel = torch.randint(0, n_experts, (bs,), dtype=torch.int32, device=device)</p>
<p>print(model(testvec, sel, keys).shape)</p>
<p>```</p>
<h3>Error logs</h3>
<p><code>[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [WARNING] speculate_subgraph: while introspecting the user-defined autograd.Function, we were unable to trace function `trampoline_autograd_bwd` into a single graph. This means that Dynamo was unable to prove safety for this API and will fall back to eager-mode PyTorch, which could lead to a slowdown.
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR] Illegal getattr invocation stride in strict mode
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR] Traceback (most recent call last):
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/higher_order_ops.py", line 231, in speculate_subgraph
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     output = f.call_function(tx, args, sub_kwargs)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 248, in call_function
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return super().call_function(tx, args, kwargs)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 81, in call_function
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_user_function_return(
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 688, in inline_user_function_return
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2261, in inline_call
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2376, in inline_call_
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 818, in run
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 781, in step
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 470, in wrapper
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return inner_fn(self, inst)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1252, in CALL_FUNCTION_EX
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.call_function(fn, argsvars.items, kwargsvars.items)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 652, in call_function
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.push(fn.call_function(self, args, kwargs))
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py", line 660, in call_function
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return self.obj.call_method(tx, self.name, args, kwargs)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py", line 505, in call_method
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_call(tx, backward, args, kwargs)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2261, in inline_call
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2376, in inline_call_
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 818, in run
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 781, in step
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 470, in wrapper
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return inner_fn(self, inst)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1213, in CALL_FUNCTION
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.call_function(fn, args, {})
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 652, in call_function
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.push(fn.call_function(self, args, kwargs))
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 248, in call_function
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return super().call_function(tx, args, kwargs)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 81, in call_function
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_user_function_return(
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 688, in inline_user_function_return
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2261, in inline_call
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2376, in inline_call_
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 818, in run
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 781, in step
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1303, in LOAD_ATTR
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     result = BuiltinVariable(getattr).call_function(
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py", line 651, in call_function
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     result = handler(tx, *args, **kwargs)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py", line 1229, in call_getattr
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return obj.var_getattr(tx, name).clone(source=source)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/tensor.py", line 218, in var_getattr
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     unimplemented(f"Illegal getattr invocation {name} in strict mode")
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/exc.py", line 193, in unimplemented
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     raise Unsupported(msg)
[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR] torch._dynamo.exc.Unsupported: Illegal getattr invocation stride in strict mode
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [WARNING] speculate_subgraph: while introspecting the user-defined autograd.Function, we were unable to trace function `trampoline_autograd_bwd` into a single graph. This means that Dynamo was unable to prove safety for this API and will fall back to eager-mode PyTorch, which could lead to a slowdown.
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR] Illegal getattr invocation stride in strict mode
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR] Traceback (most recent call last):
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/higher_order_ops.py", line 231, in speculate_subgraph
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     output = f.call_function(tx, args, sub_kwargs)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 248, in call_function
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return super().call_function(tx, args, kwargs)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 81, in call_function
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_user_function_return(
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 688, in inline_user_function_return
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2261, in inline_call
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2376, in inline_call_
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 818, in run
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 781, in step
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 470, in wrapper
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return inner_fn(self, inst)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1252, in CALL_FUNCTION_EX
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.call_function(fn, argsvars.items, kwargsvars.items)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 652, in call_function
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.push(fn.call_function(self, args, kwargs))
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py", line 660, in call_function
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return self.obj.call_method(tx, self.name, args, kwargs)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py", line 505, in call_method
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_call(tx, backward, args, kwargs)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2261, in inline_call
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2376, in inline_call_
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 818, in run
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 781, in step
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 470, in wrapper
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return inner_fn(self, inst)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1213, in CALL_FUNCTION
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.call_function(fn, args, {})
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 652, in call_function
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.push(fn.call_function(self, args, kwargs))
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 248, in call_function
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return super().call_function(tx, args, kwargs)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 81, in call_function
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_user_function_return(
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 688, in inline_user_function_return
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2261, in inline_call
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2376, in inline_call_
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 818, in run
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 781, in step
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1303, in LOAD_ATTR
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     result = BuiltinVariable(getattr).call_function(
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py", line 651, in call_function
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     result = handler(tx, *args, **kwargs)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py", line 1229, in call_getattr
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return obj.var_getattr(tx, name).clone(source=source)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/tensor.py", line 218, in var_getattr
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     unimplemented(f"Illegal getattr invocation {name} in strict mode")
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/exc.py", line 193, in unimplemented
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     raise Unsupported(msg)
[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR] torch._dynamo.exc.Unsupported: Illegal getattr invocation stride in strict mode
Traceback (most recent call last):
  File "/home/robert/rnn_generalization_test/compile_test3.py", line 167, in &lt;module&gt;
    print(model(testvec, sel, keys).shape)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/robert/rnn_generalization_test/compile_test3.py", line 148, in forward
    return cvmm(x, sel, w)
  File "/home/robert/rnn_generalization_test/compile_test3.py", line 141, in cvmm
    return CVMM.apply(x, sel.sel_index, sel.sel, keys, sel.out_index)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/autograd/function.py", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/robert/rnn_generalization_test/compile_test3.py", line 114, in forward
    @staticmethod
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 901, in forward
    return compiled_fn(full_args)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 81, in g
    return f(*args)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 94, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 105, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 118, in rng_functionalization_wrapper
    return compiled_fw(args)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 864, in __call__
    return self.get_current_callable()(inputs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 611, in run
    return model(new_inputs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 892, in _run_from_cache
    return compiled_graph.compiled_artifact(inputs)
  File "/tmp/torchinductor_robert/vr/cvrlscv7n2ne4vcxcvozlnvsvyiddsior53it7ifax2hfs2uosni.py", line 111, in call
    cvmm_kernel_0.run(a_ptr=arg0_1, b_ptr=arg1_1, c_ptr=buf0, index_ptr=arg3_1, sel_ptr=arg2_1, out_index_ptr=None, M=64, N=64, K=64, stride_cm=64, stride_cn=1, stride_index=1, stride_sel=1, stride_out_index=0, grid=grid_wrapper_for_cvmm_kernel_0, stream=stream0)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/triton_heuristics.py", line 540, in run
    self.autotune_to_one_config(*args, grid=grid, **kwargs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/triton_heuristics.py", line 444, in autotune_to_one_config
    timings = self.benchmark_all_configs(*args, **kwargs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/triton_heuristics.py", line 420, in benchmark_all_configs
    timings = {
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/triton_heuristics.py", line 421, in &lt;dictcomp&gt;
    launcher: self.bench(launcher, *args, **kwargs)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/triton_heuristics.py", line 392, in bench
    return do_bench(kernel_call, rep=40, fast_flush=True)
  File "/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/utils.py", line 167, in do_bench
    return triton_do_bench(*args, **kwargs)[0]
  File "/home/robert/.local/lib/python3.10/site-packages/triton/testing.py", line 103, in do_bench
    torch.cuda.synchronize()
  File "/home/robert/.local/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
    return torch._C._cuda_synchronize()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.</code></p>
<h3>Minified repro</h3>
<p>The minifier creates a directory structure which has no files. I'm not sure what I'm supposed to paste here. The directory structure looks like this:</p>
<p><code>~/rnn_generalization_test/torch_compile_debug &gt;&gt;&gt; find .                                                                                                                                                                                                                                                                                                    
.
./run_2023_12_07_14_23_32_124760-pid_906807
./run_2023_12_07_14_23_32_124760-pid_906807/minifier
./run_2023_12_07_14_23_32_124760-pid_906807/minifier/checkpoints</code></p>
<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.2.0.dev20231206+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Manjaro Linux (x86_64)<br />
GCC version: (GCC) 12.2.1 20230201<br />
Clang version: 15.0.7<br />
CMake version: version 3.25.0<br />
Libc version: glibc-2.37</p>
<p>Python version: 3.10.10 (main, Mar  5 2023, 22:26:53) [GCC 12.2.1 20230201] (64-bit runtime)<br />
Python platform: Linux-5.15.109-1-MANJARO-x86_64-with-glibc2.37<br />
Is CUDA available: True<br />
CUDA runtime version: 12.1.105<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA TITAN V<br />
Nvidia driver version: 530.41.03<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/libcudnn.so.8.8.0<br />
/usr/lib/libcudnn_adv_infer.so.8.8.0<br />
/usr/lib/libcudnn_adv_train.so.8.8.0<br />
/usr/lib/libcudnn_cnn_infer.so.8.8.0<br />
/usr/lib/libcudnn_cnn_train.so.8.8.0<br />
/usr/lib/libcudnn_ops_infer.so.8.8.0<br />
/usr/lib/libcudnn_ops_train.so.8.8.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Address sizes:                   39 bits physical, 48 bits virtual<br />
Byte Order:                      Little Endian<br />
CPU(s):                          12<br />
On-line CPU(s) list:             0-11<br />
Vendor ID:                       GenuineIntel<br />
Model name:                      Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz<br />
CPU family:                      6<br />
Model:                           158<br />
Thread(s) per core:              2<br />
Core(s) per socket:              6<br />
Socket(s):                       1<br />
Stepping:                        10<br />
CPU(s) scaling MHz:              25%<br />
CPU max MHz:                     3200.0000<br />
CPU min MHz:                     800.0000<br />
BogoMIPS:                        6402.62<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d arch_capabilities<br />
L1d cache:                       192 KiB (6 instances)<br />
L1i cache:                       192 KiB (6 instances)<br />
L2 cache:                        1.5 MiB (6 instances)<br />
L3 cache:                        12 MiB (1 instance)<br />
NUMA node(s):                    1<br />
NUMA node0 CPU(s):               0-11<br />
Vulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported<br />
Vulnerability L1tf:              Mitigation; PTE Inversion<br />
Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Meltdown:          Mitigation; PTI<br />
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:          Mitigation; IBRS<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:             Mitigation; Microcode<br />
Vulnerability Tsx async abort:   Mitigation; TSX disabled</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==6.0.0<br />
[pip3] kmeans-pytorch==0.3<br />
[pip3] numpy==1.24.2<br />
[pip3] pytorch-lightning==1.9.0<br />
[pip3] pytorch-triton==2.1.0+bcad9dabe1<br />
[pip3] torch==2.2.0.dev20231206+cu121<br />
[pip3] torch-dct==0.1.6<br />
[pip3] torch-tb-profiler==0.4.1<br />
[pip3] torchaudio==2.2.0.dev20231206+cu121<br />
[pip3] torchdata==0.6.1<br />
[pip3] torchmetrics==0.11.0<br />
[pip3] torchpq==0.3.0.5<br />
[pip3] torchtext==0.15.2<br />
[pip3] torchvision==0.17.0.dev20231206+cu121<br />
[conda] Could not collect<br />
```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @wconstab</p>]]></description>
      <pubDate>Thu, 07 Dec 2023 05:32:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/115344</guid>
    </item>
    <item>
      <title>[AOTAutograd] torch.compile under ambient `no_grad` is broken</title>
      <link>https://github.com/pytorch/pytorch/issues/114338</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I'm not sure how this is possible that this behaviour is occuring on main, but it is.</p>
<p>```python<br />
def fn(x):<br />
    with torch.enable_grad():<br />
        out = x + 1<br />
    return out</p>
<p>opt_fn = torch.compile(fn, backend="inductor")  # also, aot_eager, aot_eager_decomp_partition, but not eager<br />
with torch.no_grad():<br />
    res = fn(torch.zeros(10, requires_grad=True))<br />
    opt_res = opt_fn(torch.zeros(10, requires_grad=True))</p>
<pre><code>assert res.requires_grad == opt_res.requires_grad  # True != False
</code></pre>
<p>```</p>
<p>Traced Dynamo FX graph</p>
<p>```<br />
[2023-11-22 00:01:20,308] [0/0] torch.<em>dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH<br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]  ===== __compiled_fn_0 =====<br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]  <eval_with_key>.0 class GraphModule(torch.nn.Module):<br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]     def forward(self, L_x</em> : torch.Tensor):<br />
[2023-11-22 00:01:20,308] [0/0] torch.<em>dynamo.output_graph.__graph_code: [DEBUG]         l_x</em> = L_x_<br />
[2023-11-22 00:01:20,308] [0/0] torch.<em>dynamo.output_graph.__graph_code: [DEBUG]       <br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # No stacktrace found for following nodes<br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         _set_grad_enabled = torch._C._set_grad_enabled(True)<br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]       <br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /home/jonch/Desktop/sdpa.py:2169, code: out = x + 1<br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         out = l_x</em> + 1;  l_x_ = None<br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]       <br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # No stacktrace found for following nodes<br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         _set_grad_enabled_1 = torch._C._set_grad_enabled(False)<br />
[2023-11-22 00:01:20,308] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         return (out,)</p>
<p>```</p>
<h3>Versions</h3>
<p>main</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @wconstab @bdhirsh @anijain2305 </p>]]></description>
      <pubDate>Tue, 21 Nov 2023 21:00:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/114338</guid>
    </item>
    <item>
      <title>[compile] output does not match eager mode</title>
      <link>https://github.com/pytorch/pytorch/issues/100075</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>```python<br />
import torch<br />
from torch import nn<br />
from torchvision.models import resnet18<br />
from torch._dynamo import allow_in_graph<br />
from functools import wraps<br />
from functorch import make_functional_with_buffers, vmap, grad</p>
<p>def traceable(f):<br />
    f = allow_in_graph(f)</p>
<pre><code>@wraps(f)
def wrapper(*args, **kwargs):
    return f(*args, **kwargs)

return wrapper
</code></pre>
<p>torch.manual_seed(42)<br />
device = 'cpu'  # also fails on CUDA</p>
<p>model = resnet18(pretrained=False, norm_layer=(lambda c: nn.GroupNorm(min(c, 32), c)))<br />
model.to(device)<br />
model.eval()</p>
<p>fnet, params, buffers = make_functional_with_buffers(model)</p>
<p>x = torch.randn(10, 3, 224, 224, device=device)<br />
f = lambda p, b, x : fnet(p, b, x).sum()</p>
<h1>Works for this simpler function</h1>
<h1>f = lambda p, b, x: (torch.sin(x) + torch.cos(x) + torch.exp(x)).sum()</h1>
<p>f = grad(f)</p>
<p>expected = f(params, buffers, x)<br />
actual = torch.compile(traceable(f))(params, buffers, x)</p>
<p>torch.testing.assert_close(actual, expected)<br />
```</p>
<p>Output<br />
```<br />
AssertionError: Tensor-likes are not close!</p>
<p>Mismatched elements: 9406 / 9408 (100.0%)<br />
Greatest absolute difference: 0.12333643436431885 at index (0, 2, 3, 4) (up to 1e-05 allowed)<br />
Greatest relative difference: 41.227630615234375 at index (5, 0, 0, 6) (up to 1.3e-06 allowed)</p>
<p>```</p>
<h3>Versions</h3>
<p>master</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @Chillee @samdow @janeyx99 @SherlockNoMad @wconstab @soumith @ngimel</p>]]></description>
      <pubDate>Tue, 25 Apr 2023 23:25:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/100075</guid>
    </item>
    <item>
      <title>torch.compile mode="max-autotune" precision appears to be lower</title>
      <link>https://github.com/pytorch/pytorch/issues/96693</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I'm sorry, this is going to be a terrible bug report, but I wasn't able to simplify the problem. </p>
<p>I'm running a training run with a model that is compiled with <code>torch.compile</code>. The blue line below is loss with the <code>default</code> mode, the red line below is loss with <code>max-autotune</code>, the training runs are otherwise identical:</p>
<p><img alt="spiky_comparison" src="https://user-images.githubusercontent.com/22680696/224837823-788ccd48-6fb0-4099-9c87-46926dca134f.png" /></p>
<p>It appears as if precision/stability is noticeably lower on the max-autotune training run. This is running on an RTXA600 GPU. The forward pass is wrapped in <br />
<code>torch.autocast</code>, there are no graph breaks or warning messages. Max-autotune code is also noticeably a bit faster, so it is definitely doing something that the default mode is not.</p>
<p>Feel free to close this issue if this is not enough info. I could also test more specific things or try to make a mini version to get to the bottom of this, but I have no idea what to look for, or which part would be relevant.</p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.1.0.dev20230307<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Pop!_OS 22.04 LTS (x86_64)<br />
GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0<br />
Clang version: Could not collect<br />
CMake version: Could not collect<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-6.2.0-76060200-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU<br />
Nvidia driver version: 525.85.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>Versions of relevant libraries:<br />
[pip3] lion-pytorch==0.0.7<br />
[pip3] numpy==1.23.5<br />
[pip3] torch==2.1.0.dev20230307<br />
[pip3] torchaudio==2.0.0.dev20230307<br />
[pip3] torchvision==0.15.0.dev20230307<br />
[conda] blas                      1.0                         mkl<br />
[conda] lion-pytorch              0.0.7                    pypi_0    pypi<br />
[conda] mkl                       2021.4.0           h06a4308_640<br />
[conda] mkl-service               2.4.0           py310h7f8727e_0<br />
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0<br />
[conda] mkl_random                1.2.2           py310h00e6091_0<br />
[conda] numpy                     1.23.5          py310hd5efca6_0<br />
[conda] numpy-base                1.23.5          py310h8e6c178_0<br />
[conda] pytorch                   2.1.0.dev20230307 py3.10_cuda11.8_cudnn8.7.0_0    pytorch-nightly<br />
[conda] pytorch-cuda              11.8                 h7e8668a_3    pytorch-nightly<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly<br />
[conda] torchaudio                2.0.0.dev20230307     py310_cu118    pytorch-nightly<br />
[conda] torchtriton               2.0.0+b8b470bc59           py310    pytorch-nightly<br />
[conda] torchvision               0.15.0.dev20230307     py310_cu118    pytorch-nightly</p>
<p>```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @wconstab @soumith @ngimel</p>]]></description>
      <pubDate>Mon, 13 Mar 2023 13:46:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/96693</guid>
    </item>
    <item>
      <title>Cannot access data pointer of Tensor that doesn't have storage when using `torch.func.jvp` with `torch.compile`</title>
      <link>https://github.com/pytorch/pytorch/issues/96041</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When attempting to run below code snippet, </p>
<p>```<br />
import torch</p>
<p>@torch.compile()<br />
def compute(log_probs):<br />
    lit_weights = torch.stack(((1 - log_probs.exp()).log(), log_probs), dim=-1).permute(1, 2, 0)</p>
<pre><code>levels = [torch.tensor([5, 7], device='cuda'), torch.tensor([8], device='cuda')]
lit_indices = torch.tensor([0, 1, 2, 3, 4, 6], device='cuda')
id = 9
node_indices = torch.tensor([[[0, 0],[0, 0]],[[0, 0],[0, 0]],[[0, 0],[0, 0]],[[0, 0],[0, 0]],
[[0, 0],[0, 0]],[[1, 2],[3, 4]],[[0, 0],[0, 0]],[[1, 4],[9, 9]],[[0, 5],[6, 7]]], device='cuda')
lit_mask = (torch.tensor([0, 1, 2, 1, 2, 0], device='cuda'), torch.tensor([1, 1, 0, 0, 1, 0], device='cuda'))
lit_indices = torch.tensor([0, 1, 2, 3, 4, 6], device='cuda')

data = torch.empty(id+1, log_probs.size(0), device='cuda')
data[id] =  -float(1000)
data[lit_indices] = lit_weights[lit_mask[0], lit_mask[1]]

data[levels[0]] = data[node_indices[levels[0]]].sum(-2).logsumexp(-2)
data[levels[1]] = data[node_indices[levels[1]]].sum(-2).logsumexp(-2)

return data[levels[-1]]
</code></pre>
<h1>Prepare probabilities</h1>
<p>batch_size = 5<br />
log_probs = torch.rand((batch_size, 10), device='cuda', requires_grad=True).log()<br />
grad = torch.ones((batch_size, 10), device='cuda')</p>
<p>out, grad = torch.func.jvp(compute, (log_probs,), (grad,))<br />
```</p>
<p>which makes use of both <code>torch.compile</code> as well as <code>torch.func.jvp</code>, I get a </p>
<blockquote>
<p>RuntimeError: Cannot access data pointer of Tensor that doesn't have storage</p>
</blockquote>
<p>When commenting out to the <code>torch.compile</code> decorator, however, the code runs seamlessly.</p>
<h3>Error logs</h3>
<p>[2023-03-04 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping <strong>init</strong> /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py<br />
[2023-03-04 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping <strong>enter</strong> /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py<br />
[2023-03-04 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping <strong>init</strong> /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py<br />
[2023-03-04 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping <strong>enter</strong> /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py<br />
[2023-03-04 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping enable_dynamic /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py<br />
Traceback (most recent call last):<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 547, in preserve_rng_state<br />
    yield<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py", line 925, in wrap_fx_proxy_cls<br />
    example_value = wrap_to_fake_tensor_and_record(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py", line 1062, in wrap_to_fake_tensor_and_record<br />
    fake_e = wrap_fake_exception(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 808, in wrap_fake_exception<br />
    return fn()<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py", line 1063, in <lambda><br />
    lambda: tx.fake_mode.from_tensor(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py", line 1395, in from_tensor<br />
    return self.fake_tensor_converter(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py", line 314, in <strong>call</strong><br />
    return self.from_real_tensor(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py", line 272, in from_real_tensor<br />
    out = self.meta_converter(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_subclasses/meta_utils.py", line 502, in <strong>call</strong><br />
    r = self.meta_tensor(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_subclasses/meta_utils.py", line 275, in meta_tensor<br />
    base = self.meta_tensor(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_subclasses/meta_utils.py", line 381, in meta_tensor<br />
    s = t.untyped_storage()<br />
NotImplementedError: Cannot access storage of TensorWrapper</p>
<p>During handling of the above exception, another exception occurred:</p>
<p>Traceback (most recent call last):<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 324, in _compile<br />
    out_code = transform_code_object(code, transform)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 530, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 299, in transform<br />
    tracer = InstructionTranslator(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1806, in <strong>init</strong><br />
    self.symbolic_locals = collections.OrderedDict(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1809, in <genexpr><br />
    VariableBuilder(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py", line 174, in <strong>call</strong><br />
    return self._wrap(value).clone(**self.options())<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py", line 300, in _wrap<br />
    return type_dispatch(self, value)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py", line 748, in wrap_tensor<br />
    tensor_variable = wrap_fx_proxy(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py", line 865, in wrap_fx_proxy<br />
    return wrap_fx_proxy_cls(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py", line 898, in wrap_fx_proxy_cls<br />
    with preserve_rng_state():<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py", line 153, in <strong>exit</strong><br />
    self.gen.throw(typ, value, traceback)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 549, in preserve_rng_state<br />
    torch.random.set_rng_state(rng)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/random.py", line 18, in set_rng_state<br />
    default_generator.set_state(new_state)<br />
RuntimeError: Cannot access data pointer of Tensor that doesn't have storage</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    torch._dynamo.config.suppress_errors = True</p>
<p>The above exception was the direct cause of the following exception:</p>
<p>Traceback (most recent call last):<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 104, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 262, in _convert_frame_assert<br />
    return _compile(<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 164, in time_wrapper<br />
    r = func(</em>args, **kwargs)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 394, in _compile<br />
    raise InternalTorchDynamoError() from e<br />
torch._dynamo.exc.InternalTorchDynamoError</p>
<p>During handling of the above exception, another exception occurred:</p>
<p>Traceback (most recent call last):<br />
  File "/scratch/ahmedk/simple-graphs/exactly-k/github_issue.py", line 33, in <module><br />
    out, grad = torch.func.jvp(compute, (log_probs,), (grad,))<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 916, in jvp<br />
    return _jvp_with_argnums(func, primals, tangents, argnums=None, strict=strict, has_aux=has_aux)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 39, in fn<br />
    return f(<em>args, </em><em>kwargs)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 965, in _jvp_with_argnums<br />
    result_duals = func(</em>duals)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 231, in _fn<br />
    return fn(<em>args, </em>*kwargs)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 368, in catch_errors<br />
    return callback(frame, cache_size, hooks)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 404, in _convert_frame<br />
    result = inner_convert(frame, cache_size, hooks)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 109, in _fn<br />
    torch.cuda.set_rng_state(cuda_rng_state)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/cuda/random.py", line 64, in set_rng_state<br />
    _lazy_call(cb)<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/cuda/<strong>init</strong>.py", line 192, in _lazy_call<br />
    callable()<br />
  File "/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/site-packages/torch/cuda/random.py", line 62, in cb<br />
    default_generator.set_state(new_state_copy)<br />
RuntimeError: Cannot access data pointer of Tensor that doesn't have storage</p>
<h3>Minified repro</h3>
<p>I couldn't get it to produce a minified code, unfortunately, despite running with both <code>env TORCHDYNAMO_REPRO_AFTER='dynamo'</code> and <code>env TORCHDYNAMO_REPRO_AFTER='aot'</code></p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.1.0.dev20230304+cu117<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.7<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.4 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.25.0<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.13.0-39-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA RTX A5000<br />
GPU 1: NVIDIA RTX A5000<br />
GPU 2: NVIDIA RTX A5000<br />
GPU 3: NVIDIA RTX A5000</p>
<p>Nvidia driver version: 495.29.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   48 bits physical, 48 bits virtual<br />
CPU(s):                          32<br />
On-line CPU(s) list:             0-31<br />
Thread(s) per core:              2<br />
Core(s) per socket:              16<br />
Socket(s):                       1<br />
NUMA node(s):                    1<br />
Vendor ID:                       AuthenticAMD<br />
CPU family:                      25<br />
Model:                           1<br />
Model name:                      AMD EPYC 7313P 16-Core Processor<br />
Stepping:                        1<br />
Frequency boost:                 enabled<br />
CPU MHz:                         1500.000<br />
CPU max MHz:                     3729.4919<br />
CPU min MHz:                     1500.0000<br />
BogoMIPS:                        5988.68<br />
Virtualization:                  AMD-V<br />
L1d cache:                       512 KiB<br />
L1i cache:                       512 KiB<br />
L2 cache:                        8 MiB<br />
L3 cache:                        128 MiB<br />
NUMA node0 CPU(s):               0-31<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; LFENCE, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.1<br />
[pip3] pytorch-triton==2.0.0+b8b470bc59<br />
[pip3] torch==2.1.0.dev20230304+cu117<br />
[pip3] torch-geometric==2.2.0<br />
[pip3] torch-scatter==2.1.0<br />
[pip3] torch-sparse==0.6.16<br />
[pip3] torchaudio==2.0.0.dev20230223+cu117<br />
[pip3] torchvision==0.15.0.dev20230227+cu117<br />
[conda] numpy                     1.24.1                   pypi_0    pypi<br />
[conda] pytorch-triton            2.0.0+b8b470bc59          pypi_0    pypi<br />
[conda] torch                     2.1.0.dev20230304+cu117          pypi_0    pypi<br />
[conda] torch-geometric           2.2.0                    pypi_0    pypi<br />
[conda] torch-scatter             2.1.0                    pypi_0    pypi<br />
[conda] torch-sparse              0.6.16                   pypi_0    pypi<br />
[conda] torchaudio                2.0.0.dev20230223+cu117          pypi_0    pypi<br />
[conda] torchvision               0.15.0.dev20230227+cu117          pypi_0    pypi</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @Chillee @samdow @kshitij12345 @janeyx99 @soumith @ngimel</p>]]></description>
      <pubDate>Sat, 04 Mar 2023 13:50:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/96041</guid>
    </item>
    <item>
      <title>Inductor may merge two output tensors into one</title>
      <link>https://github.com/pytorch/pytorch/issues/88813</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Consider the following example:<br />
```python<br />
import torch._dynamo<br />
import logging</p>
<p>def g(a):<br />
    b = a * 2<br />
    c = a * 2<br />
    return b, c</p>
<p>x = torch.rand((1000000,), device="cuda", requires_grad=True)<br />
expect = g(x)<br />
actual = torch._dynamo.optimize("inductor")(g)(x)<br />
assert expect[0] is not expect[1]<br />
assert actual[0] is actual[1]<br />
```</p>
<p>The outputs have been merged into a single tensor, so downstream users of this function may get silently wrong results if <code>a</code> or <code>b</code> are mutated.</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 1.14.0a0+git034872d<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.7<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (conda-forge gcc 9.5.0-17) 9.5.0<br />
Clang version: Could not collect<br />
CMake version: version 3.24.1</p>
<p>Python version: 3.8 (64-bit runtime)<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
GPU models and configuration: <br />
GPU 0: NVIDIA GeForce RTX 2060<br />
GPU 1: NVIDIA GeForce RTX 2060</p>
<p>Nvidia driver version: 515.65.01<br />
cuDNN version: Probably one of the following:<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @mlazos @soumith @yanboliang @chunyuan-w @Xia-Weiwen</p>]]></description>
      <pubDate>Thu, 10 Nov 2022 07:09:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/88813</guid>
    </item>
    <item>
      <title>TorchInductor CPU Performance Dashboard</title>
      <link>https://github.com/pytorch/pytorch/issues/93531</link>
      <description><![CDATA[<p>Dashboard to track the performance of torchinductor on CPU.</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @soumith @ngimel @chauhang </p>]]></description>
      <pubDate>Wed, 12 Oct 2022 18:29:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/93531</guid>
    </item>
  </channel>
</rss>
