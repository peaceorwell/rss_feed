<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>DISABLED test_select_expanded_v (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/125056</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_select_expanded_v&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24293934542">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 76 workflow(s) with 228 failures and 76 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_select_expanded_v</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/var/lib/jenkins/workspace/test/test_autograd.py", line 3306, in test_select_expanded_v
    (result,) = torch.autograd.grad(a[0], a, v_expanded)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/autograd/__init__.py", line 412, in grad
    result = _engine_run_backward(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/autograd/graph.py", line 767, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/fx/graph_module.py", line 735, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/fx/graph_module.py", line 315, in __call__
    raise e
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/fx/graph_module.py", line 302, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert
    return _compile(
           ^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_utils_internal.py", line 70, in wrapper_function
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner
    out_code = transform_code_object(code, transform)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform
    tracer.run()
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 2241, in run
    super().run()
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 875, in run
    while self.step():
          ^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 790, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 2398, in RETURN_VALUE
    self._return(inst)
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 2383, in _return
    self.output.compile_subgraph(
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1093, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/opt/conda/envs/py_3.12/lib/python3.12/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1285, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1376, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1357, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/__init__.py", line 1781, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/var/lib/jenkins/workspace/test/inductor/test_compiled_autograd.py", line 27, in inner_compiler
    return inductor.compile(gm_, example_inputs_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/__init__.py", line 28, in compile
    return compile_fx(gm, example_inputs, config_patches=options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 1277, in compile_fx
    return flatten_graph_inputs(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 2669, in flatten_graph_inputs
    compiled_fn = compile_gm(GmWrapper(gm, spec), inputs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 1455, in compile_fx
    return aot_autograd(
           ^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(
                  ^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 156, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 1359, in fw_compiler_base
    return inner_compile(
           ^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 486, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 780, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/graph.py", line 1622, in compile_to_fn
    return self.compile_to_module().call
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/graph.py", line 1569, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 2447, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_jenkins/qa/cqa546fz5okasm6eccfqpphag5tyyiagwblqag6xkmg2ofotzkna.py", line 60, in <module>
    async_compile.wait(globals())
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 3049, in wait
    scope[key] = result.result()
                 ^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 2858, in result
    return self.result_fn()
           ^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 2327, in future
    result = get_result()
             ^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 2164, in load_fn
    future.result()
  File "/opt/conda/envs/py_3.12/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/conda/envs/py_3.12/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 2189, in _worker_compile_cpp
    compile_file(input_path, output_path, shlex.split(cmd))
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 2060, in compile_file
    raise exc.CppCompileError(cmd, output) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inner_compiler' raised:
CppCompileError: C++ compile error

Command:
g++ /tmp/torchinductor_jenkins/4o/c4oz5l55ya7unrrexue4dm364qelz66hkcteowwfqhtx5ihz5pdr.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=1 -I/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/include -I/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/include/TH -I/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/include/THC -I/opt/conda/envs/py_3.12/include/python3.12 -L/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/lib -L/opt/conda/envs/py_3.12/lib -L/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma -DCPU_CAPABILITY_AVX512 -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_jenkins/4o/c4oz5l55ya7unrrexue4dm364qelz66hkcteowwfqhtx5ihz5pdr.so

Output:
/tmp/torchinductor_jenkins/4o/c4oz5l55ya7unrrexue4dm364qelz66hkcteowwfqhtx5ihz5pdr.cpp:2:10: fatal error: /tmp/tmpseqqrsyi/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h: No such file or directory
    2 | #include "/tmp/tmpseqqrsyi/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h"
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


To execute this test, run the following from the base repo dir:
     python test/test_autograd.py -k test_select_expanded_v

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 26 Apr 2024 10:39:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125056</guid>
    </item>
    <item>
      <title>DISABLED test_dynamic_shapes (__main__.TestCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/125055</link>
      <description><![CDATA[<p>Platforms: linux, rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_dynamic_shapes&amp;suite=TestCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24293934542">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 10 workflow(s) with 30 failures and 10 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_dynamic_shapes</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/test_case.py", line 34, in tearDown
    super().tearDown()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/test_case.py", line 67, in tearDown
    reset()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/__init__.py", line 79, in reset
    _reset_guarded_backend_cache()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 88, in _reset_guarded_backend_cache
    backend.reset()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/__init__.py", line 1753, in reset
    reset_cudagraph_trees()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 301, in reset_cudagraph_trees
    for device, lock in locks_dict.items():
SystemError: /croot/python-split_1711037784081/work/Objects/dictobject.c:1514: bad argument to internal function
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 26 Apr 2024 10:39:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125055</guid>
    </item>
    <item>
      <title>DISABLED test_custom_fn_output_metadata (__main__.TestCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/125024</link>
      <description><![CDATA[<p>Platforms: linux</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_custom_fn_output_metadata&amp;suite=TestCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24291937877">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_custom_fn_output_metadata</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/var/lib/jenkins/workspace/test/inductor/test_compiled_autograd.py", line 783, in test_custom_fn_output_metadata
    self.check_output_and_recompiles(fn, 1, my_compiler_fn)
  File "/var/lib/jenkins/workspace/test/inductor/test_compiled_autograd.py", line 57, in check_output_and_recompiles
    actual = list(opt_fn())
  File "/var/lib/jenkins/workspace/test/inductor/test_compiled_autograd.py", line 778, in fn
    loss.backward()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_tensor.py", line 523, in backward
    torch.autograd.backward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/graph.py", line 767, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 735, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 315, in __call__
    raise e
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 302, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.138", line 4, in forward
    def forward(self, inputs, sizes, hooks):
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 2686, in wrapper
    return compiled_fn(flat_args)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 974, in boxed_forward
    return compiled_fn(flat_args)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 130, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 118, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 188, in rng_functionalization_wrapper
    return compiled_fw(args)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 968, in __call__
    return self.current_callable(inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 947, in run
    return compiled_fn(new_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 368, in deferred_cudagraphify
    fn, out = cudagraphify(model, inputs, new_static_input_idxs, *args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 390, in cudagraphify
    manager = get_container(device_index).get_tree_manager()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 325, in get_container
    lock = get_obj(local, "tree_manager_locks")[device_index]
TypeError: 'WeakIdRef' object is not subscriptable

To execute this test, run the following from the base repo dir:
     python test/inductor/test_compiled_autograd.py -k test_custom_fn_output_metadata

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 26 Apr 2024 04:46:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125024</guid>
    </item>
    <item>
      <title>DISABLED test_saved_variable_packing_unpacking_saved_original_with_default_hooks (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/125023</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_saved_variable_packing_unpacking_saved_original_with_default_hooks&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24291968372">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 88 workflow(s) with 264 failures and 88 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_saved_variable_packing_unpacking_saved_original_with_default_hooks</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/var/lib/jenkins/workspace/test/test_autograd.py", line 9179, in test_saved_variable_packing_unpacking_saved_original_with_default_hooks
    y.sum().backward()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_tensor.py", line 523, in backward
    torch.autograd.backward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/graph.py", line 767, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 735, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 315, in __call__
    raise e
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 302, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert
    return _compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py", line 70, in wrapper_function
    return function(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform
    tracer.run()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2241, in run
    super().run()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 875, in run
    while self.step():
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 790, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2398, in RETURN_VALUE
    self._return(inst)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2383, in _return
    self.output.compile_subgraph(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1093, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1285, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1376, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1357, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/__init__.py", line 1781, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
  File "/var/lib/jenkins/workspace/test/inductor/test_compiled_autograd.py", line 27, in inner_compiler
    return inductor.compile(gm_, example_inputs_)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/__init__.py", line 28, in compile
    return compile_fx(gm, example_inputs, config_patches=options)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1277, in compile_fx
    return flatten_graph_inputs(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 2669, in flatten_graph_inputs
    compiled_fn = compile_gm(GmWrapper(gm, spec), inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1455, in compile_fx
    return aot_autograd(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 156, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1359, in fw_compiler_base
    return inner_compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 486, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 780, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1622, in compile_to_fn
    return self.compile_to_module().call
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1569, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2447, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_jenkins/27/c27poz3hqgplnlamgagy3hnt42zo2vcgph27sm3ld744t6y2hqt4.py", line 54, in <module>
    async_compile.wait(globals())
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3049, in wait
    scope[key] = result.result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2858, in result
    return self.result_fn()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2327, in future
    result = get_result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2164, in load_fn
    future.result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2189, in _worker_compile_cpp
    compile_file(input_path, output_path, shlex.split(cmd))
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2060, in compile_file
    raise exc.CppCompileError(cmd, output) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inner_compiler' raised:
CppCompileError: C++ compile error

Command:
g++ /tmp/torchinductor_jenkins/kp/ckpw46oaqo5ikfaqrgv7m2ktzpdooaong3mg453tmrawik3awibq.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=1 -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/py_3.10/include/python3.10 -L/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/lib -L/opt/conda/envs/py_3.10/lib -L/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma -DCPU_CAPABILITY_AVX512 -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_jenkins/kp/ckpw46oaqo5ikfaqrgv7m2ktzpdooaong3mg453tmrawik3awibq.so

Output:
/tmp/torchinductor_jenkins/kp/ckpw46oaqo5ikfaqrgv7m2ktzpdooaong3mg453tmrawik3awibq.cpp:2:10: fatal error: /tmp/tmpl8xp8egh/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h: No such file or directory
    2 | #include "/tmp/tmpl8xp8egh/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h"
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


To execute this test, run the following from the base repo dir:
     python test/test_autograd.py -k test_saved_variable_packing_unpacking_saved_original_with_default_hooks

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 26 Apr 2024 04:46:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125023</guid>
    </item>
    <item>
      <title>DISABLED test_unbacked_cat_backwards_cuda (__main__.TestInductorDynamicCUDA)</title>
      <link>https://github.com/pytorch/pytorch/issues/125019</link>
      <description><![CDATA[<p>Platforms: linux, rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_unbacked_cat_backwards_cuda&amp;suite=TestInductorDynamicCUDA&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24289492429">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 17 workflow(s) with 51 failures and 17 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_unbacked_cat_backwards_cuda</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/var/lib/jenkins/workspace/test/inductor/test_torchinductor_dynamic_shapes.py", line 424, in test_unbacked_cat_backwards
    torch.compile(fullgraph=True)(f)(x, w).backward()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert
    return _compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py", line 70, in wrapper_function
    return function(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform
    tracer.run()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2241, in run
    super().run()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 875, in run
    while self.step():
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 790, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2398, in RETURN_VALUE
    self._return(inst)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2383, in _return
    self.output.compile_subgraph(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1068, in compile_subgraph
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1285, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1376, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1357, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/__init__.py", line 1742, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1455, in compile_fx
    return aot_autograd(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 434, in aot_dispatch_autograd
    compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1359, in fw_compiler_base
    return inner_compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 486, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 780, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1622, in compile_to_fn
    return self.compile_to_module().call
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1569, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2447, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_jenkins/eo/ceomvltnmh7ysnnwzonsh3tpcldidozuj5sfjxowgb7sp5sj3nzp.py", line 132, in <module>
    async_compile.wait(globals())
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3049, in wait
    scope[key] = result.result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2858, in result
    return self.result_fn()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2327, in future
    result = get_result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2164, in load_fn
    future.result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2189, in _worker_compile_cpp
    compile_file(input_path, output_path, shlex.split(cmd))
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2060, in compile_file
    raise exc.CppCompileError(cmd, output) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
CppCompileError: C++ compile error

Command:
g++ /tmp/torchinductor_jenkins/ml/cmllccvnqj7jsqoeror2omktgeid5lmod3nd42zitlwncy5x7xxq.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=1 -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/py_3.10/include/python3.10 -L/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/lib -L/opt/conda/envs/py_3.10/lib -L/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -mavx2 -mfma -DCPU_CAPABILITY_AVX2 -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_jenkins/ml/cmllccvnqj7jsqoeror2omktgeid5lmod3nd42zitlwncy5x7xxq.so

Output:
/tmp/torchinductor_jenkins/ml/cmllccvnqj7jsqoeror2omktgeid5lmod3nd42zitlwncy5x7xxq.cpp:2:10: fatal error: /tmp/tmpq1x8j0d9/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h: No such file or directory
    2 | #include "/tmp/tmpq1x8j0d9/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h"
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


To execute this test, run the following from the base repo dir:
     python test/inductor/test_torchinductor_dynamic_shapes.py -k test_unbacked_cat_backwards_cuda

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_torchinductor_dynamic_shapes.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 26 Apr 2024 04:46:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125019</guid>
    </item>
    <item>
      <title>[AMD] [Draft] New inductor gemm configs</title>
      <link>https://github.com/pytorch/pytorch/pull/125017</link>
      <description><![CDATA[<p>Draft state, running perf testing. Testing the removal of trying matrix_instr_nonkdim=[0, 16] for every config due to slow autotune speeds and only revert to matrix_instr_nonkdim=0 if block_m and block_n is not a multiple of matrix_instr_nonkdim</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 26 Apr 2024 02:49:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125017</guid>
    </item>
    <item>
      <title>add likely/unlikely macro for unsupport c++20 compiler.</title>
      <link>https://github.com/pytorch/pytorch/pull/124997</link>
      <description><![CDATA[<h1>Issue:</h1>
<p>Intel validation team found some low version gcc which not support c++20 will occur below issue:<br />
<code>cmd
[2024-04-13T08:03:25.142Z] g++ /tmp/torchinductor_root/vd/cvdytwwwlhi63ofh3pwzqfpjga4w4xe7bjfdoavpblbo5khzf3b2.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=0 -I/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/include -I/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/include/TH -I/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/include/THC -I/root/anaconda3/envs/pytorch/include/python3.8 -L/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/lib -L/root/anaconda3/envs/pytorch/lib -L/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -mavx2 -mfma -DCPU_CAPABILITY_AVX2 -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_root/vd/cvdytwwwlhi63ofh3pwzqfpjga4w4xe7bjfdoavpblbo5khzf3b2.so
[2024-04-13T08:03:25.142Z]
[2024-04-13T08:03:25.142Z] Output:
[2024-04-13T08:03:25.142Z] /tmp/torchinductor_root/vd/cvdytwwwlhi63ofh3pwzqfpjga4w4xe7bjfdoavpblbo5khzf3b2.cpp: In function ‘T parse_arg(PyObject*, size_t) [with T = long int; PyObject = _object; size_t = long unsigned int]’:
[2024-04-13T08:03:25.142Z] /tmp/torchinductor_root/vd/cvdytwwwlhi63ofh3pwzqfpjga4w4xe7bjfdoavpblbo5khzf3b2.cpp:117:10: error: expected identifier before ‘[’ token
[2024-04-13T08:03:25.142Z] [[unlikely]] throw std::runtime_error("expected int arg");
[2024-04-13T08:03:25.142Z] ^</code></p>
<p>The season is <code>unlikely</code> need c++20 attribute, ref: https://en.cppreference.com/w/cpp/language/attributes/likely</p>
<h1>Solution:</h1>
<p>Add MACRO to enable non-c++20 attribute GNU compiler.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 18:41:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124997</guid>
    </item>
    <item>
      <title>Remove Inductor IR for legacy functional collectives</title>
      <link>https://github.com/pytorch/pytorch/pull/124992</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124992<br />
* #124979</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang @d4l3k @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 17:06:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124992</guid>
    </item>
    <item>
      <title>[compiled autograd] verbose logs for debugging cache misses</title>
      <link>https://github.com/pytorch/pytorch/pull/124980</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124980<br />
* #124954</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 15:36:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124980</guid>
    </item>
    <item>
      <title>[compiled autograd] introduce verbose logs, add autograd node info to graph</title>
      <link>https://github.com/pytorch/pytorch/pull/124954</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124980<br />
* <strong>-&gt;</strong> #124954</p>
<ul>
<li>sets it as a fake stack trace as we don't have a generic comment feature</li>
<li>when verbose is disabled, still adds a contextmanager and flag checks. the alternative is to use MACROS, but that wouldn't be usable with TORCH_LOGS</li>
</ul>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 10:06:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124954</guid>
    </item>
    <item>
      <title>torch.compile fails on hugging face Mistral7b</title>
      <link>https://github.com/pytorch/pytorch/issues/124946</link>
      <description><![CDATA[<h3>🐛 Describe the bug</h3>
<p><strong>The following code fails:</strong></p>
<p>import torch<br />
from transformers import AutoModelForCausalLM, AutoTokenizer</p>
<p>model = AutoModelForCausalLM.from_pretrained(<br />
    "mistralai/Mistral-7B-v0.1",<br />
    torch_dtype=torch.float16,<br />
    use_cache=True,<br />
)<br />
model = torch.compile(model)</p>
<p>input_ids =torch.randint(low=0, high=60000, size=(1, 32), dtype=torch.int64)<br />
attention_mask = torch.ones(1, 32, dtype=torch.int64)<br />
model(input_ids=input_ids, attention_mask=attention_mask)</p>
<p><strong>It generates the following error:</strong></p>
<p>Traceback (most recent call last):<br />
  File "/work1/sleduc/torch-trials/compile_mistral7b.py", line 13, in <module><br />
    model(input_ids=input_ids, attention_mask=attention_mask)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl                                                                                                                                                                                                                                                                                                                      return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 1157, in forward<br />
    outputs = self.model(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 655, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 727, in _convert_frame<br />
    result = inner_convert(frame, cache_entry, hooks, frame_state)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 383, in _convert_frame_assert<br />
    compiled_product = _compile(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 646, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 562, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 151, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 527, in transform<br />
    tracer.run()<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2128, in run<br />
    super().run()<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 818, in run<br />
    and self.step()<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 781, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 465, in wrapper<br />
    return handle_graph_break(self, inst, speculation.reason)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 521, in handle_graph_break<br />
    self.output.compile_subgraph(self, reason=reason)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 945, in compile_subgraph<br />
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)<br />
  File "/usr/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(<em>args, </em><em>kwds)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1087, in compile_and_call_fx_graph<br />
    compiled_fn = self.call_user_compiler(gm)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/<em>dynamo/output_graph.py", line 1159, in call_user_compiler<br />
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1140, in call_user_compiler<br />
    compiled_fn = compiler_fn(gm, self.example_inputs())<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper<br />
    compiled_gm = compiler_fn(gm, example_inputs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/<strong>init</strong>.py", line 1668, in <strong>call</strong><br />
    return compile_fx(model</em>, inputs_, config_patches=self.config)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1168, in compile_fx<br />
    return aot_autograd(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 55, in compiler_fn<br />
    cg = aot_module_simplified(gm, example_inputs, </strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 887, in aot_module_simplified<br />
    compiled_fn = create_aot_dispatcher_function(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(<em>args, </em>*kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 600, in create_aot_dispatcher_function<br />
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 425, in aot_wrapper_dedupe<br />
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 630, in aot_wrapper_synthetic_base<br />
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 181, in aot_dispatch_autograd<br />
    fw_module, bw_module = aot_config.partition_fn(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1128, in partition_fn<br />
    return min_cut_rematerialization_partition(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/partitioners.py", line 650, in min_cut_rematerialization_partition<br />
    import networkx as nx<br />
  File "/usr/lib/python3/dist-packages/networkx/<strong>init</strong>.py", line 115, in <module><br />
    import networkx.readwrite<br />
  File "/usr/lib/python3/dist-packages/networkx/readwrite/<strong>init</strong>.py", line 15, in <module><br />
    from networkx.readwrite.graphml import *<br />
  File "/usr/lib/python3/dist-packages/networkx/readwrite/graphml.py", line 314, in <module><br />
    class GraphML(object):<br />
  File "/usr/lib/python3/dist-packages/networkx/readwrite/graphml.py", line 346, in GraphML<br />
    (np.int, "int"), (np.int8, "int"),<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/numpy/<strong>init</strong>.py", line 324, in <strong>getattr</strong><br />
    raise AttributeError(<strong>former_attrs</strong>[attr])<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
AttributeError: module 'numpy' has no attribute 'int'.<br />
<code>np.int</code> was a deprecated alias for the builtin <code>int</code>. To avoid this error in existing code, use <code>int</code> by itself. Doing this will not modify any behavior and is safe. When replacing <code>np.int</code>, you may wish to use e.g. <code>np.int64</code> or <code>np.int32</code> to specify the precision. If you wish to review your current use, check the release note link for additional information.<br />
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:<br />
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True</p>
<h3>Versions</h3>
<p><strong>Versions:</strong></p>
<p>PyTorch version: 2.2.2+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.4 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-73-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA L40S<br />
Nvidia driver version: 550.67<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Address sizes:                   52 bits physical, 57 bits virtual<br />
Byte Order:                      Little Endian<br />
CPU(s):                          64<br />
On-line CPU(s) list:             0-63<br />
Vendor ID:                       AuthenticAMD<br />
Model name:                      AMD EPYC 9334 32-Core Processor<br />
CPU family:                      25<br />
Model:                           17<br />
Thread(s) per core:              1<br />
Core(s) per socket:              32<br />
Socket(s):                       2<br />
Stepping:                        1<br />
Frequency boost:                 enabled<br />
CPU max MHz:                     3910.2529<br />
CPU min MHz:                     1500.0000<br />
BogoMIPS:                        5399.98<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d<br />
Virtualization:                  AMD-V<br />
L1d cache:                       2 MiB (64 instances)<br />
L1i cache:                       2 MiB (64 instances)<br />
L2 cache:                        64 MiB (64 instances)<br />
L3 cache:                        256 MiB (8 instances)<br />
NUMA node(s):                    2<br />
NUMA node0 CPU(s):               0-31<br />
NUMA node1 CPU(s):               32-63<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Not affected<br />
Vulnerability Retbleed:          Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] onnx==1.16.0<br />
[pip3] onnx-graphsurgeon==0.5.2<br />
[pip3] onnxruntime==1.16.3<br />
[pip3] onnxsim==0.4.36<br />
[pip3] torch==2.2.2<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 08:34:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124946</guid>
    </item>
    <item>
      <title>[RFC] Support reinplaceble ops for custom ops in Inductor</title>
      <link>https://github.com/pytorch/pytorch/issues/124933</link>
      <description><![CDATA[<h3>🚀 The feature, motivation and pitch</h3>
<p>Currently, the inductor backend in PyTorch supports the reinplace optimization for a fixed list of ops in the reinplace fx pass (<code>torch/_inductor/fx_passes/reinplace.py</code>). However, for custom inplace ops that are functionalized into out-of-place ops using <code>copy_</code>, they cannot be reinplaced, which impacts performance. To address this, we propose adding a configuration option to the <code>torch.compile</code> inductor backend that allows users to specify the mapping between out-of-place ops and their inplace counterparts. This would enable the reinplace pass to support the reinplace optimization for these custom ops as well, potentially improving performance.</p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @zou3519</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 05:12:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124933</guid>
    </item>
    <item>
      <title>[inductor] share more cse cache during swap buffer</title>
      <link>https://github.com/pytorch/pytorch/pull/124921</link>
      <description><![CDATA[<p><code>swap_buffer</code> will make the <code>cse_cache</code> cannot be shared inside/outside of the lambda function scope.<br />
For example,</p>
<p><code>auto tmp8 = -std::numeric_limits&lt;float&gt;::infinity();
auto tmp9 = [&amp;]
{
    auto tmp12 = -std::numeric_limits&lt;float&gt;::infinity();
    return tmp12;
}</code><br />
<code>tmp12</code> should not be created since it is same with <code>tmp8</code>.</p>
<p>We make the <code>cse_cache</code> as a read only cache inside the scope (because it is unsafe to expose cache inside the scope,the outside scope cannot use it.)</p>
<p><strong>Test Plan</strong><br />
<code>python test/inductor/test_torchinductor.py -k test_AllenaiLongformerBase_repro_cpu</code><br />
the <code>static_cast&lt;int&gt;(256)</code> will only occur once after this PR since the inside scope can share the cse buffer outside the scope.</p>
<p>Before this PR, <br />
```<br />
cpp_fused_copy_full_like_0 = async_compile.cpp_pybinding(['const float<em>', 'float</em>'], '''</p>
<h1>include "/tmp/torchinductor_root/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h"</h1>
<p>extern "C" void kernel(const float<em> in_ptr0,<br />
                       float</em> out_ptr1)<br />
{<br />
    #pragma omp parallel num_threads(128)<br />
    {<br />
        int tid = omp_get_thread_num();<br />
        {<br />
            #pragma omp for collapse(2)<br />
            for(long x0=static_cast<long>(0L); x0&lt;static_cast<long>(4L); x0+=static_cast<long>(1L))<br />
            {<br />
                for(long x1=static_cast<long>(0L); x1&lt;static_cast<long>(1024L); x1+=static_cast<long>(1L))<br />
                {<br />
                    #pragma GCC ivdep<br />
                    for(long x2=static_cast<long>(0L); x2&lt;static_cast<long>(12L); x2+=static_cast<long>(1L))<br />
                    {<br />
                        for(long x3=static_cast<long>(0L); x3&lt;static_cast<long>(512L); x3+=static_cast<long>(16L))<br />
                        {<br />
                            auto tmp0 = c10::convert<int>(x1);<br />
                            auto tmp1 = static_cast<int>(256);<br />
                            auto tmp2 = tmp0 &lt; tmp1;<br />
                            auto tmp3 = [&amp;]<br />
                            {<br />
                                auto tmp4 = c10::convert<int>(x3);<br />
                                auto tmp5 = at::vec::Vectorized<int>::arange(tmp4, 1);<br />
                                auto tmp6 = static_cast<int>(257);<br />
                                auto tmp7 = at::vec::Vectorized<int>(tmp6);<br />
                                auto tmp8 = at::vec::VecMask<int,1>(tmp5 &lt; tmp7);<br />
                                auto tmp10 = at::vec::VecMask<float,1>::from(tmp2);<br />
                                auto tmp11 = tmp8 &amp; tmp10;<br />
                                auto tmp9 = [&amp;]<br />
                                {<br />
                                    auto tmp12 = -std::numeric_limits<float>::infinity();<br />
                                    return tmp12;<br />
                                }<br />
                                ;<br />
                                auto tmp13 =<br />
                                [&amp;]<br />
                                {<br />
                                    if (tmp11.all_zero())<br />
                                    {<br />
                                        return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                    }<br />
                                    else<br />
                                    {<br />
                                        return decltype(at::vec::Vectorized<float>(tmp9()))::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), at::vec::Vectorized<float>(tmp9()), tmp11.template cast<float,1>());<br />
                                    }<br />
                                }<br />
                                ()<br />
                                ;<br />
                                auto tmp14 = c10::convert<int>(c10::div_floor_integer(x1, 256L));<br />
                                auto tmp15 = static_cast<int>(3);<br />
                                auto tmp16 = tmp14 &lt; tmp15;<br />
                                auto tmp18 = tmp16 &amp; tmp2;<br />
                                auto tmp17 = [&amp;]<br />
                                {<br />
                                    auto tmp19 = c10::convert<int>(x3);<br />
                                    auto tmp20 = at::vec::Vectorized<int>::arange(tmp19, 1);<br />
                                    auto tmp21 = static_cast<int>(256);<br />
                                    auto tmp22 = at::vec::Vectorized<int>(tmp21);<br />
                                    auto tmp23 = at::vec::VecMask<int,1>(tmp20 &gt;= tmp22);<br />
                                    auto tmp25 = at::vec::VecMask<float,1>::from(tmp18);<br />
                                    auto tmp26 = tmp23 &amp; tmp25;<br />
                                    auto tmp24 = [&amp;]<br />
                                    {<br />
                                        auto tmp27 = tmp26.template cast<float,1>().template loadu<float,1>(in_ptr0 + static_cast<long>((-256L) + x3 + (513L<em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L</em>(c10::div_floor_integer(x1, 256L))) + (787968L<em>x2) + (9455616L</em>x0)));<br />
                                        return tmp27;<br />
                                    }<br />
                                    ;<br />
                                    auto tmp28 =<br />
                                    [&amp;]<br />
                                    {<br />
                                        if (tmp26.all_zero())<br />
                                        {<br />
                                            return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                        }<br />
                                        else<br />
                                        {<br />
                                            return decltype(tmp24())::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), tmp24(), tmp26.template cast<float,1>());<br />
                                        }<br />
                                    }<br />
                                    ()<br />
                                    ;<br />
                                    auto tmp29 = static_cast<float>(0.0);<br />
                                    auto tmp30 = at::vec::Vectorized<float>(tmp29);<br />
                                    auto tmp31 = decltype(tmp28)::blendv(tmp30, tmp28, tmp23.template cast<float,1>());<br />
                                    return tmp31;<br />
                                }<br />
                                ;<br />
                                auto tmp32 = tmp16 ? tmp17() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                auto tmp33 = static_cast<float>(0.0);<br />
                                auto tmp34 = at::vec::VecMask<float,1>::from(tmp16);<br />
                                auto tmp35 = at::vec::Vectorized<float>(tmp33);<br />
                                auto tmp36 = decltype(tmp32)::blendv(tmp35, tmp32, tmp34.template cast<float,1>());<br />
                                auto tmp37 = decltype(tmp13)::blendv(tmp36, tmp13, tmp8.template cast<float,1>());<br />
                                return tmp37;<br />
                            }<br />
                            ;<br />
                            auto tmp38 = tmp2 ? tmp3() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                            auto tmp39 = c10::convert<int>(c10::div_floor_integer(x1, 256L));<br />
                            auto tmp40 = static_cast<int>(3);<br />
                            auto tmp41 = tmp39 &lt; tmp40;<br />
                            auto tmp42 = [&amp;]<br />
                            {<br />
                                auto tmp43 = c10::convert<int>(x3);<br />
                                auto tmp44 = at::vec::Vectorized<int>::arange(tmp43, 1);<br />
                                auto tmp45 = static_cast<int>(256);<br />
                                auto tmp46 = at::vec::Vectorized<int>(tmp45);<br />
                                auto tmp47 = at::vec::VecMask<int,1>(tmp44 &gt;= tmp46);<br />
                                auto tmp49 = at::vec::VecMask<float,1>::from(tmp41);<br />
                                auto tmp50 = tmp47 &amp; tmp49;<br />
                                auto tmp48 = [&amp;]<br />
                                {<br />
                                    auto tmp51 = tmp50.template cast<float,1>().template loadu<float,1>(in_ptr0 + static_cast<long>((-256L) + x3 + (513L<em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L</em>(c10::div_floor_integer(x1, 256L))) + (787968L<em>x2) + (9455616L</em>x0)));<br />
                                    return tmp51;<br />
                                }<br />
                                ;<br />
                                auto tmp52 =<br />
                                [&amp;]<br />
                                {<br />
                                    if (tmp50.all_zero())<br />
                                    {<br />
                                        return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                    }<br />
                                    else<br />
                                    {<br />
                                        return decltype(tmp48())::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), tmp48(), tmp50.template cast<float,1>());<br />
                                    }<br />
                                }<br />
                                ()<br />
                                ;<br />
                                auto tmp53 = static_cast<float>(0.0);<br />
                                auto tmp54 = at::vec::Vectorized<float>(tmp53);<br />
                                auto tmp55 = decltype(tmp52)::blendv(tmp54, tmp52, tmp47.template cast<float,1>());<br />
                                return tmp55;<br />
                            }<br />
                            ;<br />
                            auto tmp56 = tmp41 ? tmp42() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                            auto tmp57 = static_cast<float>(0.0);<br />
                            auto tmp58 = at::vec::VecMask<float,1>::from(tmp41);<br />
                            auto tmp59 = at::vec::Vectorized<float>(tmp57);<br />
                            auto tmp60 = decltype(tmp56)::blendv(tmp59, tmp56, tmp58.template cast<float,1>());<br />
                            auto tmp61 = at::vec::VecMask<float,1>::from(tmp2);<br />
                            auto tmp62 = decltype(tmp38)::blendv(tmp60, tmp38, tmp61.template cast<float,1>());<br />
                            tmp62.store(out_ptr1 + static_cast<long>(x3 + (513L<em>x1) + (525312L</em>x2) + (6303744L<em>x0)));<br />
                        }<br />
                        #pragma omp simd simdlen(8) <br />
                        for(long x3=static_cast<long>(512L); x3&lt;static_cast<long>(513L); x3+=static_cast<long>(1L))<br />
                        {<br />
                            auto tmp0 = c10::convert<int64_t>(x1);<br />
                            auto tmp1 = static_cast<int64_t>(256);<br />
                            auto tmp2 = tmp0 &lt; tmp1;<br />
                            auto tmp3 = [&amp;]<br />
                            {<br />
                                auto tmp4 = c10::convert<int64_t>(x3);<br />
                                auto tmp5 = static_cast<int64_t>(257);<br />
                                auto tmp6 = tmp4 &lt; tmp5;<br />
                                auto tmp7 = [&amp;]<br />
                                {<br />
                                    auto tmp8 = -std::numeric_limits<float>::infinity();<br />
                                    return tmp8;<br />
                                }<br />
                                ;<br />
                                auto tmp9 = tmp6 ? tmp7() : static_cast<decltype(tmp7())>(0.0);<br />
                                auto tmp10 = c10::convert<int64_t>(c10::div_floor_integer(x1, 256L));<br />
                                auto tmp11 = static_cast<int64_t>(3);<br />
                                auto tmp12 = tmp10 &lt; tmp11;<br />
                                auto tmp13 = [&amp;]<br />
                                {<br />
                                    auto tmp14 = c10::convert<int64_t>(x3);<br />
                                    auto tmp15 = static_cast<int64_t>(256);<br />
                                    auto tmp16 = tmp14 &gt;= tmp15;<br />
                                    auto tmp17 = [&amp;]<br />
                                    {<br />
                                        auto tmp18 = in_ptr0[static_cast<long>((-256L) + x3 + (513L</em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L<em>(c10::div_floor_integer(x1, 256L))) + (787968L</em>x2) + (9455616L<em>x0))];<br />
                                        return tmp18;<br />
                                    }<br />
                                    ;<br />
                                    auto tmp19 = tmp16 ? tmp17() : static_cast<decltype(tmp17())>(0.0);<br />
                                    auto tmp20 = static_cast<float>(0.0);<br />
                                    auto tmp21 = tmp16 ? tmp19 : tmp20;<br />
                                    return tmp21;<br />
                                }<br />
                                ;<br />
                                auto tmp22 = tmp12 ? tmp13() : static_cast<decltype(tmp13())>(0.0);<br />
                                auto tmp23 = static_cast<float>(0.0);<br />
                                auto tmp24 = tmp12 ? tmp22 : tmp23;<br />
                                auto tmp25 = tmp6 ? tmp9 : tmp24;<br />
                                return tmp25;<br />
                            }<br />
                            ;<br />
                            auto tmp26 = tmp2 ? tmp3() : static_cast<decltype(tmp3())>(0.0);<br />
                            auto tmp27 = c10::convert<int64_t>(c10::div_floor_integer(x1, 256L));<br />
                            auto tmp28 = static_cast<int64_t>(3);<br />
                            auto tmp29 = tmp27 &lt; tmp28;<br />
                            auto tmp30 = [&amp;]<br />
                            {<br />
                                auto tmp31 = c10::convert<int64_t>(x3);<br />
                                auto tmp32 = static_cast<int64_t>(256);<br />
                                auto tmp33 = tmp31 &gt;= tmp32;<br />
                                auto tmp34 = [&amp;]<br />
                                {<br />
                                    auto tmp35 = in_ptr0[static_cast<long>((-256L) + x3 + (513L</em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L<em>(c10::div_floor_integer(x1, 256L))) + (787968L</em>x2) + (9455616L<em>x0))];<br />
                                    return tmp35;<br />
                                }<br />
                                ;<br />
                                auto tmp36 = tmp33 ? tmp34() : static_cast<decltype(tmp34())>(0.0);<br />
                                auto tmp37 = static_cast<float>(0.0);<br />
                                auto tmp38 = tmp33 ? tmp36 : tmp37;<br />
                                return tmp38;<br />
                            }<br />
                            ;<br />
                            auto tmp39 = tmp29 ? tmp30() : static_cast<decltype(tmp30())>(0.0);<br />
                            auto tmp40 = static_cast<float>(0.0);<br />
                            auto tmp41 = tmp29 ? tmp39 : tmp40;<br />
                            auto tmp42 = tmp2 ? tmp26 : tmp41;<br />
                            out_ptr1[static_cast<long>(x3 + (513L</em>x1) + (525312L<em>x2) + (6303744L</em>x0))] = tmp42;<br />
                        }<br />
                    }<br />
                }<br />
            }<br />
        }<br />
    }<br />
}<br />
''')<br />
<code>After this PR,</code><br />
cpp_fused_copy_full_like_0 = async_compile.cpp_pybinding(['const float<em>', 'float</em>'], '''</p>
<h1>include "/tmp/torchinductor_root/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h"</h1>
<p>extern "C" void kernel(const float<em> in_ptr0,<br />
                       float</em> out_ptr1)<br />
{<br />
    #pragma omp parallel num_threads(128)<br />
    {<br />
        int tid = omp_get_thread_num();<br />
        {<br />
            #pragma omp for collapse(2)<br />
            for(long x0=static_cast<long>(0L); x0&lt;static_cast<long>(4L); x0+=static_cast<long>(1L))<br />
            {<br />
                for(long x1=static_cast<long>(0L); x1&lt;static_cast<long>(1024L); x1+=static_cast<long>(1L))<br />
                {<br />
                    #pragma GCC ivdep<br />
                    for(long x2=static_cast<long>(0L); x2&lt;static_cast<long>(12L); x2+=static_cast<long>(1L))<br />
                    {<br />
                        for(long x3=static_cast<long>(0L); x3&lt;static_cast<long>(512L); x3+=static_cast<long>(16L))<br />
                        {<br />
                            auto tmp0 = c10::convert<int>(x1);<br />
                            auto tmp1 = static_cast<int>(256);<br />
                            auto tmp2 = tmp0 &lt; tmp1;<br />
                            auto tmp3 = [&amp;]<br />
                            {<br />
                                auto tmp4 = c10::convert<int>(x3);<br />
                                auto tmp5 = at::vec::Vectorized<int>::arange(tmp4, 1);<br />
                                auto tmp6 = static_cast<int>(257);<br />
                                auto tmp7 = at::vec::Vectorized<int>(tmp6);<br />
                                auto tmp8 = at::vec::VecMask<int,1>(tmp5 &lt; tmp7);<br />
                                auto tmp10 = at::vec::VecMask<float,1>::from(tmp2);<br />
                                auto tmp11 = tmp8 &amp; tmp10;<br />
                                auto tmp9 = [&amp;]<br />
                                {<br />
                                    auto tmp12 = -std::numeric_limits<float>::infinity();<br />
                                    return tmp12;<br />
                                }<br />
                                ;<br />
                                auto tmp13 =<br />
                                [&amp;]<br />
                                {<br />
                                    if (tmp11.all_zero())<br />
                                    {<br />
                                        return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                    }<br />
                                    else<br />
                                    {<br />
                                        return decltype(at::vec::Vectorized<float>(tmp9()))::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), at::vec::Vectorized<float>(tmp9()), tmp11.template cast<float,1>());<br />
                                    }<br />
                                }<br />
                                ()<br />
                                ;<br />
                                auto tmp14 = c10::convert<int>(c10::div_floor_integer(x1, 256L));<br />
                                auto tmp15 = static_cast<int>(3);<br />
                                auto tmp16 = tmp14 &lt; tmp15;<br />
                                auto tmp18 = tmp16 &amp; tmp2;<br />
                                auto tmp17 = [&amp;]<br />
                                {<br />
                                    auto tmp19 = at::vec::Vectorized<int>(tmp1);<br />
                                    auto tmp20 = at::vec::VecMask<int,1>(tmp5 &gt;= tmp19);<br />
                                    auto tmp22 = at::vec::VecMask<float,1>::from(tmp18);<br />
                                    auto tmp23 = tmp20 &amp; tmp22;<br />
                                    auto tmp21 = [&amp;]<br />
                                    {<br />
                                        auto tmp24 = tmp23.template cast<float,1>().template loadu<float,1>(in_ptr0 + static_cast<long>((-256L) + x3 + (513L<em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L</em>(c10::div_floor_integer(x1, 256L))) + (787968L<em>x2) + (9455616L</em>x0)));<br />
                                        return tmp24;<br />
                                    }<br />
                                    ;<br />
                                    auto tmp25 =<br />
                                    [&amp;]<br />
                                    {<br />
                                        if (tmp23.all_zero())<br />
                                        {<br />
                                            return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                        }<br />
                                        else<br />
                                        {<br />
                                            return decltype(tmp21())::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), tmp21(), tmp23.template cast<float,1>());<br />
                                        }<br />
                                    }<br />
                                    ()<br />
                                    ;<br />
                                    auto tmp26 = static_cast<float>(0.0);<br />
                                    auto tmp27 = at::vec::Vectorized<float>(tmp26);<br />
                                    auto tmp28 = decltype(tmp25)::blendv(tmp27, tmp25, tmp20.template cast<float,1>());<br />
                                    return tmp28;<br />
                                }<br />
                                ;<br />
                                auto tmp29 = tmp16 ? tmp17() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                auto tmp30 = static_cast<float>(0.0);<br />
                                auto tmp31 = at::vec::VecMask<float,1>::from(tmp16);<br />
                                auto tmp32 = at::vec::Vectorized<float>(tmp30);<br />
                                auto tmp33 = decltype(tmp29)::blendv(tmp32, tmp29, tmp31.template cast<float,1>());<br />
                                auto tmp34 = decltype(tmp13)::blendv(tmp33, tmp13, tmp8.template cast<float,1>());<br />
                                return tmp34;<br />
                            }<br />
                            ;<br />
                            auto tmp35 = tmp2 ? tmp3() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                            auto tmp36 = c10::convert<int>(c10::div_floor_integer(x1, 256L));<br />
                            auto tmp37 = static_cast<int>(3);<br />
                            auto tmp38 = tmp36 &lt; tmp37;<br />
                            auto tmp39 = [&amp;]<br />
                            {<br />
                                auto tmp40 = c10::convert<int>(x3);<br />
                                auto tmp41 = at::vec::Vectorized<int>::arange(tmp40, 1);<br />
                                auto tmp42 = at::vec::Vectorized<int>(tmp1);<br />
                                auto tmp43 = at::vec::VecMask<int,1>(tmp41 &gt;= tmp42);<br />
                                auto tmp45 = at::vec::VecMask<float,1>::from(tmp38);<br />
                                auto tmp46 = tmp43 &amp; tmp45;<br />
                                auto tmp44 = [&amp;]<br />
                                {<br />
                                    auto tmp47 = tmp46.template cast<float,1>().template loadu<float,1>(in_ptr0 + static_cast<long>((-256L) + x3 + (513L<em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L</em>(c10::div_floor_integer(x1, 256L))) + (787968L<em>x2) + (9455616L</em>x0)));<br />
                                    return tmp47;<br />
                                }<br />
                                ;<br />
                                auto tmp48 =<br />
                                [&amp;]<br />
                                {<br />
                                    if (tmp46.all_zero())<br />
                                    {<br />
                                        return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                    }<br />
                                    else<br />
                                    {<br />
                                        return decltype(tmp44())::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), tmp44(), tmp46.template cast<float,1>());<br />
                                    }<br />
                                }<br />
                                ()<br />
                                ;<br />
                                auto tmp49 = static_cast<float>(0.0);<br />
                                auto tmp50 = at::vec::Vectorized<float>(tmp49);<br />
                                auto tmp51 = decltype(tmp48)::blendv(tmp50, tmp48, tmp43.template cast<float,1>());<br />
                                return tmp51;<br />
                            }<br />
                            ;<br />
                            auto tmp52 = tmp38 ? tmp39() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                            auto tmp53 = static_cast<float>(0.0);<br />
                            auto tmp54 = at::vec::VecMask<float,1>::from(tmp38);<br />
                            auto tmp55 = at::vec::Vectorized<float>(tmp53);<br />
                            auto tmp56 = decltype(tmp52)::blendv(tmp55, tmp52, tmp54.template cast<float,1>());<br />
                            auto tmp57 = at::vec::VecMask<float,1>::from(tmp2);<br />
                            auto tmp58 = decltype(tmp35)::blendv(tmp56, tmp35, tmp57.template cast<float,1>());<br />
                            tmp58.store(out_ptr1 + static_cast<long>(x3 + (513L<em>x1) + (525312L</em>x2) + (6303744L<em>x0)));<br />
                        }<br />
                        #pragma omp simd simdlen(8) <br />
                        for(long x3=static_cast<long>(512L); x3&lt;static_cast<long>(513L); x3+=static_cast<long>(1L))<br />
                        {<br />
                            auto tmp0 = c10::convert<int64_t>(x1);<br />
                            auto tmp1 = static_cast<int64_t>(256);<br />
                            auto tmp2 = tmp0 &lt; tmp1;<br />
                            auto tmp3 = [&amp;]<br />
                            {<br />
                                auto tmp4 = c10::convert<int64_t>(x3);<br />
                                auto tmp5 = static_cast<int64_t>(257);<br />
                                auto tmp6 = tmp4 &lt; tmp5;<br />
                                auto tmp7 = [&amp;]<br />
                                {<br />
                                    auto tmp8 = -std::numeric_limits<float>::infinity();<br />
                                    return tmp8;<br />
                                }<br />
                                ;<br />
                                auto tmp9 = tmp6 ? tmp7() : static_cast<decltype(tmp7())>(0.0);<br />
                                auto tmp10 = c10::convert<int64_t>(c10::div_floor_integer(x1, 256L));<br />
                                auto tmp11 = static_cast<int64_t>(3);<br />
                                auto tmp12 = tmp10 &lt; tmp11;<br />
                                auto tmp13 = [&amp;]<br />
                                {<br />
                                    auto tmp14 = tmp4 &gt;= tmp1;<br />
                                    auto tmp15 = [&amp;]<br />
                                    {<br />
                                        auto tmp16 = in_ptr0[static_cast<long>((-256L) + x3 + (513L</em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L<em>(c10::div_floor_integer(x1, 256L))) + (787968L</em>x2) + (9455616L<em>x0))];<br />
                                        return tmp16;<br />
                                    }<br />
                                    ;<br />
                                    auto tmp17 = tmp14 ? tmp15() : static_cast<decltype(tmp15())>(0.0);<br />
                                    auto tmp18 = static_cast<float>(0.0);<br />
                                    auto tmp19 = tmp14 ? tmp17 : tmp18;<br />
                                    return tmp19;<br />
                                }<br />
                                ;<br />
                                auto tmp20 = tmp12 ? tmp13() : static_cast<decltype(tmp13())>(0.0);<br />
                                auto tmp21 = static_cast<float>(0.0);<br />
                                auto tmp22 = tmp12 ? tmp20 : tmp21;<br />
                                auto tmp23 = tmp6 ? tmp9 : tmp22;<br />
                                return tmp23;<br />
                            }<br />
                            ;<br />
                            auto tmp24 = tmp2 ? tmp3() : static_cast<decltype(tmp3())>(0.0);<br />
                            auto tmp25 = c10::convert<int64_t>(c10::div_floor_integer(x1, 256L));<br />
                            auto tmp26 = static_cast<int64_t>(3);<br />
                            auto tmp27 = tmp25 &lt; tmp26;<br />
                            auto tmp28 = [&amp;]<br />
                            {<br />
                                auto tmp29 = c10::convert<int64_t>(x3);<br />
                                auto tmp30 = tmp29 &gt;= tmp1;<br />
                                auto tmp31 = [&amp;]<br />
                                {<br />
                                    auto tmp32 = in_ptr0[static_cast<long>((-256L) + x3 + (513L</em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L<em>(c10::div_floor_integer(x1, 256L))) + (787968L</em>x2) + (9455616L<em>x0))];<br />
                                    return tmp32;<br />
                                }<br />
                                ;<br />
                                auto tmp33 = tmp30 ? tmp31() : static_cast<decltype(tmp31())>(0.0);<br />
                                auto tmp34 = static_cast<float>(0.0);<br />
                                auto tmp35 = tmp30 ? tmp33 : tmp34;<br />
                                return tmp35;<br />
                            }<br />
                            ;<br />
                            auto tmp36 = tmp27 ? tmp28() : static_cast<decltype(tmp28())>(0.0);<br />
                            auto tmp37 = static_cast<float>(0.0);<br />
                            auto tmp38 = tmp27 ? tmp36 : tmp37;<br />
                            auto tmp39 = tmp2 ? tmp24 : tmp38;<br />
                            out_ptr1[static_cast<long>(x3 + (513L</em>x1) + (525312L<em>x2) + (6303744L</em>x0))] = tmp39;<br />
                        }<br />
                    }<br />
                }<br />
            }<br />
        }<br />
    }<br />
}<br />
''')<br />
```</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* (to be filled)</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 00:23:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124921</guid>
    </item>
    <item>
      <title>Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?</title>
      <link>https://github.com/pytorch/pytorch/issues/124918</link>
      <description><![CDATA[<h3>🐛 Describe the bug</h3>
<p>Hello PyTorch Community,</p>
<p>I am currently developing a custom operator and considering its compatibility with torch.compile. I have a specific question regarding the function signature of the operator: Does the output tensor need to be explicitly returned by the function, or can it be passed as an input argument?</p>
<p>If I pass the output as an input parameter when creating a custom operator, with the operator interface having no return value, I encounter incorrect results when running with torch.compile, but correct results in eager mode. Based on my analysis, torch.compile seems to optimize away the custom operators that do not return any value, leading to incorrect outcomes.</p>
<h3>Error logs</h3>
<p>Traceback (most recent call last):<br />
  File "/tmp/test.py", line 40, in <module><br />
    assert torch.allclose(out, x * y)<br />
AssertionError</p>
<h3>Minified repro</h3>
<p>```python<br />
import torch<br />
import numpy as np                                                                                                                                                           </p>
<p>def custom_func(x, y, out):                                                                                                          <br />
    torch.<em>check(x.shape == y.shape)<br />
    torch._check(x.device == y.device)<br />
    x_np = x.numpy()<br />
    y_np = y.numpy()<br />
    z_np = np.multiply(x_np, y_np)<br />
    out.copy</em>(torch.from_numpy(z_np))<br />
    return</p>
<p>torch.library.define("mylib::custom_func", "(Tensor x, Tensor y, Tensor out) -&gt; None")<br />
 # Add the implementation of the custom op                                                                                                                               <br />
torch.library.impl("mylib::custom_func", "default", custom_func)</p>
<h1>Add an abstract impl that describes what the properties of the output  tensor are, given the properties of the input Tensors.</h1>
<p>@torch.library.impl_abstract("mylib::custom_func")<br />
def custom_func_abstract(x, y, out):<br />
    torch._check(x.shape == y.shape)<br />
    torch._check(x.device == y.device)<br />
    torch._check(out.shape == y.shape)<br />
    torch._check(out.device == y.device)<br />
    return</p>
<p>@torch.compile(backend="inductor", fullgraph=True)<br />
def f(x, y, out):<br />
    return torch.ops.mylib.custom_func.default(x, y, out)</p>
<p>x = torch.randn(3)<br />
y = torch.randn(3)<br />
out = torch.empty_like(x)<br />
z = f(x, y, out)<br />
assert torch.allclose(out, x * y)<br />
```</p>
<h3>Versions</h3>
<p>PyTorch version: 2.3.0a0+40ec155e58.nv24.03                                                                                                                                <br />
Is debug build: False                                                                                                                                                      <br />
CUDA used to build PyTorch: 12.4                                                                                                                                           <br />
ROCM used to build PyTorch: N/A                                                                                                                                              </p>
<p>OS: Ubuntu 22.04.4 LTS (x86_64)                                                                                                                                            <br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0                                                                                                                         <br />
Clang version: Could not collect                                                                                                                                           <br />
CMake version: version 3.28.3                                                                                                                                              <br />
Libc version: glibc-2.35                                                                                                                                                     </p>
<p>Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)                                                                                        <br />
Python platform: Linux-5.4.0-150-generic-x86_64-with-glibc2.35                                                                                                             <br />
Is CUDA available: True                                                                                                                                                    <br />
CUDA runtime version: 12.4.99                                                                                                                                              <br />
CUDA_MODULE_LOADING set to: LAZY                                                                                                                                           <br />
GPU models and configuration:                                                                                                                                              <br />
GPU 0: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 1: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 2: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 3: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 4: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 5: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 6: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 7: NVIDIA A100 80GB PCIe                                                                                                                                                 </p>
<p>Nvidia driver version: 525.89.02                                                                                                                                           <br />
cuDNN version: Probably one of the following:                                                                                                                              <br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0                                                                                                                                <br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0                                                                                                                            <br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.4<br />
[pip3] onnx==1.15.0rc2<br />
[pip3] optree==0.10.0<br />
[pip3] pytorch-quantization==2.1.2<br />
[pip3] pytorch-triton==2.2.0+e28a256d7<br />
[pip3] torch==2.3.0a0+40ec155e58.nv24.3<br />
[pip3] torch-tensorrt==2.3.0a0<br />
[pip3] torchdata==0.7.1a0<br />
[pip3] torchtext==0.17.0a0<br />
[pip3] torchvision==0.18.0a0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @zou3519</p>]]></description>
      <pubDate>Wed, 24 Apr 2024 23:45:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124918</guid>
    </item>
    <item>
      <title>`RuntimeError: invalid dtype for bias` when use compile + autocast</title>
      <link>https://github.com/pytorch/pytorch/issues/124901</link>
      <description><![CDATA[<h3>🐛 Describe the bug</h3>
<p>When I tried using <code>torch.compile</code> along with <code>autocast</code> to infereance a llama's decoder block, I encountered <code>RuntimeError: invalid dtype for bias - should match query's dtype</code>.</p>
<p>```python<br />
import torch<br />
from transformers import AutoModelForCausalLM, AutoTokenizer<br />
from torch.amp import autocast</p>
<p>model_name = "meta-llama/Llama-2-7b-hf"<br />
model = AutoModelForCausalLM.from_pretrained(model_name)<br />
tokenizer = AutoTokenizer.from_pretrained(model_name)<br />
device = "cuda"<br />
amp_dtype = torch.float16<br />
block = model.model.layers[0]<br />
block = block.to(device).to(amp_dtype)</p>
<p>input_ids = torch.randn(8, 2048, 4096).to(amp_dtype).to(device)<br />
input_others = {<br />
    "attention_mask": torch.randn(8, 1, 4096, 4096).to(amp_dtype).to(device),<br />
    "position_ids": torch.arange(2048).unsqueeze(0).to(torch.int64).to(device),<br />
    "cache_position": torch.arange(2048).to(torch.int64).to(device),<br />
}</p>
<p>opt_block = torch.compile(block)<br />
out_without_amp = opt_block.forward(input_ids, **input_others)<br />
print(f"out_without_amp[0].shape: {out_without_amp[0].shape}")</p>
<p>with autocast(device_type=device, dtype=amp_dtype):<br />
    out = opt_block.forward(input_ids, **input_others)<br />
    print(f"out[0].shape: {out[0].shape}")<br />
```</p>
<h3>Versions</h3>
<p>```bash<br />
$ python collect_env.py <br />
Collecting environment information...<br />
PyTorch version: 2.4.0.dev20240318+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 10.5.0-1ubuntu1~20.04) 10.5.0<br />
Clang version: Could not collect<br />
CMake version: version 3.29.0<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.4.0-169-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 12.3.52<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A100-SXM4-80GB<br />
GPU 1: NVIDIA A100-SXM4-80GB<br />
GPU 2: NVIDIA A100-SXM4-80GB<br />
GPU 3: NVIDIA A100-SXM4-80GB<br />
GPU 4: NVIDIA A100-SXM4-80GB<br />
GPU 5: NVIDIA A100-SXM4-80GB<br />
GPU 6: NVIDIA A100-SXM4-80GB<br />
GPU 7: NVIDIA A100-SXM4-80GB</p>
<p>...<br />
Versions of relevant libraries:<br />
[pip3] functorch==1.14.0a0+b71aa0b<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+989adb9a29<br />
[pip3] torch==2.4.0.dev20240318+cu121<br />
[pip3] torchaudio==2.2.1<br />
[pip3] torchvision==0.18.0.dev20240318+cu121<br />
[pip3] triton==2.2.0<br />
[conda] blas                      1.0                         mkl<br />
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch<br />
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch<br />
[conda] mkl                       2023.1.0         h213fc3f_46344<br />
[conda] mkl-service               2.4.0            py39h5eee18b_1<br />
[conda] mkl_fft                   1.3.8            py39h5eee18b_0<br />
[conda] mkl_random                1.2.4            py39hdb19cb5_0<br />
[conda] numpy                     1.26.4           py39h5f9d8c6_0<br />
[conda] numpy-base                1.26.4           py39hb5e798b_0<br />
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch<br />
[conda] pytorch-triton            3.0.0+989adb9a29          pypi_0    pypi<br />
[conda] torch                     2.1.2                    pypi_0    pypi<br />
[conda] torchaudio                2.2.1                py39_cu121    pytorch<br />
[conda] torchtriton               2.2.0                      py39    pytorch<br />
[conda] torchvision               0.18.0.dev20240318+cu121          pypi_0    pypi<br />
```</p>
<p>cc @mcarilli @ptrblck @leslie-fang-intel @jgong5 @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Wed, 24 Apr 2024 17:42:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124901</guid>
    </item>
    <item>
      <title>Codegen runtime asserts in Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/124874</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #125034<br />
* <strong>-&gt;</strong> #124874<br />
* #124864</p>
<p>This completely subsumes https://github.com/pytorch/pytorch/pull/120816</p>
<p>This makes use of the unbacked binding machinery to teach Inductor how to generate deferred runtime asserts directly. There is some back story about why I did it this way, let me explain.</p>
<p>Previously, our strategy for generating runtime asserts was that Dynamo would insert them into the FX graph after finishing tracing, and we would attempt to code generate them based on the FX graph. This is a good strategy for export, where we immediately export the graph. However, this strategy was afflicted by problems in eager, where we reuse the same ShapeEnv as before. In particular, on subsequent graph passes, we would immediately turn all of these assertions into noops, because when we evaluated their expressions, we would see that because we had a deferred runtime assert in the ShapeEnv, we know "oh, of course this expression is True" already. Oops!</p>
<p>So, with this PR, we take the attitude that as long as the ShapeEnv sticks around, the ShapeEnv's list of deferred runtime asserts is the source of truth, and we don't put anything in the graph. So we just need to decide when to actually generate asserts, and the place I picked was Inductor lowering, since we already have an AssertScalar buffer concept, and so I just need to insert them at this point. AssertScalar also uses raw sympy.Expr rather than SymInt/Bool, so it is easier to prevent unrestricted simplification at this point.</p>
<p>There are a few things jumbled together in this PR. I can split them if you want, but some of the changes are before I changed my strategy, but they're useful changes anyway.</p>
<p><strong>torch/_dynamo/output_graph.py</strong> and <strong>torch/_inductor/lowering.py</strong> - Here, we stop putting deferred runtime asserts in the graph. I also have to make sure we don't DCE unused symbol arguments; we're going to get some goofy graph arguments this way, will be good to restore that optimization eventually. We also just disable codegen for <code>_assert_scalar</code>  entirely; we assume that ShapeEnv will be good enough to capture all of these. </p>
<p><strong>torch/_inductor/codegen/wrapper.py</strong> and <strong>torch/_inductor/ir.py</strong> - Add a way to codegen sizevars without forcing simplification</p>
<p><strong>torch/_inductor/graph.py</strong> - The main logic. Our strategy is to interpose in the same place we are testing that unbacked SymInts are properly showing up in lowered code. The logic is directly analogous to the logic in the existing insert deferred runtime asserts FX pass, but it's simpler because sympy expressions can be directly stored on inductor IR nodes.</p>
<p><strong>torch/fx/experimental/symbolic_shapes.py</strong> - For extra safety, we have a way of freezing runtime asserts, so that if you try to add more we error. This prevents us from adding runtime asserts after we've done lowering. There's a funny interaction with backwards which there's a comment for in graph.py</p>
<p><strong>torch/fx/passes/runtime_assert.py</strong> - This is not really needed in this PR, but I rewrote the runtime assert logic to use unbacked_bindings rather than inferring it by looking for unbacked SymInts. Now, keypaths are translated into FX node acessors. Unfortunately, I couldn't delete the old inference code, because you still need it to find backed SymInts from arguments (as this pass may be used on graphs which don't explicitly bind all their shape variables as argments). There are some new tests exercising this.</p>
<p>TODO: I think we need to generate asserts for replacements too. This is a preexisting problem that the old FX pass had too.</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 24 Apr 2024 11:47:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124874</guid>
    </item>
    <item>
      <title>[Inductor] Support fusion of chained reductions even if keepdims=True</title>
      <link>https://github.com/pytorch/pytorch/pull/124843</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124843</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 24 Apr 2024 06:14:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124843</guid>
    </item>
    <item>
      <title>[cpu] [inductor] decompose bmm for memory bound in lowering</title>
      <link>https://github.com/pytorch/pytorch/pull/124826</link>
      <description><![CDATA[<p>Fixes #124697. Resolve the issue of large regression of GPT-FAST MOE with <code>coordinate_descent_tuning</code> disabled.</p>
<p>To get better perf for memory bound case, we decompose bmm in lowering.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 23:21:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124826</guid>
    </item>
    <item>
      <title>AOTInductor error: Unsupported reduction type from torch.float32 to torch.int64 </title>
      <link>https://github.com/pytorch/pytorch/issues/124821</link>
      <description><![CDATA[<h3>🐛 Describe the bug</h3>
<p>when trying the following code, it will throw an error. I think it's related to int64, when I change the input type int float32, there is no such a problem.<br />
```<br />
import numpy as np<br />
import torch</p>
<h1>Model definition</h1>
<p>class M(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.v2_0 = torch.nn.Parameter(torch.empty([1, 22, 51], dtype=torch.int64), requires_grad=False)</p>
<pre><code>def forward(self, _args):
    v2_0 = self.v2_0
    getitem = _args
    max_1 = getitem.max(0)
    getattr_1 = max_1.values
    max_2 = torch.max(getitem, v2_0)
    return (getattr_1, max_2)
</code></pre>
<p>m = M()</p>
<p>inp =  torch.from_numpy(np.zeros((22, 51),dtype=np.int64))<br />
m(inp) # this line is OK<br />
opt = torch.compile(m, fullgraph=True, backend='inductor', mode=None)<br />
opt(inp) # this line will crash<br />
```</p>
<h3>Error logs</h3>
<p>C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0] Error in codegen for ComputedBuffer(name='buf0', layout=FixedLayout('cpu', torch.int64, size=[51], stride=[1]), data=Reduction(<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   'cpu',<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   torch.int64,<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   def inner_fn(index, rindex):<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]       i0 = index<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]       r0 = rindex<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]       tmp0 = ops.load(arg1_1, i0 + 51 * r0)<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]       return tmp0<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   ,<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   ranges=[51],<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   reduction_ranges=[22],<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   reduction_type=max,<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   origin_node=getitem,<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   origins={max_1}<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0] ))<br />
W0424 14:47:08.389000 139979197380416 torch/_dynamo/repro/after_dynamo.py:108] [0/0] Compiled Fx GraphModule failed. Creating script to minify the error.<br />
W0424 14:47:08.390000 139979197380416 torch/_dynamo/debug_utils.py:276] [0/0] Writing minified repro to:<br />
W0424 14:47:08.390000 139979197380416 torch/_dynamo/debug_utils.py:276] [0/0] /home/zhangzihan/nnsmith/torch_compile_debug/run_2024_04_24_14_47_08_390122-pid_1824613/minifier/minifier_launcher.py<br />
W0424 14:47:08.390000 139979197380416 torch/_dynamo/repro/after_dynamo.py:108] [0/0] Compiled Fx GraphModule failed. Creating script to minify the error.<br />
W0424 14:47:08.391000 139979197380416 torch/_dynamo/debug_utils.py:276] [0/0] Writing minified repro to:<br />
W0424 14:47:08.391000 139979197380416 torch/_dynamo/debug_utils.py:276] [0/0] /home/zhangzihan/nnsmith/torch_compile_debug/run_2024_04_24_14_47_08_390122-pid_1824613/minifier/minifier_launcher.py<br />
Traceback (most recent call last):<br />
  File "/home/zhangzihan/nnsmith/./bug_1.py", line 26, in <module><br />
    opt(inp) # this line will crash<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state, skip=1)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert<br />
    return _compile(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_utils_internal.py", line 70, in wrapper_function<br />
    return function(*args, </strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(<em>args, </em><em>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform<br />
    tracer.run()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2237, in run<br />
    super().run()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 875, in run<br />
    while self.step():<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 790, in step<br />
    self.dispatch_table<a href="self, inst">inst.opcode</a><br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2394, in RETURN_VALUE<br />
    self._return(inst)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2379, in _return<br />
    self.output.compile_subgraph(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1082, in compile_subgraph<br />
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(<em>args, </em><em>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1274, in compile_and_call_fx_graph<br />
    compiled_fn = self.call_user_compiler(gm)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/<em>dynamo/output_graph.py", line 1365, in call_user_compiler<br />
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1346, in call_user_compiler<br />
    compiled_fn = compiler_fn(gm, self.example_inputs())<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 105, in debug_wrapper<br />
    compiled_gm = compiler_fn(copy.deepcopy(gm), example_inputs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 105, in debug_wrapper<br />
    compiled_gm = compiler_fn(copy.deepcopy(gm), example_inputs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/<strong>init</strong>.py", line 1742, in <strong>call</strong><br />
    return compile_fx(model</em>, inputs_, config_patches=self.config)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1416, in compile_fx<br />
    return aot_autograd(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn<br />
    cg = aot_module_simplified(gm, example_inputs, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified<br />
    compiled_fn = create_aot_dispatcher_function(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function<br />
    compiled_fn = compiler_fn(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe<br />
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base<br />
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 149, in aot_dispatch_base<br />
    compiled_fw = compiler(fw_module, updated_flat_args)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(<em>args, </em><em>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1320, in fw_compiler_base<br />
    return inner_compile(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper<br />
    inner_compiled_fn = compiler_fn(gm, example_inputs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/debug.py", line 304, in inner<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(<em>args, </em><em>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 477, in compile_fx_inner<br />
    compiled_graph = fx_codegen_and_compile(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 752, in fx_codegen_and_compile<br />
    compiled_fn = graph.compile_to_fn()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1541, in compile_to_fn<br />
    return self.compile_to_module().call<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(<em>args, </em><em>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1484, in compile_to_module<br />
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1440, in codegen<br />
    self.scheduler.codegen()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 2485, in codegen<br />
    self.get_backend(device).codegen_node(node)  # type: ignore[possibly-undefined]<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/cpp.py", line 3865, in codegen_node<br />
    cpp_kernel_proxy.codegen_nodes(nodes)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/cpp.py", line 3517, in codegen_nodes<br />
    vec_kernel = codegen_kernel(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/cpp.py", line 3411, in codegen_kernel<br />
    run(kernel)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/cpp.py", line 3423, in run<br />
    node.run(vars, reduction_vars)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 765, in run<br />
    self.codegen(index_vars)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 787, in codegen<br />
    self._body(<em>index_vars)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/ir.py", line 7760, in <strong>call</strong><br />
    result = self.root_block()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/ir.py", line 7917, in <strong>call</strong><br />
    return InterpreterShim(graph, submodules).run(V.get_ops_handler())<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/ir.py", line 7662, in run<br />
    return super().run(</em>args, </strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/fx/interpreter.py", line 145, in run<br />
    self.env[node] = self.run_node(node)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/ir.py", line 7658, in run_node<br />
    return super().run_node(n)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/fx/interpreter.py", line 202, in run_node<br />
    return getattr(self, n.op)(n.target, args, kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/fx/interpreter.py", line 296, in call_method<br />
    return getattr(self_obj, target)(<em>args_tail, </em>*kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/sizevars.py", line 641, in store_reduction<br />
    return self._inner.store_reduction(name, self._simplify(index), value)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/common.py", line 1570, in store_reduction<br />
    return self.store_reduction(name, index, value)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/cpp.py", line 2585, in store_reduction<br />
    raise AssertionError(<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
AssertionError: Unsupported reduction type from torch.float32 to torch.int64</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>Minifier script written to /home/zhangzihan/nnsmith/torch_compile_debug/run_2024_04_24_14_47_08_390122-pid_1824613/minifier/minifier_launcher.py. Run this script to find the smallest traced graph which reproduces this error.</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True</p>
<h3>Minified repro</h3>
<p>from math import inf<br />
import torch<br />
from torch import tensor, device<br />
import torch.fx as fx<br />
import torch._dynamo<br />
from torch._dynamo.testing import rand_strided<br />
from torch._dynamo.debug_utils import run_fwd_maybe_bwd</p>
<p>import torch._dynamo.config<br />
import torch._inductor.config<br />
import torch._functorch.config<br />
import torch.fx.experimental._config</p>
<p>from torch.nn import *<br />
class Repro(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.L__self___v2_0 = torch.nn.Parameter(torch.randn([1, 22, 51], dtype=torch.int64))</p>
<pre><code>def forward(self, L_args_ : torch.Tensor):
    l_args_ = L_args_
    v2_0 = self.L__self___v2_0
    max_1 = l_args_.max(0)
    getattr_1 = max_1[0];  max_1 = None
    max_2 = torch.max(l_args_, v2_0);  l_args_ = v2_0 = None
    return (getattr_1, max_2)
</code></pre>
<p>mod = Repro()</p>
<p>def load_args(reader):<br />
    buf0 = reader.storage('fdc2a09f82bac0b6462614f84f565597f805bdca', 8976, dtype_hint=torch.int64)<br />
    reader.tensor(buf0, (22, 51), dtype=torch.int64, is_leaf=True)  # L_args_<br />
load_args._version = 0</p>
<p>if <strong>name</strong> == '<strong>main</strong>':<br />
    from torch._dynamo.repro.after_dynamo import run_repro<br />
    run_repro(mod, load_args, accuracy=False, command='minify',<br />
        save_dir='/home/zhangzihan/nnsmith/torch_compile_debug/run_2024_04_24_14_47_08_390122-pid_1824613/minifier/checkpoints', autocast=False, backend=None)</p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.4.0.dev20240422+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.29.2<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-15-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA GeForce RTX 4090<br />
GPU 1: NVIDIA GeForce RTX 4090</p>
<p>Nvidia driver version: 525.147.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             32<br />
On-line CPU(s) list:                0-31<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         13th Gen Intel(R) Core(TM) i9-13900K<br />
CPU family:                         6<br />
Model:                              183<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 24<br />
Socket(s):                          1<br />
Stepping:                           1<br />
CPU max MHz:                        5800.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           5990.40<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          896 KiB (24 instances)<br />
L1i cache:                          1.3 MiB (24 instances)<br />
L2 cache:                           32 MiB (12 instances)<br />
L3 cache:                           36 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-31<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+45fff310c8<br />
[pip3] torch==2.4.0.dev20240422+cu118<br />
[pip3] torchaudio==2.2.0.dev20240422+cu118<br />
[pip3] torchvision==0.19.0.dev20240422+cu118<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @desertfire @chenyang78</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 22:51:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124821</guid>
    </item>
    <item>
      <title>[inductor] share cse cache during vectorized indirect load</title>
      <link>https://github.com/pytorch/pytorch/pull/124597</link>
      <description><![CDATA[<p>Fix https://github.com/pytorch/pytorch/issues/123502</p>
<p><code>swap_buffer</code> in not needed in vectorized indirect load, remove it to share cse buffer.<br />
<code>auto tmp8 =
[&amp;]
{
    __at_align__ std::array&lt;int64_t, 16&gt; tmpbuf;
    tmp7.store(tmpbuf.data());
    return tmpbuf;
}
()
;
//
// other codes
//
// also store tmp7 here (redundant tmp16)
auto tmp16 =
[&amp;]
{
    __at_align__ std::array&lt;int64_t, 16&gt; tmpbuf;
    tmp7.store(tmpbuf.data());
    return tmpbuf;
}
()
;</code></p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124921<br />
* <strong>-&gt;</strong> #124597</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 21 Apr 2024 22:47:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124597</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Improved GEMM template</title>
      <link>https://github.com/pytorch/pytorch/pull/124577</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124930<br />
* #124929<br />
* #124928<br />
* <strong>-&gt;</strong> #124577<br />
* #124576</p>
<p>Improves the Cutlass backend GEMM template:</p>
<ul>
<li>Adds code which allows to create stand-alone test runners for Cutlass GEMM Kernels, which allows (manual) debugging of, for example, CUDA IMA errors or similar problems which occur in practice. Includes some utility code and tests to actually compile and run these standalone tests.</li>
<li>Cleans up the GEMM template code through various refactorings</li>
<li>Eliminates code sections and options that are unneccessary now that epilogue fusions are being removed.</li>
<li>Limits the scope of a workaround for (flaky) Cutlass issues with bias broadcasting to neccessary cases.</li>
<li>Puts some CPU runtime checks into #if / #endif blocks, such that it's possible to compile CUTLASS Kernels with lower CPU overhead.</li>
<li>Add documentation comments</li>
</ul>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 21 Apr 2024 14:58:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124577</guid>
    </item>
    <item>
      <title>DISABLED test_isolated_node (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124460</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, slow, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_isolated_node&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24005337184">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 21 failures and 7 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_isolated_node</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/inductor/test_compiled_autograd.py -1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)<br />
headers: {}</p>
<p>cc @clee2000 @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 22:40:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124460</guid>
    </item>
    <item>
      <title>DISABLED test_inplace_on_view_weak_grad_fn (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124453</link>
      <description><![CDATA[<p>Platforms: linux, rocm, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_inplace_on_view_weak_grad_fn&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24000916402">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_inplace_on_view_weak_grad_fn</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 19:39:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124453</guid>
    </item>
    <item>
      <title>[inductor] switch assume_aligned_inputs to False</title>
      <link>https://github.com/pytorch/pytorch/pull/124336</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124336</p>
<p>In #123319, we guard some behavior behind the <code>assume_aligned_inputs</code> config option. If we set this to <code>False</code>, then the behavior added in #123319 becomes the default behavior. See the referenced PR for more details about the behavior affected.</p>
<p>Side effects:<br />
* It's possible that this will hurt performance in some scenarios. For example, if an unaligned input is used in a matmul, it might be better to perform the clone to align it first.<br />
* This will occasionally cause recompiles. Specifically: the check we perform (<code>(storage_offset * get_dtype_size(dtype)) % ALIGNMENT == 0</code>) can be guarded on if the storage_offset becomes dynamic. storage_offset becomes dynamic during automatic_dynamic_shapes after a shape or stride changes. Previously, this was increasing graph breaks in cpu inductor torchbench tests (but is fixed by more carefully guarding checks on alignment, so that we don't run them and generate guards unless actually needed).</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 15:14:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124336</guid>
    </item>
    <item>
      <title>[inductor][cpp] GEMM template</title>
      <link>https://github.com/pytorch/pytorch/pull/124021</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124021</p>
<p>cc @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 14 Apr 2024 06:21:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124021</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Enable lowering of qlinear-binary(-unary) fusion for X86Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/122593</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122667<br />
* <strong>-&gt;</strong> #122593<br />
* #122387<br />
* #123240</p>
<p><strong>Description</strong><br />
Lower the qlinear binary post op pattern to Inductor. Use post op sum (in-place) if the extra input has the same dtype as output. Otherwise, it uses binary add.</p>
<p><strong>Supported linear-binary(-unary) patterns</strong><br />
```<br />
    linear(X)   extra input<br />
           \   /<br />
            Add<br />
             |<br />
        Optional(relu)<br />
             |<br />
             Y</p>
<ol>
<li>
<p>int8-mixed-fp32<br />
+---+---------------+-----------+------------------------------+---------+<br />
| # | Add type      | Quant out | Pattern                      | Post op |<br />
+---+---------------+-----------+------------------------------+---------+<br />
| 1 | In-/out-place | Yes       | linear + fp32 -&gt; (relu) -&gt; q | add     |<br />
+---+---------------+-----------+------------------------------+---------+<br />
| 2 | In-/out-place | No        | linear + fp32 -&gt; (relu)      | sum     |<br />
+---+---------------+-----------+------------------------------+---------+</p>
</li>
<li>
<p>int8-mixed-bf16<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| # | X2 dtype | Add type      | Quant out | Pattern                                          | Post op |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 1 | BF16     | In-/out-place | Yes       | linear + bf16 -&gt; (relu) -&gt; to_fp32 -&gt; q          | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 2 | BF16     | In-/out-place | No        | linear + bf16 -&gt; (relu)                          | sum     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 3 | FP32     | Out-place     | Yes       | linear + fp32 -&gt; (relu) -&gt; q                     | add     |<br />
|   |          | In-place right|           |                                                  |         |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 4 | FP32     | Out-place     | No        | linear + fp32 -&gt; (relu)                          | sum     |<br />
|   |          | In-place right|           |                                                  |         |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 5 | FP32     | In-place left | Yes       | linear + fp32 -&gt; to_bf16 -&gt; relu -&gt; to_fp32 -&gt; q | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 6 | FP32     | In-place left | No        | linear + fp32 -&gt; to_bf16 -&gt; (relu)               | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
```<br />
Note<br />
(1) The positions of linear and the extra input can be swapped.<br />
(2) we don't insert q-dq before the extra input of linear-add by recipe. But if q-dq is found at the<br />
extra input, we don't match that pattern because we cannot match all these patterns in 3 passes.</p>
</li>
</ol>
<p><strong>Test plan</strong><br />
python test/inductor/test_mkldnn_pattern_matcher.py -k test_qlinear_add<br />
python test/inductor/test_cpu_cpp_wrapper.py -k test_qlinear_add</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Mar 2024 23:49:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122593</guid>
    </item>
    <item>
      <title>[Quant][PT2E] Enable linear-binary(-unary) post-op recipe for X86Inductor quantizer</title>
      <link>https://github.com/pytorch/pytorch/pull/122387</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122667<br />
* #122593<br />
* <strong>-&gt;</strong> #122387<br />
* #123240</p>
<p>As the title<br />
<strong>Test plan</strong><br />
python test/test_quantization.py -k test_linear_binary</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D56288440">D56288440</a></p>]]></description>
      <pubDate>Wed, 20 Mar 2024 22:29:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122387</guid>
    </item>
    <item>
      <title>Add registration API for torch.compile-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/121387</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we intend to provide a registration API dedicated to eager-through-torch.compile. The major workflow of this API will be as follows.</p>
<ul>
<li>Load cache</li>
<li>Check cache according to the input tensors</li>
<li>Cache Hit: Run the cached kernel directly</li>
<li>Cache Miss: Run the AOTI to produce kernel and run the produced kernel. If AOTI fails to produce the kernel, invoke the python fallback function.</li>
</ul>
<p>Currently, this PR always fallback to python kernel now and cache mechanism will be implemented in another PR - https://github.com/pytorch/pytorch/pull/116368</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124926<br />
* #124070<br />
* #124177<br />
* #116368<br />
* #124836<br />
* <strong>-&gt;</strong> #121387</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121387</guid>
    </item>
    <item>
      <title>Torch compile does not work on python 3.12</title>
      <link>https://github.com/pytorch/pytorch/issues/120233</link>
      <description><![CDATA[<h3>🐛 Describe the bug</h3>
<p>Currently torch, as of 2.2.0 does not support torch compile with python 3.12</p>
<p>See following PR for example: https://github.com/pytorch/pytorch/pull/117853</p>
<ol>
<li>We need to be able to use python 3.12 with torch.compile feature. </li>
<li>Include triton with Linux 3.12 wheel </li>
<li>Enable Python 3.12 and torch.compile CI testsing</li>
</ol>
<p>cc: @albanD @malfet </p>
<h3>Versions</h3>
<p>2.2.0</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Tue, 20 Feb 2024 06:14:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120233</guid>
    </item>
    <item>
      <title>Fix typo under torch/_inductor directory</title>
      <link>https://github.com/pytorch/pytorch/pull/119658</link>
      <description><![CDATA[<p>This PR fixes typo in comments and msgs under <code>torch/_inductor</code> directory, and also changes the corresponding test.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov</p>]]></description>
      <pubDate>Sun, 11 Feb 2024 10:05:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119658</guid>
    </item>
    <item>
      <title>Investigate Strictness of torch.compile `is_big_gpu`</title>
      <link>https://github.com/pytorch/pytorch/issues/109489</link>
      <description><![CDATA[<h3>🐛 Describe the bug</h3>
<p>The check was introduced in https://github.com/pytorch/pytorch/pull/90738 to fix: https://github.com/pytorch/torchdynamo/issues/2015</p>
<p>However, this causes Triton matmul template codegen not to occur on smaller GPUs (sms &lt; 80), including most people's commercial GPUs.</p>
<p>We make two observations:<br />
1. This check was a hotfix that is too strict for most cases<br />
2. V100 is on the testing path of Triton, so it is likely that this issue is fixed</p>
<p>Hence, my proposal is to get rid of the check entirely, as long as CI passes.</p>
<p>CC @jansel as prev discussed (and original code author)</p>
<h3>Error logs</h3>
<p><code>[WARNING] not enough SMs to use max_autotune_gemm mode</code></p>
<h3>Minified repro</h3>
<p>Use torch.compile on small GPU</p>
<h3>Versions</h3>
<details>

PyTorch version: 2.2.0a0+gitc29c493
Is debug build: False
CUDA used to build PyTorch: 12.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.27.2
Libc version: glibc-2.35

Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35
Is CUDA available: False
CUDA runtime version: 12.2.128
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU
Nvidia driver version: 535.86.10
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.4
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      46 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             20
On-line CPU(s) list:                0-19
Vendor ID:                          GenuineIntel
Model name:                         13th Gen Intel(R) Core(TM) i9-13900H
CPU family:                         6
Model:                              186
Thread(s) per core:                 2
Core(s) per socket:                 14
Socket(s):                          1
Stepping:                           2
CPU max MHz:                        5400.0000
CPU min MHz:                        400.0000
BogoMIPS:                           5990.40
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities
Virtualization:                     VT-x
L1d cache:                          544 KiB (14 instances)
L1i cache:                          704 KiB (14 instances)
L2 cache:                           11.5 MiB (8 instances)
L3 cache:                           24 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-19
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] flake8==6.0.0
[pip3] flake8-bugbear==23.3.23
[pip3] flake8-comprehensions==3.12.0
[pip3] flake8-executable==2.1.3
[pip3] flake8-logging-format==0.9.0
[pip3] flake8-pyi==23.3.1
[pip3] flake8-simplify==0.19.3
[pip3] jax-triton==0.1.4
[pip3] mypy==1.4.1
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.24.3
[pip3] pytorch-triton==2.1.0+6e4932cda8
[pip3] torch==2.2.0a0+git84680cb
[pip3] torchaudio==2.0.2
[pip3] torchvision==0.15.2
[pip3] triton==2.1.0
[conda] Could not collect

</details>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @Xia-Weiwen @ngimel</p>]]></description>
      <pubDate>Mon, 18 Sep 2023 02:44:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/109489</guid>
    </item>
  </channel>
</rss>
