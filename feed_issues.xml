<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[compiled autograd] introduce verbose logs, add autograd node info to graph</title>
      <link>https://github.com/pytorch/pytorch/pull/124954</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124954</p>
<ul>
<li>sets it as a fake stack trace as we don't have a generic comment feature</li>
<li>adds a context and a flag check to non-verbose compiles</li>
</ul>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 10:06:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124954</guid>
    </item>
    <item>
      <title>torch.compile fails on hugging face Mistral7b</title>
      <link>https://github.com/pytorch/pytorch/issues/124946</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><strong>The following code fails:</strong></p>
<p>import torch<br />
from transformers import AutoModelForCausalLM, AutoTokenizer</p>
<p>model = AutoModelForCausalLM.from_pretrained(<br />
    "mistralai/Mistral-7B-v0.1",<br />
    torch_dtype=torch.float16,<br />
    use_cache=True,<br />
)<br />
model = torch.compile(model)</p>
<p>input_ids =torch.randint(low=0, high=60000, size=(1, 32), dtype=torch.int64)<br />
attention_mask = torch.ones(1, 32, dtype=torch.int64)<br />
model(input_ids=input_ids, attention_mask=attention_mask)</p>
<p><strong>It generates the following error:</strong></p>
<p>Traceback (most recent call last):<br />
  File "/work1/sleduc/torch-trials/compile_mistral7b.py", line 13, in <module><br />
    model(input_ids=input_ids, attention_mask=attention_mask)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl                                                                                                                                                                                                                                                                                                                      return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 1157, in forward<br />
    outputs = self.model(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 655, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 727, in _convert_frame<br />
    result = inner_convert(frame, cache_entry, hooks, frame_state)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 383, in _convert_frame_assert<br />
    compiled_product = _compile(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 646, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 562, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 151, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 527, in transform<br />
    tracer.run()<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2128, in run<br />
    super().run()<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 818, in run<br />
    and self.step()<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 781, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 465, in wrapper<br />
    return handle_graph_break(self, inst, speculation.reason)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 521, in handle_graph_break<br />
    self.output.compile_subgraph(self, reason=reason)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 945, in compile_subgraph<br />
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)<br />
  File "/usr/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(<em>args, </em><em>kwds)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1087, in compile_and_call_fx_graph<br />
    compiled_fn = self.call_user_compiler(gm)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/<em>dynamo/output_graph.py", line 1159, in call_user_compiler<br />
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1140, in call_user_compiler<br />
    compiled_fn = compiler_fn(gm, self.example_inputs())<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper<br />
    compiled_gm = compiler_fn(gm, example_inputs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/<strong>init</strong>.py", line 1668, in <strong>call</strong><br />
    return compile_fx(model</em>, inputs_, config_patches=self.config)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1168, in compile_fx<br />
    return aot_autograd(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 55, in compiler_fn<br />
    cg = aot_module_simplified(gm, example_inputs, </strong>kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 887, in aot_module_simplified<br />
    compiled_fn = create_aot_dispatcher_function(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(<em>args, </em>*kwargs)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 600, in create_aot_dispatcher_function<br />
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 425, in aot_wrapper_dedupe<br />
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 630, in aot_wrapper_synthetic_base<br />
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 181, in aot_dispatch_autograd<br />
    fw_module, bw_module = aot_config.partition_fn(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1128, in partition_fn<br />
    return min_cut_rematerialization_partition(<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/torch/_functorch/partitioners.py", line 650, in min_cut_rematerialization_partition<br />
    import networkx as nx<br />
  File "/usr/lib/python3/dist-packages/networkx/<strong>init</strong>.py", line 115, in <module><br />
    import networkx.readwrite<br />
  File "/usr/lib/python3/dist-packages/networkx/readwrite/<strong>init</strong>.py", line 15, in <module><br />
    from networkx.readwrite.graphml import *<br />
  File "/usr/lib/python3/dist-packages/networkx/readwrite/graphml.py", line 314, in <module><br />
    class GraphML(object):<br />
  File "/usr/lib/python3/dist-packages/networkx/readwrite/graphml.py", line 346, in GraphML<br />
    (np.int, "int"), (np.int8, "int"),<br />
  File "/work1/sleduc/.python/lib/python3.10/site-packages/numpy/<strong>init</strong>.py", line 324, in <strong>getattr</strong><br />
    raise AttributeError(<strong>former_attrs</strong>[attr])<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
AttributeError: module 'numpy' has no attribute 'int'.<br />
<code>np.int</code> was a deprecated alias for the builtin <code>int</code>. To avoid this error in existing code, use <code>int</code> by itself. Doing this will not modify any behavior and is safe. When replacing <code>np.int</code>, you may wish to use e.g. <code>np.int64</code> or <code>np.int32</code> to specify the precision. If you wish to review your current use, check the release note link for additional information.<br />
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:<br />
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True</p>
<h3>Versions</h3>
<p><strong>Versions:</strong></p>
<p>PyTorch version: 2.2.2+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.4 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-73-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA L40S<br />
Nvidia driver version: 550.67<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Address sizes:                   52 bits physical, 57 bits virtual<br />
Byte Order:                      Little Endian<br />
CPU(s):                          64<br />
On-line CPU(s) list:             0-63<br />
Vendor ID:                       AuthenticAMD<br />
Model name:                      AMD EPYC 9334 32-Core Processor<br />
CPU family:                      25<br />
Model:                           17<br />
Thread(s) per core:              1<br />
Core(s) per socket:              32<br />
Socket(s):                       2<br />
Stepping:                        1<br />
Frequency boost:                 enabled<br />
CPU max MHz:                     3910.2529<br />
CPU min MHz:                     1500.0000<br />
BogoMIPS:                        5399.98<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d<br />
Virtualization:                  AMD-V<br />
L1d cache:                       2 MiB (64 instances)<br />
L1i cache:                       2 MiB (64 instances)<br />
L2 cache:                        64 MiB (64 instances)<br />
L3 cache:                        256 MiB (8 instances)<br />
NUMA node(s):                    2<br />
NUMA node0 CPU(s):               0-31<br />
NUMA node1 CPU(s):               32-63<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Not affected<br />
Vulnerability Retbleed:          Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] onnx==1.16.0<br />
[pip3] onnx-graphsurgeon==0.5.2<br />
[pip3] onnxruntime==1.16.3<br />
[pip3] onnxsim==0.4.36<br />
[pip3] torch==2.2.2<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 08:34:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124946</guid>
    </item>
    <item>
      <title>[RFC] Support reinplaceble ops for custom ops in Inductor</title>
      <link>https://github.com/pytorch/pytorch/issues/124933</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>Currently, the inductor backend in PyTorch supports the reinplace optimization for a fixed list of ops in the reinplace fx pass (<code>torch/_inductor/fx_passes/reinplace.py</code>). However, for custom inplace ops that are functionalized into out-of-place ops using <code>copy_</code>, they cannot be reinplaced, which impacts performance. To address this, we propose adding a configuration option to the <code>torch.compile</code> inductor backend that allows users to specify the mapping between out-of-place ops and their inplace counterparts. This would enable the reinplace pass to support the reinplace optimization for these custom ops as well, potentially improving performance.</p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @zou3519</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 05:12:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124933</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Fix cutlass_utils.get_max_alignment() for strided layouts.</title>
      <link>https://github.com/pytorch/pytorch/pull/124930</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* <strong>-&gt;</strong> #124930<br />
* #124929<br />
* #124928<br />
* #124577<br />
* #124576</p>
<p>Fixes cutlass_utils.get_max_alignment() which was so far not checking the alignment properly. Basically<br />
the method so far assumed that the passed layout is contiguous and row-major, which does not have to be true.</p>
<p>Test Plan:<br />
CI - test_cutlass_backend.py to prevent regressions<br />
TODO: Add unit test</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 03:21:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124930</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Remove epilogue nodes from Kernel call</title>
      <link>https://github.com/pytorch/pytorch/pull/124929</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124930<br />
* <strong>-&gt;</strong> #124929<br />
* #124928<br />
* #124577<br />
* #124576</p>
<p>Minor refactoring:<br />
Remove unused "fused epilogue node" arguments from some method  Kernel call signatures.</p>
<p>Test Plan:<br />
Covered by current tests in test_cutlass_backend.py - no functional change.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 03:14:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124929</guid>
    </item>
    <item>
      <title>[Inductor max autotune] Make autotune_select_algorithm more robust</title>
      <link>https://github.com/pytorch/pytorch/pull/124928</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124930<br />
* #124929<br />
* <strong>-&gt;</strong> #124928<br />
* #124577<br />
* #124576</p>
<p>This diff makes sure that a custom exception is thrown when no valid<br />
choices remain during autotuning. This allows to gracefully fall back<br />
to a default choice, even if that default choice has not been passed to<br />
autotune_select_algorithm.</p>
<p>Additionally, this diff handles RuntimeErrors during autotuning gracefully, e.g. the corresponding choice is ignored but it does not lead to the compilation failure of the entire model if a problematic choice is encountered during autotuning.<br />
( An error is being logged, though).</p>
<p>TODO:<br />
 * Add unit test<br />
 * Add an assertion that we use autune_in_subproc when CUTLASS backend is enabled</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 03:14:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124928</guid>
    </item>
    <item>
      <title>[inductor] share more cse cache during swap buffer</title>
      <link>https://github.com/pytorch/pytorch/pull/124921</link>
      <description><![CDATA[<p><code>swap_buffer</code> will make the <code>cse_cache</code> cannot be shared inside/outside of the lambda function scope.<br />
For example,</p>
<p><code>auto tmp8 = -std::numeric_limits&lt;float&gt;::infinity();
auto tmp9 = [&amp;]
{
    auto tmp12 = -std::numeric_limits&lt;float&gt;::infinity();
    return tmp12;
}</code><br />
<code>tmp12</code> should not be created since it is same with <code>tmp8</code>.</p>
<p>We make the <code>cse_cache</code> as a read only cache inside the scope (because it is unsafe to expose cache inside the scope because the outside scope cannot use it.</p>
<p><strong>Test Plan</strong><br />
<code>python test/inductor/test_torchinductor.py -k test_AllenaiLongformerBase_repro_cpu</code><br />
the <code>static_cast&lt;int&gt;(256)</code> will only occur once after this PR since the inside scope can share the cse buffer outside the scope.</p>
<p>Before this PR, <br />
```<br />
cpp_fused_copy_full_like_0 = async_compile.cpp_pybinding(['const float<em>', 'float</em>'], '''</p>
<h1>include "/tmp/torchinductor_root/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h"</h1>
<p>extern "C" void kernel(const float<em> in_ptr0,<br />
                       float</em> out_ptr1)<br />
{<br />
    #pragma omp parallel num_threads(128)<br />
    {<br />
        int tid = omp_get_thread_num();<br />
        {<br />
            #pragma omp for collapse(2)<br />
            for(long x0=static_cast<long>(0L); x0&lt;static_cast<long>(4L); x0+=static_cast<long>(1L))<br />
            {<br />
                for(long x1=static_cast<long>(0L); x1&lt;static_cast<long>(1024L); x1+=static_cast<long>(1L))<br />
                {<br />
                    #pragma GCC ivdep<br />
                    for(long x2=static_cast<long>(0L); x2&lt;static_cast<long>(12L); x2+=static_cast<long>(1L))<br />
                    {<br />
                        for(long x3=static_cast<long>(0L); x3&lt;static_cast<long>(512L); x3+=static_cast<long>(16L))<br />
                        {<br />
                            auto tmp0 = c10::convert<int>(x1);<br />
                            auto tmp1 = static_cast<int>(256);<br />
                            auto tmp2 = tmp0 &lt; tmp1;<br />
                            auto tmp3 = [&amp;]<br />
                            {<br />
                                auto tmp4 = c10::convert<int>(x3);<br />
                                auto tmp5 = at::vec::Vectorized<int>::arange(tmp4, 1);<br />
                                auto tmp6 = static_cast<int>(257);<br />
                                auto tmp7 = at::vec::Vectorized<int>(tmp6);<br />
                                auto tmp8 = at::vec::VecMask<int,1>(tmp5 &lt; tmp7);<br />
                                auto tmp10 = at::vec::VecMask<float,1>::from(tmp2);<br />
                                auto tmp11 = tmp8 &amp; tmp10;<br />
                                auto tmp9 = [&amp;]<br />
                                {<br />
                                    auto tmp12 = -std::numeric_limits<float>::infinity();<br />
                                    return tmp12;<br />
                                }<br />
                                ;<br />
                                auto tmp13 =<br />
                                [&amp;]<br />
                                {<br />
                                    if (tmp11.all_zero())<br />
                                    {<br />
                                        return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                    }<br />
                                    else<br />
                                    {<br />
                                        return decltype(at::vec::Vectorized<float>(tmp9()))::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), at::vec::Vectorized<float>(tmp9()), tmp11.template cast<float,1>());<br />
                                    }<br />
                                }<br />
                                ()<br />
                                ;<br />
                                auto tmp14 = c10::convert<int>(c10::div_floor_integer(x1, 256L));<br />
                                auto tmp15 = static_cast<int>(3);<br />
                                auto tmp16 = tmp14 &lt; tmp15;<br />
                                auto tmp18 = tmp16 &amp; tmp2;<br />
                                auto tmp17 = [&amp;]<br />
                                {<br />
                                    auto tmp19 = c10::convert<int>(x3);<br />
                                    auto tmp20 = at::vec::Vectorized<int>::arange(tmp19, 1);<br />
                                    auto tmp21 = static_cast<int>(256);<br />
                                    auto tmp22 = at::vec::Vectorized<int>(tmp21);<br />
                                    auto tmp23 = at::vec::VecMask<int,1>(tmp20 &gt;= tmp22);<br />
                                    auto tmp25 = at::vec::VecMask<float,1>::from(tmp18);<br />
                                    auto tmp26 = tmp23 &amp; tmp25;<br />
                                    auto tmp24 = [&amp;]<br />
                                    {<br />
                                        auto tmp27 = tmp26.template cast<float,1>().template loadu<float,1>(in_ptr0 + static_cast<long>((-256L) + x3 + (513L<em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L</em>(c10::div_floor_integer(x1, 256L))) + (787968L<em>x2) + (9455616L</em>x0)));<br />
                                        return tmp27;<br />
                                    }<br />
                                    ;<br />
                                    auto tmp28 =<br />
                                    [&amp;]<br />
                                    {<br />
                                        if (tmp26.all_zero())<br />
                                        {<br />
                                            return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                        }<br />
                                        else<br />
                                        {<br />
                                            return decltype(tmp24())::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), tmp24(), tmp26.template cast<float,1>());<br />
                                        }<br />
                                    }<br />
                                    ()<br />
                                    ;<br />
                                    auto tmp29 = static_cast<float>(0.0);<br />
                                    auto tmp30 = at::vec::Vectorized<float>(tmp29);<br />
                                    auto tmp31 = decltype(tmp28)::blendv(tmp30, tmp28, tmp23.template cast<float,1>());<br />
                                    return tmp31;<br />
                                }<br />
                                ;<br />
                                auto tmp32 = tmp16 ? tmp17() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                auto tmp33 = static_cast<float>(0.0);<br />
                                auto tmp34 = at::vec::VecMask<float,1>::from(tmp16);<br />
                                auto tmp35 = at::vec::Vectorized<float>(tmp33);<br />
                                auto tmp36 = decltype(tmp32)::blendv(tmp35, tmp32, tmp34.template cast<float,1>());<br />
                                auto tmp37 = decltype(tmp13)::blendv(tmp36, tmp13, tmp8.template cast<float,1>());<br />
                                return tmp37;<br />
                            }<br />
                            ;<br />
                            auto tmp38 = tmp2 ? tmp3() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                            auto tmp39 = c10::convert<int>(c10::div_floor_integer(x1, 256L));<br />
                            auto tmp40 = static_cast<int>(3);<br />
                            auto tmp41 = tmp39 &lt; tmp40;<br />
                            auto tmp42 = [&amp;]<br />
                            {<br />
                                auto tmp43 = c10::convert<int>(x3);<br />
                                auto tmp44 = at::vec::Vectorized<int>::arange(tmp43, 1);<br />
                                auto tmp45 = static_cast<int>(256);<br />
                                auto tmp46 = at::vec::Vectorized<int>(tmp45);<br />
                                auto tmp47 = at::vec::VecMask<int,1>(tmp44 &gt;= tmp46);<br />
                                auto tmp49 = at::vec::VecMask<float,1>::from(tmp41);<br />
                                auto tmp50 = tmp47 &amp; tmp49;<br />
                                auto tmp48 = [&amp;]<br />
                                {<br />
                                    auto tmp51 = tmp50.template cast<float,1>().template loadu<float,1>(in_ptr0 + static_cast<long>((-256L) + x3 + (513L<em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L</em>(c10::div_floor_integer(x1, 256L))) + (787968L<em>x2) + (9455616L</em>x0)));<br />
                                    return tmp51;<br />
                                }<br />
                                ;<br />
                                auto tmp52 =<br />
                                [&amp;]<br />
                                {<br />
                                    if (tmp50.all_zero())<br />
                                    {<br />
                                        return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                    }<br />
                                    else<br />
                                    {<br />
                                        return decltype(tmp48())::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), tmp48(), tmp50.template cast<float,1>());<br />
                                    }<br />
                                }<br />
                                ()<br />
                                ;<br />
                                auto tmp53 = static_cast<float>(0.0);<br />
                                auto tmp54 = at::vec::Vectorized<float>(tmp53);<br />
                                auto tmp55 = decltype(tmp52)::blendv(tmp54, tmp52, tmp47.template cast<float,1>());<br />
                                return tmp55;<br />
                            }<br />
                            ;<br />
                            auto tmp56 = tmp41 ? tmp42() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                            auto tmp57 = static_cast<float>(0.0);<br />
                            auto tmp58 = at::vec::VecMask<float,1>::from(tmp41);<br />
                            auto tmp59 = at::vec::Vectorized<float>(tmp57);<br />
                            auto tmp60 = decltype(tmp56)::blendv(tmp59, tmp56, tmp58.template cast<float,1>());<br />
                            auto tmp61 = at::vec::VecMask<float,1>::from(tmp2);<br />
                            auto tmp62 = decltype(tmp38)::blendv(tmp60, tmp38, tmp61.template cast<float,1>());<br />
                            tmp62.store(out_ptr1 + static_cast<long>(x3 + (513L<em>x1) + (525312L</em>x2) + (6303744L<em>x0)));<br />
                        }<br />
                        #pragma omp simd simdlen(8) <br />
                        for(long x3=static_cast<long>(512L); x3&lt;static_cast<long>(513L); x3+=static_cast<long>(1L))<br />
                        {<br />
                            auto tmp0 = c10::convert<int64_t>(x1);<br />
                            auto tmp1 = static_cast<int64_t>(256);<br />
                            auto tmp2 = tmp0 &lt; tmp1;<br />
                            auto tmp3 = [&amp;]<br />
                            {<br />
                                auto tmp4 = c10::convert<int64_t>(x3);<br />
                                auto tmp5 = static_cast<int64_t>(257);<br />
                                auto tmp6 = tmp4 &lt; tmp5;<br />
                                auto tmp7 = [&amp;]<br />
                                {<br />
                                    auto tmp8 = -std::numeric_limits<float>::infinity();<br />
                                    return tmp8;<br />
                                }<br />
                                ;<br />
                                auto tmp9 = tmp6 ? tmp7() : static_cast<decltype(tmp7())>(0.0);<br />
                                auto tmp10 = c10::convert<int64_t>(c10::div_floor_integer(x1, 256L));<br />
                                auto tmp11 = static_cast<int64_t>(3);<br />
                                auto tmp12 = tmp10 &lt; tmp11;<br />
                                auto tmp13 = [&amp;]<br />
                                {<br />
                                    auto tmp14 = c10::convert<int64_t>(x3);<br />
                                    auto tmp15 = static_cast<int64_t>(256);<br />
                                    auto tmp16 = tmp14 &gt;= tmp15;<br />
                                    auto tmp17 = [&amp;]<br />
                                    {<br />
                                        auto tmp18 = in_ptr0[static_cast<long>((-256L) + x3 + (513L</em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L<em>(c10::div_floor_integer(x1, 256L))) + (787968L</em>x2) + (9455616L<em>x0))];<br />
                                        return tmp18;<br />
                                    }<br />
                                    ;<br />
                                    auto tmp19 = tmp16 ? tmp17() : static_cast<decltype(tmp17())>(0.0);<br />
                                    auto tmp20 = static_cast<float>(0.0);<br />
                                    auto tmp21 = tmp16 ? tmp19 : tmp20;<br />
                                    return tmp21;<br />
                                }<br />
                                ;<br />
                                auto tmp22 = tmp12 ? tmp13() : static_cast<decltype(tmp13())>(0.0);<br />
                                auto tmp23 = static_cast<float>(0.0);<br />
                                auto tmp24 = tmp12 ? tmp22 : tmp23;<br />
                                auto tmp25 = tmp6 ? tmp9 : tmp24;<br />
                                return tmp25;<br />
                            }<br />
                            ;<br />
                            auto tmp26 = tmp2 ? tmp3() : static_cast<decltype(tmp3())>(0.0);<br />
                            auto tmp27 = c10::convert<int64_t>(c10::div_floor_integer(x1, 256L));<br />
                            auto tmp28 = static_cast<int64_t>(3);<br />
                            auto tmp29 = tmp27 &lt; tmp28;<br />
                            auto tmp30 = [&amp;]<br />
                            {<br />
                                auto tmp31 = c10::convert<int64_t>(x3);<br />
                                auto tmp32 = static_cast<int64_t>(256);<br />
                                auto tmp33 = tmp31 &gt;= tmp32;<br />
                                auto tmp34 = [&amp;]<br />
                                {<br />
                                    auto tmp35 = in_ptr0[static_cast<long>((-256L) + x3 + (513L</em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L<em>(c10::div_floor_integer(x1, 256L))) + (787968L</em>x2) + (9455616L<em>x0))];<br />
                                    return tmp35;<br />
                                }<br />
                                ;<br />
                                auto tmp36 = tmp33 ? tmp34() : static_cast<decltype(tmp34())>(0.0);<br />
                                auto tmp37 = static_cast<float>(0.0);<br />
                                auto tmp38 = tmp33 ? tmp36 : tmp37;<br />
                                return tmp38;<br />
                            }<br />
                            ;<br />
                            auto tmp39 = tmp29 ? tmp30() : static_cast<decltype(tmp30())>(0.0);<br />
                            auto tmp40 = static_cast<float>(0.0);<br />
                            auto tmp41 = tmp29 ? tmp39 : tmp40;<br />
                            auto tmp42 = tmp2 ? tmp26 : tmp41;<br />
                            out_ptr1[static_cast<long>(x3 + (513L</em>x1) + (525312L<em>x2) + (6303744L</em>x0))] = tmp42;<br />
                        }<br />
                    }<br />
                }<br />
            }<br />
        }<br />
    }<br />
}<br />
''')<br />
<code>After this PR,</code><br />
cpp_fused_copy_full_like_0 = async_compile.cpp_pybinding(['const float<em>', 'float</em>'], '''</p>
<h1>include "/tmp/torchinductor_root/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h"</h1>
<p>extern "C" void kernel(const float<em> in_ptr0,<br />
                       float</em> out_ptr1)<br />
{<br />
    #pragma omp parallel num_threads(128)<br />
    {<br />
        int tid = omp_get_thread_num();<br />
        {<br />
            #pragma omp for collapse(2)<br />
            for(long x0=static_cast<long>(0L); x0&lt;static_cast<long>(4L); x0+=static_cast<long>(1L))<br />
            {<br />
                for(long x1=static_cast<long>(0L); x1&lt;static_cast<long>(1024L); x1+=static_cast<long>(1L))<br />
                {<br />
                    #pragma GCC ivdep<br />
                    for(long x2=static_cast<long>(0L); x2&lt;static_cast<long>(12L); x2+=static_cast<long>(1L))<br />
                    {<br />
                        for(long x3=static_cast<long>(0L); x3&lt;static_cast<long>(512L); x3+=static_cast<long>(16L))<br />
                        {<br />
                            auto tmp0 = c10::convert<int>(x1);<br />
                            auto tmp1 = static_cast<int>(256);<br />
                            auto tmp2 = tmp0 &lt; tmp1;<br />
                            auto tmp3 = [&amp;]<br />
                            {<br />
                                auto tmp4 = c10::convert<int>(x3);<br />
                                auto tmp5 = at::vec::Vectorized<int>::arange(tmp4, 1);<br />
                                auto tmp6 = static_cast<int>(257);<br />
                                auto tmp7 = at::vec::Vectorized<int>(tmp6);<br />
                                auto tmp8 = at::vec::VecMask<int,1>(tmp5 &lt; tmp7);<br />
                                auto tmp10 = at::vec::VecMask<float,1>::from(tmp2);<br />
                                auto tmp11 = tmp8 &amp; tmp10;<br />
                                auto tmp9 = [&amp;]<br />
                                {<br />
                                    auto tmp12 = -std::numeric_limits<float>::infinity();<br />
                                    return tmp12;<br />
                                }<br />
                                ;<br />
                                auto tmp13 =<br />
                                [&amp;]<br />
                                {<br />
                                    if (tmp11.all_zero())<br />
                                    {<br />
                                        return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                    }<br />
                                    else<br />
                                    {<br />
                                        return decltype(at::vec::Vectorized<float>(tmp9()))::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), at::vec::Vectorized<float>(tmp9()), tmp11.template cast<float,1>());<br />
                                    }<br />
                                }<br />
                                ()<br />
                                ;<br />
                                auto tmp14 = c10::convert<int>(c10::div_floor_integer(x1, 256L));<br />
                                auto tmp15 = static_cast<int>(3);<br />
                                auto tmp16 = tmp14 &lt; tmp15;<br />
                                auto tmp18 = tmp16 &amp; tmp2;<br />
                                auto tmp17 = [&amp;]<br />
                                {<br />
                                    auto tmp19 = at::vec::Vectorized<int>(tmp1);<br />
                                    auto tmp20 = at::vec::VecMask<int,1>(tmp5 &gt;= tmp19);<br />
                                    auto tmp22 = at::vec::VecMask<float,1>::from(tmp18);<br />
                                    auto tmp23 = tmp20 &amp; tmp22;<br />
                                    auto tmp21 = [&amp;]<br />
                                    {<br />
                                        auto tmp24 = tmp23.template cast<float,1>().template loadu<float,1>(in_ptr0 + static_cast<long>((-256L) + x3 + (513L<em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L</em>(c10::div_floor_integer(x1, 256L))) + (787968L<em>x2) + (9455616L</em>x0)));<br />
                                        return tmp24;<br />
                                    }<br />
                                    ;<br />
                                    auto tmp25 =<br />
                                    [&amp;]<br />
                                    {<br />
                                        if (tmp23.all_zero())<br />
                                        {<br />
                                            return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                        }<br />
                                        else<br />
                                        {<br />
                                            return decltype(tmp21())::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), tmp21(), tmp23.template cast<float,1>());<br />
                                        }<br />
                                    }<br />
                                    ()<br />
                                    ;<br />
                                    auto tmp26 = static_cast<float>(0.0);<br />
                                    auto tmp27 = at::vec::Vectorized<float>(tmp26);<br />
                                    auto tmp28 = decltype(tmp25)::blendv(tmp27, tmp25, tmp20.template cast<float,1>());<br />
                                    return tmp28;<br />
                                }<br />
                                ;<br />
                                auto tmp29 = tmp16 ? tmp17() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                auto tmp30 = static_cast<float>(0.0);<br />
                                auto tmp31 = at::vec::VecMask<float,1>::from(tmp16);<br />
                                auto tmp32 = at::vec::Vectorized<float>(tmp30);<br />
                                auto tmp33 = decltype(tmp29)::blendv(tmp32, tmp29, tmp31.template cast<float,1>());<br />
                                auto tmp34 = decltype(tmp13)::blendv(tmp33, tmp13, tmp8.template cast<float,1>());<br />
                                return tmp34;<br />
                            }<br />
                            ;<br />
                            auto tmp35 = tmp2 ? tmp3() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                            auto tmp36 = c10::convert<int>(c10::div_floor_integer(x1, 256L));<br />
                            auto tmp37 = static_cast<int>(3);<br />
                            auto tmp38 = tmp36 &lt; tmp37;<br />
                            auto tmp39 = [&amp;]<br />
                            {<br />
                                auto tmp40 = c10::convert<int>(x3);<br />
                                auto tmp41 = at::vec::Vectorized<int>::arange(tmp40, 1);<br />
                                auto tmp42 = at::vec::Vectorized<int>(tmp1);<br />
                                auto tmp43 = at::vec::VecMask<int,1>(tmp41 &gt;= tmp42);<br />
                                auto tmp45 = at::vec::VecMask<float,1>::from(tmp38);<br />
                                auto tmp46 = tmp43 &amp; tmp45;<br />
                                auto tmp44 = [&amp;]<br />
                                {<br />
                                    auto tmp47 = tmp46.template cast<float,1>().template loadu<float,1>(in_ptr0 + static_cast<long>((-256L) + x3 + (513L<em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L</em>(c10::div_floor_integer(x1, 256L))) + (787968L<em>x2) + (9455616L</em>x0)));<br />
                                    return tmp47;<br />
                                }<br />
                                ;<br />
                                auto tmp48 =<br />
                                [&amp;]<br />
                                {<br />
                                    if (tmp46.all_zero())<br />
                                    {<br />
                                        return at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                                    }<br />
                                    else<br />
                                    {<br />
                                        return decltype(tmp44())::blendv(at::vec::Vectorized<float>(static_cast<float>(0.0)), tmp44(), tmp46.template cast<float,1>());<br />
                                    }<br />
                                }<br />
                                ()<br />
                                ;<br />
                                auto tmp49 = static_cast<float>(0.0);<br />
                                auto tmp50 = at::vec::Vectorized<float>(tmp49);<br />
                                auto tmp51 = decltype(tmp48)::blendv(tmp50, tmp48, tmp43.template cast<float,1>());<br />
                                return tmp51;<br />
                            }<br />
                            ;<br />
                            auto tmp52 = tmp38 ? tmp39() : at::vec::Vectorized<float>(static_cast<float>(0.0));<br />
                            auto tmp53 = static_cast<float>(0.0);<br />
                            auto tmp54 = at::vec::VecMask<float,1>::from(tmp38);<br />
                            auto tmp55 = at::vec::Vectorized<float>(tmp53);<br />
                            auto tmp56 = decltype(tmp52)::blendv(tmp55, tmp52, tmp54.template cast<float,1>());<br />
                            auto tmp57 = at::vec::VecMask<float,1>::from(tmp2);<br />
                            auto tmp58 = decltype(tmp35)::blendv(tmp56, tmp35, tmp57.template cast<float,1>());<br />
                            tmp58.store(out_ptr1 + static_cast<long>(x3 + (513L<em>x1) + (525312L</em>x2) + (6303744L<em>x0)));<br />
                        }<br />
                        #pragma omp simd simdlen(8) <br />
                        for(long x3=static_cast<long>(512L); x3&lt;static_cast<long>(513L); x3+=static_cast<long>(1L))<br />
                        {<br />
                            auto tmp0 = c10::convert<int64_t>(x1);<br />
                            auto tmp1 = static_cast<int64_t>(256);<br />
                            auto tmp2 = tmp0 &lt; tmp1;<br />
                            auto tmp3 = [&amp;]<br />
                            {<br />
                                auto tmp4 = c10::convert<int64_t>(x3);<br />
                                auto tmp5 = static_cast<int64_t>(257);<br />
                                auto tmp6 = tmp4 &lt; tmp5;<br />
                                auto tmp7 = [&amp;]<br />
                                {<br />
                                    auto tmp8 = -std::numeric_limits<float>::infinity();<br />
                                    return tmp8;<br />
                                }<br />
                                ;<br />
                                auto tmp9 = tmp6 ? tmp7() : static_cast<decltype(tmp7())>(0.0);<br />
                                auto tmp10 = c10::convert<int64_t>(c10::div_floor_integer(x1, 256L));<br />
                                auto tmp11 = static_cast<int64_t>(3);<br />
                                auto tmp12 = tmp10 &lt; tmp11;<br />
                                auto tmp13 = [&amp;]<br />
                                {<br />
                                    auto tmp14 = tmp4 &gt;= tmp1;<br />
                                    auto tmp15 = [&amp;]<br />
                                    {<br />
                                        auto tmp16 = in_ptr0[static_cast<long>((-256L) + x3 + (513L</em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L<em>(c10::div_floor_integer(x1, 256L))) + (787968L</em>x2) + (9455616L<em>x0))];<br />
                                        return tmp16;<br />
                                    }<br />
                                    ;<br />
                                    auto tmp17 = tmp14 ? tmp15() : static_cast<decltype(tmp15())>(0.0);<br />
                                    auto tmp18 = static_cast<float>(0.0);<br />
                                    auto tmp19 = tmp14 ? tmp17 : tmp18;<br />
                                    return tmp19;<br />
                                }<br />
                                ;<br />
                                auto tmp20 = tmp12 ? tmp13() : static_cast<decltype(tmp13())>(0.0);<br />
                                auto tmp21 = static_cast<float>(0.0);<br />
                                auto tmp22 = tmp12 ? tmp20 : tmp21;<br />
                                auto tmp23 = tmp6 ? tmp9 : tmp22;<br />
                                return tmp23;<br />
                            }<br />
                            ;<br />
                            auto tmp24 = tmp2 ? tmp3() : static_cast<decltype(tmp3())>(0.0);<br />
                            auto tmp25 = c10::convert<int64_t>(c10::div_floor_integer(x1, 256L));<br />
                            auto tmp26 = static_cast<int64_t>(3);<br />
                            auto tmp27 = tmp25 &lt; tmp26;<br />
                            auto tmp28 = [&amp;]<br />
                            {<br />
                                auto tmp29 = c10::convert<int64_t>(x3);<br />
                                auto tmp30 = tmp29 &gt;= tmp1;<br />
                                auto tmp31 = [&amp;]<br />
                                {<br />
                                    auto tmp32 = in_ptr0[static_cast<long>((-256L) + x3 + (513L</em>(static_cast<long>(x1) % static_cast<long>(256L))) + (262656L<em>(c10::div_floor_integer(x1, 256L))) + (787968L</em>x2) + (9455616L<em>x0))];<br />
                                    return tmp32;<br />
                                }<br />
                                ;<br />
                                auto tmp33 = tmp30 ? tmp31() : static_cast<decltype(tmp31())>(0.0);<br />
                                auto tmp34 = static_cast<float>(0.0);<br />
                                auto tmp35 = tmp30 ? tmp33 : tmp34;<br />
                                return tmp35;<br />
                            }<br />
                            ;<br />
                            auto tmp36 = tmp27 ? tmp28() : static_cast<decltype(tmp28())>(0.0);<br />
                            auto tmp37 = static_cast<float>(0.0);<br />
                            auto tmp38 = tmp27 ? tmp36 : tmp37;<br />
                            auto tmp39 = tmp2 ? tmp24 : tmp38;<br />
                            out_ptr1[static_cast<long>(x3 + (513L</em>x1) + (525312L<em>x2) + (6303744L</em>x0))] = tmp39;<br />
                        }<br />
                    }<br />
                }<br />
            }<br />
        }<br />
    }<br />
}<br />
''')<br />
```</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* (to be filled)</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 00:23:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124921</guid>
    </item>
    <item>
      <title>Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?</title>
      <link>https://github.com/pytorch/pytorch/issues/124918</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Hello PyTorch Community,</p>
<p>I am currently developing a custom operator and considering its compatibility with torch.compile. I have a specific question regarding the function signature of the operator: Does the output tensor need to be explicitly returned by the function, or can it be passed as an input argument?</p>
<p>If I pass the output as an input parameter when creating a custom operator, with the operator interface having no return value, I encounter incorrect results when running with torch.compile, but correct results in eager mode. Based on my analysis, torch.compile seems to optimize away the custom operators that do not return any value, leading to incorrect outcomes.</p>
<h3>Error logs</h3>
<p>Traceback (most recent call last):<br />
  File "/tmp/test.py", line 40, in <module><br />
    assert torch.allclose(out, x * y)<br />
AssertionError</p>
<h3>Minified repro</h3>
<p>```python<br />
import torch<br />
import numpy as np                                                                                                                                                           </p>
<p>def custom_func(x, y, out):                                                                                                          <br />
    torch.<em>check(x.shape == y.shape)<br />
    torch._check(x.device == y.device)<br />
    x_np = x.numpy()<br />
    y_np = y.numpy()<br />
    z_np = np.multiply(x_np, y_np)<br />
    out.copy</em>(torch.from_numpy(z_np))<br />
    return</p>
<p>torch.library.define("mylib::custom_func", "(Tensor x, Tensor y, Tensor out) -&gt; None")<br />
 # Add the implementation of the custom op                                                                                                                               <br />
torch.library.impl("mylib::custom_func", "default", custom_func)</p>
<h1>Add an abstract impl that describes what the properties of the output  tensor are, given the properties of the input Tensors.</h1>
<p>@torch.library.impl_abstract("mylib::custom_func")<br />
def custom_func_abstract(x, y, out):<br />
    torch._check(x.shape == y.shape)<br />
    torch._check(x.device == y.device)<br />
    torch._check(out.shape == y.shape)<br />
    torch._check(out.device == y.device)<br />
    return</p>
<p>@torch.compile(backend="inductor", fullgraph=True)<br />
def f(x, y, out):<br />
    return torch.ops.mylib.custom_func.default(x, y, out)</p>
<p>x = torch.randn(3)<br />
y = torch.randn(3)<br />
out = torch.empty_like(x)<br />
z = f(x, y, out)<br />
assert torch.allclose(out, x * y)<br />
```</p>
<h3>Versions</h3>
<p>PyTorch version: 2.3.0a0+40ec155e58.nv24.03                                                                                                                                <br />
Is debug build: False                                                                                                                                                      <br />
CUDA used to build PyTorch: 12.4                                                                                                                                           <br />
ROCM used to build PyTorch: N/A                                                                                                                                              </p>
<p>OS: Ubuntu 22.04.4 LTS (x86_64)                                                                                                                                            <br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0                                                                                                                         <br />
Clang version: Could not collect                                                                                                                                           <br />
CMake version: version 3.28.3                                                                                                                                              <br />
Libc version: glibc-2.35                                                                                                                                                     </p>
<p>Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)                                                                                        <br />
Python platform: Linux-5.4.0-150-generic-x86_64-with-glibc2.35                                                                                                             <br />
Is CUDA available: True                                                                                                                                                    <br />
CUDA runtime version: 12.4.99                                                                                                                                              <br />
CUDA_MODULE_LOADING set to: LAZY                                                                                                                                           <br />
GPU models and configuration:                                                                                                                                              <br />
GPU 0: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 1: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 2: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 3: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 4: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 5: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 6: NVIDIA A100 80GB PCIe                                                                                                                                               <br />
GPU 7: NVIDIA A100 80GB PCIe                                                                                                                                                 </p>
<p>Nvidia driver version: 525.89.02                                                                                                                                           <br />
cuDNN version: Probably one of the following:                                                                                                                              <br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0                                                                                                                                <br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0                                                                                                                            <br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.4<br />
[pip3] onnx==1.15.0rc2<br />
[pip3] optree==0.10.0<br />
[pip3] pytorch-quantization==2.1.2<br />
[pip3] pytorch-triton==2.2.0+e28a256d7<br />
[pip3] torch==2.3.0a0+40ec155e58.nv24.3<br />
[pip3] torch-tensorrt==2.3.0a0<br />
[pip3] torchdata==0.7.1a0<br />
[pip3] torchtext==0.17.0a0<br />
[pip3] torchvision==0.18.0a0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Wed, 24 Apr 2024 23:45:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124918</guid>
    </item>
    <item>
      <title>Nested wrapper subclasses with torch.compile is broken</title>
      <link>https://github.com/pytorch/pytorch/issues/124878</link>
      <description><![CDATA[<p>This came in in the FSDP2 workstream, which needs a DTensor that holds some sort of float8 tensor (cc @Chillee @ezyang @zou3519 @albanD @samdow @msaroufim @anijain2305 @chauhang @awgu / @drisspg ).</p>
<p>We should probably add general testing that nested wrapper subclasses work properly in compile, as part of fixing that issue. The first thing we should probably do is audit all of the places that we call <code>__tensor_flatten__</code> in dynamo/AOTAutograd, and make sure that we are recursively calling <code>__tensor_flatten__</code> when the inner tensors are also wrapper subclasses.</p>
<p>In particular, I tried a simple repro using TwoTensor's, and noticed that the gradients we produce with compile are wrong (this runs properly when you remove the compile decorator):<br />
```<br />
import torch<br />
from torch.testing._internal.two_tensor import TwoTensor</p>
<p>@torch.compile(backend='aot_eager')<br />
def f(x):<br />
    return x.sin().cos()</p>
<p>a = torch.ones(4, requires_grad=True)<br />
a2 = a.clone().detach().requires_grad_()<br />
aa = TwoTensor(a, a2)<br />
aa2 = aa.clone().detach().requires_grad_()<br />
aaaa = TwoTensor(aa, aa2)<br />
out = f(aaaa)<br />
print(type(out))<br />
print(type(out.a))<br />
print(type(out.b))<br />
print(type(out.a.a))<br />
print(type(out.a.b))<br />
print(type(out.b.a))<br />
print(type(out.b.b))<br />
out.sum().backward()</p>
<h1>aaaa.grad should be a TwoTensor(TwoTensor, TwoTensor)</h1>
<h1>but instead it is a TwoTensor(tensor, tensor)</h1>
<p>print(type(aaaa.grad))<br />
print(type(aaaa.grad.a))<br />
print(type(aaaa.grad.b))<br />
print(type(aaaa.grad.a.a))<br />
print(type(aaaa.grad.a.b))<br />
print(type(aaaa.grad.b.a))<br />
```</p>]]></description>
      <pubDate>Wed, 24 Apr 2024 12:08:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124878</guid>
    </item>
    <item>
      <title>Codegen runtime asserts in Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/124874</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124874<br />
* #124864</p>
<p>This completely subsumes https://github.com/pytorch/pytorch/pull/120816</p>
<p>This makes use of the unbacked binding machinery to teach Inductor how to generate deferred runtime asserts directly. There is some back story about why I did it this way, let me explain.</p>
<p>Previously, our strategy for generating runtime asserts was that Dynamo would insert them into the FX graph after finishing tracing, and we would attempt to code generate them based on the FX graph. This is a good strategy for export, where we immediately export the graph. However, this strategy was afflicted by problems in eager, where we reuse the same ShapeEnv as before. In particular, on subsequent graph passes, we would immediately turn all of these assertions into noops, because when we evaluated their expressions, we would see that because we had a deferred runtime assert in the ShapeEnv, we know "oh, of course this expression is True" already. Oops!</p>
<p>So, with this PR, we take the attitude that as long as the ShapeEnv sticks around, the ShapeEnv's list of deferred runtime asserts is the source of truth, and we don't put anything in the graph. So we just need to decide when to actually generate asserts, and the place I picked was Inductor lowering, since we already have an AssertScalar buffer concept, and so I just need to insert them at this point. AssertScalar also uses raw sympy.Expr rather than SymInt/Bool, so it is easier to prevent unrestricted simplification at this point.</p>
<p>There are a few things jumbled together in this PR. I can split them if you want, but some of the changes are before I changed my strategy, but they're useful changes anyway.</p>
<p><strong>torch/_dynamo/output_graph.py</strong> and <strong>torch/_inductor/lowering.py</strong> - Here, we stop putting deferred runtime asserts in the graph. I also have to make sure we don't DCE unused symbol arguments; we're going to get some goofy graph arguments this way, will be good to restore that optimization eventually. We also just disable codegen for <code>_assert_scalar</code>  entirely; we assume that ShapeEnv will be good enough to capture all of these. </p>
<p><strong>torch/_inductor/codegen/wrapper.py</strong> and <strong>torch/_inductor/ir.py</strong> - Add a way to codegen sizevars without forcing simplification</p>
<p><strong>torch/_inductor/graph.py</strong> - The main logic. Our strategy is to interpose in the same place we are testing that unbacked SymInts are properly showing up in lowered code. The logic is directly analogous to the logic in the existing insert deferred runtime asserts FX pass, but it's simpler because sympy expressions can be directly stored on inductor IR nodes.</p>
<p><strong>torch/fx/experimental/symbolic_shapes.py</strong> - For extra safety, we have a way of freezing runtime asserts, so that if you try to add more we error. This prevents us from adding runtime asserts after we've done lowering. There's a funny interaction with backwards which there's a comment for in graph.py</p>
<p><strong>torch/fx/passes/runtime_assert.py</strong> - This is not really needed in this PR, but I rewrote the runtime assert logic to use unbacked_bindings rather than inferring it by looking for unbacked SymInts. Now, keypaths are translated into FX node acessors. Unfortunately, I couldn't delete the old inference code, because you still need it to find backed SymInts from arguments (as this pass may be used on graphs which don't explicitly bind all their shape variables as argments). There are some new tests exercising this.</p>
<p>TODO: I think we need to generate asserts for replacements too. This is a preexisting problem that the old FX pass had too.</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 24 Apr 2024 11:47:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124874</guid>
    </item>
    <item>
      <title>[inductor] unexpected cuda:0 device usage when compiling and runing a model on cuda:1</title>
      <link>https://github.com/pytorch/pytorch/issues/124854</link>
      <description><![CDATA[<p>When user works with "cuda:1" device and compiles a model, there is unexpected cuda context initialization for device "cuda:0" happenning, which can be surprising to the user seeing with nvidia-smi the device 0 utilisation.</p>
<p><strong>Reproduction code:</strong><br />
```python<br />
import torch<br />
from torchvision.models import resnet18</p>
<p>def print_memory_usage():<br />
    for d in [0, 1]:<br />
        stats = torch.cuda.memory_stats(device=d)<br />
        m = stats["allocated_bytes.all.allocated"] + stats["inactive_split_bytes.all.allocated"] + stats["reserved_bytes.all.allocated"]<br />
        print(f"\t- CUDA Device: {d}, allocated + reserved + non-released in MB: {m / 1024 / 1024}")</p>
<p>device = "cuda:1"<br />
model = resnet18()<br />
compiled_model = torch.compile(model)</p>
<p>print("--- Before compiled model to device")<br />
print_memory_usage()</p>
<p>compiled_model.to(device)<br />
x = torch.rand(16, 3, 320, 320, device=device)</p>
<p>print("--- Before compiled model forward")<br />
print_memory_usage()</p>
<p>y = compiled_model(x)</p>
<p>print("--- Before compiled model backward")<br />
print_memory_usage()</p>
<p>y.sum().backward()</p>
<p>print("--- After compiled model backward")<br />
print_memory_usage()<br />
<code>Output:</code><br />
--- Before compiled model to device<br />
        - CUDA Device: 0, allocated + reserved + non-released in MB: 0.0<br />
        - CUDA Device: 1, allocated + reserved + non-released in MB: 0.0<br />
--- Before compiled model forward<br />
        - CUDA Device: 0, allocated + reserved + non-released in MB: 0.0<br />
        - CUDA Device: 1, allocated + reserved + non-released in MB: 192.966796875<br />
--- Before compiled model backward<br />
        - CUDA Device: 0, allocated + reserved + non-released in MB: 8.044921875    # &lt;--- this should be zero<br />
        - CUDA Device: 1, allocated + reserved + non-released in MB: 2054.27197265625<br />
--- After compiled model backward<br />
        - CUDA Device: 0, allocated + reserved + non-released in MB: 8.044921875    # &lt;--- this should be zero<br />
        - CUDA Device: 1, allocated + reserved + non-released in MB: 5654.61962890625<br />
```</p>
<p>Similar, unexpected cuda device utilisation can be seen when we compile a model and run on CPU inputs.</p>
<p>All of them are mostly due to pattern registrations using cuda device if a cuda is available (and no matter the input device).</p>
<p>In <a href="https://github.com/pytorch/pytorch/pull/124722">this PR</a>, I tried to fix this problem but this ends up into the pattern registration device choice, for example:<br />
https://github.com/pytorch/pytorch/blob/bf834d388b9037a27675f3d27a008b323bec4311/torch/_inductor/fx_passes/pad_mm.py#L444-L448</p>
<p>an open question whether we would like to:<br />
1) register patterns multiple times according to the input's device and add <code>exist_ok</code> option for already registered device agnostic patterns. </p>
<p>2) or register patterns once (current behaviour) and use current cuda device from the inputs. This option will still howver show cuda:0 utilization when first compiled model call is done on cpu input...</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire </p>]]></description>
      <pubDate>Wed, 24 Apr 2024 08:32:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124854</guid>
    </item>
    <item>
      <title>[Inductor] Support fusion of chained reductions even if keepdims=True</title>
      <link>https://github.com/pytorch/pytorch/pull/124843</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124843</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 24 Apr 2024 06:14:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124843</guid>
    </item>
    <item>
      <title>[inductor] add uint8 SDPA pattern</title>
      <link>https://github.com/pytorch/pytorch/pull/124832</link>
      <description><![CDATA[<p>Under the setting of static shape and type int8-fp32, the PR can match a uint8 SDPA, tested with Bert-Large. The current uint8 SDPA internally calls fp32 SDPA kernel by converting input into fp32 and output into uint8.</p>
<p><strong>TODO:</strong></p>
<p>Pattern:<br />
- [ ] Support dynamic shape.<br />
- [ ] Support type int8-bf16.</p>
<p>Kernel:<br />
- [ ] Add uint8 SDPA API with additional inputs of scale and zero point pairs.<br />
- [ ] Support uint8 SDPA kernel with e.g. brgemm.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 24 Apr 2024 00:21:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124832</guid>
    </item>
    <item>
      <title>Compiled autograd doesn't support reading attribute from autograd context if the attribute is created in eager forward</title>
      <link>https://github.com/pytorch/pytorch/issues/124827</link>
      <description><![CDATA[<p>Repro:<br />
```python<br />
import torch<br />
from torch._dynamo import compiled_autograd</p>
<p>class CommOpGradientScaling(torch.autograd.Function):<br />
    @staticmethod<br />
    def forward(<br />
        ctx, input_tensor, scale_gradient_factor<br />
    ) -&gt; torch.Tensor:<br />
        ctx.scale_gradient_factor = scale_gradient_factor<br />
        return input_tensor</p>
<pre><code>@staticmethod
def backward(
    ctx, grad_output
):
    grad_output.mul_(ctx.scale_gradient_factor)
    return grad_output, None
</code></pre>
<p>class Net(torch.nn.Module):<br />
    def <strong>init</strong>(self, checkpoint=False):<br />
        super().<strong>init</strong>()<br />
        self.fc1 = torch.nn.Linear(4, 4)</p>
<pre><code>def forward(self, x):
    x = CommOpGradientScaling.apply(x, 0.5)
    return self.fc1(x)
</code></pre>
<p>model = Net()</p>
<h1>model = torch.compile(model)</h1>
<p>input = torch.randn([1, 4], requires_grad=True)</p>
<p>def compiler_fn(gm):<br />
    return torch.compile(gm, backend="aot_eager", fullgraph=True)</p>
<p>with compiled_autograd.enable(compiler_fn):<br />
    loss = model(input).sum()<br />
    loss.backward()<br />
```</p>
<p>The above code as-is will throw error:<br />
```<br />
  File "/data/users/willfeng/pytorch_yf225/torch/fx/proxy.py", line 298, in create_arg<br />
    raise NotImplementedError(f"argument of type: {type(a)}")<br />
torch._dynamo.exc.InternalTorchDynamoError: argument of type: <class 'type'></p>
<p>from user code:<br />
   File "<eval_with_key>.0", line 23, in forward<br />
    call_backward = torch__dynamo_external_utils_call_backward(getitem_6, (), mm);  getitem_6 = mm = None<br />
  File "/data/users/willfeng/pytorch_yf225/torch/<em>dynamo/external_utils.py", line 78, in call_backward<br />
    grads = backward_fn(FakeContext(saved_tensors), *args)<br />
  File "/data/users/willfeng/pytorch_yf225/test/test_ca_custom_autograd_func.py", line 16, in backward<br />
    grad_output.mul</em>(ctx.scale_gradient_factor)<br />
<code>``
If we print out what</code>a<code>is in</code>raise NotImplementedError(f"argument of type: {type(a)}")<code>, we can see it's a</code>NO_SUCH_SUBOBJ<code>. And then printing more info from</code>NO_SUCH_SUBOBJ<code>creation site will tell us that it's because we are trying to look for the</code>scale_gradient_factor<code>attribute in the</code>torch._dynamo.external_utils.FakeContext` object and failed at doing so.</p>
<p>Compare this with compiled forward (uncomment <code>model = torch.compile(model)</code>), and we will see that only eager forward has this problem.</p>
<p>This is currently a blocker for enabling compiled autograd and compiled DDP on Ads models.</p>
<p>cc. @xmfan @fegin</p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 23:37:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124827</guid>
    </item>
    <item>
      <title>[dynamo] Refactor into torch/_inductor/runtime/compile_tasks.py</title>
      <link>https://github.com/pytorch/pytorch/pull/124681</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122941<br />
* #124682<br />
* <strong>-&gt;</strong> #124681<br />
* #124592</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 15:47:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124681</guid>
    </item>
    <item>
      <title>[inductor] optimize isa dry compile time.</title>
      <link>https://github.com/pytorch/pytorch/pull/124602</link>
      <description><![CDATA[<p>Fixes #100378<br />
Original issue caused by startup dry compile need cost almost 1 second.</p>
<p>This PR add compiler version info, isa build options and pytorch version info to the test binary path hash.<br />
So same compile, same isa and same pytorch can skip the dry compile.</p>
<p>Local test:<br />
First time:<br />
<img width="1588" alt="image" src="https://github.com/pytorch/pytorch/assets/8433590/d0b83f5d-849e-4f37-9977-3b0276e5a5a5"><br />
We need to compile all c++ modules and it cost 16.5s.</p>
<p>Second time:<br />
<img width="1589" alt="image" src="https://github.com/pytorch/pytorch/assets/8433590/44f07fb0-5a15-4342-b0f6-dfe2c880b5d3"><br />
We skipped dry compile due to the same isa fingerprint. It is only cost 0.36s.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 01:14:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124602</guid>
    </item>
    <item>
      <title>[inductor] allow clone cse cache during vectorized indirect load</title>
      <link>https://github.com/pytorch/pytorch/pull/124597</link>
      <description><![CDATA[<p>Fix https://github.com/pytorch/pytorch/issues/123502</p>
<p><code>swap_buffer</code> do not clone the <code>cse.cache</code> which will bring redundant computation.<br />
We may able to clone the <code>cse.cache</code> if there is no cse value in the <code>expr</code><br />
<code>auto tmp8 =
[&amp;]
{
    __at_align__ std::array&lt;int64_t, 16&gt; tmpbuf;
    tmp7.store(tmpbuf.data());
    return tmpbuf;
}
()
;
//
// other codes
//
// also store tmp7 here (redundant tmp16)
auto tmp16 =
[&amp;]
{
    __at_align__ std::array&lt;int64_t, 16&gt; tmpbuf;
    tmp7.store(tmpbuf.data());
    return tmpbuf;
}
()
;</code></p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124921<br />
* <strong>-&gt;</strong> #124597</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 21 Apr 2024 22:47:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124597</guid>
    </item>
    <item>
      <title>log pt2 config dict to signpost from inductor post grad</title>
      <link>https://github.com/pytorch/pytorch/pull/124593</link>
      <description><![CDATA[<p>Summary:<br />
previous attempts don't work eventually.  D49720297 causes online train SEV due to extra importing.  D56299408 mitigates a tricky bug from Distributed Shampoo constructor but unfortutenaly didn't correct the scuba logging either.</p>
<p>see f552546983</p>
<p>Test Plan: {F1491621504}</p>
<p>Differential Revision: D56378270</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 21 Apr 2024 21:15:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124593</guid>
    </item>
    <item>
      <title>[inductor] Remove usage of device_interface from _inductor.runtime</title>
      <link>https://github.com/pytorch/pytorch/pull/124592</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122941<br />
* #124682<br />
* #124681<br />
* <strong>-&gt;</strong> #124592</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 21 Apr 2024 20:53:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124592</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Improved GEMM template</title>
      <link>https://github.com/pytorch/pytorch/pull/124577</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124930<br />
* #124929<br />
* #124928<br />
* <strong>-&gt;</strong> #124577<br />
* #124576</p>
<p>Improves the Cutlass backend GEMM template:</p>
<ul>
<li>Adds code which allows to create stand-alone test runners for Cutlass GEMM Kernels, which allows (manual) debugging of, for example, CUDA IMA errors or similar problems which occur in practice. Includes some utility code and tests to actually compile and run these standalone tests.</li>
<li>Cleans up the GEMM template code through various refactorings</li>
<li>Eliminates code sections and options that are unneccessary now that epilogue fusions are being removed.</li>
<li>Limits the scope of a workaround for (flaky) Cutlass issues with bias broadcasting to neccessary cases.</li>
<li>Puts some CPU runtime checks into #if / #endif blocks, such that it's possible to compile CUTLASS Kernels with lower CPU overhead.</li>
<li>Add documentation comments</li>
</ul>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 21 Apr 2024 14:58:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124577</guid>
    </item>
    <item>
      <title>fix torch.compile with triton kernels under inference_mode</title>
      <link>https://github.com/pytorch/pytorch/pull/124489</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124489<br />
* #124840<br />
* #124839</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 07:25:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124489</guid>
    </item>
    <item>
      <title>[inductor] switch assume_aligned_inputs to False</title>
      <link>https://github.com/pytorch/pytorch/pull/124336</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124336<br />
* #123319</p>
<p>In #123319, we guard some behavior behind the <code>assume_aligned_inputs</code> config option. If we set this to <code>False</code>, then the behavior added in #123319 becomes the default behavior. See the referenced PR for more details about the behavior affected.</p>
<p>Side effects:<br />
* It's possible that this will hurt performance in some scenarios. For example, if an unaligned input is used in a matmul, it might be better to perform the clone to align it first.<br />
* This will occasionally cause recompiles. Specifically: the check we perform (<code>(storage_offset * get_dtype_size(dtype)) % ALIGNMENT == 0</code>) can be guarded on if the storage_offset becomes dynamic. storage_offset becomes dynamic during automatic_dynamic_shapes after a shape or stride changes. Previously, this was increasing graph breaks in cpu inductor torchbench tests (but is fixed by more carefully guarding checks on alignment, so that we don't run them and generate guards unless actually needed).</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 15:14:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124336</guid>
    </item>
    <item>
      <title>[1/N] Scalar Support: Enable aot compile to support aten operations with scalar input like alpha</title>
      <link>https://github.com/pytorch/pytorch/pull/124177</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124926<br />
* #124070<br />
* <strong>-&gt;</strong> #124177<br />
* #116368<br />
* #124836<br />
* #121387</p>
<p>Some operations have a scalar input parameter, like <code>torch.add(a, b, alpha=2.0)</code>.  Currently, the aot compile does not support such a case because it requires the signature of the captured graph to align with the operation's signature. This means that some inputs in the captured graph may be scalar(float, int, bool, etc.). It breaks the assumption of <code>compile_fx_aot</code> as it assumes all the example inputs are tensor - https://github.com/pytorch/pytorch/blob/0f6ce45bcbd7026c00da43db0317ede10830378b/torch/_inductor/compile_fx.py#L1048</p>
<p>This PR intends to support such cases by allowing not-aligned signature and filtering out the non-Tensor parameters. </p>
<p>Captured graph for <code>torch.add(a, b, alpha=2.0)</code></p>
<p>```<br />
opcode         name      target           args              kwargs</p>
<hr />
<p>placeholder    arg0_1    arg0_1           ()                {}<br />
placeholder    arg1_1    arg1_1           ()                {}<br />
call_function  add       aten.add.Tensor  (arg0_1, arg1_1)  {'alpha': 2.0}<br />
output         output_1  output           ((add,),)         {}<br />
```</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 07:05:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124177</guid>
    </item>
    <item>
      <title>[2/N] Scalar Support: Add scalar to the cache for eager-through-torch.compile</title>
      <link>https://github.com/pytorch/pytorch/pull/124070</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124926<br />
* <strong>-&gt;</strong> #124070<br />
* #124177<br />
* #116368<br />
* #124836<br />
* #121387</p>
<p>Add scalar information to the kernel configuration.</p>
<h4>Additional Context</h4>
<p>Currently, the input parameters are orchestrated by input order in the kernel configuration and loaded/mapped to the kernel at runtime. For example, the cache order of the input parameters of <code>torch.add(a, b, alpha=2.0)</code> is <code>a' first, followed by</code>b<code>and then</code>alpha`. The same order is for cache loading.</p>
<p>However, the orchestration mechanism does not support kwargs because the order of kwargs is useless. For example, the <code>out</code> of <code>aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -&gt; Tensor(a!)</code> may be before <code>approximate</code>. We will support it with subsequent PRs. </p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 07:04:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124070</guid>
    </item>
    <item>
      <title>[inductor] add cpp builder code.</title>
      <link>https://github.com/pytorch/pytorch/pull/124045</link>
      <description><![CDATA[<p>Previous full PR https://github.com/pytorch/pytorch/pull/115248 is failed to merge due to fb_code is hard to debug.<br />
I also tried to submit them as two pieces, https://github.com/pytorch/pytorch/pull/118514 https://github.com/pytorch/pytorch/pull/118515. And they have passed PreCI at that time.</p>
<p>Now I tried to split https://github.com/pytorch/pytorch/pull/115248 into smaller piece, and it is the first step of RFC https://github.com/pytorch/pytorch/issues/124245.<br />
Changes:<br />
1. Add cpp builder code, the new cpp_builder support Windows OS.<br />
2. Add CPU ISA checker which is cross OS and exported from backend cpuinfo.<br />
3. Switch compiler ISA checker to new cpp builder.<br />
4. CppCodeCache use the new ISA checker.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 14 Apr 2024 23:55:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124045</guid>
    </item>
    <item>
      <title>[inductor] Specialize on unguarded alignment of example inputs</title>
      <link>https://github.com/pytorch/pytorch/pull/123319</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124336<br />
* <strong>-&gt;</strong> #123319</p>
<p>When inductor generates triton code, the triton code can either assume that the inputs given to it are aligned or unaligned. If they are aligned, triton can use more efficient instructions (like vectorized loads or tensor cores). However, if we generate "aligned" code and pass in unaligned inputs, the triton code will error out; to fix this, we clone unaligned inputs that are passed to triton kernels that expect aligned inputs. This can lead to excessive clones if we have inputs that are not expected to be aligned.</p>
<p>In this PR, we use the example input to decide whether the generated triton code should assume alignment or not. If the example input is aligned, then we will generate triton code that assumes alignment; if at runtime we receive an unaligned input, we'll make a clone. Meanwhile, if the example input is not aligned, the generated triton code will not assume inputs are aligned and we won't ever need to clone.</p>
<p>Note that the alignment of the inputs is not guarded on; we found that adding guards on tensor offsets (a) was slow in cases where we do a lot of comparisons on tensor offsets, and (b) led to a lot of recompilations.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 18:06:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123319</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Enable lowering of qlinear-binary(-unary) fusion for X86Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/122593</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #123240<br />
* #122667<br />
* <strong>-&gt;</strong> #122593<br />
* #122387</p>
<p><strong>Description</strong><br />
Lower the qlinear binary post op pattern to Inductor. Use post op sum (in-place) if the extra input has the same dtype as output. Otherwise, it uses binary add.</p>
<p><strong>Supported linear-binary(-unary) patterns</strong><br />
```<br />
    linear(X)   extra input<br />
           \   /<br />
            Add<br />
             |<br />
        Optional(relu)<br />
             |<br />
             Y</p>
<ol>
<li>
<p>int8-mixed-fp32<br />
+---+---------------+-----------+------------------------------+---------+<br />
| # | Add type      | Quant out | Pattern                      | Post op |<br />
+---+---------------+-----------+------------------------------+---------+<br />
| 1 | In-/out-place | Yes       | linear + fp32 -&gt; (relu) -&gt; q | add     |<br />
+---+---------------+-----------+------------------------------+---------+<br />
| 2 | In-/out-place | No        | linear + fp32 -&gt; (relu)      | sum     |<br />
+---+---------------+-----------+------------------------------+---------+</p>
</li>
<li>
<p>int8-mixed-bf16<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| # | X2 dtype | Add type      | Quant out | Pattern                                          | Post op |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 1 | BF16     | In-/out-place | Yes       | linear + bf16 -&gt; (relu) -&gt; to_fp32 -&gt; q          | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 2 | BF16     | In-/out-place | No        | linear + bf16 -&gt; (relu)                          | sum     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 3 | FP32     | Out-place     | Yes       | linear + fp32 -&gt; (relu) -&gt; q                     | add     |<br />
|   |          | In-place right|           |                                                  |         |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 4 | FP32     | Out-place     | No        | linear + fp32 -&gt; (relu)                          | sum     |<br />
|   |          | In-place right|           |                                                  |         |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 5 | FP32     | In-place left | Yes       | linear + fp32 -&gt; to_bf16 -&gt; relu -&gt; to_fp32 -&gt; q | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 6 | FP32     | In-place left | No        | linear + fp32 -&gt; to_bf16 -&gt; (relu)               | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
```<br />
Note<br />
(1) The positions of linear and the extra input can be swapped.<br />
(2) we don't insert q-dq before the extra input of linear-add by recipe. But if q-dq is found at the<br />
extra input, we don't match that pattern because we cannot match all these patterns in 3 passes.</p>
</li>
</ol>
<p><strong>Test plan</strong><br />
python test/inductor/test_mkldnn_pattern_matcher.py -k test_qlinear_add<br />
python test/inductor/test_cpu_cpp_wrapper.py -k test_qlinear_add</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Mar 2024 23:49:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122593</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] DO NOT REVIEW - to be split up</title>
      <link>https://github.com/pytorch/pytorch/pull/121492</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121492<br />
* #124930<br />
* #124929<br />
* #124928<br />
* #124577<br />
* #124576</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121492</guid>
    </item>
    <item>
      <title>Add registration API for torch.compile-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/121387</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we intend to provide a registration API dedicated to eager-through-torch.compile. The major workflow of this API will be as follows.</p>
<ul>
<li>Load cache</li>
<li>Check cache according to the input tensors</li>
<li>Cache Hit: Run the cached kernel directly</li>
<li>Cache Miss: Run the AOTI to produce kernel and run the produced kernel. If AOTI fails to produce the kernel, invoke the python fallback function.</li>
</ul>
<p>Currently, this PR always fallback to python kernel now and cache mechanism will be implemented in another PR - https://github.com/pytorch/pytorch/pull/116368</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124926<br />
* #124070<br />
* #124177<br />
* #116368<br />
* #124836<br />
* <strong>-&gt;</strong> #121387</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121387</guid>
    </item>
    <item>
      <title>Add a cache mechanism to accelerate torch.compile-for-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/116368</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124926<br />
* #124070<br />
* #124177<br />
* <strong>-&gt;</strong> #116368<br />
* #124836<br />
* #121387</p>
<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to enable a cache mechanism to accelerate <strong>eager-through-torch.compile</strong>. When <strong>eager-through-torch.compile</strong> is enabled, we will store a persistent config to cache the kernel information for the aten operation. </p>
<p>The persistent config consists of two parts - meta_info and kernel_path.</p>
<ul>
<li>meta_info: The input tensors' shape, stride, device type, data type, and symbolic flag.</li>
<li>kernel_path: The path of the kernel produced by Inductor.</li>
</ul>
<p>When an aten operation is registered, the <code>kernel_holder</code> will load the persistent config and parse it to build the cache map;  the meta_info is key, and the kernel library is the value.</p>
<p>Currently, this PR only supports static shape to guard the kernel.</p>
<p>Take a <code>mul</code> as an example.<br />
```python<br />
class MulKernel:<br />
  def <strong>init</strong>(self) -&gt; None:<br />
    pass</p>
<p>def <strong>call</strong>(self, <em>args: Any, </em><em>kwargs: Any) -&gt; Any:<br />
    with torch._C._SetExcludeDispatchKeyGuard(torch._C.DispatchKey.Python, False):<br />
      opt_fn = torch.compile(torch.ops.aten.mul, dynamic=False, options={<br />
          "aot_inductor.eager_mode": True,<br />
          "aot_inductor.eager_op_name": "mul_Tensor"<br />
        }<br />
      )<br />
      return opt_fn(</em>args, **kwargs)</p>
<p>torch_compile_op_lib_impl = torch.library.Library("aten", "IMPL")</p>
<p>_, overload_names = torch._C._jit_get_operation("aten::mul")<br />
schema = torch._C._get_schema("aten::mul", overload_name)<br />
reg_name = schema.name<br />
if schema.overload_name:<br />
  reg_name = f"{reg_name}.{schema.overload_name}"<br />
  torch_compile_op_lib_impl.impl(<br />
    reg_name,<br />
    MulKernel(),<br />
    "CUDA",<br />
    compile_mode=True)</p>
<p>a = torch.randn(1024, 1024, device=device)<br />
b = torch.randn(1024, 1024, device=device)<br />
warm_up_iter = 1000<br />
iter = 10000<br />
fn = torch.mul</p>
<h1>Warm up</h1>
<p>for _ in range(warm_up_iter):<br />
    fn(a, b)</p>
<h1>Collect performance</h1>
<p>beg = time.time()<br />
for _ in range(iter):<br />
    fn(a, b)<br />
end = time.time()<br />
print(f"E2E run: {end - beg}")<br />
```<br />
It will produce the config as follows.</p>
<p><code>json
[
    {
        "meta_info": [
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            },
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            }
        ],
        "kernel_path": "/tmp/torchinductor_eikan/e4/ce4jw46i5l2e7v3tvr2pyglpjmahnp7x3hxaqotrvxwoeh5t6qzc.so"
    }
]</code></p>
<p>Performance-wise, we collected mul.Tensor through torch.compile w/ 10000 runs(e2e). The data is as follows. And we will collect data when we support dynamic shape.</p>
<ul>
<li>Eager: ~266.11ms</li>
<li>W/O Cache: ~3455.54ms</li>
<li>W/ Cache and Cache Miss: ~3555.3ms</li>
<li>W/ Cache and Cache Hit: ~267.12ms</li>
</ul>
<p>Hardware:</p>
<ul>
<li>CPU: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz</li>
<li>GPU: CUDA A10</li>
</ul>
<p>Software:</p>
<ul>
<li>PyTorch Version: 39df084001c54cca5fe3174176f9b0206ddb7dcf</li>
<li>GPU Driver Version: 525.147.05</li>
<li>CUDA Version: 12.0</li>
</ul>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 03:21:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116368</guid>
    </item>
    <item>
      <title>calling nn.utils.parametrize inside torch.compile leads to error</title>
      <link>https://github.com/pytorch/pytorch/issues/115744</link>
      <description><![CDATA[<p>I would expect it to graph break gracefully.<br />
```<br />
import torch.nn as nn<br />
import torch.nn.utils.parametrize as parametrize<br />
torch.manual_seed(0)</p>
<p>class Symmetric(nn.Module):<br />
    def forward(self, X):<br />
        return X.triu() + X.triu(1).transpose(-1, -2)</p>
<p>x = torch.randn(3)</p>
<p>def f(x):<br />
    layer = nn.Linear(3, 3, bias=False)<br />
    parametrize.register_parametrization(layer, "weight", Symmetric())<br />
    y = layer(x)<br />
    return y, layer.weight</p>
<p>f_compiled = torch.compile(f, backend="aot_eager")</p>
<p>out_compiled, weight = f_compiled(x)<br />
out = torch.nn.functional.linear(x, weight)<br />
assert torch.allclose(out_compiled, out)<br />
<code>gives</code><br />
Traceback (most recent call last):<br />
  File "/home/rzou/dev/nightly/baz.py", line 20, in <module><br />
    out_compiled, weight = f_compiled(x)<br />
                           ^^^^^^^^^^^^^<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 501, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/rzou/dev/nightly/baz.py", line 12, in f<br />
    def f(x):<br />
  File "/home/rzou/dev/nightly/baz.py", line 13, in resume_in_f<br />
    layer = nn.Linear(3, 3, bias=False)<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 667, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 730, in _convert_frame<br />
    result = inner_convert(frame, cache_entry, hooks, frame_state)<br />
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 383, in _convert_frame_assert<br />
    compiled_product = _compile(<br />
                       ^^^^^^^^^<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 665, in _compile<br />
    raise InternalTorchDynamoError(str(e)).with_traceback(<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 646, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(</em>args, **kwargs)<br />
        ^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 626, in compile_inner<br />
    check_fn = CheckFunctionManager(<br />
               ^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/_dynamo/guards.py", line 1004, in <strong>init</strong><br />
    self.check_fn = self.compile_check_fn(builder, guards, guard_fail_fn)<br />
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/_dynamo/guards.py", line 1087, in compile_check_fn<br />
    dynamic_dims_sizes = [<br />
                         ^<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/_dynamo/guards.py", line 1088, in <listcomp><br />
    convert(self.output_graph.tensor_weakref_to_sizes_strides[t]["size"])<br />
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^<br />
  File "/home/rzou/dev/nightly/env/lib/python3.11/site-packages/torch/utils/weak.py", line 170, in <strong>getitem</strong><br />
    return self.data[self.ref_type(key)]  # CHANGED<br />
           ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^<br />
torch._dynamo.exc.InternalTorchDynamoError: <weakref at 0x7f6963e11a20; to 'Tensor' at 0x7f6963e115b0></p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng</p>]]></description>
      <pubDate>Wed, 13 Dec 2023 08:28:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/115744</guid>
    </item>
  </channel>
</rss>
