<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>DISABLED test_cond_simple_non_abi_compatible_cuda (__main__.AOTInductorTestNonABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121533</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestNonABICompatibleCuda%3A%3Atest_cond_simple_non_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>Broken by https://github.com/pytorch/pytorch/pull/121120</p>
<p>cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 11:59:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121533</guid>
    </item>
    <item>
      <title>DISABLED test_cond_with_multiple_outputs_abi_compatible_cuda (__main__.AOTInductorTestABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121532</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestABICompatibleCuda%3A%3Atest_cond_with_multiple_outputs_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>Broken by https://github.com/pytorch/pytorch/pull/121120.</p>
<p>cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 11:58:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121532</guid>
    </item>
    <item>
      <title>DISABLED test_cond_use_buffers_from_outer_scope_non_abi_compatible_cuda (__main__.AOTInductorTestNonABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121531</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestNonABICompatibleCuda%3A%3Atest_cond_use_buffers_from_outer_scope_non_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>Another one broken by https://github.com/pytorch/pytorch/pull/121120 ?</p>
<p>cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 11:57:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121531</guid>
    </item>
    <item>
      <title>[inductor][experimental] new score fusion heuristic</title>
      <link>https://github.com/pytorch/pytorch/pull/121525</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121525</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 10:59:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121525</guid>
    </item>
    <item>
      <title>Skip AOT Inductor test_cond_* tests on ROCm</title>
      <link>https://github.com/pytorch/pytorch/pull/121522</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121522<br />
* #121120</p>
<p>Summary: The newly added tests in https://github.com/pytorch/pytorch/pull/121120 are failing in the <code>ciflow/periodic</code> jobs. Here we skip those on ROCm to avoid the need to disable those tests manually on ROCm.</p>
<p>Test Plan:</p>
<p>```<br />
$ python test/inductor/test_aot_inductor.py -k test_cond_nested<br />
...</p>
<hr />
<p>Ran 6 tests in 72.122s</p>
<p>OK<br />
```</p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 10:50:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121522</guid>
    </item>
    <item>
      <title>Enable FX graph cache for a bunch of inductor tests</title>
      <link>https://github.com/pytorch/pytorch/pull/121521</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121521<br />
* #121520</p>
<p>Summary: Get more FX graph cache coverage by enabling for these unit tests (<code>torch._inductor.test_case.TestCase</code> enables the caching and isolates the caching tmp subdir)</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 10:47:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121521</guid>
    </item>
    <item>
      <title>SIGILL / ILL_ILLOPN crash when using AOT Inductor pre-compiled model (torch 2.2)</title>
      <link>https://github.com/pytorch/pytorch/issues/121516</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>We are getting a crash with this backtrace:</p>
<p>torch::inductor::AOTIModelContainerRunner::AOTIModelContainerRunner <br />
at::DynamicLibrary::sym <br />
(dso) (no symbol)<br />
(libc) (no symbol)<br />
(dso) -&gt; crash</p>
<p>We checked the processors flags were the problem happened (it doesn't happen everywhere), and the machine were we compile has more flags (avx512xxx) than the machines where it runs.</p>
<p>I think our next step will be to add a wrapper around g++, that disable some avx instructions, and point CXX to it, but maybe there's a better way.</p>
<p>(I noticed an 'ISA' option but I'm not sure on how to use it).</p>
<p>```</p>
<h1>config specific to codegen/cpp.py</h1>
<p>class cpp:<br />
    # set to torch.get_num_threads()<br />
    threads = -1</p>
<pre><code># Do not generate loops when the condition doesn't hold, like:
# for(long i0=4096; i0&lt;4096; i0+=1)
no_redundant_loops = True

# Assume number of threads is dynamic, don't specialize thread number.
# Kernels don't recompile on thread number changes with this flag on.
# For single-threaded workload, turning it on would incur a slight
# performance degradation.
dynamic_threads = False

simdlen: Optional[int] = None
min_chunk_size = 4096
cxx = (
    None,  # download gcc12 from conda-forge if conda is installed
    # "g++-12",
    # "g++-11",
    # "g++-10",
    # "clang++",
    os.environ.get("CXX", "clang++" if sys.platform == "darwin" else "g++"),
    # "g++.par",
)
# Allow kernel performance profiling via PyTorch profiler
enable_kernel_profile = False

# enable weight prepacking to get a better performance; may lead to large memory footprint
weight_prepack = True

# Inject a bug into our relu implementation; useful for testing our repro
# extraction and minification functionality.
# Valid values: "compile_error", "runtime_error", "accuracy"
inject_relu_bug_TESTING_ONLY: Optional[str] = None
inject_log1p_bug_TESTING_ONLY: Optional[str] = None

# If None, autodetect whether or not AVX512/AVX2 can be used.  Otherwise,
# force usage as specified, without testing.
vec_isa_ok: Optional[bool] = None
</code></pre>
<p>```</p>
<p>We could also try to hack and remove this:</p>
<p><code>if sys.platform == "darwin":
        # Per https://mac.r-project.org/openmp/ right way to pass `openmp` flags to MacOS is via `-Xclang`
        # Also, `-march=native` is unrecognized option on M1
        base_flags += " -Xclang"
    else:
        if platform.machine() == "ppc64le":
            base_flags += " -mcpu=native"
        else:
            base_flags += " -march=native"</code></p>
<p>(-march=native)</p>
<h3>Versions</h3>
<p>pytorch-2.2.0</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 10:12:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121516</guid>
    </item>
    <item>
      <title>DISABLED test_cond_with_multiple_outputs_non_abi_compatible_cuda (__main__.AOTInductorTestNonABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121515</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestNonABICompatibleCuda%3A%3Atest_cond_with_multiple_outputs_non_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>Broken by https://github.com/pytorch/pytorch/pull/121120</p>
<p>cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 10:03:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121515</guid>
    </item>
    <item>
      <title>DISABLED test_cond_nested_non_abi_compatible_cuda (__main__.AOTInductorTestNonABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121514</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestNonABICompatibleCuda%3A%3Atest_cond_nested_non_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>Broken by https://github.com/pytorch/pytorch/pull/121120</p>
<p>cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 10:01:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121514</guid>
    </item>
    <item>
      <title>DISABLED test_cond_with_reinterpret_view_inputs_outputs_abi_compatible_cuda (__main__.AOTInductorTestABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121513</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestABICompatibleCuda%3A%3Atest_cond_with_reinterpret_view_inputs_outputs_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>Broken by https://github.com/pytorch/pytorch/pull/121120</p>
<p>cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 10:01:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121513</guid>
    </item>
    <item>
      <title>DISABLED test_cond_simple_abi_compatible_cuda (__main__.AOTInductorTestABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121512</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestABICompatibleCuda%3A%3Atest_cond_simple_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>Broken by https://github.com/pytorch/pytorch/pull/121120</p>
<p>cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 09:59:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121512</guid>
    </item>
    <item>
      <title>DISABLED test_compiled_fsdp (__main__.TestStateDict)</title>
      <link>https://github.com/pytorch/pytorch/issues/121510</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22distributed%2Fcheckpoint%2Ftest_state_dict.py%3A%3ATestStateDict%3A%3Atest_compiled_fsdp%22%5D">recent examples</a>).</p>
<p>cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 09:55:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121510</guid>
    </item>
    <item>
      <title>DISABLED test_cond_use_buffers_from_outer_scope_abi_compatible_cuda (__main__.AOTInductorTestABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121502</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>Newly added test fails on rocm: https://github.com/pytorch/pytorch/pull/121120#issuecomment-1985926321.</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestABICompatibleCuda%3A%3Atest_cond_use_buffers_from_outer_scope_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 08:10:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121502</guid>
    </item>
    <item>
      <title>DISABLED test_cond_with_outer_code_before_after_abi_compatible_cuda (__main__.AOTInductorTestABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121500</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>Newly added test fails on rocm: https://github.com/pytorch/pytorch/pull/121120#issuecomment-1985926321.</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestABICompatibleCuda%3A%3Atest_cond_with_outer_code_before_after_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 08:08:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121500</guid>
    </item>
    <item>
      <title>DISABLED test_cond_with_parameters_abi_compatible_cuda (__main__.AOTInductorTestABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121499</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>Newly added test fails on rocm: https://github.com/pytorch/pytorch/pull/121120#issuecomment-1985926321.</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestABICompatibleCuda%3A%3Atest_cond_with_parameters_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 08:07:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121499</guid>
    </item>
    <item>
      <title>DISABLED test_cond_nested_abi_compatible_cuda (__main__.AOTInductorTestABICompatibleCuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121498</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>Newly added test fails on rocm: https://github.com/pytorch/pytorch/pull/121120#issuecomment-1985926321.</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_aot_inductor.py%3A%3AAOTInductorTestABICompatibleCuda%3A%3Atest_cond_nested_abi_compatible_cuda%22%5D">recent examples</a>).</p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 08:07:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121498</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Tolerate dynamic shapes</title>
      <link>https://github.com/pytorch/pytorch/pull/121497</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* <strong>-&gt;</strong> #121497<br />
* #121491<br />
* #121490<br />
* #121489</p>
<p>Previously, when the Cutlass backend was<br />
enabled, using dynamic shapes could lead<br />
to exceptions during JIT.</p>
<p>With this change, there are guards in place to just disable the Cutlass backend if dynamic dimensions are involved.</p>
<p>In addition, if no choices for a GEMM are available using the selected backends,<br />
then an ATen Kernel is used as fallback,<br />
even if the ATen backend is not enabled.</p>
<p>Test:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 07:55:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121497</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] DO NOT REVIEW - to be split up</title>
      <link>https://github.com/pytorch/pytorch/pull/121492</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121492<br />
* #121497<br />
* #121491<br />
* #121490<br />
* #121489</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121492</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] GEMM size threshold for Cutlass backend usage</title>
      <link>https://github.com/pytorch/pytorch/pull/121491</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121497<br />
* <strong>-&gt;</strong> #121491<br />
* #121490<br />
* #121489</p>
<p>Adds a configurable GEMM size threshold for the usage of Cutlass GEMM Kernels</p>
<p>Test plan:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121491</guid>
    </item>
    <item>
      <title>[Inductor] Make codecache CUDA compilation more robust &amp; flexible</title>
      <link>https://github.com/pytorch/pytorch/pull/121490</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121497<br />
* #121491<br />
* <strong>-&gt;</strong> #121490<br />
* #121489</p>
<p>Minor changes which make the CUDA compilation within _inductor/codecache.py<br />
more robust and flexible.</p>
<p>Test plan:<br />
CI<br />
Additional test in test_codecache.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121490</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Move tests to separate file</title>
      <link>https://github.com/pytorch/pytorch/pull/121489</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121497<br />
* #121491<br />
* #121490<br />
* <strong>-&gt;</strong> #121489</p>
<p>Move Cutlass backend related tests to test/inductor/test_cutlass_backend.py - no changes to the tests themselves.</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121489</guid>
    </item>
    <item>
      <title>[inductor] Skip welford combine on first reduciton loop iteration</title>
      <link>https://github.com/pytorch/pytorch/pull/121488</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120330<br />
* <strong>-&gt;</strong> #121488</p>
<p>On the first iteration we short circuit <code>welford_reduce</code> since we know<br />
the accumulators are filled with the default values.</p>
<p>This is split out from #120330 to hopefully avoid the meta-internal failure.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 05:14:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121488</guid>
    </item>
    <item>
      <title>[inductor] use generic launcher</title>
      <link>https://github.com/pytorch/pytorch/pull/121483</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121483</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 00:32:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121483</guid>
    </item>
    <item>
      <title>[inductor] Refactor common triton imports into one function</title>
      <link>https://github.com/pytorch/pytorch/pull/121438</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121268<br />
* #121267<br />
* <strong>-&gt;</strong> #121438</p>
<p>This means when codegen depends on a particular import we only need to<br />
add it in one place and it's applied to all triton kernels.</p>
<p>This also changes codegen slightly so instead of generating<br />
<code>@pointwise</code> we now generate <code>@triton_heuristics.pointwise</code> just so<br />
the imports are the same for all kernel types.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 07 Mar 2024 11:42:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121438</guid>
    </item>
    <item>
      <title>Inductor: fix Conv output stride for dynamic shapes</title>
      <link>https://github.com/pytorch/pytorch/pull/121400</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121400</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/120873.<br />
Fixes the output stride of Conv in the case of dynamic shapes. The previous logic in inductor assumed that the output stride of Conv is always channels last while it is actually contiguous if <code>dynamic_shapes and is_contiguous_storage_and_layout(x)</code>.</p>
<h3>Static shape</h3>
<p>In static shape cases, since weight is prepacked (<code>weight_t.is_mkldnn()</code> will be <code>true</code>), we'll always force output to be channels last in the Conv kernel, thus it's fine to have the assumption in Inductor that the output stride of Conv is always channels last.<br />
https://github.com/pytorch/pytorch/blob/96ed37ac13366cc9a7e6645b8955061d0a14f80b/aten/src/ATen/native/mkldnn/Conv.cpp#L357-L358</p>
<h3>Dynamic shape</h3>
<p>In dynamic shape cases, we won't do weight prepack for Conv, in this case, the Conv kernel decides the output layout based on the input and weight layout.<br />
https://github.com/pytorch/pytorch/blob/96ed37ac13366cc9a7e6645b8955061d0a14f80b/torch/_inductor/fx_passes/mkldnn_fusion.py#L1024-L1025</p>
<p>For input with <code>channels = 1</code>, like tensor of size <code>(s0, 1, 28, 28)</code> and stride <code>(784, 784, 28, 1)</code>, in Inductor, with <code>req_stride_order</code> in channels last order, the <code>require_stride_order</code> on <code>x</code> of such size and stride won't change the stride of the tensor since stride for dimensions of size 1 is ignored<br />
https://github.com/pytorch/pytorch/blob/96ed37ac13366cc9a7e6645b8955061d0a14f80b/torch/_inductor/ir.py#L5451</p>
<p>While in Conv kernel, such tensor is consider it as <strong>contiguous</strong> tensor instead of channels last tensor thus the output of the Conv kernel will be in contiguous format.<br />
https://github.com/pytorch/pytorch/blob/96ed37ac13366cc9a7e6645b8955061d0a14f80b/aten/src/ATen/native/ConvUtils.h#L396-L404</p>
<p>To align the behavior of the Conv kernel, we set the output_stride in such case to be contiguous instead of channels last.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 07 Mar 2024 01:42:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121400</guid>
    </item>
    <item>
      <title>[inductor][cpu]AMP models regression and crash in 2024-03-04 nightly release</title>
      <link>https://github.com/pytorch/pytorch/issues/121377</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>AMP models regression and crash in 2024-03-04 nightly release.</p>
<p>AMP static shape default wrapper regression</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>llama</td>
      <td>multiple</td>
      <td>32</td>
      <td>1.372195</td>
      <td>0.014102689</td>
      <td>0.019351639332355</td>
      <td>19.282797</td>
      <td>32.0</td>
      <td>2.142721</td>
      <td>0.009058953000000002</td>
      <td>0.019410808831113003</td>
      <td>19.388413</td>
      <td>0.64</td>
      <td>1.0</td>
      <td>0.64</td>
      <td>1.01</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>multiple</td>
      <td>8</td>
      <td>0.975399</td>
      <td>0.09829916</td>
      <td>0.09588090236484</td>
      <td>17.287654</td>
      <td>8.0</td>
      <td>3.491892</td>
      <td>0.02860036</td>
      <td>0.09986936828111999</td>
      <td>21.98618</td>
      <td>0.28</td>
      <td>1.04</td>
      <td>0.29</td>
      <td>1.27</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>multiple</td>
      <td>64</td>
      <td>1.364183</td>
      <td>0.251519204</td>
      <td>0.34311822227033195</td>
      <td>36.793275</td>
      <td>64.0</td>
      <td>1.990777</td>
      <td>0.172512283</td>
      <td>0.343433485213891</td>
      <td>31.588458</td>
      <td>0.69</td>
      <td>1.0</td>
      <td>0.69</td>
      <td>0.86</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>llama</td>
      <td>single</td>
      <td>1</td>
      <td>0.68803</td>
      <td>0.061452501</td>
      <td>0.042281164263030004</td>
      <td>20.256173</td>
      <td>1.0</td>
      <td>3.788769</td>
      <td>0.01094602</td>
      <td>0.04147194124938</td>
      <td>19.422561</td>
      <td>0.18</td>
      <td>0.98</td>
      <td>0.18</td>
      <td>0.96</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>single</td>
      <td>1</td>
      <td>0.99071</td>
      <td>0.28245702899999997</td>
      <td>0.27983300320058996</td>
      <td>15.963345</td>
      <td>1.0</td>
      <td>3.867883</td>
      <td>0.07098655599999999</td>
      <td>0.274567693180948</td>
      <td>19.435037</td>
      <td>0.26</td>
      <td>0.98</td>
      <td>0.25</td>
      <td>1.22</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1</td>
      <td>1.220285</td>
      <td>0.116050149</td>
      <td>0.141614256072465</td>
      <td>29.88047</td>
      <td>1.0</td>
      <td>1.501315</td>
      <td>0.098143494</td>
      <td>0.14734429969461</td>
      <td>24.300461</td>
      <td>0.81</td>
      <td>1.04</td>
      <td>0.85</td>
      <td>0.81</td>
    </tr>
  </tbody>

</table>
<p>AMP static shape default wrapper failure</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>accuracy</th>
      <th>perf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>huggingface</td>
      <td>BlenderbotForCausalLM</td>
      <td>single</td>
      <td>‚àö</td>
      <td>X</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>stable_diffusion_text_encoder</td>
      <td>single</td>
      <td>X</td>
      <td>‚àö</td>
    </tr>

</table>

<p>AMP dynamic shape default wrapper regression</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>multiple</td>
      <td>8</td>
      <td>0.974551</td>
      <td>0.09871308399999999</td>
      <td>0.09620093472528399</td>
      <td>18.658825</td>
      <td>8.0</td>
      <td>2.825746</td>
      <td>0.034874982000000006</td>
      <td>0.09854784088657202</td>
      <td>26.072793</td>
      <td>0.34</td>
      <td>1.02</td>
      <td>0.35</td>
      <td>1.4</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>llama</td>
      <td>single</td>
      <td>1</td>
      <td>0.732906</td>
      <td>0.060439407</td>
      <td>0.044296404026741995</td>
      <td>20.219663</td>
      <td>1.0</td>
      <td>3.801087</td>
      <td>0.01084628</td>
      <td>0.04122765390636</td>
      <td>19.354458</td>
      <td>0.19</td>
      <td>0.93</td>
      <td>0.18</td>
      <td>0.96</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>single</td>
      <td>1</td>
      <td>0.989944</td>
      <td>0.281489406</td>
      <td>0.278658748533264</td>
      <td>15.981785</td>
      <td>1.0</td>
      <td>3.866154</td>
      <td>0.07068060400000001</td>
      <td>0.273262099877016</td>
      <td>19.418051</td>
      <td>0.26</td>
      <td>0.98</td>
      <td>0.25</td>
      <td>1.22</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1</td>
      <td>1.262171</td>
      <td>0.116195749</td>
      <td>0.14665890471107898</td>
      <td>29.754619</td>
      <td>1.0</td>
      <td>1.492518</td>
      <td>0.09633620799999999</td>
      <td>0.143783524491744</td>
      <td>24.215046</td>
      <td>0.85</td>
      <td>0.98</td>
      <td>0.83</td>
      <td>0.81</td>
    </tr>
  </tbody>

</table>

<p>AMP dynamic shape default wrapper failure</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>accuracy</th>
      <th>perf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>huggingface</td>
      <td>BlenderbotForCausalLM</td>
      <td>single</td>
      <td>‚àö</td>
      <td>X</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>stable_diffusion_text_encoder</td>
      <td>single</td>
      <td>X</td>
      <td>‚àö</td>
    </tr>
  </tbody>

</table>
<p>SW info</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>ff42d907</td>
      <td>main</td>
      <td>ff42d907</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>581fe26792849a80f04feaa7f8bdec69b7f41dd8</td>
      <td>main</td>
      <td>5c7b761f8e748fe45c8e2e29563df637ae651917</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
      <td>main</td>
      <td>0.18.0a0+a52607e</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
      <td>main</td>
      <td>2.2.0a0+5286f9f</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>

</table>

<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh <strong>thread</strong> inference performance/accuracy <strong>suite</strong> <strong>model</strong> amp first static/dynamic<br />
regression bisect log:<br />
<a href="https://github.com/pytorch/pytorch/files/14517983/torchbench-yolov3-inference-amp-dynamic-default-multiple-performance-drop_guilty_commit.log">torchbench-yolov3-inference-amp-dynamic-default-multiple-performance-drop_guilty_commit.log</a><br />
failure bisect log:<br />
<a href="https://github.com/pytorch/pytorch/files/14517991/huggingface-BlenderbotForCausalLM-inference-amp-dynamic-default-single-performance-crash_guilty_commit.log">huggingface-BlenderbotForCausalLM-inference-amp-dynamic-default-single-performance-crash_guilty_commit.log</a><br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/1104e0798c8206e0226f2d68f6bb065645e6276f<br />
cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 18:41:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121377</guid>
    </item>
    <item>
      <title>[inductor][cpu]fp32 models performance regression in 2024-03-04 nightly release</title>
      <link>https://github.com/pytorch/pytorch/issues/121376</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>fp32 models regression in 2024-03-04 nightly release.</p>
<p>fp32 static shape default wrapper</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>multiple</td>
      <td>8</td>
      <td>0.991369</td>
      <td>0.255836803</td>
      <td>0.253628675553307</td>
      <td>21.008619</td>
      <td>8</td>
      <td>1.591373</td>
      <td>0.156282283</td>
      <td>0.24870340554455897</td>
      <td>26.609133</td>
      <td>0.62</td>
      <td>0.98</td>
      <td>0.61</td>
      <td>1.27</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>multiple</td>
      <td>64</td>
      <td>1.269294</td>
      <td>0.8729793100000001</td>
      <td>1.10806740030714</td>
      <td>43.814319</td>
      <td>64</td>
      <td>1.540159</td>
      <td>0.7080052130000001</td>
      <td>1.0904406008488672</td>
      <td>38.130222</td>
      <td>0.82</td>
      <td>0.98</td>
      <td>0.81</td>
      <td>0.87</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>single</td>
      <td>1</td>
      <td>0.99786</td>
      <td>0.590719669</td>
      <td>0.58945552890834</td>
      <td>19.799038</td>
      <td>1</td>
      <td>1.387904</td>
      <td>0.428259948</td>
      <td>0.594383694868992</td>
      <td>24.260618</td>
      <td>0.72</td>
      <td>1.01</td>
      <td>0.72</td>
      <td>1.23</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1</td>
      <td>1.208036</td>
      <td>0.2238699</td>
      <td>0.2704428985164</td>
      <td>35.575396</td>
      <td>1</td>
      <td>1.372201</td>
      <td>0.196654595</td>
      <td>0.26984963191359496</td>
      <td>30.385211</td>
      <td>0.88</td>
      <td>1.0</td>
      <td>0.88</td>
      <td>0.85</td>
    </tr>
  </tbody>

</table>
<p>fp32 static shape cpp wrapper</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>multiple</td>
      <td>64.0</td>
      <td>1.284969</td>
      <td>0.844753783</td>
      <td>1.085482423787727</td>
      <td>33.745031</td>
      <td>64.0</td>
      <td>1.561684</td>
      <td>0.7009253529999999</td>
      <td>1.0946239089744518</td>
      <td>29.064628</td>
      <td>0.82</td>
      <td>1.01</td>
      <td>0.83</td>
      <td>0.86</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1.0</td>
      <td>1.213455</td>
      <td>0.221748512</td>
      <td>0.26908184062896</td>
      <td>30.388094</td>
      <td>1.0</td>
      <td>1.383058</td>
      <td>0.194475219</td>
      <td>0.26897050743970197</td>
      <td>25.875338</td>
      <td>0.88</td>
      <td>1.0</td>
      <td>0.88</td>
      <td>0.85</td>
    </tr>
  </tbody>

</table>
<p>fp32 dynamic shape default wrapper</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>multiple</td>
      <td>8</td>
      <td>0.983977</td>
      <td>0.25779631399999997</td>
      <td>0.253665643660778</td>
      <td>22.344479</td>
      <td>8.0</td>
      <td>1.503107</td>
      <td>0.16544836799999998</td>
      <td>0.24868660007937599</td>
      <td>30.578669</td>
      <td>0.65</td>
      <td>0.98</td>
      <td>0.64</td>
      <td>1.37</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>single</td>
      <td>1</td>
      <td>0.997312</td>
      <td>0.592453853</td>
      <td>0.5908613370431359</td>
      <td>20.022786</td>
      <td>1.0</td>
      <td>1.37728</td>
      <td>0.428594964</td>
      <td>0.5902952720179201</td>
      <td>24.479398</td>
      <td>0.72</td>
      <td>1.0</td>
      <td>0.72</td>
      <td>1.22</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1</td>
      <td>1.208458</td>
      <td>0.223794506</td>
      <td>0.270446261131748</td>
      <td>35.944872</td>
      <td>1.0</td>
      <td>1.374786</td>
      <td>0.196470478</td>
      <td>0.270104862567708</td>
      <td>30.699722</td>
      <td>0.88</td>
      <td>1.0</td>
      <td>0.88</td>
      <td>0.85</td>
    </tr>
  </tbody>

</table>
<p>fp32 dynamic shape cpp wrapper</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1.0</td>
      <td>1.222613</td>
      <td>0.221231601</td>
      <td>0.270480631393413</td>
      <td>30.374674</td>
      <td>1.0</td>
      <td>1.379473</td>
      <td>0.194968928</td>
      <td>0.268954372014944</td>
      <td>25.781443</td>
      <td>0.89</td>
      <td>0.99</td>
      <td>0.88</td>
      <td>0.85</td>
    </tr>
  </tbody>


</table>
<p>SW info</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>ff42d907</td>
      <td>main</td>
      <td>ff42d907</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>581fe26792849a80f04feaa7f8bdec69b7f41dd8</td>
      <td>main</td>
      <td>5c7b761f8e748fe45c8e2e29563df637ae651917</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
      <td>main</td>
      <td>0.18.0a0+a52607e</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
      <td>main</td>
      <td>2.2.0a0+5286f9f</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>

</table>

<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh <strong>thread</strong> inference performance <strong>suite</strong> <strong>model</strong> float32 first static/dynamic cpp/default<br />
<a href="https://github.com/pytorch/pytorch/files/14517683/torchbench-yolov3-inference-float32-static-default-multiple-performance-drop_guilty_commit.log">torchbench-yolov3-inference-float32-static-default-multiple-performance-drop_guilty_commit.log</a><br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/8c2e569928a200893fe971e615b82a2f9ce32630<br />
Seems the guilty commit pointed to PR is reverted, will double check in next test result.<br />
cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 18:04:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121376</guid>
    </item>
    <item>
      <title>Match HuggingFace T5 SDPA pattern in Inductor</title>
      <link>https://github.com/pytorch/pytorch/issues/121371</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>The <code>hf_T5</code> MHA/SDPA inference pattern is currently not being matched by Inductor.<br />
Matching &amp; replacing it with a call to the SDPA kernel may boost performance.</p>
<p>Thanks!</p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p>There are two extra <code>view</code> nodes after addition of the attention mask, which are not present in existing patterns.</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 16:54:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121371</guid>
    </item>
    <item>
      <title>[inductor] Update triton pin</title>
      <link>https://github.com/pytorch/pytorch/pull/121268</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121268<br />
* #121267<br />
* #121438</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 13:25:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121268</guid>
    </item>
    <item>
      <title>[inductor] Changes to support newer triton pin</title>
      <link>https://github.com/pytorch/pytorch/pull/121267</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121268<br />
* <strong>-&gt;</strong> #121267<br />
* #121438</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 13:25:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121267</guid>
    </item>
    <item>
      <title>can't compile torchvision RPN with AOTInductor</title>
      <link>https://github.com/pytorch/pytorch/issues/121036</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I try to compile an object detection model that uses torchvison's rpn module. it fails when generating anchors with <code>AssertionError: Mutating module attribute cell_anchors during export.</code> I think this is related to https://github.com/pytorch/pytorch/issues/108805</p>
<h3>Error logs</h3>
<p>```python<br />
I0301 19:15:52.323000 140521716340544 torch/fx/experimental/symbolic_shapes.py:1869] [0/0] create_env<br />
I0301 19:15:52.435000 140521716340544 torch/fx/experimental/symbolic_shapes.py:2620] [0/0] create_symbol s0 = 2 for L['batch_tensor'].size()[0] [2, 13] (_dynamo/variables/builder.py:1791 in <lambda>)<br />
V0301 19:15:52.437000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval True == True [statically known]<br />
V0301 19:15:52.437000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval False == False [statically known]<br />
V0301 19:15:52.537000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(s0, 9223372036854775807) == False [statically known]<br />
V0301 19:15:52.538000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval False == False [statically known]<br />
V0301 19:15:52.539000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval s0 &lt; 9223372036854775807 == True [statically known]<br />
V0301 19:15:52.575000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Ne(s0, 1) == True [statically known]<br />
V0301 19:15:52.592000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(s0, 1) == False [statically known]<br />
V0301 19:15:52.659000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval True == True [statically known]<br />
V0301 19:16:11.974000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(3<em>s0, 3) == False [statically known]<br />
V0301 19:16:11.975000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Ne(3</em>s0, 3) == True [statically known]<br />
I0301 19:16:12.160000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3622] [0/0] set_replacement s0 = 2 (solve_backed) ValueRanges(lower=2, upper=2, is_bool=False)<br />
I0301 19:16:12.162000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3622] [0/0] set_replacement s0 = 2 (find) ValueRanges(lower=2, upper=2, is_bool=False)<br />
I0301 19:16:12.163000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3832] [0/0] eval Eq(s0, 2) [guard added] at torchvision/models/detection/transform.py:243 in batch_images (_dynamo/variables/tensor.py:892 in evaluate_expr)<br />
V0301 19:16:12.171000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3902] [0/0] eval 2 [trivial]</p>
<hr />
<p>AssertionError                            Traceback (most recent call last)<br />
Cell In[2], line 15<br />
     12 width_dim = torch.export.Dim("width")<br />
     14 if not os.path.exists(model_path):<br />
---&gt; 15     so_path = torch._export.aot_compile(<br />
     16         f = model,<br />
     17         args = (x, ),<br />
     18         # Specify the first dimension of the input x as dynamic<br />
     19         dynamic_shapes={"batch_tensor": {0: batch_dim}},<br />
     20         # Specify the generated shared library path<br />
     21         options={<br />
     22             "aot_inductor.output_path": model_path,<br />
     23             "max_autotune": True,<br />
     24         },<br />
     25     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_export/<strong>init</strong>.py:382, in aot_compile(f, args, kwargs, constraints, dynamic_shapes, options, remove_runtime_assertions, disable_constraint_solver)<br />
    378     gm = torch.export._trace._export(f, args, kwargs, constraints, pre_dispatch=True).module()<br />
    379 else:<br />
    380     # We want to export to Torch IR here to utilize the pre_grad passes in<br />
    381     # inductor, which run on Torch IR.<br />
--&gt; 382     gm = _export_to_torch_ir(<br />
    383         f,<br />
    384         args,<br />
    385         kwargs,<br />
    386         constraints,<br />
    387         disable_constraint_solver=disable_constraint_solver,<br />
    388         # Disabling this flag, because instead we can rely on the mapping<br />
    389         # dynamo_flat_name_to_original_fqn which is coming from Dynamo.<br />
    390         restore_fqn=False,<br />
    391     )<br />
    392 flat_example_inputs = pytree.arg_tree_leaves(<em>args, </em>*(kwargs or {}))<br />
    394 with torch.no_grad():</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/export/_trace.py:320, in _export_to_torch_ir(f, args, kwargs, constraints, preserve_module_call_signature, disable_constraint_solver, restore_fqn, _log_export_usage)<br />
    316     module_call_specs: Dict[str, Dict[str, pytree.TreeSpec]] = {}<br />
    317     with _wrap_submodules(<br />
    318         f, preserve_module_call_signature, module_call_specs<br />
    319     ), _ignore_backend_decomps():<br />
--&gt; 320         gm_torch_level, _ = torch._dynamo.export(<br />
    321             f,<br />
    322             constraints=constraints,<br />
    323             assume_static_by_default=True,<br />
    324             tracing_mode="symbolic",<br />
    325             disable_constraint_solver=disable_constraint_solver,<br />
    326             _log_export_usage=_log_export_usage,<br />
    327         )(<br />
    328             <em>args,<br />
    329             </em>*kwargs,<br />
    330         )<br />
    331 except (ConstraintViolationError, ValueRangeError) as e:<br />
    332     raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: TRY200</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1314, in export.<locals>.inner(<em>args, </em><em>kwargs)<br />
   1312 # TODO(voz): We may have instances of <code>f</code> that mutate inputs, we should track sideeffects and reject.<br />
   1313 try:<br />
-&gt; 1314     result_traced = opt_f(</em>args, **kwargs)<br />
   1315 except ConstraintViolationError as e:<br />
   1316     constraint_violation_error = e</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1529, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1527     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1528 else:<br />
-&gt; 1529     return self._call_impl(*args, </strong>kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1538, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1533 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1534 # this function, and just call forward.<br />
   1535 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1536         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1537         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1538     return forward_call(</em>args, **kwargs)<br />
   1540 try:<br />
   1541     result = None</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:455, in _TorchDynamoContext.<strong>call</strong>.<locals>._fn(<em>args, </em><em>kwargs)<br />
    453 prior = set_eval_frame(callback)<br />
    454 try:<br />
--&gt; 455     return fn(</em>args, **kwargs)<br />
    456 finally:<br />
    457     set_eval_frame(prior)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1529, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1527     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1528 else:<br />
-&gt; 1529     return self._call_impl(*args, </strong>kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1538, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1533 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1534 # this function, and just call forward.<br />
   1535 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1536         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1537         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1538     return forward_call(</em>args, **kwargs)<br />
   1540 try:<br />
   1541     result = None</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:912, in catch_errors_wrapper.<locals>.catch_errors(frame, cache_entry, frame_state)<br />
    908             return hijacked_callback(frame, cache_entry, hooks, frame_state)<br />
    910 with compile_lock, _disable_current_modes():<br />
    911     # skip=1: skip this frame<br />
--&gt; 912     return callback(frame, cache_entry, hooks, frame_state, skip=1)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:398, in convert_frame_assert.<locals>._convert_frame_assert(frame, cache_entry, hooks, frame_state, skip)<br />
    384 compile_id = CompileId(frame_id, frame_compile_id)<br />
    386 signpost_event(<br />
    387     "dynamo",<br />
    388     "_convert_frame_assert._compile",<br />
   (...)<br />
    395     },<br />
    396 )<br />
--&gt; 398 return _compile(<br />
    399     frame.f_code,<br />
    400     frame.f_globals,<br />
    401     frame.f_locals,<br />
    402     frame.f_builtins,<br />
    403     compiler_fn,<br />
    404     one_graph,<br />
    405     export,<br />
    406     export_constraints,<br />
    407     hooks,<br />
    408     cache_size,<br />
    409     frame,<br />
    410     frame_state=frame_state,<br />
    411     compile_id=compile_id,<br />
    412     skip=skip + 1,<br />
    413 )</p>
<p>File /opt/conda/lib/python3.10/contextlib.py:79, in ContextDecorator.<strong>call</strong>.<locals>.inner(<em>args, </em><em>kwds)<br />
     76 @wraps(func)<br />
     77 def inner(</em>args, <strong>kwds):<br />
     78     with self._recreate_cm():<br />
---&gt; 79         return func(*args, </strong>kwds)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:669, in _compile(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)<br />
    659 log.debug(<br />
    660     "torchdynamo start compiling %s %s:%s, stack (elided %s frames):\n%s",<br />
    661     code.co_name,<br />
   (...)<br />
    666     "".join(traceback.format_list(traceback.extract_stack()[: -2 - skip])),<br />
    667 )<br />
    668 try:<br />
--&gt; 669     guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
    670     return guarded_code<br />
    671 except (<br />
    672     Unsupported,<br />
    673     TorchRuntimeError,<br />
   (...)<br />
    680     BisectValidationException,<br />
    681 ) as e:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py:256, in dynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper(<em>args, </em><em>kwargs)<br />
    254 with torch.profiler.record_function(f"{key} (dynamo_timed)"):<br />
    255     t0 = time.time()<br />
--&gt; 256     r = func(</em>args, **kwargs)<br />
    257     time_spent = time.time() - t0<br />
    258 compilation_time_metrics[key].append(time_spent)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:542, in _compile.<locals>.compile_inner(code, one_graph, hooks, transform)<br />
    540 CompileContext.get().attempt = attempt<br />
    541 try:<br />
--&gt; 542     out_code = transform_code_object(code, transform)<br />
    543     break<br />
    544 except exc.RestartAnalysis as e:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1033, in transform_code_object(code, transformations, safe)<br />
   1030 instructions = cleaned_instructions(code, safe)<br />
   1031 propagate_line_nums(instructions)<br />
-&gt; 1033 transformations(instructions, code_options)<br />
   1034 return clean_and_assemble_instructions(instructions, keys, code_options)[1]</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:163, in preserve_global_state.<locals>._fn(<em>args, </em><em>kwargs)<br />
    161 cleanup = setup_compile_debug()<br />
    162 try:<br />
--&gt; 163     return fn(</em>args, **kwargs)<br />
    164 finally:<br />
    165     cleanup.close()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:507, in _compile.<locals>.transform(instructions, code_options)<br />
    505 try:<br />
    506     with tracing(tracer.output.tracing_context), tracer.set_current_tx():<br />
--&gt; 507         tracer.run()<br />
    508 except exc.UnspecializeRestartAnalysis:<br />
    509     speculation_log.clear()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2130, in InstructionTranslator.run(self)<br />
   2129 def run(self):<br />
-&gt; 2130     super().run()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1308, in InstructionTranslatorBase.STORE_ATTR(self, inst)<br />
   1303 val, obj = self.popn(2)<br />
   1305 if isinstance(obj, NNModuleVariable):<br />
   1306     # We don't allow side effects during export<br />
   1307     # https://github.com/pytorch/torchdynamo/issues/1475<br />
-&gt; 1308     assert (<br />
   1309         not self.export<br />
   1310     ), f"Mutating module attribute {inst.argval} during export."<br />
   1312 try:<br />
   1313     BuiltinVariable(setattr).call_function(<br />
   1314         self, [obj, ConstantVariable.create(inst.argval), val], {}<br />
   1315     )</p>
<p>AssertionError: Mutating module attribute cell_anchors during export.</p>
<p>from user code:<br />
   File "/opt/workspace/./satlas-src/satlas/model/model.py", line 841, in forward<br />
    cur_outputs, _ = head(batch_tensor, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/opt/workspace/<a href="http://127.0.0.1:8888/lab/tree/satlas-src/satlas/model/model.py#line=530">./satlas-src/satlas/model/model.py", line 531</a>, in forward<br />
    proposals, proposal_losses = self.rpn(images, features, targets)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/rpn.py", line 362, in forward<br />
    anchors = self.anchor_generator(images, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 126, in forward<br />
    self.set_cell_anchors(dtype, device)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 77, in set_cell_anchors<br />
    self.cell_anchors = [cell_anchor.to(dtype=dtype, device=device) for cell_anchor in self.cell_anchors]<br />
```</p>
<h3>Minified repro</h3>
<p>os.environ["TORCH_DYNAMO_REPRO_AFTER"] = "aot" doesn't seem to do anything. after I run the code, the traceback is the same.</p>
<p>a minimal repro is to do</p>
<p>```python<br />
import torch<br />
import torch._dynamo as torchdynamo<br />
from torchvision.models.detection import (<br />
    maskrcnn_resnet50_fpn,<br />
)</p>
<p>import torch._dynamo.config</p>
<h1>torch._dynamo.config.capture_scalar_outputs = True</h1>
<p>net = maskrcnn_resnet50_fpn()<br />
net.eval()</p>
<h1>Single input which is a list of images.</h1>
<p>images = [torch.rand(3, 16, 16)]</p>
<h1>Double-check that the inputs work for the normal net.</h1>
<p>net(images)<br />
torch._dynamo.export(net, images, tracing_mode="symbolic", aten_graph=True)</p>
<p>```</p>
<p>```python<br />
AssertionError: Mutating module attribute cell_anchors during export.</p>
<p>from user code:<br />
   File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py", line 104, in forward<br />
    proposals, proposal_losses = self.rpn(images, features, targets)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/rpn.py", line 362, in forward<br />
    anchors = self.anchor_generator(images, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(</em>args, **kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 126, in forward<br />
    self.set_cell_anchors(dtype, device)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 77, in set_cell_anchors<br />
    self.cell_anchors = [cell_anchor.to(dtype=dtype, device=device) for cell_anchor in self.cell_anchors]</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information<br />
```</p>
<p>from the test script here: https://docs.google.com/document/d/159NTQQhz8ovIBxbQvGQ-fZ10pF9e2RPXm1JZYqdEzt4/edit from this issue https://github.com/pytorch/pytorch/issues/108805</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.3.0.dev20240221<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.26.4<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.1.105<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090<br />
Nvidia driver version: 535.161.07<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      39 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             12<br />
On-line CPU(s) list:                0-11<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz<br />
CPU family:                         6<br />
Model:                              158<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 6<br />
Socket(s):                          1<br />
Stepping:                           10<br />
CPU max MHz:                        4600.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           6399.96<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp vnmi md_clear flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          192 KiB (6 instances)<br />
L1i cache:                          192 KiB (6 instances)<br />
L2 cache:                           1.5 MiB (6 instances)<br />
L3 cache:                           12 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-11<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable<br />
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Meltdown:             Mitigation; PTI<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Mitigation; IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Mitigation; Microcode<br />
Vulnerability Tsx async abort:      Mitigation; TSX disabled</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==5.0.4<br />
[pip3] mypy==1.0.1<br />
[pip3] mypy-extensions==0.4.4<br />
[pip3] numpy==1.26.4<br />
[pip3] optree==0.10.0<br />
[pip3] torch==2.3.0.dev20240221<br />
[pip3] torchaudio==2.2.0.dev20240221<br />
[pip3] torchelastic==0.2.2<br />
[pip3] torchvision==0.18.0.dev20240221<br />
[pip3] triton==2.2.0<br />
[pip3] vit-pytorch==1.6.5<br />
[conda] blas                      1.0                         mkl<br />
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch-nightly<br />
[conda] mkl                       2023.1.0         h213fc3f_46344<br />
[conda] mkl-service               2.4.0           py310h5eee18b_1<br />
[conda] mkl_fft                   1.3.8           py310h5eee18b_0<br />
[conda] mkl_random                1.2.4           py310hdb19cb5_0<br />
[conda] numpy                     1.26.4                   pypi_0    pypi<br />
[conda] optree                    0.10.0                   pypi_0    pypi<br />
[conda] pytorch                   2.3.0.dev20240221 py3.10_cuda12.1_cudnn8.9.2_0    pytorch-nightly<br />
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch-nightly<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly<br />
[conda] torchaudio                2.2.0.dev20240221     py310_cu121    pytorch-nightly<br />
[conda] torchelastic              0.2.2                    pypi_0    pypi<br />
[conda] torchvision               0.18.0.dev20240221     py310_cu121    pytorch-nightly<br />
[conda] triton                    2.2.0                    pypi_0    pypi<br />
[conda] vit-pytorch               1.6.5                    pypi_0    pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4</p>]]></description>
      <pubDate>Fri, 01 Mar 2024 11:34:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121036</guid>
    </item>
    <item>
      <title>[compiled autograd] support custom ops backed by c++ autograd::Function</title>
      <link>https://github.com/pytorch/pytorch/pull/120681</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120681</p>
<ul>
<li>Adds support for custom ops backed by c++ custom autograd functions, e.g. fbgemm</li>
<li>Include files more granularly to avoid namespace pollution and circular imports</li>
</ul>
<p>limitations:<br />
- requires user to audit their code and opt-in their custom autograd::Function via autograd::Function::is_traceable and maybe additional compiled_args + apply_with_saved implementation. this was the only way I can think of for soundness<br />
- will throw if we can't hash the saved_data i.e. for any non implemented type other than list and dict in at::IValue::hash https://github.com/pytorch/pytorch/blob/b0cfa96e82d7fbd02f5dbcef2632714caf89615d/aten/src/ATen/core/ivalue.cpp#L364<br />
- can technically silently fail if both the typeid hash and the typeid string name of the custom autograd::Function collide at the same time, and an identical autograd graph containing a different custom autograd::Function, yet that has an identical implementation, is called. this case seems extremely unlikely, and the only alternative to hash collision i can think of is compiling with reflection<br />
- tensors not saved via save_variables are not lifted, and are specialized on TensorImpl*'s hash (treated as a memory address). if needed, we can lift them.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 26 Feb 2024 17:38:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120681</guid>
    </item>
    <item>
      <title>[inductor] Optimize welford reduction</title>
      <link>https://github.com/pytorch/pytorch/pull/120330</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120330<br />
* #121488</p>
<p>This does two things,<br />
1) Short circuit <code>welford_reduce</code> on the first iteration to ignore the accumulator (big win for small <code>rnumel</code>)<br />
2) Replace division with multiplication by reciprocal</p>
<p>Currently this is not enough to match two pass reduction with bfloat16 but it is still a significant improvement.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames</p>]]></description>
      <pubDate>Wed, 21 Feb 2024 11:43:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120330</guid>
    </item>
    <item>
      <title>Inductor oneDNN Graph integration</title>
      <link>https://github.com/pytorch/pytorch/pull/120301</link>
      <description><![CDATA[<h2>Inductor-CPU integration with oneDNN Graph (base PR)</h2>
<p>Uses Inductor pattern-matcher to offload some compute to oneDNN Graph library, which has fusions for compute-intensive ops such as MHA &amp; MLP. This is the first PR that adds oneDNN Graph support, so that subsequent PRs can add more pattern replacements that use oneDNN Graph. This PR only adds a replacement for one pattern (GPT2 MHA/SDPA pattern, which is different for batch size 1 &amp; batch size &gt; 1). Currently, functionality is limited to inference &amp; only Linux is supported.</p>
<h4>Details</h4>
<p>In Inductor joint graph passes, if <code>config.onednn_graph</code> is enabled, or the env variable <code>TORCHINDUCTOR_ONEDNN_GRAPH=1</code> is used on a machine that supports some advanced ISAs, then if some specific patterns are matched, the corresponding ops are handled by oneDNN Graph. However, if oneDNN Graph is unable to match a pattern, then it's handled by the same ops that'd have handled it sans oneDNN Graph.</p>
<p>As opposed to other oneDNN kernels (created with oneDNN primitives API) &amp; oneMKL kernels, these kernels are compiled at Inductor compilation-time (in case of shape-mismatch at runtime, compilation for those static input shapes will occur again).</p>
<p>BF16 dtype requires a machine to support at least <code>AVX512_BF16</code> ISA. <code>AMX</code> ISA is also used, if available.<br />
While FP32 dtype support may also work on Xeon SP generation 1 (SkyLake SP), which does not support <code>AVX512_VNNI</code> ISA, the lowest machine configuration for which this functionality will be enabled in this PR is Xeon SP generation 2 (Cascade Lake), but may be changed in a subsequent PR to include Skylake SP platform support as well. The reason to not support Skylake SP at this point is that some consumer-grade CPUs also support the same AVX512 ISAs as SkyLake SP, but this implementation has not been verified on consumer-grade CPUs.</p>
<h4>Summary of other changes</h4>
<ul>
<li>Builds the Graph Compiler backend of oneDNN Graph, which is included in <code>libdnnl.a</code></li>
<li>Adds extensible support for more patterns to be handled with oneDNN Graph in subsequent PRs</li>
<li>Adds an API to check if oneDNN Graph was built.</li>
<li>Serialization of SDPA patterns can now be done for some corner-cases, such as only inference.</li>
<li>Serialized attention patterns  can also have <code>getitem</code> in them, which should be replaced by <code>operator.getitem</code>.</li>
<li>Currently, oneDNN Graph is not used if dynamic shapes are provided at compile time.</li>
<li>In oneDNN v3.3.x, the GPT2 MHA pattern needs an attention mask, so in order to match cases of attention mask not being present, an attention mask of all zeros is being created in this implementation. The performance is still better compared to the default implementation, which means it'd be even better with oneDNN v3.5, which would make attention mask optional for GPT2 MHA pattern.</li>
</ul>
<h3>Performance data</h3>
<p>This PR accelerates inference of <code>hf_GPT2</code> by offloading its MHA computation to oneDNN Graph. Other models did not suffer any regression/breakage.</p>
<p>These performance datapoints were collected for a single TorchBench run (TorchBench may have run the workload multiple times) of each configuration, and were not cherry-picked, so it's possible that performance may be even better with this feature enabled -</p>
<p>|MODEL |BATCH SIZE | DTYPE | BEFORE: Speedup over eager-mode with the default Inductor implementation | AFTER: Speedup over eager mode with the default Inductor implementation| Perf boost = (AFTER - BEFORE)/BEFORE * 100|<br />
|--------------------------|-------------|---------|-----------------------------|--------------------------|------------|<br />
|hf_GPT2| 1 | FP32 | 1.522x | 1.784x| 17.21%|<br />
|hf_GPT2| 1 | BF16 (AMP) | 1.795x | 2.602x| 44.91%|<br />
|hf_GPT2| 2 | FP32 |  1.313x |1.629x | 24.06%|<br />
|hf_GPT2|2| BF16 (AMP) | 1.556x | 2.107x | 35.41%|<br />
|hf_GPT2_large| 1 | FP32 | 1.380x |1.510x | 9.4%|<br />
|hf_GPT2_large| 1 | BF16 (AMP) | 1.208x | 1.653x | 36.83%|<br />
|hf_GPT2_large| 2 | FP32 | 1.188x | 1.508x | 26.93%|<br />
|hf_GPT2_large|2| BF16 (AMP) | 0.991x | 1.771x | 78.70%|</p>
<p>Config: 48 physical cores of one socket of Intel Xeon Platinum 8468H (Xeon SP 4th gen a.k.a. Sapphire Rapids) . Only one logical core of a physical core was used. Intel OpenMP &amp; tcmalloc were preloaded.</p>
<p>Example of running the workload:<br />
<code>TORCHINDUCTOR_ONEDNN_GRAPH=1 OMP_NUM_THREADS=48 MKL_NUM_THREADS=48 numactl --membind=0 -C 0-47 python benchmarks/dynamo/torchbench.py --performance --inference --inductor --float32 -dcpu --only hf_GPT2 --freezing --batch-size 2</code></p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @ashokei @jingxu10 @gujinghui @PenghuiCheng @jianyuh @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @voznesenskym @penguinwu @EikanWang @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 21 Feb 2024 01:26:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120301</guid>
    </item>
    <item>
      <title>Increased compile time max GPUs to 512. Switched to int16_t DeviceIndex.</title>
      <link>https://github.com/pytorch/pytorch/pull/119639</link>
      <description><![CDATA[<p>Fixes #115331.</p>
<p>This PR increases the number of valid GPU devices to 512 (from 64) in order to future-proof PyTorch for providers that offer <a href="https://www.tensorwave.com/">single nodes with a large device count</a>. Until now, <code>DeviceIndex</code> was an <code>int8_t</code>, thus multiple changes were necessary:</p>
<ul>
<li><code>DeviceIndex</code> changed to <code>int16_t</code>. Updated consumers that assume it to be an <code>int8_t</code>.</li>
<li>Updated bounds checking for <code>torch.device()</code> in the Python frontend. Right now, we allow funny things like <code>torch.device('cpu', 200).index == -56</code>, which is undefined behavior. I inserted some checks to only allow values between 0 and <code>c10::Device::MAX_NUM_DEVICES - 1</code>.</li>
<li>Updated the <code>ArgumentInfo</code> struct as it hardcodes the device index as 8 bit field [^1]. Might be a breaking change, not sure if users rely on this.</li>
<li>Introduced <code>c10::Device::MAX_NUM_DEVICES</code> as a replacement for the old <code>C10_COMPILE_TIME_MAX_GPUS</code></li>
</ul>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225</p>
<p>[^1]: This field was unsigned, so I guess this has also been undef behavior the whole time? Our default device index is -1, so this always wrapped around to 255 when written to the <code>ArgumentInfo</code> struct. When I switched the <code>DeviceIndex</code> to <code>int16_t</code>, it actually stayed 255 after unpacking from <code>ArgumentInfo</code> again, as the <code>DeviceIndex</code> was now wide enough that it didn't wrap back to -1.</p>]]></description>
      <pubDate>Sat, 10 Feb 2024 05:14:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119639</guid>
    </item>
    <item>
      <title>`torch.compile()` makes loss `nan`</title>
      <link>https://github.com/pytorch/pytorch/issues/114109</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>torch.compile()</code> causes instability issue during training in my use case. Specifically, I observed that the loss goes <code>nan</code> with <code>torch.compile()</code> enabled.</p>
<p>To reproduce, I first tried below, but the minifier gave me <code>RuntimeError: Input graph did not fail the tester</code>:<br />
```console<br />
$ TORCHDYNAMO_REPRO_AFTER="aot" TORCHDYNAMO_REPRO_LEVEL=4 python my_script.py<br />
...<br />
torch._dynamo.debug_utils.AccuracyError: Bad accuracy detected<br />
$ python torch_compile_debug/run_2023_11_20_08_23_04_362462-pid_45740/minifier/minifier_launcher.py<br />
Loading inputs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:02&lt;00:00, 45.68it/s]</p>
<blockquote>
<blockquote>
<p>Loading inputs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53/53 [00:01&lt;00:00, 26.81it/s]<br />
 [2023-11-20 08:23:46,301] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph</p>
</blockquote>
</blockquote>
<p>Traceback (most recent call last):<br />
  File "/home/akihiro/work/github.com/kumo-ai/some-repo/torch_compile_debug/run_2023_11_20_08_23_04_362462-pid_45740/minifier/minifier_launcher.py", line 540, in <module><br />
    run_repro(mod, load_args, accuracy=True, command='minify', save_dir='/home/akihiro/work/github.com/kumo-ai/some-repo/torch_compile_debug/run_2023_11_20_08_23_04_362462-pid_45740/minifier/checkpoints', tracing_mode='real', check_str=None)<br />
  File "/home/akihiro/.conda/envs/kumo310/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 927, in run_repro<br />
    COMMAND_FNS<a href="options, mod, load_args">options.command</a><br />
  File "/home/akihiro/.conda/envs/kumo310/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 530, in repro_minify<br />
    minifier(<br />
  File "/home/akihiro/.conda/envs/kumo310/lib/python3.10/site-packages/torch/_functorch/fx_minifier.py", line 189, in minifier<br />
    raise RuntimeError("Input graph did not fail the tester")<br />
RuntimeError: Input graph did not fail the tester<br />
<code>and then, I tried below instead:</code>console<br />
$ TORCHDYNAMO_REPRO_AFTER="dynamo" TORCHDYNAMO_REPRO_LEVEL=4 python my_script.py<br />
...<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
E                   AccuracyError: Bad accuracy detected.<br />
$  python torch_compile_debug/run_2023_11_20_08_45_58_834796-pid_52789/minifier/minifier_launcher.py<br />
...<br />
Wrote minimal repro out to repro.py<br />
```</p>
<h3>Error logs</h3>
<p><code>console
$ python repro.py
Loading inputs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02&lt;00:00,  1.04s/it]
[2023-11-20 08:54:20,386] torch._dynamo.debug_utils: [WARNING] Could not generate fp64 outputs
[2023-11-20 08:54:20,388] torch._dynamo.debug_utils: [WARNING] Could not generate fp64 outputs
[2023-11-20 08:54:24,498] torch._dynamo.utils: [ERROR] Accuracy failed: allclose not within tol=0.001
Traceback (most recent call last):
  File "/home/akihiro/work/github.com/kumo-ai/some-repo/repro.py", line 44, in &lt;module&gt;
    run_repro(mod, load_args, accuracy=True, command='run',
  File "/home/akihiro/.conda/envs/kumo310/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 566, in run_repro
    COMMAND_FNS[options.command](options, mod, load_args)
  File "/home/akihiro/.conda/envs/kumo310/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 428, in repro_run
    raise AccuracyError("Dynamo failed")
torch._dynamo.debug_utils.AccuracyError: Dynamo failed</code></p>
<h3>Minified repro</h3>
<p>This is the minified <code>repro.py</code>:</p>
<p>```python<br />
from math import inf<br />
import torch<br />
from torch import tensor, device<br />
import torch.fx as fx<br />
import torch._dynamo<br />
from torch._dynamo.testing import rand_strided<br />
from torch._dynamo.debug_utils import run_fwd_maybe_bwd<br />
import torch._dynamo.config<br />
import torch._inductor.config<br />
import torch._functorch.config<br />
torch._dynamo.config.assume_static_by_default = False<br />
from torch.nn import *</p>
<p>class Repro(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
    def forward(self, view_22, getitem_51):<br />
        expand_as_16 = view_22.expand_as(getitem_51);  view_22 = getitem_51 = None<br />
        return (expand_as_16,)</p>
<p>mod = Repro()</p>
<p>def load_args(reader):<br />
    buf0 = reader.storage('d4bfc3c81078c64c193b38e7b2189c50a5687e7a', 16, device=device(type='cuda', index=0), dtype_hint=torch.int64)<br />
    reader.tensor(buf0, (1, 1), dtype=torch.int64, storage_offset=1, is_leaf=True)  # view_22<br />
    buf1 = reader.storage('549a91c2aa84c73c8e9be35307048f20aeb4fd99', 128, device=device(type='cuda', index=0))<br />
    reader.tensor(buf1, (1, 32), requires_grad=True)  # getitem_51<br />
load_args._version = 0</p>
<p>if <strong>name</strong> == '<strong>main</strong>':<br />
    from torch._dynamo.repro.after_dynamo import run_repro<br />
    run_repro(mod, load_args, accuracy=True, command='run',<br />
        save_dir='/home/akihiro/work/github.com/kumo-ai/some-repo/checkpoints', autocast=False, backend='inductor')<br />
```</p>
<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.1.0+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.22.5<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0] (64-bit runtime)<br />
Python platform: Linux-5.13.0-1031-aws-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: Tesla T4<br />
Nvidia driver version: 510.47.03<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
CPU(s):                          16<br />
On-line CPU(s) list:             0-15<br />
Thread(s) per core:              2<br />
Core(s) per socket:              8<br />
Socket(s):                       1<br />
NUMA node(s):                    1<br />
Vendor ID:                       GenuineIntel<br />
CPU family:                      6<br />
Model:                           85<br />
Model name:                      Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz<br />
Stepping:                        7<br />
CPU MHz:                         2499.996<br />
BogoMIPS:                        4999.99<br />
Hypervisor vendor:               KVM<br />
Virtualization type:             full<br />
L1d cache:                       256 KiB<br />
L1i cache:                       256 KiB<br />
L2 cache:                        8 MiB<br />
L3 cache:                        35.8 MiB<br />
NUMA node0 CPU(s):               0-15<br />
Vulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported<br />
Vulnerability L1tf:              Mitigation; PTE Inversion<br />
Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown<br />
Vulnerability Meltdown:          Mitigation; PTI<br />
Vulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown<br />
Vulnerability Spec store bypass: Vulnerable<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke avx512_vnni</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==6.1.0<br />
[pip3] mypy-boto3-s3==1.26.62<br />
[pip3] numpy==1.23.5<br />
[pip3] pytorch-lightning==2.1.0<br />
[pip3] pytorch-triton==2.1.0+6e4932cda8<br />
[pip3] torch==2.1.0+cu118<br />
[pip3] torch_geometric==2.4.0<br />
[pip3] torch-scatter==2.1.2+pt21cu118<br />
[pip3] torchdata==0.7.0<br />
[pip3] torchmetrics==0.11.4<br />
[pip3] torchtext==0.16.0<br />
[pip3] triton==2.1.0<br />
[conda] numpy                     1.23.5                   pypi_0    pypi<br />
[conda] pytorch-lightning         2.1.0                    pypi_0    pypi<br />
[conda] pytorch-triton            2.1.0+6e4932cda8          pypi_0    pypi<br />
[conda] torch                     2.1.0+cu118              pypi_0    pypi<br />
[conda] torch-geometric           2.4.0                    pypi_0    pypi<br />
[conda] torch-scatter             2.1.2+pt21cu118          pypi_0    pypi<br />
[conda] torchdata                 0.7.0                    pypi_0    pypi<br />
[conda] torchmetrics              0.11.4                   pypi_0    pypi<br />
[conda] torchtext                 0.16.0                   pypi_0    pypi<br />
[conda] triton                    2.1.0                    pypi_0    pypi<br />
```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @wconstab @bdhirsh @anijain2305</p>]]></description>
      <pubDate>Mon, 20 Nov 2023 01:15:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/114109</guid>
    </item>
  </channel>
</rss>
