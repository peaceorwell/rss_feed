<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>WIP: [inductor] Be more strict about down-casting to {b,}float16</title>
      <link>https://github.com/pytorch/pytorch/pull/122915</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122736<br />
* <strong>-&gt;</strong> #122915<br />
* #115435<br />
* #122518<br />
* #121924</p>
<p>Our triton codegen currently treats all float16 and bfloat16 intermediates<br />
as float32 and this may lead to numerical discrepancies from eager.</p>
<p>For example, say a float16 value is inlined into one kernel but also realized<br />
as an input argument to another kernel. These kernels will observe two different<br />
values for the same variable! One in full float32 precision and the other<br />
truncated to float16.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 11:37:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122915</guid>
    </item>
    <item>
      <title>[inductor][Autotune] Add matrix_instr_nonkdim to triton_meta (#122852)</title>
      <link>https://github.com/pytorch/pytorch/pull/122906</link>
      <description><![CDATA[<p>Summary:</p>
<p>Previous work <code>https://github.com/pytorch/pytorch/pull/120742</code> to enable <code>matrix_instr_nonkdim</code> only dealt with the autotuner benchmarking, but failed to enable the parameter in Triton meta for real runs. <code>matrix_instr_nonkdim</code> needs to be visible to the compiler driver to set up the optimization pipeline, so it's unlike other kernel parameters such as <code>BLOCK_N</code> that can be just set inside the kernel itself.</p>
<p>Test Plan:<br />
P1201466917</p>
<p>triton_heuristics.template(<br />
    num_stages=1,<br />
    num_warps=4,<br />
    triton_meta={'signature': {0: '<em>fp32', 1: '</em>fp32', 2: '*fp32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())], 'matrix_instr_nonkdim': 16},<br />
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_0', 'backend_hash': None},<br />
  )</p>
<p>Differential Revision: D55456401</p>
<p>Fixes #ISSUE_NUMBER</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 09:48:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122906</guid>
    </item>
    <item>
      <title>Add inductor fx pass unit test for shape propagation</title>
      <link>https://github.com/pytorch/pytorch/pull/122897</link>
      <description><![CDATA[<p>Summary: Pre-grad fx passes expect information from shape propagation to be present. D55221119 ensured that <code>pass_execution_and_save</code> invokes shape propagation, and this diff adds a covering unit test to prevent regression.</p>
<p>Test Plan: New UT passes locally.</p>
<p>Differential Revision: D55440240</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 07:22:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122897</guid>
    </item>
    <item>
      <title>Less error prone way of ensuring Inductor cache is disabled when doing local development on Inductor</title>
      <link>https://github.com/pytorch/pytorch/issues/122891</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I recently spent half an hour debugging because I added a breakpoint to inductor, ran a PyTorch unit test, and scratched my head when seemingly the breakpoint wasn't being called at all. Well, the reason was because I had a warm cache and was hitting the cache entry instead.</p>
<p>I'm not sure what I'd suggest. It would be nice if the cache knew to automatically invalidate itself when PyTorch code itself changes, but that might be difficult to implement efficiently. A convenient way to force runs without hitting cache would help maybe.</p>
<p>cc @masnesral </p>
<h3>Versions</h3>
<p>main</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 06:20:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122891</guid>
    </item>
    <item>
      <title>fix amp for AOTInductor</title>
      <link>https://github.com/pytorch/pytorch/pull/122883</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122883</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 01:38:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122883</guid>
    </item>
    <item>
      <title>[FSDP2][PT2D] enable torch.compile for compute in CI</title>
      <link>https://github.com/pytorch/pytorch/pull/122876</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122876<br />
* #122851</p>
<p>34 tests are eligible for torch.compile (call forward) <br />
* 70% passed<br />
* 30% failed for 3 reasons</p>
<p>failure on 2D<br />
* test_fully_shard_init.py::test_meta_device_2d_init<br />
* test_fully_shard_training.py::test_train_parity_2d_mlp<br />
* test_fully_shard_training.py::test_train_parity_2d_transformer_checkpoint_resume<br />
* test_fully_shard_clip_grad_norm_.py::test_clip_grad_norm_2d</p>
<p>failure on composable AC<br />
* test_fully_shard_training.py::test_train_parity_with_shared_params<br />
* test_fully_shard_training.py::test_train_parity_with_activation_checkpointing<br />
* test_fully_shard_comm.py::test_fully_shard_backward_prefetch<br />
* test_fully_shard_frozen.py::test_train_mixed_requires_grad_per_group</p>
<p>failure on numerics<br />
* test_fully_shard_mixed_precision.py::test_compute_dtype<br />
* test_fully_shard_mixed_precision.py::test_reduce_dtype</p>
<p>following 2 tests called forward but did not count as eligible since memory and time are different in compiler mode<br />
* test_fully_shard_memory.py::test_fully_shard_training_memory<br />
* test_fully_shard_overlap.py::test_fully_shard_training_overlap</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 23:13:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122876</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Reuse inductor test for Intel GPU (PART 1)</title>
      <link>https://github.com/pytorch/pytorch/pull/122866</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122866<br />
* #121895<br />
* #122254<br />
* #121883</p>
<p>backend.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 20:28:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122866</guid>
    </item>
    <item>
      <title>[inductor][cpu]DebertaV2ForQuestionAnswering AMP static shape multiple thread default wrapper regression in 2024-03-23</title>
      <link>https://github.com/pytorch/pytorch/issues/122862</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>new_perf_regression in 2024-03-23</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>huggingface</td>
      <td>DebertaV2ForQuestionAnswering</td>
      <td>multiple</td>
      <td>1.0</td>
      <td>1.012825</td>
      <td>0.118098464</td>
      <td>0.11961307680080001</td>
      <td>26.196844</td>
      <td>1.0</td>
      <td>2.290567</td>
      <td>0.068807462</td>
      <td>0.157608101810954</td>
      <td>27.15124</td>
      <td>0.44</td>
      <td>1.32</td>
      <td>0.58</td>
      <td>1.04</td>
    </tr>
  </tbody>

</table>

<p>42624bc<br />
/workspace/pytorch# bash inductor_single_run.sh multiple inference performance huggingface DebertaV2ForQuestionAnswering amp first static<br />
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles<br />
cpu,DebertaV2ForQuestionAnswering,1,1.287133,95.115025,30.050526,0.930185,1250.272051,1344.110592,1088,1,0,0,0,0</p>
<p>e26280a<br />
/workspace/pytorch# bash inductor_single_run.sh multiple inference performance huggingface DebertaV2ForQuestionAnswering amp first static<br />
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles<br />
cpu,DebertaV2ForQuestionAnswering,1,1.989292,62.880099,19.385791,0.930492,1270.559949,1365.470413,1088,1,0,0,0,0</p>
<h3>Versions</h3>
<p></table><p>SW info</p><table border="1" class="dataframe table"><br />
<br />
<thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead></p>
<tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>d6015d42</td>
      <td>main</td>
      <td>d6015d42</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>f0d461beacded34abe196c72ec4bcdb55bf01793</td>
      <td>main</td>
      <td>5bc7f7f97760d2d485621f9f30d0316e1f2440c6</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.19.0a0+a0c79b3</td>
      <td>main</td>
      <td>0.18.0a0+0325175</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+17a7081</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>

</table>
<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh multiple inference performance huggingface DebertaV2ForQuestionAnswering amp first static</p>
<p><a href="https://github.com/pytorch/pytorch/files/14783477/huggingface-DebertaV2ForQuestionAnswering-inference-amp-static-default-multiple-performance-drop_guilty_commit.1.log">huggingface-DebertaV2ForQuestionAnswering-inference-amp-static-default-multiple-performance-drop_guilty_commit (1).log</a></p>
<p>Suspected guilty commit: https://github.com/pytorch/pytorch/commit/42624bceb6856a8934a1fcacb8a621a418f1a2c3<br />
cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 19:11:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122862</guid>
    </item>
    <item>
      <title>Short-term fix to preserve NJT metadata cache in torch.compile</title>
      <link>https://github.com/pytorch/pytorch/pull/122836</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121947<br />
* <strong>-&gt;</strong> #122836</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D55448636">D55448636</a></p>]]></description>
      <pubDate>Wed, 27 Mar 2024 13:48:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122836</guid>
    </item>
    <item>
      <title>Add Matmul recipe into x86_inductor_quantizer</title>
      <link>https://github.com/pytorch/pytorch/pull/122776</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122777<br />
* <strong>-&gt;</strong> #122776<br />
* #122775</p>
<p><strong>Summary</strong><br />
Add <code>matmul</code> in the quantization recipes, noting that it's not a general recipe but tailored to meet accuracy criteria for specific models. <code>matmul</code> recipe is disabled by default.</p>
<p><strong>Test Plan</strong><br />
<code>python -m pytest quantization/pt2e/test_x86inductor_quantizer.py -k test_attention_block</code></p>]]></description>
      <pubDate>Wed, 27 Mar 2024 01:18:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122776</guid>
    </item>
    <item>
      <title>Add Quantization recipe filter per operator type for x86_inductor_quantizer</title>
      <link>https://github.com/pytorch/pytorch/pull/122775</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122777<br />
* #122776<br />
* <strong>-&gt;</strong> #122775</p>
<p><strong>Summary</strong><br />
Default recipes are enabled in <code>X86InductorQuantizer</code> and request comes to customize recipes based on these defaults.</p>
<ul>
<li>Avoid annotation propagation and restrict annotation only to annotate <code>conv</code>/<code>linear</code>.</li>
<li>Add <code>matmul</code>  in the quantization recipes, noting that it's not a general recipe but tailored to meet accuracy criteria for specific models.</li>
</ul>
<p>To meet these requests, we made changes in this PR by using the same interface of  <code>set_operator_type</code> as <code>XNNPACKQuantizer</code>.</p>
<ul>
<li>To disable the recipe for this operator, user can simply exclude it from the list of operations as <code>quantizer.set_operator_type(op, None)</code>.</li>
<li>To modify or extend the recipe for this operator with default recipe, user can customize as <code>quantizer.set_operator_type(op, config)</code>.</li>
</ul>
<p><strong>Test Plan</strong><br />
<code>python -m pytest quantization/pt2e/test_x86inductor_quantizer.py -k test_filter_conv2d_recipe
python -m pytest quantization/pt2e/test_x86inductor_quantizer.py -k test_filter_linear_recipe
python -m pytest quantization/pt2e/test_x86inductor_quantizer.py -k test_filter_maxpool2d_recipe</code></p>]]></description>
      <pubDate>Wed, 27 Mar 2024 01:18:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122775</guid>
    </item>
    <item>
      <title>Dont precompile already seen keys, limit epilogue choices</title>
      <link>https://github.com/pytorch/pytorch/pull/122642</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122825<br />
* #122643<br />
* <strong>-&gt;</strong> #122642<br />
* #121999</p>
<p>Two changes: <br />
- in epilogue benchmark fusion, only take top 6 choices. There were basically no choices taken after this in HF.<br />
- Share a single precompilation function among matmuls with same key. </p>]]></description>
      <pubDate>Mon, 25 Mar 2024 12:24:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122642</guid>
    </item>
    <item>
      <title>[fix] 'from' in python code generated by inductor, resulting in SyntaxError</title>
      <link>https://github.com/pytorch/pytorch/pull/122632</link>
      <description><![CDATA[<p><code>from</code> is  a python keyword.  Python code containing <code>aten.random.from</code> will result in syntax errors. This PR handle this special case in the codegen of inductor. </p>
<p>Although only <code>aten.random(_).</code>  has an <code>from</code> overload for now, all possible cases were handled.</p>
<p>Fixes #121621 </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 10:47:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122632</guid>
    </item>
    <item>
      <title>[Inductor] add contiguous layout optm for bmm input</title>
      <link>https://github.com/pytorch/pytorch/pull/122599</link>
      <description><![CDATA[<p>Fixes #117743.</p>
<p>Add contiguous layout optimization for <code>bmm</code> input, to avoid additional copies.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 01:24:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122599</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Enable lowering of qlinear-binary(-unary) fusion for X86Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/122593</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122667<br />
* <strong>-&gt;</strong> #122593<br />
* #122387<br />
* #122288</p>
<p><strong>Description</strong><br />
Lower the qlinear binary post op pattern to Inductor. Use post op sum (in-place) if the extra input has the same dtype as output. Otherwise, it uses binary add.<br />
<code>+-------------------+--------------+---------------+
| Extra input dtype | Output dtype | Post op       |
+-------------------+--------------+---------------+
| Fp32/bf16         | fp32/bf16    | sum           |
+-------------------+--------------+---------------+
| Fp32/bf16         | int8         | add           |
+-------------------+--------------+---------------+
| int8              | fp32/bf16    | not supported |
+-------------------+--------------+---------------+
| int8              | int8         | sum           |
+-------------------+--------------+---------------+</code></p>
<p>The following patterns can be matched:<br />
<code>qx -&gt; qlinear -&gt; add -&gt; optional relu -&gt; optional type convert -&gt; optional quant</code></p>
<p><strong>Test plan</strong><br />
python test/inductor/test_mkldnn_pattern_matcher.py -k test_qlinear_add<br />
python test/inductor/test_cpu_cpp_wrapper.py -k test_qlinear_add</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Mar 2024 23:49:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122593</guid>
    </item>
    <item>
      <title>[inductor] Add explicit ops.fma and use it in softmax_backward</title>
      <link>https://github.com/pytorch/pytorch/pull/122518</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122736<br />
* #122915<br />
* #115435<br />
* <strong>-&gt;</strong> #122518<br />
* #121924</p>
<p>This allows us to generate an fma even when fp-fusion is disabled<br />
in the compiler.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 22 Mar 2024 12:48:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122518</guid>
    </item>
    <item>
      <title>[Inductor] Pass device interface to the worker compile</title>
      <link>https://github.com/pytorch/pytorch/pull/122492</link>
      <description><![CDATA[<p>Summary: In <code>codecache.py</code> pass the device_interface directly to <code>_worker_compile()</code> instead of calling <code>get_device_interface()</code> from inside the function.</p>
<p>If the device_interface is registered by an out-of-tree module then it will only be registered inside the main process and not inside the worker process. This fixes this issue. Happy to add a test if required. </p>
<p>Test plan:<br />
No tests added</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 22 Mar 2024 04:05:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122492</guid>
    </item>
    <item>
      <title>[Quant][PT2E] Enable linear-binary(-unary) post-op recipe for X86Inductor quantizer</title>
      <link>https://github.com/pytorch/pytorch/pull/122387</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122667<br />
* #122593<br />
* <strong>-&gt;</strong> #122387<br />
* #122288</p>
<p>As the title<br />
<strong>Test plan</strong><br />
python test/test_quantization.py -k test_linear_binary</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 22:29:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122387</guid>
    </item>
    <item>
      <title>[inductor]re-enable cpu reduction ut</title>
      <link>https://github.com/pytorch/pytorch/pull/122289</link>
      <description><![CDATA[<p>Re-enable these two ut. I can pass these two ut on my local and we can see the status in the CI for this PR.</p>
<p>See the background about why they are disabled https://github.com/pytorch/pytorch/issues/93542, https://github.com/pytorch/pytorch/issues/87157.</p>
<p>After https://github.com/pytorch/pytorch/pull/115620. The reduction orders should be deterministic.<br />
However, the orders may not exactly same with ref path (<code>aten</code>). We may can set larger tolerance if they still cannot be passed in CI.</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122289</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 23:43:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122289</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Add qlinear_pointwise.binary op for X86Inductor backend</title>
      <link>https://github.com/pytorch/pytorch/pull/122288</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122667<br />
* #122593<br />
* #122387<br />
* <strong>-&gt;</strong> #122288</p>
<p><strong>Description</strong><br />
Add qlinear_binary op for X86Inductor backend of quantization PT2E. It only supports <code>add</code> and <code>add_relu</code> now.<br />
It will use post op sum if the extra input has the same dtype as output. Otherwise, it uses binary add.<br />
<code>+-------------------+--------------+---------------+
| Extra input dtype | Output dtype | Post op       |
+-------------------+--------------+---------------+
| Fp32/bf16         | fp32/bf16    | sum           |
+-------------------+--------------+---------------+
| Fp32/bf16         | int8         | add           |
+-------------------+--------------+---------------+
| int8              | fp32/bf16    | not supported |
+-------------------+--------------+---------------+
| int8              | int8         | sum           |
+-------------------+--------------+---------------+</code></p>
<p><strong>Test plan</strong><br />
python test_quantization.py -k test_qlinear_add_pt2e<br />
python test_quantization.py -k test_qlinear_add_relu_pt2e</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 23:40:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122288</guid>
    </item>
    <item>
      <title>[aoti] Change aot_compile callsites</title>
      <link>https://github.com/pytorch/pytorch/pull/122225</link>
      <description><![CDATA[<p>Summary:<br />
Replacing <code>torch._export.aot_compile</code> callsites with <br />
<code>ep = torch.export._trace._export(.., predispatch=True)   # Traces the given program into predispatch IR
so_path = torch._inductor.aot_compile_ep(ep, ...)  # Takes an exported program and compiles it into a .so</code></p>
<p>This allows us to explicitly split up the export step from AOTInductor. We can later modify tests to do <code>export + serialize + deserialize + inductor</code> to mimic internal production use cases better.</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D54808612</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 12:18:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122225</guid>
    </item>
    <item>
      <title>Update torch.compile_faq w.r.t to functorch</title>
      <link>https://github.com/pytorch/pytorch/pull/122213</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122213<br />
* #122212<br />
* #122211</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 09:47:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122213</guid>
    </item>
    <item>
      <title>Error "module compiled against ABI version" when using a device on MacBook Pro M2 with MacOS Sonoma 14.4</title>
      <link>https://github.com/pytorch/pytorch/issues/122056</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<ul>
<li>I created a brand new virtual environment and Python script containing the following code.</li>
<li>I installed the beta / nightly version of Pytorch, using the <a href="https://developer.apple.com/metal/pytorch/">directions on Apple's website for MPS (M2) support</a>.</li>
</ul>
<p><code>pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu</code></p>
<p><code>python
import torch
mps = torch.device("mps")</code></p>
<p>When I run the code, I get this error:</p>
<p>```</p>
<p>A module that was compiled using NumPy 1.x cannot be run in<br />
NumPy 2.0.0b1 as it may crash. To support both 1.x and 2.x<br />
versions of NumPy, modules must be compiled against NumPy 2.0.</p>
<p>If you are a user of the module, the easiest solution will be to<br />
either downgrade NumPy or update the failing module (if available).</p>
<p>Traceback (most recent call last):  File "/Users/trevor.sullivan/git/pytorch-audio-classifier/main.py", line 7, in <module><br />
    mps = torch.device("cpu")<br />
/Users/trevor.sullivan/git/pytorch-audio-classifier/main.py:7: DeprecationWarning: numpy.core._multiarray_umath is deprecated and has been renamed to numpy._core._multiarray_umath. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core._multiarray_umath._ARRAY_API.<br />
  mps = torch.device("cpu")<br />
/Users/trevor.sullivan/git/pytorch-audio-classifier/main.py:7: UserWarning: Failed to initialize NumPy: module compiled against ABI version 0x1000009 but this version of numpy is 0x2000000 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)<br />
  mps = torch.device("cpu")<br />
```</p>
<p>How can I go about resolving this?</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.4.0.dev20240317<br />
Is debug build: False<br />
CUDA used to build PyTorch: None<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: macOS 14.4 (arm64)<br />
GCC version: Could not collect<br />
Clang version: 15.0.0 (clang-1500.3.9.4)<br />
CMake version: Could not collect<br />
Libc version: N/A</p>
<p>Python version: 3.12.2 (main, Feb  6 2024, 20:19:44) [Clang 15.0.0 (clang-1500.1.0.2.5)] (64-bit runtime)<br />
Python platform: macOS-14.4-arm64-arm-64bit<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Apple M2 Pro</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==2.0.0b1<br />
[pip3] torch==2.4.0.dev20240317<br />
[pip3] torchaudio==2.2.0.dev20240317<br />
[pip3] torchvision==0.18.0.dev20240317<br />
[conda] Could not collect<br />
```</p>
<p>cc @seemethere @malfet @osalpekar @atalman @mruberry @rgommers @kulinseth @albanD @DenisVieriu97 @razarmehr</p>]]></description>
      <pubDate>Sun, 17 Mar 2024 14:11:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122056</guid>
    </item>
    <item>
      <title>[inductor] Lower divide by constant as multiplication by reciprocal</title>
      <link>https://github.com/pytorch/pytorch/pull/121924</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122736<br />
* #122915<br />
* #115435<br />
* #122518<br />
* <strong>-&gt;</strong> #121924</p>
<p>Fixes #101039</p>
<p>This lowers division by a constant value to be multipication by reciprocal.<br />
The same optimization is applied in eager mode on CUDA:</p>
<p>https://github.com/pytorch/pytorch/blob/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu#L36-L38</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 11:41:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121924</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Add Inductor Intel GPU backend.</title>
      <link>https://github.com/pytorch/pytorch/pull/121895</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122866<br />
* <strong>-&gt;</strong> #121895<br />
* #122254<br />
* #121883</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 00:00:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121895</guid>
    </item>
    <item>
      <title>[Inductor] [ReImplement] Outer Loop Fusion for CPP Backend</title>
      <link>https://github.com/pytorch/pytorch/pull/121625</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121625</p>
<p><strong>Summary</strong><br />
Re-implement of https://github.com/pytorch/pytorch/pull/121064</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_outer_loop_fusion</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 10 Mar 2024 23:44:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121625</guid>
    </item>
    <item>
      <title>[WIP][inductor] padding</title>
      <link>https://github.com/pytorch/pytorch/pull/120758</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120758</p>
<p>This is not fully ready for review yet. Sending it out in case people want to give early feedback. Also it can allow me run CI perf test earlier.</p>
<p>By testing BlenderbotSmallForConditionalGeneration I already see 2.5ms speedup. I feel the solution can be further improved to get more speedup.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang @jansel @Chillee @eellison </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames</p>]]></description>
      <pubDate>Tue, 27 Feb 2024 17:00:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120758</guid>
    </item>
    <item>
      <title>[inductor] Disable fp contraction and add option to use precise division</title>
      <link>https://github.com/pytorch/pytorch/pull/115435</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122736<br />
* #122915<br />
* <strong>-&gt;</strong> #115435<br />
* #122518<br />
* #121924</p>
<p>Fixes #101039</p>
<p>In the issue we have something like:<br />
<code>python
(x / 1e-6) + (y / 1e-6)</code><br />
which triton turns into<br />
<code>python
fma(x, 1e6, y * 1e6)</code></p>
<p>Where we have one division calculated to float32 precision,<br />
and the other calculated to infinite precision as part of the <code>fma</code>.<br />
In the issue this leads to a bad normalization in softmax.</p>
<p>Here we disallow the "fp fusion" optimization which generates these fma instructions.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Fri, 08 Dec 2023 10:25:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/115435</guid>
    </item>
    <item>
      <title>Make compiled models serializable</title>
      <link>https://github.com/pytorch/pytorch/issues/101107</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Serializing a compiled model with <code>pickle</code> fails with <code>Can't pickle local object 'convert_frame.&lt;locals&gt;._convert_frame'</code> and <code>cannot pickle 'ConfigModuleInstance' object</code> when using <code>dill</code>.</p>
<p>A Colab with an example: <br />
https://colab.research.google.com/drive/1v6jUUq86ql1Era4X47cIDj7bzrrz2RZe?usp=sharing</p>
<p>In Hugging Face Datasets, this error stops us from generating (deterministic) hashes for transforms (functions) that reference a compiled model, meaning such transforms cannot be cached and must be re-computed each time when transforming a dataset.   </p>
<p>(The "export" API for the compiled models would also work for us.)</p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<details>
<summary>Colab env with torch 2.0.1 installed</summary>

```
PyTorch version: 2.0.1+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: 10.0.0-4ubuntu1 
CMake version: version 3.25.2
Libc version: glibc-2.31

Python version: 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.10.147+-x86_64-with-glibc2.31
Is CUDA available: False
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.7.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.7.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          2
On-line CPU(s) list:             0,1
Thread(s) per core:              2
Core(s) per socket:              1
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           79
Model name:                      Intel(R) Xeon(R) CPU @ 2.20GHz
Stepping:                        0
CPU MHz:                         2200.196
BogoMIPS:                        4400.39
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       32 KiB
L1i cache:                       32 KiB
L2 cache:                        256 KiB
L3 cache:                        55 MiB
NUMA node0 CPU(s):               0,1
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Mitigation; PTE Inversion
Vulnerability Mds:               Vulnerable; SMT Host state unknown
Vulnerability Meltdown:          Vulnerable
Vulnerability Mmio stale data:   Vulnerable
Vulnerability Retbleed:          Vulnerable
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Vulnerable
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities

Versions of relevant libraries:
[pip3] numpy==1.22.4
[pip3] torch==2.0.1+cu118
[pip3] torchaudio==2.0.2+cu118
[pip3] torchdata==0.6.0
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.15.1
[pip3] torchvision==0.15.2+cu118
[pip3] triton==2.0.0
[conda] Could not collect
```
</details>

<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4 @soumith @wconstab @ngimel</p>]]></description>
      <pubDate>Wed, 10 May 2023 10:48:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/101107</guid>
    </item>
    <item>
      <title>Custom recurrent network takes very long to compile for long sequences</title>
      <link>https://github.com/pytorch/pytorch/issues/97155</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I am currently experimenting around <code>torch.compile</code> for use with a custom, pure-Python LiGRU implementation, which is a variant of the typical GRU architecture. This is not a crash/miscompile issue but a performance bug, resulting in really long compile times (&gt;15 minutes for 1024 sequence length as an extreme example), ~~and worsened runtime performance (worse than eager even at a 64 sequence length on an A100 40GB)~~ (that part needs double checking).<br />
For smaller sequences, the compiled variant is typically much faster than either eager mode or <code>torch.jit.script</code> compilation.</p>
<p>It seems likely to me that the recurrent architecture is being fully unrolled. Is this the expected behavior (e.g. at graph construction time)? If so, what can be done to work around the issue?<br />
Note that I have been unable to try <code>dynamic=True</code> for now, as I am encountering an error with it enabled. If you believe dynamic shape support can help with this issue, I can also look deeper into that and report back.</p>
<p>The recurrent architecture in the below code is implemented as a <code>for</code>-loop over the sequence length as determined from the shape of the input.<br />
I am not very familiar with <code>torch.compile</code>'s architecture, or with GPU compilers in general. However, I am wondering if something like partial unrolling in the CPU world could be applied here.</p>
<p>I have attached the full real-world code including the toy benchmark I've been using, which hopefully is correct.<br />
I assumed it would not be helpful to create a smaller repro as this is not a crash/miscompile. If I'm wrong, I could try creating a smaller repro that shows similar behavior.</p>
<p><strong><code>def _ligru_cell(self, w, ht):</code> is the relevant function.</strong></p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Repro</h3>
<p>```python<br />
import logging<br />
logging.basicConfig(<br />
    format='%(asctime)s,%(msecs)03d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s',<br />
    datefmt='%Y-%m-%d:%H:%M:%S',<br />
    level=logging.INFO<br />
)<br />
logging.info("importing pytorch")</p>
<p>import torch<br />
import torch._dynamo as dynamo<br />
import torch.nn as nn<br />
import time</p>
<p>from torch import Tensor<br />
from typing import Optional</p>
<p>class LiGRU(torch.nn.Module):<br />
    """ This function implements a Light GRU (liGRU).<br />
    LiGRU is single-gate GRU model based on batch-norm + relu<br />
    activations + recurrent dropout. For more info see:<br />
    "M. Ravanelli, P. Brakel, M. Omologo, Y. Bengio,<br />
    Light Gated Recurrent Units for Speech Recognition,<br />
    in IEEE Transactions on Emerging Topics in Computational Intelligence,<br />
    2018" (https://arxiv.org/abs/1803.10225)<br />
    This is a custm RNN and to speed it up it must be compiled with<br />
    the torch just-in-time compiler (jit) right before using it.<br />
    You can compile it with:<br />
    compiled_model = torch.jit.script(model)<br />
    It accepts in input tensors formatted as (batch, time, fea).<br />
    In the case of 4d inputs like (batch, time, fea, channel) the tensor is<br />
    flattened as (batch, time, fea*channel).<br />
    Arguments<br />
    ---------<br />
    hidden_size : int<br />
        Number of output neurons (i.e, the dimensionality of the output).<br />
        values (i.e, time and frequency kernel sizes respectively).<br />
    input_shape : tuple<br />
        The shape of an example input.<br />
    nonlinearity : str<br />
        Type of nonlinearity (tanh, relu).<br />
    normalization : str<br />
        Type of normalization for the ligru model (batchnorm, layernorm).<br />
        Every string different from batchnorm and layernorm will result<br />
        in no normalization.<br />
    num_layers : int<br />
        Number of layers to employ in the RNN architecture.<br />
    bias : bool<br />
        If True, the additive bias b is adopted.<br />
    dropout : float<br />
        It is the dropout factor (must be between 0 and 1).<br />
    re_init : bool<br />
        If True, orthogonal initialization is used for the recurrent weights.<br />
        Xavier initialization is used for the input connection weights.<br />
    bidirectional : bool<br />
        If True, a bidirectional model that scans the sequence both<br />
        right-to-left and left-to-right is used.<br />
    Example<br />
    -------<br />
    &gt;&gt;&gt; inp_tensor = torch.rand([4, 10, 20])<br />
    &gt;&gt;&gt; net = LiGRU(input_shape=inp_tensor.shape, hidden_size=5)<br />
    &gt;&gt;&gt; out_tensor, _ = net(inp_tensor)<br />
    &gt;&gt;&gt;<br />
    torch.Size([4, 10, 5])<br />
    """</p>
<pre><code>def __init__(
    self,
    hidden_size,
    input_shape,
    nonlinearity="relu",
    normalization="batchnorm",
    num_layers=1,
    bias=True,
    dropout=0.0,
    re_init=True,
    bidirectional=False,
):
    super().__init__()
    self.hidden_size = hidden_size
    self.nonlinearity = nonlinearity
    self.num_layers = num_layers
    self.normalization = normalization
    self.bias = bias
    self.dropout = dropout
    self.re_init = re_init
    self.bidirectional = bidirectional
    self.reshape = False

    # Computing the feature dimensionality
    if len(input_shape) &gt; 3:
        self.reshape = True
    self.fea_dim = float(torch.prod(torch.tensor(input_shape[2:])))
    self.batch_size = input_shape[0]
    self.rnn = self._init_layers()

    if self.re_init:
        rnn_init(self.rnn)

def _init_layers(self):
    """Initializes the layers of the liGRU."""
    rnn = torch.nn.ModuleList([])
    current_dim = self.fea_dim

    for i in range(self.num_layers):
        rnn_lay = LiGRU_Layer(
            current_dim,
            self.hidden_size,
            self.num_layers,
            self.batch_size,
            dropout=self.dropout,
            nonlinearity=self.nonlinearity,
            normalization=self.normalization,
            bidirectional=self.bidirectional,
        )
        rnn.append(rnn_lay)

        if self.bidirectional:
            current_dim = self.hidden_size * 2
        else:
            current_dim = self.hidden_size
    return rnn

def forward(self, x, hx: Optional[Tensor] = None):
    """Returns the output of the liGRU.
    Arguments
    ---------
    x : torch.Tensor
        The input tensor.
    hx : torch.Tensor
        Starting hidden state.
    """
    # Reshaping input tensors for 4d inputs
    if self.reshape:
        if x.ndim == 4:
            x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3])

    # run ligru
    output, hh = self._forward_ligru(x, hx=hx)

    return output, hh

def _forward_ligru(self, x, hx: Optional[Tensor]):
    """Returns the output of the vanilla liGRU.
    Arguments
    ---------
    x : torch.Tensor
        Input tensor.
    hx : torch.Tensor
    """
    h = []
    if hx is not None:
        if self.bidirectional:
            hx = hx.reshape(
                self.num_layers, self.batch_size * 2, self.hidden_size
            )
    # Processing the different layers
    for i, ligru_lay in enumerate(self.rnn):
        if hx is not None:
            x = ligru_lay(x, hx=hx[i])
        else:
            x = ligru_lay(x, hx=None)
        h.append(x[:, -1, :])
    h = torch.stack(h, dim=1)

    if self.bidirectional:
        h = h.reshape(h.shape[1] * 2, h.shape[0], self.hidden_size)
    else:
        h = h.transpose(0, 1)

    return x, h
</code></pre>
<p>def chunker(seq, size):<br />
    return (seq[pos:pos + size] for pos in range(0, len(seq), size))</p>
<p>class LiGRU_Layer(torch.nn.Module):<br />
    """ This function implements Light-Gated Recurrent Units (ligru) layer.<br />
    Arguments<br />
    ---------<br />
    input_size : int<br />
        Feature dimensionality of the input tensors.<br />
    batch_size : int<br />
        Batch size of the input tensors.<br />
    hidden_size : int<br />
        Number of output neurons.<br />
    num_layers : int<br />
        Number of layers to employ in the RNN architecture.<br />
    nonlinearity : str<br />
        Type of nonlinearity (tanh, relu).<br />
    normalization : str<br />
        Type of normalization (batchnorm, layernorm).<br />
        Every string different from batchnorm and layernorm will result<br />
        in no normalization.<br />
    dropout : float<br />
        It is the dropout factor (must be between 0 and 1).<br />
    bidirectional : bool<br />
        if True, a bidirectional model that scans the sequence both<br />
        right-to-left and left-to-right is used.<br />
    """</p>
<pre><code>def __init__(
    self,
    input_size,
    hidden_size,
    num_layers,
    batch_size,
    dropout=0.0,
    nonlinearity="relu",
    normalization="batchnorm",
    bidirectional=False,
):

    super(LiGRU_Layer, self).__init__()
    self.hidden_size = int(hidden_size)
    self.input_size = int(input_size)
    self.batch_size = batch_size
    self.bidirectional = bidirectional
    self.dropout = dropout

    self.w = nn.Linear(self.input_size, 2 * self.hidden_size, bias=False)

    self.u = nn.Linear(self.hidden_size, 2 * self.hidden_size, bias=False)

    if self.bidirectional:
        self.batch_size = self.batch_size * 2

    # Initializing batch norm
    self.normalize = False

    if normalization == "batchnorm":
        self.norm = nn.BatchNorm1d(2 * self.hidden_size, momentum=0.05)
        self.normalize = True

    elif normalization == "layernorm":
        self.norm = torch.nn.LayerNorm(2 * self.hidden_size)
        self.normalize = True
    else:
        # Normalization is disabled here. self.norm is only  formally
        # initialized to avoid jit issues.
        self.norm = torch.nn.LayerNorm(2 * self.hidden_size)
        self.normalize = True

    # Initial state
    self.register_buffer("h_init", torch.zeros(1, self.hidden_size))

    # Preloading dropout masks (gives some speed improvement)
    self._init_drop(self.batch_size)

    # Setting the activation function
    if nonlinearity == "tanh":
        self.act = torch.nn.Tanh()
    elif nonlinearity == "sin":
        self.act = torch.sin
    elif nonlinearity == "leaky_relu":
        self.act = torch.nn.LeakyReLU()
    else:
        self.act = torch.nn.ReLU()

def forward(self, x, hx: Optional[Tensor] = None):
    # type: (Tensor, Optional[Tensor]) -&gt; Tensor # noqa F821
    """Returns the output of the liGRU layer.
    Arguments
    ---------
    x : torch.Tensor
        Input tensor.
    """
    if self.bidirectional:
        x_flip = x.flip(1)
        x = torch.cat([x, x_flip], dim=0)

    # Change batch size if needed
    self._change_batch_size(x)

    # Feed-forward affine transformations (all steps in parallel)
    w = self.w(x)

    # Apply batch normalization
    if self.normalize:
        w_bn = self.norm(w.reshape(w.shape[0] * w.shape[1], w.shape[2]))
        w = w_bn.reshape(w.shape[0], w.shape[1], w.shape[2])

    # Processing time steps
    if hx is not None:
        h = self._ligru_cell(w, hx)
    else:
        h = self._ligru_cell(w, self.h_init)

    if self.bidirectional:
        h_f, h_b = h.chunk(2, dim=0)
        h_b = h_b.flip(1)
        h = torch.cat([h_f, h_b], dim=2)

    return h

def _ligru_cell(self, w, ht):
    """Returns the hidden states for each time step.
    Arguments
    ---------
    wx : torch.Tensor
        Linearly transformed input.
    """
    hiddens = []

    # Sampling dropout mask
    drop_mask = self._sample_drop_mask(w)

    # Loop over time axis
    for k in range(w.shape[1]):
        # HACK: compile: broadcast_to is necessary for the compiler to understand
        # the initial shape of ht correctly
        gates = w[:, k] + self.u(ht).broadcast_to(w[:, k].shape)
        at, zt = gates.chunk(2, 1)
        zt = torch.sigmoid(zt)
        hcand = self.act(at) * drop_mask
        ht = zt * ht + (1 - zt) * hcand
        hiddens.append(ht)

    # Stacking hidden states
    h = torch.stack(hiddens, dim=1)
    return h

def _init_drop(self, batch_size):
    """Initializes the recurrent dropout operation. To speed it up,
    the dropout masks are sampled in advance.
    """
    self.drop = torch.nn.Dropout(p=self.dropout, inplace=False)
    self.N_drop_masks = 16000
    self.drop_mask_cnt = 0

    self.register_buffer(
        "drop_masks",
        self.drop(torch.ones(self.N_drop_masks, self.hidden_size)).data,
    )
    self.register_buffer("drop_mask_te", torch.tensor([1.0]).float())

def _sample_drop_mask(self, w):
    """Selects one of the pre-defined dropout masks"""
    if self.training:

        # Sample new masks when needed
        if self.drop_mask_cnt + self.batch_size &gt; self.N_drop_masks:
            self.drop_mask_cnt = 0
            self.drop_masks = self.drop(
                torch.ones(
                    self.N_drop_masks, self.hidden_size, device=w.device
                )
            ).data

        # Sampling the mask
        drop_mask = self.drop_masks[
            self.drop_mask_cnt : self.drop_mask_cnt + self.batch_size
        ]
        self.drop_mask_cnt = self.drop_mask_cnt + self.batch_size

    else:
        # FIXME: compile: breaks fullgraph capture
        # self.drop_mask_te = self.drop_mask_te.to(w.device)
        drop_mask = self.drop_mask_te.to(w.device)

    return drop_mask

def _change_batch_size(self, x):
    """This function changes the batch size when it is different from
    the one detected in the initialization method. This might happen in
    the case of multi-gpu or when we have different batch sizes in train
    and test. We also update the h_int and drop masks.
    """
    if self.batch_size != x.shape[0]:
        self.batch_size = x.shape[0]

        if self.training:
            self.drop_masks = self.drop(
                torch.ones(
                    self.N_drop_masks, self.hidden_size, device=x.device,
                )
            ).data
</code></pre>
<p>def rnn_init(module):<br />
    """This function is used to initialize the RNN weight.<br />
    Recurrent connection: orthogonal initialization.<br />
    Arguments<br />
    ---------<br />
    module: torch.nn.Module<br />
        Recurrent neural network module.<br />
    Example<br />
    -------<br />
    &gt;&gt;&gt; inp_tensor = torch.rand([4, 10, 20])<br />
    &gt;&gt;&gt; net = RNN(hidden_size=5, input_shape=inp_tensor.shape)<br />
    &gt;&gt;&gt; out_tensor = net(inp_tensor)<br />
    &gt;&gt;&gt; rnn_init(net)<br />
    """<br />
    for name, param in module.named_parameters():<br />
        if "weight_hh" in name or ".u.weight" in name:<br />
            nn.init.orthogonal_(param)</p>
<p>def time_it(func):<br />
    start = time.time()<br />
    ret = func()<br />
    end = time.time()<br />
    logging.info(f"... took {end - start:.2f}s")<br />
    return ret</p>
<p>def benchmark(func, count=100):<br />
    logging.info("running 10 dry runs for function")<br />
    for _ in range(10):<br />
        func()</p>
<pre><code>torch.cuda.synchronize()

logging.info("true runs:")
start = time.time()
for _ in range(count):
    torch.cuda.synchronize()
    func()
    torch.cuda.synchronize()
end = time.time()
spent = end - start
logging.info(f"{spent / count:.4f}s/iter for {count} total")
</code></pre>
<p>if <strong>name</strong> == "<strong>main</strong>":<br />
    batch, time_steps, feats = 128, 64, 512<br />
    hidden_size, num_layer, dropout = 1024, 4, 0.0<br />
    nonlinearity = "relu" # works also with sine, leakyrelu and tanh</p>
<pre><code>device = "cuda"

# torch._dynamo.config.verbose=False

# smaller_toy = torch.randn((batch, 8, feats), requires_grad=False).to(device).half()
# frozen_toy_example = dynamo.run(toy_example)

logging.info("loading model &amp; input onto device")
torch.manual_seed(0)
inp_tensor = torch.randn((batch, time_steps, feats), requires_grad=False).to(device).half()
net = LiGRU(
    input_shape=inp_tensor.shape,
    hidden_size=hidden_size,
    num_layers=num_layer,
    dropout=dropout,
    nonlinearity=nonlinearity,
).to(device).half()

net.eval()

logging.info("=== evaluating model (not compiled)")
benchmark(lambda: net(inp_tensor)[0].sum().backward())

logging.info("=== torch.jit.script (JIT)")
net = time_it(lambda: torch.jit.script(net))

logging.info("=== testing JIT model")
benchmark(lambda: net(inp_tensor)[0].sum().backward())

logging.info("=== torch.compile (torchinductor)")
net = LiGRU(
    input_shape=inp_tensor.shape,
    hidden_size=hidden_size,
    num_layers=num_layer,
    dropout=dropout,
    nonlinearity=nonlinearity,
).to(device).half()
net.eval()

net = time_it(lambda: torch.compile(net, mode="reduce-overhead", fullgraph=True))

logging.info("=== testing compiled model")
benchmark(lambda: net(inp_tensor)[0].sum().backward())
</code></pre>
<p>```</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.0.0<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 18.04.5 LTS (x86_64)<br />
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0<br />
Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)<br />
CMake version: version 3.10.2<br />
Libc version: glibc-2.27</p>
<p>Python version: 3.10.9 (main, Mar  8 2023, 10:47:38) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.4.0-73-generic-x86_64-with-glibc2.27<br />
Is CUDA available: False<br />
CUDA runtime version: 11.3.58<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Probably one of the following:<br />
/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.2.1<br />
/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.7.2.1<br />
/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn.so.8.2.0<br />
/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.2.0<br />
/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.2.0<br />
/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.2.0<br />
/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.2.0<br />
/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.2.0<br />
/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.2.0<br />
/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:        x86_64<br />
CPU op-mode(s):      32-bit, 64-bit<br />
Byte Order:          Little Endian<br />
CPU(s):              16<br />
On-line CPU(s) list: 0-15<br />
Thread(s) per core:  2<br />
Core(s) per socket:  4<br />
Socket(s):           2<br />
NUMA node(s):        2<br />
Vendor ID:           GenuineIntel<br />
CPU family:          6<br />
Model:               44<br />
Model name:          Intel(R) Xeon(R) CPU           E5640  @ 2.67GHz<br />
Stepping:            2<br />
CPU MHz:             1600.237<br />
BogoMIPS:            5334.11<br />
Virtualization:      VT-x<br />
L1d cache:           32K<br />
L1i cache:           32K<br />
L2 cache:            256K<br />
L3 cache:            12288K<br />
NUMA node0 CPU(s):   0,2,4,6,8,10,12,14<br />
NUMA node1 CPU(s):   1,3,5,7,9,11,13,15<br />
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida arat flush_l1d</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.23.5<br />
[pip3] torch==2.0.0<br />
[pip3] torchaudio==2.0.0<br />
[pip3] torchvision==0.15.0<br />
[pip3] triton==2.0.0<br />
[conda] blas                      1.0                         mkl<br />
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch<br />
[conda] mkl                       2021.4.0           h06a4308_640<br />
[conda] mkl-service               2.4.0           py310h7f8727e_0<br />
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0<br />
[conda] mkl_random                1.2.2           py310h00e6091_0<br />
[conda] numpy                     1.23.5          py310hd5efca6_0<br />
[conda] numpy-base                1.23.5          py310h8e6c178_0<br />
[conda] pytorch                   2.0.0           py3.10_cuda11.8_cudnn8.7.0_0    pytorch<br />
[conda] pytorch-cuda              11.8                 h7e8668a_3    pytorch<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch<br />
[conda] torchaudio                2.0.0               py310_cu118    pytorch<br />
[conda] torchtriton               2.0.0                     py310    pytorch<br />
[conda] torchvision               0.15.0              py310_cu118    pytorch<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @soumith @wconstab @ngimel</p>]]></description>
      <pubDate>Mon, 20 Mar 2023 07:28:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/97155</guid>
    </item>
  </channel>
</rss>
