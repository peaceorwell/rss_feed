<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[inductor] Remove `divisible_by_8` from kernel argument descriptor</title>
      <link>https://github.com/pytorch/pytorch/pull/121566</link>
      <description><![CDATA[<p>Triton has deprecated the <code>divisible_by_8</code> hint.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 18:23:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121566</guid>
    </item>
    <item>
      <title>Enable FX graph cache for even more inductor tests</title>
      <link>https://github.com/pytorch/pytorch/pull/121540</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121540<br />
* #121521<br />
* #121520</p>
<p>Summary: Get even more FX graph cache coverage by enabling caching for another batch of inductor unit tests (<code>torch._inductor.test_case.TestCase</code> enables the caching and isolates the caching tmp subdir)</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 12:55:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121540</guid>
    </item>
    <item>
      <title>Enable FX graph cache for a bunch of inductor tests</title>
      <link>https://github.com/pytorch/pytorch/pull/121521</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121540<br />
* <strong>-&gt;</strong> #121521<br />
* #121520</p>
<p>Summary: Get more FX graph cache coverage by enabling for these unit tests (<code>torch._inductor.test_case.TestCase</code> enables the caching and isolates the caching tmp subdir)</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 10:47:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121521</guid>
    </item>
    <item>
      <title>torch.compile fails with .view(dtype)</title>
      <link>https://github.com/pytorch/pytorch/issues/120998</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>torch.compile</code> fails when there's a dtype view call from float to int:</p>
<p>Here's a minimal code to reproduce it:<br />
``` Python<br />
import torch<br />
@torch.compile()<br />
def call(data):<br />
    return data.view(torch.uint8) + 1</p>
<p>data = torch.randint(0, 2**4, [4096, 4096], device='cuda', dtype=torch.uint8)<br />
out = call(data) #OK <br />
out = call(data.view(torch.float16)) #Error <br />
<code></code> Python<br />
BackendCompilerFailed: backend='inductor' raised:<br />
LoweringException: NotImplementedError: bitcast torch.float16 to different bitwidth type torch.uint8 is not supported yet.<br />
```</p>
<p>Wrapping the view call inside another function with <code>torch.jit.ignore</code> doesn't work either. </p>
<p>Is there by any chance a way to avoid this without converting types back and forth?<br />
Thanks a lot in advance!</p>
<h3>Versions</h3>
<p>```<br />
Versions of relevant libraries:<br />
PyTorch version: 2.1.2+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>Nvidia driver version: 535.129.03<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: Could not collect<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-89-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.2.140<br />
CUDA_MODULE_LOADING set to: LAZY</p>
<p>[pip3] numpy==1.26.4<br />
[pip3] onnxruntime==1.17.1<br />
[pip3] open-clip-torch==2.24.0<br />
[pip3] pytorch-triton==2.2.0+e28a256d71<br />
[pip3] torch==2.1.2<br />
[pip3] torchaudio==2.2.0.dev20231221+cu121<br />
[pip3] torchvision==0.16.2<br />
[pip3] triton==2.1.0<br />
[conda] Could not collect<br />
```</p>
<p>cc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Fri, 01 Mar 2024 06:56:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120998</guid>
    </item>
    <item>
      <title>Add warning when trying to use nccl version while not compiled with nccl</title>
      <link>https://github.com/pytorch/pytorch/pull/116989</link>
      <description><![CDATA[<p>Trying to get nccl version on windows return an exception:</p>
<p><code>AttributeError: module 'torch._C' has no attribute '_nccl_version'</code></p>
<p>Here is error:<br />
<code>Traceback (most recent call last):
  File "./test/smoke_test/smoke_test.py", line 317, in &lt;module&gt;
    main()
  File "./test/smoke_test/smoke_test.py", line 313, in main
    smoke_test_cuda(options.package, options.runtime_error_check)
  File "./test/smoke_test/smoke_test.py", line 166, in smoke_test_cuda
    print(f"torch nccl version: {torch.cuda.nccl.version()}" )
  File "C:\Jenkins\Miniconda3\envs\conda-env-7450624690\lib\site-packages\torch\cuda\nccl.py", line 35, in version
    ver = torch._C._nccl_version()
AttributeError: module 'torch._C' has no attribute '_nccl_version'</code></p>
<p>Test:<br />
```</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch<br />
print(f"torch nccl version: {torch.cuda.nccl.version()}" )<br />
C:\Jenkins\Miniconda3\envs\conda-env-7453139523\lib\site-packages\torch\cuda\nccl.py:36: UserWarning: PyTorch is not compiled with NCCL support<br />
  warnings.warn("PyTorch is not compiled with NCCL support")<br />
torch nccl version: (-1, -1, -1)<br />
```</p>
</blockquote>
</blockquote>
</blockquote>]]></description>
      <pubDate>Mon, 08 Jan 2024 12:49:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116989</guid>
    </item>
    <item>
      <title>Dynamic shapes support for inductor foreach codegen</title>
      <link>https://github.com/pytorch/pytorch/issues/107005</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>Currently foreach op compilation doesn't support dynamic shapes. </p>
<p>See the restriction here: https://github.com/pytorch/pytorch/blob/4df84c3b4d93ea53c9ca169f32a0dc4619bee797/torch/_inductor/lowering.py#L419</p>
<p>Since this feature was originally intended for optimizers, dynamic shapes aren't strictly necessary, but in order for this codegen to be used elsewhere in the stack (think input concatenation, more generalized horizontal fusion) dynamic shapes support is necessary. </p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p>A good place to get started is here: https://github.com/pytorch/pytorch/blob/4df84c3b4d93ea53c9ca169f32a0dc4619bee797/torch/_inductor/codegen/triton_foreach.py#L142</p>
<p>This file is what handles codegen for the foreach operators. At a high level additional runtime args and a more sophisticated kernel launch grid calculation will be needed to take into account runtime sizes vs static sizes. Contact @mlazos for more details</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov</p>]]></description>
      <pubDate>Thu, 10 Aug 2023 18:05:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/107005</guid>
    </item>
    <item>
      <title>[inductor] Updated upsample_bicubic2d decomposition</title>
      <link>https://github.com/pytorch/pytorch/pull/104248</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #104248<br />
* #104182</p>
<ul>
<li>fixed dispatch for upsample_bicubic2d_vec and use inductor lowering</li>
<li>added support for uint8</li>
<li>updated tests</li>
</ul>
<p>Performance benchmarks results</p>
<p>```<br />
[-------------------------- Interpolate bicubic, AA=false, cpu -------------------------]<br />
                                                                   |  Eager   |  Compiled<br />
1 threads: ------------------------------------------------------------------------------<br />
- main/nightly:<br />
      Input (3, 345, 456), torch.uint8, torch.contiguous_format    |   647.2  |   2828.6<br />
      Input (3, 345, 456), torch.float32, torch.contiguous_format  |  2163.4  |   2076.7<br />
      Input (3, 345, 456), torch.uint8, torch.channels_last        |   263.5  |   3284.6<br />
      Input (3, 345, 456), torch.float32, torch.channels_last      |  3585.1  |   2400.3<br />
- PR:<br />
      Input (3, 345, 456), torch.uint8, torch.contiguous_format    |   634.5  |   3579.6   &lt;--- worse than in main<br />
      Input (3, 345, 456), torch.float32, torch.contiguous_format  |  2143.6  |   2237.8   &lt;--- worse than in main<br />
      Input (3, 345, 456), torch.uint8, torch.channels_last        |   261.2  |   2883.2 <br />
      Input (3, 345, 456), torch.float32, torch.channels_last      |  3544.3  |   1814.4</p>
<p>Times are in microseconds (us).</p>
<p>[------------------------ Interpolate bicubic, AA=false, cuda -------------------------]<br />
                                                                   |  Eager  |  Compiled<br />
1 threads: -----------------------------------------------------------------------------<br />
- main/nightly:<br />
      Input (3, 345, 456), torch.float32, torch.contiguous_format  |   17.2  |   352.0<br />
      Input (3, 345, 456), torch.float32, torch.channels_last      |   17.5  |   357.4<br />
- PR:<br />
      Input (3, 345, 456), torch.float32, torch.contiguous_format  |   14.3  |    43.2<br />
      Input (3, 345, 456), torch.float32, torch.channels_last      |   14.4  |    40.1</p>
<p>Times are in microseconds (us).<br />
```</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang @Xia-Weiwen @ngimel</p>]]></description>
      <pubDate>Tue, 27 Jun 2023 03:26:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/104248</guid>
    </item>
  </channel>
</rss>
