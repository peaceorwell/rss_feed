<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[inductor][cpp] GEMM template</title>
      <link>https://github.com/pytorch/pytorch/pull/124021</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124021</p>
<p>cc @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 14 Apr 2024 06:21:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124021</guid>
    </item>
    <item>
      <title>Inconsistency error of `torch.Tensor.atan2_` with torch.compile</title>
      <link>https://github.com/pytorch/pytorch/issues/124020</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When running the below model with torch.compile,<br />
the result is different with eager mode. </p>
<p>The minimized repro would be :<br />
```python <br />
import torch<br />
import torch.nn as nn<br />
from copy import deepcopy<br />
print("torch version: ",torch.<strong>version</strong>)<br />
p0 =  torch.randn((), requires_grad=False)<br />
class Model(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        # nn.parameter.Parameter objects (with comments for shapes)<br />
        self.p0 = p0</p>
<pre><code>def forward(self, v0_0):
    # v0_0: [], torch.float32
    v6_0 = torch.Tensor.sigmoid_(self.p0)
    v5_0 = torch.Tensor.atan2_(v0_0, other=self.p0)
    return v6_0, v5_0
</code></pre>
<p>inputs = {"v0_0": torch.randn(()).to(torch.device("cpu"))}<br />
model = Model().to(torch.device("cpu"))<br />
copied = deepcopy(inputs)<br />
for k, v in inputs.items():<br />
    inputs[k] = v.to(torch.device("cpu"))<br />
print('==== Eager mode ====')<br />
ret_eager = model(**inputs)</p>
<p>print('==== TorchComp mode ====')<br />
ret_exported = torch.compile(model)(**copied)<br />
print(ret_eager, ret_exported)<br />
print('==== Check ====')<br />
for r1, r2 in zip(ret_eager, ret_exported):<br />
    if not torch.allclose(r1, r2, rtol=1e-2, atol=1e-3, equal_nan=True):<br />
        print("r1: ",r1,"r2: ",r2)<br />
        raise ValueError("Tensors are different.")<br />
print('OK!')</p>
<p>```</p>
<h3>Error logs</h3>
<p>torch version:  2.4.0.dev20240413+cu118<br />
==== Eager mode ====<br />
==== TorchComp mode ====<br />
(tensor(0.6052), tensor(-0.8890)) (tensor(0.6052), tensor(-0.7159))<br />
==== Check ====<br />
r1:  tensor(-0.8890) r2:  tensor(-0.7159)<br />
Traceback (most recent call last):<br />
  File "prog.py", line 36, in <module><br />
    raise ValueError("Tensors are different.")<br />
ValueError: Tensors are different.</p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>PyTorch version: 2.4.0.dev20240413+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.28.3<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.8.17 (default, Jul  5 2023, 21:04:15)  [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-27-generic-x86_64-with-glibc2.17<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      48 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             256<br />
On-line CPU(s) list:                0-255<br />
Vendor ID:                          AuthenticAMD<br />
Model name:                         AMD EPYC 7763 64-Core Processor<br />
CPU family:                         25<br />
Model:                              1<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 64<br />
Socket(s):                          2<br />
Stepping:                           1<br />
Frequency boost:                    enabled<br />
CPU max MHz:                        3529.0520<br />
CPU min MHz:                        1500.0000<br />
BogoMIPS:                           4890.76<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm<br />
Virtualization:                     AMD-V<br />
L1d cache:                          4 MiB (128 instances)<br />
L1i cache:                          4 MiB (128 instances)<br />
L2 cache:                           64 MiB (128 instances)<br />
L3 cache:                           512 MiB (16 instances)<br />
NUMA node(s):                       2<br />
NUMA node0 CPU(s):                  0-63,128-191<br />
NUMA node1 CPU(s):                  64-127,192-255<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Mitigation; Safe RET<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.23.5<br />
[pip3] pytorch-triton==3.0.0+989adb9a29<br />
[pip3] torch==2.4.0.dev20240413+cu118<br />
[pip3] torchaudio==2.2.0.dev20240413+cu118<br />
[pip3] torchvision==0.19.0.dev20240413+cu118<br />
[conda] numpy                     1.23.5                   pypi_0    pypi<br />
[conda] pytorch-triton            3.0.0+989adb9a29          pypi_0    pypi<br />
[conda] torch                     2.4.0.dev20240413+cu118          pypi_0    pypi<br />
[conda] torchaudio                2.2.0.dev20240413+cu118          pypi_0    pypi<br />
[conda] torchvision               0.19.0.dev20240413+cu118          pypi_0    pypi</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Sun, 14 Apr 2024 05:37:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124020</guid>
    </item>
    <item>
      <title>[Inductor] Fix endless recursion in codecache.DLLWrapper.__getattr__</title>
      <link>https://github.com/pytorch/pytorch/pull/123931</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #123932<br />
* #121734<br />
* #121497<br />
* <strong>-&gt;</strong> #123931<br />
* #123930</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 12 Apr 2024 03:33:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123931</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Fix tests: skipIfROCm always skips when using as class annotation</title>
      <link>https://github.com/pytorch/pytorch/pull/123930</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #123932<br />
* #121734<br />
* #121497<br />
* #123931<br />
* <strong>-&gt;</strong> #123930</p>
<p>I previously added @skipIfRocm as a class annotation within test/inductor/test_cutlass_backend.py - turns out this annotation always skips if applied at class level, so I need to skip Cutlass tests on ROCm differently..</p>]]></description>
      <pubDate>Fri, 12 Apr 2024 03:30:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123930</guid>
    </item>
    <item>
      <title>Fixing logging errors uing TORCH_COMPILE_DEBUG</title>
      <link>https://github.com/pytorch/pytorch/pull/119124</link>
      <description><![CDATA[<p>Instead of (trying) to log None to an decimal integer value with the formattin, I checked with an inline if-statement if it's None, if so a default value -1 gets logged. <br />
Maybe an improvement could be to change the strings of the logs to the newer f-strings?x</p>
<p>It's my first ever contribution to any open-source projects. Feel free to give me critical feedback. </p>
<p>Fixes #118476 </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sat, 03 Feb 2024 02:32:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119124</guid>
    </item>
  </channel>
</rss>
