<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>Add inductor level stack trace stitching</title>
      <link>https://github.com/pytorch/pytorch/issues/123201</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>When the inductor generated python file is executed via inductor's main process and it crashes, the stack trace emitted only comes from the parent inductor process and it does not include the error from the generated python file. We should add support for stitching the child process stack trace into parent process.</p>
<p>This will help debugging some internal errors.</p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Tue, 02 Apr 2024 10:52:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123201</guid>
    </item>
    <item>
      <title>[aot_inductor] Fix issues in mts_gpu_benchmark and pre_grad passes to support AF_OC gated module</title>
      <link>https://github.com/pytorch/pytorch/pull/123181</link>
      <description><![CDATA[<p>Summary:<br />
This diff helps to support request from https://fb.workplace.com/groups/gpuinference/permalink/2767700120045305/<br />
Fixed a few things:<br />
- found an issue in mts_gpu_benchmark, we cast the model to float at the beginning of the program. The fix was to fix issues for bf16 lowering since we encountered some models which are casted to fp16 and we need to cast to fp32 first.<br />
The solution is to disable the casting and ask the user to set dense_type as fp32 at config level<br />
- fixed a bug in <code>sink_cat_after_pointwise</code> pass for PT IR. The root cause is asumption of existence of input in kwargs or args</p>
<p>BE work: remove aten2ait relevant code since it is deprecated</p>
<p>Test Plan: CUDA_VISIBLE_DEVICES=7 TORCH_COMPILE_DEBUG=1 TORCHINDUCTOR_MAX_AUTOTUNE=1 buck2 run mode/opt  mode/inplace  -c fbcode.platform010_cuda_version=12    -c fbcode.nvcc_arch=h100  caffe2/torch/fb/model_transform/experimental/benchmark:mts_gpu_benchmark -- --local-model /data/local/models/546241546/0/gpu_lowering/input.predictor.disagg.gpu.merge --lower-backend="AOT_INDUCTOR"</p>
<p>Differential Revision: D55617545</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 02 Apr 2024 08:27:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123181</guid>
    </item>
    <item>
      <title>torch.compile+cudagraphs asserts in multithreaded context</title>
      <link>https://github.com/pytorch/pytorch/issues/123177</link>
      <description><![CDATA[<p>```<br />
import threading<br />
import torch</p>
<p>def foo(x, y):<br />
    a = torch.sin(x)<br />
    b = torch.cos(y)<br />
    return a + b</p>
<p>opt_foo1 = torch.compile(foo, mode="max-autotune")</p>
<p>threads = []<br />
for _ in range(32):<br />
    threads.append(<br />
        threading.Thread(<br />
            target=opt_foo1,<br />
            args=(torch.rand(2, 3).to("cuda"), torch.rand(2, 3).to("cuda")),<br />
        )<br />
    )</p>
<p>for t in threads:<br />
    t.start()</p>
<p>for t in threads:<br />
    t.join()<br />
```</p>
<p>Stack trace looks like:<br />
<code>Exception in thread Thread-2 (foo):
Traceback (most recent call last):
  File "/usr/local/fbcode/platform010/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/local/fbcode/platform010/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_dynamo/eval_frame.py", line 410, in _fn
    return fn(*args, **kwargs)
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/scripts/suo/foo.py", line 6, in foo
    def foo(x, y):
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_dynamo/eval_frame.py", line 410, in _fn
    return fn(*args, **kwargs)
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_functorch/aot_autograd.py", line 917, in forward
    return compiled_fn(full_args)
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_functorch/_aot_autograd/utils.py", line 89, in g
    return f(*args)
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 101, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_functorch/_aot_autograd/utils.py", line 113, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 181, in rng_functionalization_wrapper
    return compiled_fw(args)
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_inductor/codecache.py", line 939, in __call__
    return self.current_callable(inputs)
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_inductor/compile_fx.py", line 902, in run
    return compiled_fn(new_inputs)
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_inductor/cudagraph_trees.py", line 382, in deferred_cudagraphify
    fn, out = cudagraphify(model, inputs, new_static_input_idxs, *args, **kwargs)
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_inductor/cudagraph_trees.py", line 402, in cudagraphify
    manager = get_container(device_index).get_tree_manager()
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_inductor/cudagraph_trees.py", line 338, in get_container
    container_dict = get_obj(local, "tree_manager_containers")
  File "/data/users/suo/fbsource/buck-out/v2/gen/fbcode/da91e1664cbd9051/scripts/suo/__foo__/foo-inplace#link-tree/torch/_inductor/cudagraph_trees.py", line 333, in get_obj
    assert torch._C._is_key_in_tls(attr_name)
AssertionError</code></p>
<p>Context: it would be useful in inference contexts to be able to run multiple threads per GPU, to improve GPU utilization in the presence of small-batch inference requests. </p>
<p>As far as I know we don't have any thread-safety guarantees in PT2, so this issue is not really about this particular stack trace but rather to track the general question of using torch.compile in a multithreaded context.</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 02 Apr 2024 08:12:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123177</guid>
    </item>
    <item>
      <title>[Inductor] Add the possible fusions group by priority</title>
      <link>https://github.com/pytorch/pytorch/pull/123067</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123067<br />
* #121625</p>
<p><strong>Summary</strong></p>
<p>Refactor the <code>Scheduler.fuse_nodes</code> changes in https://github.com/pytorch/pytorch/pull/121625. In the previous implementation of <code>Scheduler.fuse_nodes</code> in https://github.com/pytorch/pytorch/pull/121625, we use the <code>enable_outer_loop_fusion</code> context to ensure <code>OuterLoopFusion</code> happens after all the norm fusions.</p>
<p>And there is a discussion in https://github.com/pytorch/pytorch/pull/121625/files#r1527177141 to reuse current <code>score_fusion</code> mechanism. However, given that <a href="https://github.com/pytorch/pytorch/blob/f4ff063c333f286d4384523bac67c047aca4d7b9/torch/_inductor/scheduler.py#L1679-L1698">fuse_nodes</a> will invoke <code>fuse_nodes_once</code> 10 times. We are concerned that the score approach may potentially disrupt pairs of regular fusion nodes in the 2rd invocation of <code>fuse_nodes_once</code> if they have been pick up by the outer loop fusion in the 1st invocation of <code>fuse_nodes_once</code>.</p>
<p>In this PR, we propose adding an abstract of <code>filter_possible_fusions_by_priority</code>. In each invoking of <code>fuse_nodes_once</code>, the possible fusions will be grouped by their priority from the backend. And only the group of possible fusions with highest priority will be fused in this invocation. In this way, we can ensure <code>OuterLoopFusion</code> happens after all the norm fusions.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 31 Mar 2024 18:24:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123067</guid>
    </item>
    <item>
      <title>ImportError: cannot import name 'triton_key' from 'triton.compiler.compiler'</title>
      <link>https://github.com/pytorch/pytorch/issues/123042</link>
      <description><![CDATA[<p>Code:<br />
```python<br />
torch._dynamo.config.suppress_errors = True</p>
<p>self.pipe = OotdPipeline.from_pretrained(<br />
    MODEL_PATH,<br />
    unet_garm=unet_garm,<br />
    unet_vton=unet_vton,<br />
    vae=vae,<br />
    torch_dtype=torch.float16,<br />
    variant="fp16",<br />
    use_safetensors=True,<br />
    safety_checker=None,<br />
    requires_safety_checker=False,<br />
    local_files_only=True,<br />
).to(self.gpu_id)</p>
<p>self.pipe.unet_garm = torch.compile(self.pipe.unet_garm, mode="reduce-overhead", fullgraph=True)<br />
self.pipe.unet_vton = torch.compile(self.pipe.unet_vton, mode="reduce-overhead", fullgraph=True)<br />
<code>Error Message:</code>bash<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_triton.py", line 57, in triton_hash_with_backend<br />
    from triton.compiler.compiler import triton_key<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
ImportError: cannot import name 'triton_key' from 'triton.compiler.compiler' (/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py)<br />
```</p>
<h3>Versions</h3>
<p>PyTorch version: 2.4.0.dev20240330+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.29.0<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.10.0-28-cloud-amd64-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.1.105<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA L4<br />
Nvidia driver version: 535.104.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             96<br />
On-line CPU(s) list:                0-95<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Xeon(R) CPU @ 2.20GHz<br />
CPU family:                         6<br />
Model:                              85<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 24<br />
Socket(s):                          2<br />
Stepping:                           7<br />
BogoMIPS:                           4400.42<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities<br />
Hypervisor vendor:                  KVM<br />
Virtualization type:                full<br />
L1d cache:                          1.5 MiB (48 instances)<br />
L1i cache:                          1.5 MiB (48 instances)<br />
L2 cache:                           48 MiB (48 instances)<br />
L3 cache:                           77 MiB (2 instances)<br />
NUMA node(s):                       2<br />
NUMA node0 CPU(s):                  0-23,48-71<br />
NUMA node1 CPU(s):                  24-47,72-95<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT Host state unknown<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown<br />
Vulnerability Retbleed:             Mitigation; Enhanced IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT Host state unknown</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.4<br />
[pip3] onnxruntime==1.16.2<br />
[pip3] pytorch-triton==0.0.1<br />
[pip3] torch==2.4.0.dev20240330+cu121<br />
[pip3] torchaudio==2.2.0.dev20240330+cu121<br />
[pip3] torchvision==0.19.0.dev20240330+cu121<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @seemethere @malfet @osalpekar @atalman @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Sat, 30 Mar 2024 15:54:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123042</guid>
    </item>
    <item>
      <title>[fix] inductor `split` lowering fails if `item()` is captured</title>
      <link>https://github.com/pytorch/pytorch/pull/123032</link>
      <description><![CDATA[<p>Fixes #122937</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sat, 30 Mar 2024 10:57:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123032</guid>
    </item>
    <item>
      <title>[Inductor pattern] support int8 woq mm pattern matcher with freezing passe</title>
      <link>https://github.com/pytorch/pytorch/pull/122955</link>
      <description><![CDATA[<p>There exist some issues in the previous PR (https://github.com/pytorch/pytorch/pull/120985) of supporting int8 WOQ mm pattern matcher. This PR tends to further optimize it.</p>
<ol>
<li>
<p>Although the UT code is the same as that in gpt-fast model: <code>F.linear(x, weight.to(dtype=x.dtype)) * scales</code>, the traced graphs are not the same because gpt-fast enables the config <code>coordinate_descent_tuning</code>. The PR changes the pattern to the traced graph with <code>coordinate_descent_tuning=True</code>.</p>
</li>
<li>
<p>Two patterns are added to match int8 woq mm in gpt-fast model, due to different input shapes.</p>
</li>
<li>
<p>If one activation is used in several woq mms, its dequantization part would be removed. Hence, we do dequantization promotion for woq mm to match all the linear ops: 161 in gpt-fast.</p>
</li>
</ol>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 29 Mar 2024 00:27:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122955</guid>
    </item>
    <item>
      <title>[aotinductor] Fix benchmarks with self.autocast for run_performance_test</title>
      <link>https://github.com/pytorch/pytorch/pull/122954</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122954<br />
* #122883</p>
<p>https://github.com/pytorch/pytorch/pull/110490 fixes the self.autocast in the <code>check_accuracy</code> function. Fix it in the <code>run_performance_test</code> function as well.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 23:29:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122954</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Reuse inductor test for Intel GPU (PART 1)</title>
      <link>https://github.com/pytorch/pytorch/pull/122866</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122866<br />
* #121895<br />
* #122254<br />
* #121883</p>
<p>backend.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 20:28:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122866</guid>
    </item>
    <item>
      <title>[dynamo] Emit FUNCTORCH_STACK_MATCH guard in vmap(compile(f)) case</title>
      <link>https://github.com/pytorch/pytorch/pull/122786</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122786</p>
<p>Fixes: #122201</p>
<p>cc @zou3519 @Chillee @samdow @kshitij12345 @janeyx99 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 06:02:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122786</guid>
    </item>
    <item>
      <title>`torch.compile` is not usable on MacOS out of box</title>
      <link>https://github.com/pytorch/pytorch/issues/122705</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Attempts to run any <code>torch.compile</code>d code fails out-of-box with 2.3.0 wheels, as it attempts to link against wrong copy of <code>libomp.dylib</code>:<br />
<code>OMP: Error #15: Initializing libomp.dylib, but found libomp.dylib already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/</code></p>
<p>This happens, because <code>delocate</code> embeds libomp.dylib into <code>torch/.libs</code> folder, but another instance is shipped in <code>torch/libs</code></p>
<h3>Versions</h3>
<p>2.3.0</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @seemethere @osalpekar @atalman @albanD @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 07:38:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122705</guid>
    </item>
    <item>
      <title>[fix] 'from' in python code generated by inductor, resulting in SyntaxError</title>
      <link>https://github.com/pytorch/pytorch/pull/122632</link>
      <description><![CDATA[<p><code>from</code> is  a python keyword.  Python code containing <code>aten.random.from</code> will result in syntax errors. This PR handle this special case in the codegen of inductor. </p>
<p>Also add fallback lowering for aten.random</p>
<p>Fixes #121621 </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 10:47:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122632</guid>
    </item>
    <item>
      <title>[Inductor] add contiguous layout optm for bmm input</title>
      <link>https://github.com/pytorch/pytorch/pull/122599</link>
      <description><![CDATA[<p>Fixes #117743.</p>
<p>Add contiguous layout optimization for <code>bmm</code> input, to avoid additional copies.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 25 Mar 2024 01:24:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122599</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Enable lowering of qlinear-binary(-unary) fusion for X86Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/122593</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122667<br />
* <strong>-&gt;</strong> #122593<br />
* #122387<br />
* #123144</p>
<p><strong>Description</strong><br />
Lower the qlinear binary post op pattern to Inductor. Use post op sum (in-place) if the extra input has the same dtype as output. Otherwise, it uses binary add.</p>
<p><strong>Supported linear-binary(-unary) patterns</strong><br />
```<br />
    linear(X)   extra input<br />
           \   /<br />
            Add<br />
             |<br />
        Optional(relu)<br />
             |<br />
             Y</p>
<ol>
<li>
<p>int8-mixed-fp32<br />
+---+---------------+-----------+------------------------------+---------+<br />
| # | Add type      | Quant out | Pattern                      | Post op |<br />
+---+---------------+-----------+------------------------------+---------+<br />
| 1 | In-/out-place | Yes       | linear + fp32 -&gt; (relu) -&gt; q | add     |<br />
+---+---------------+-----------+------------------------------+---------+<br />
| 2 | In-/out-place | No        | linear + fp32 -&gt; (relu)      | sum     |<br />
+---+---------------+-----------+------------------------------+---------+</p>
</li>
<li>
<p>int8-mixed-bf16<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| # | X2 dtype | Add type      | Quant out | Pattern                                          | Post op |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 1 | BF16     | In-/out-place | Yes       | linear + bf16 -&gt; (relu) -&gt; to_fp32 -&gt; q          | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 2 | BF16     | In-/out-place | No        | linear + bf16 -&gt; (relu)                          | sum     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 3 | FP32     | Out-place     | Yes       | linear + fp32 -&gt; (relu) -&gt; q                     | add     |<br />
|   |          | In-place right|           |                                                  |         |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 4 | FP32     | Out-place     | No        | linear + fp32 -&gt; (relu)                          | sum     |<br />
|   |          | In-place right|           |                                                  |         |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 5 | FP32     | In-place left | Yes       | linear + fp32 -&gt; to_bf16 -&gt; relu -&gt; to_fp32 -&gt; q | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
| 6 | FP32     | In-place left | No        | linear + fp32 -&gt; to_bf16 -&gt; (relu)               | add     |<br />
+---+----------+---------------+-----------+--------------------------------------------------+---------+<br />
```<br />
Note<br />
(1) The positions of linear and the extra input can be swapped.<br />
(2) we don't insert q-dq before the extra input of linear-add by recipe. But if q-dq is found at the<br />
extra input, we don't match that pattern because we cannot match all these patterns in 3 passes.</p>
</li>
</ol>
<p><strong>Test plan</strong><br />
python test/inductor/test_mkldnn_pattern_matcher.py -k test_qlinear_add<br />
python test/inductor/test_cpu_cpp_wrapper.py -k test_qlinear_add</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Mar 2024 23:49:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122593</guid>
    </item>
    <item>
      <title>[compiled autograd][dynamo] Make compiled graph take in boxed inputs</title>
      <link>https://github.com/pytorch/pytorch/pull/122353</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122535<br />
* <strong>-&gt;</strong> #122353<br />
* #123007<br />
* #122746<br />
* #122691</p>
<h3>Context</h3>
<p>In today's Dynamo, we lift all tensors encountered during tracing to be individual graph inputs, even when they were in a container. </p>
<p>And <a href="https://github.com/pytorch/pytorch/blob/fdc281f2587f9a5a935de1f1368e7ad7ed0f9828/torch/_dynamo/codegen.py#L371">Dynamo generates</a> the runtime function's signature using the graph's graphargs. </p>
<p>This means that the generated function will have each grapharg as an argument, which is problematic if we want to free the inputs in inductor codegen. See <a href="https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670">python function arguments are kept alive for the duration of the function call</a>. </p>
<p>```python</p>
<h1>original code</h1>
<p>def forward(inputs):<br />
  a, b, c, d, e = inputs<br />
  out = a<br />
  out += b<br />
  del b<br />
  out += c<br />
  del c<br />
  out += d<br />
  del d<br />
  out += e<br />
  del e<br />
  return out</p>
<h1>compiled code:</h1>
<p>def forward(a, b, c, d, e):<br />
  # b, c, d, e can't be freed before end of function<br />
```</p>
<p>This isn't a concern when compiling forward because a, b, c, d, e are all from user code, and should be kept alive. But when compiling backwards, a, b, c, d, e may be intermediate results i.e. activations, that we DO want to clear ASAP to remain on par with eager peak memory.</p>
<h3>Solution</h3>
<p>We have encountered similar memory problems in AOTAutograd before, where we adopted the boxed calling convention (wrapping to-be-freed objects in a list), adding list clearing to inductor codegen, and being careful about holding references to elements in the input list. We need to do something similar, but for inputs from the user program (compiled autograd fx graph in this case).</p>
<p>This PR support lists as graphargs/placeholder nodes. When tracing a list of tensors, we create a node for it, and pre-emptively initialize variable trackers for its elements before they are used in the user program. Subsequent uses of those variables will find hits in the lookup table <code>input_source_to_var</code>.</p>
<p>With the inputs as a list in the graph args, our compiled code can free inputs early.<br />
<code>python
def forward(inputs):
  # a, b, c, d, e can be freed within the function now</code></p>
<p>AOT/Inductor already support this list input via <a href="https://github.com/pytorch/pytorch/blob/597f479643f82859307ece38971f1c8e7d657c80/torch/_inductor/compile_fx.py#L1454-L1478">flatten_graph_inputs wrapper</a>. Which was fixed in the previous PR of this stack.</p>
<p>The next step after is to ensure that we are careful in forwarding the list to inductor codegen without holding additional references.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 15:17:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122353</guid>
    </item>
    <item>
      <title>[Inductor Intel GPU backend Upstream] Enable triton installation for Intel GPU</title>
      <link>https://github.com/pytorch/pytorch/pull/122254</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122866<br />
* #121895<br />
* <strong>-&gt;</strong> #122254<br />
* #121883</p>
<p>Intel GPU.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 17:14:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122254</guid>
    </item>
    <item>
      <title>Update torch.compile_faq w.r.t to functorch</title>
      <link>https://github.com/pytorch/pytorch/pull/122213</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122213<br />
* #122212<br />
* #122211</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 09:47:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122213</guid>
    </item>
    <item>
      <title>Investigate torch.compile Windows support.</title>
      <link>https://github.com/pytorch/pytorch/issues/122094</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>torch.compile is not supported on Windows. <br />
torch.compile has dependency triton: https://github.com/openai/triton/issues/1640</p>
<h3>Expected outcome</h3>
<ol>
<li>Document the outcomes.</li>
</ol>]]></description>
      <pubDate>Mon, 18 Mar 2024 06:27:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122094</guid>
    </item>
    <item>
      <title>[PT2.2] PyTorch dispacthing non core ATen ops in .compile mode for torch ops.</title>
      <link>https://github.com/pytorch/pytorch/issues/121967</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>PyTorch dispacthing non core ATen ops in .compile mode for torch ops. <br />
Same non core ATen ops are observed as part of our backend.</p>
<p><strong>List of non core ATen ops observed are:</strong><br />
<code>addmv.default
addbmm.default
cumprod.default
median.default
min.default</code></p>
<h3>Error logs</h3>
<p>aten ops getting involved for addmv ['addmv.default']<br />
aten ops getting involved for addbmm ['addbmm.default']<br />
aten ops getting involved for cumprod ['cumprod.default']<br />
aten ops getting involved for median ['median.default']<br />
aten ops getting involved for min ['min.default']</p>
<h3>Minified repro</h3>
<p>```<br />
import torch<br />
from torch._decomp import core_aten_decompositions<br />
from torch._dynamo.backends.common import aot_autograd</p>
<p>fx_info = {"fx_aten_ops": []}</p>
<p>def inner_compiler(fx_module: torch.fx.GraphModule, inputs):<br />
    for node in fx_module.graph.nodes:<br />
        if node.op == "call_function":<br />
            fx_info["fx_aten_ops"].append(node.target.<strong>name</strong>)<br />
    return fx_module</p>
<h1>UT for addmv ATen op</h1>
<p>def addmv_model(M, mat, vec):<br />
  return torch.addmv(M, mat, vec)</p>
<p>M = torch.randn(2)<br />
mat = torch.randn(2, 3)<br />
vec = torch.randn(3)</p>
<p>fx_trace_backend = aot_autograd(<br />
    fw_compiler=inner_compiler, decompositions=core_aten_decompositions()<br />
)<br />
model_compiled = torch.compile(addmv_model, backend=fx_trace_backend)<br />
result_fwd = model_compiled(M, mat, vec)</p>
<p>print(f"aten ops getting involved for addmv {fx_info['fx_aten_ops']}")</p>
<h1>UT for addbmm ATen op</h1>
<p>def addbmm_model(M, batch1, batch2):<br />
  return torch.addbmm(M, batch1, batch2)</p>
<p>M = torch.randn(3, 5)<br />
batch1 = torch.randn(10, 3, 4)<br />
batch2 = torch.randn(10, 4, 5)</p>
<p>fx_info = {"fx_aten_ops": []}<br />
fx_trace_backend = aot_autograd(<br />
    fw_compiler=inner_compiler, decompositions=core_aten_decompositions()<br />
)<br />
model_compiled = torch.compile(addbmm_model, backend=fx_trace_backend)<br />
result_fwd = model_compiled(M, batch1, batch2)<br />
print(f"aten ops getting involved for addbmm {fx_info['fx_aten_ops']}")</p>
<h1>UT for cumprod ATen op</h1>
<p>def cumprod_model(ifm):<br />
  return torch.cumprod(ifm, dim=0)</p>
<p>ifm = torch.randn(10)</p>
<p>fx_info = {"fx_aten_ops": []}<br />
fx_trace_backend = aot_autograd(<br />
    fw_compiler=inner_compiler, decompositions=core_aten_decompositions()<br />
)<br />
model_compiled = torch.compile(cumprod_model, backend=fx_trace_backend)<br />
result_fwd = model_compiled(ifm)<br />
print(f"aten ops getting involved for cumprod {fx_info['fx_aten_ops']}")</p>
<h1>UT for median ATen op</h1>
<p>def median_model(ifm):<br />
  return torch.median(ifm)</p>
<p>ifm = torch.randn(10)</p>
<p>fx_info = {"fx_aten_ops": []}<br />
fx_trace_backend = aot_autograd(<br />
    fw_compiler=inner_compiler, decompositions=core_aten_decompositions()<br />
)<br />
model_compiled = torch.compile(median_model, backend=fx_trace_backend)<br />
result_fwd = model_compiled(ifm)<br />
print(f"aten ops getting involved for median {fx_info['fx_aten_ops']}")</p>
<h1>UT for min ATen op</h1>
<p>def min_model(ifm):<br />
  return torch.min(ifm)</p>
<p>ifm = torch.randn(10)</p>
<p>fx_info = {"fx_aten_ops": []}<br />
fx_trace_backend = aot_autograd(<br />
    fw_compiler=inner_compiler, decompositions=core_aten_decompositions()<br />
)<br />
model_compiled = torch.compile(min_model, backend=fx_trace_backend)<br />
result_fwd = model_compiled(ifm)<br />
print(f"aten ops getting involved for min {fx_info['fx_aten_ops']}")</p>
<p>```</p>
<h3>Versions</h3>
<p>[pip3] numpy==1.26.4<br />
[pip3] torch==2.2.0a0<br />
[pip3] torchaudio==2.2.0<br />
[pip3] torchdata==0.7.1<br />
[pip3] torchtext==0.17.0<br />
[pip3] torchvision==0.17.0</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Fri, 15 Mar 2024 03:19:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121967</guid>
    </item>
    <item>
      <title>[Inductor Intel GPU backend Upstream] Add Inductor Intel GPU backend.</title>
      <link>https://github.com/pytorch/pytorch/pull/121895</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122866<br />
* <strong>-&gt;</strong> #121895<br />
* #122254<br />
* #121883</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 00:00:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121895</guid>
    </item>
    <item>
      <title>[Inductor Intel GPU backend Upstream] Register general runtime device for Intel GPU</title>
      <link>https://github.com/pytorch/pytorch/pull/121883</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122866<br />
* #121895<br />
* #122254<br />
* <strong>-&gt;</strong> #121883</p>
<p>interface for Intel GPU.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 18:20:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121883</guid>
    </item>
    <item>
      <title>Warning static library kineto_LIBRARY-NOTFOUND not found when trying build tutorial torch.compiler_aot_inductor.html</title>
      <link>https://github.com/pytorch/pytorch/issues/121668</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I observe following error when try to follow this tutorial:<br />
https://pytorch.org/docs/main/torch.compiler_aot_inductor.html</p>
<p>Here is the output log:</p>
<p>```<br />
CMAKE_PREFIX_PATH=/home/atalman/miniconda3/envs/py310/lib/python3.10/site-packages/torch/share/cmake cmake ..<br />
-- The C compiler identification is GNU 9.4.0<br />
-- The CXX compiler identification is GNU 9.4.0<br />
-- Detecting C compiler ABI info<br />
-- Detecting C compiler ABI info - done<br />
-- Check for working C compiler: /usr/bin/cc - skipped<br />
-- Detecting C compile features<br />
-- Detecting C compile features - done<br />
-- Detecting CXX compiler ABI info<br />
-- Detecting CXX compiler ABI info - done<br />
-- Check for working CXX compiler: /usr/bin/c++ - skipped<br />
-- Detecting CXX compile features<br />
-- Detecting CXX compile features - done<br />
-- Found CUDA: /usr/local/cuda (found version "12.1") <br />
-- The CUDA compiler identification is NVIDIA 12.1.105<br />
-- Detecting CUDA compiler ABI info<br />
-- Detecting CUDA compiler ABI info - done<br />
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped<br />
-- Detecting CUDA compile features<br />
-- Detecting CUDA compile features - done<br />
-- Found CUDAToolkit: /usr/local/cuda/include (found version "12.1.105") <br />
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD<br />
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed<br />
-- Looking for pthread_create in pthreads<br />
-- Looking for pthread_create in pthreads - not found<br />
-- Looking for pthread_create in pthread<br />
-- Looking for pthread_create in pthread - found<br />
-- Found Threads: TRUE<br />
-- Caffe2: CUDA detected: 12.1<br />
-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc<br />
-- Caffe2: CUDA toolkit directory: /usr/local/cuda<br />
-- Caffe2: Header version is: 12.1<br />
-- /usr/local/cuda/lib64/libnvrtc.so shorthash is b51b459d<br />
-- USE_CUDNN is set to 0. Compiling without cuDNN support<br />
-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support<br />
-- Autodetected CUDA architecture(s):  8.0 8.0 8.0 8.0 8.0 8.0 8.0 8.0<br />
-- Added CUDA NVCC flags for: -gencode;arch=compute_80,code=sm_80<br />
CMake Warning at /home/atalman/miniconda3/envs/py310/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):<br />
  static library kineto_LIBRARY-NOTFOUND not found.<br />
Call Stack (most recent call first):<br />
  /home/atalman/miniconda3/envs/py310/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:127 (append_torchlib_if_found)<br />
  CMakeLists.txt:4 (find_package)</p>
<p>-- Found Torch: /home/atalman/miniconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libtorch.so<br />
-- Configuring done<br />
-- Generating done<br />
-- Build files have been written to: /data/home/atalman/aot_ind/build<br />
```</p>
<p>However looking at the Wheel build log here:<br />
https://github.com/pytorch/pytorch/actions/runs/8229185939/job/22499908747</p>
<p>I do see we include kineto in the build:<br />
<code>Configuring Kineto dependency:
2024-03-11T07:42:39.8183621Z --   KINETO_SOURCE_DIR = /pytorch/third_party/kineto/libkineto
2024-03-11T07:42:39.8184187Z --   KINETO_BUILD_TESTS = OFF
2024-03-11T07:42:39.8184596Z --   KINETO_LIBRARY_TYPE = static</code></p>
<p>Without the correct settings people won't be able to use profiling in CPP code. Our nightlies and release binaries should generate the correct settings.</p>
<p>cc @seemethere @malfet @osalpekar @jbschlosser @chauhang </p>
<h3>Versions</h3>
<p>2.3.0</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 12:55:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121668</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] GEMM size threshold for Cutlass backend usage</title>
      <link>https://github.com/pytorch/pytorch/pull/121491</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121491<br />
* #121490</p>
<ul>
<li>
<p>Adds a configurable GEMM size threshold for the usage of Cutlass GEMM Kernels <strong>_inductor.config.cutlass_backend_min_gemm_size</strong></p>
</li>
<li>
<p>During GEMM algorithm choice generation: <strong>if no viable choices can be generated using the configured backends, the ATen backend will be used as a fallback backend</strong>, even if it is not enabled in <strong>_inductor.config.max_autotune_gemm_backends</strong></p>
</li>
</ul>
<p>Test plan:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121491</guid>
    </item>
    <item>
      <title>[Inductor] Make codecache CUDA compilation more robust &amp; flexible</title>
      <link>https://github.com/pytorch/pytorch/pull/121490</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121491<br />
* <strong>-&gt;</strong> #121490</p>
<p>Minor changes which make the CUDA compilation within _inductor/codecache.py<br />
more robust and flexible.</p>
<p>Test plan:<br />
CI<br />
Additional test in test_codecache.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121490</guid>
    </item>
    <item>
      <title>[inductor] use generic launcher</title>
      <link>https://github.com/pytorch/pytorch/pull/121483</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121483</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 00:32:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121483</guid>
    </item>
    <item>
      <title>Add registration API for torch.compile-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/121387</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we intend to provide a registration API dedicated to eager-through-torch.compile. The major workflow of this API will be as follows.</p>
<ul>
<li>Load cache</li>
<li>Check cache according to the input tensors</li>
<li>Cache Hit: Run the cached kernel directly</li>
<li>Cache Miss: Run the registered python kernel</li>
</ul>
<p>Currently, this PR always fallback to python kernel now and cache mechanism will be implemented in another PR.</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120595<br />
* #116449<br />
* #116368<br />
* #121727<br />
* <strong>-&gt;</strong> #121387<br />
* #121296</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121387</guid>
    </item>
    <item>
      <title>[Inductor] Add prologue fusion</title>
      <link>https://github.com/pytorch/pytorch/pull/121211</link>
      <description><![CDATA[<p>Hi, I am new in PyTorch, but trying to add prologue fusion in the Inductor-Triton path by following the implementation of epilogue fusion. </p>
<p>I mainly changed two parts: scheduler and codegen. Current scheduler gives up fusing prologues such as relu followed by mm due to the mismatch of their dependency types. I changed TemplateBuffer so that it also issues MemoryDep when prologue fusion is enabled in config to make the dependency types match.</p>
<p>Regarding codegen, as a first use case, I changed Triton mm kernel so that it can emit prologue code. Kernel is supposed to receive single type of activation function either as its first parameter, second parameter, or both.</p>
<p>Any comments or suggestions are appreciated.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 00:10:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121211</guid>
    </item>
    <item>
      <title>aot_compile: EmbeddingBag C++ compile error when torch._inductor.config.aot_inductor.abi_compatible is True.</title>
      <link>https://github.com/pytorch/pytorch/issues/120394</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>```<br />
import torch<br />
embedding = torch.nn.EmbeddingBag(num_embeddings=128, embedding_dim=32)</p>
<p>inputs = torch.randint(low=0, high=128, size=(1, 10))<br />
device = "cuda" if torch.cuda.is_available() else "cpu"</p>
<p>model = embedding.to(device=device)<br />
example_inputs = inputs.to(device)</p>
<h1>use c abi when</h1>
<p>with torch._inductor.config.patch({"aot_inductor.abi_compatible": True}):<br />
    so_path = torch._export.aot_compile(<br />
        model,<br />
        (example_inputs, ),<br />
        options={"aot_inductor.output_path": r"/tmp/test_model.so"})<br />
```</p>
<p>Error stack</p>
<p>In file included from /home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/include/torch/csrc/inductor/aoti_runtime/model_container.h:13,<br />
                 from /tmp/cmezixbshurefmdsgmafaulooifjf4hu5ofsldijijfkuo5iwvvs.cpp:2:<br />
/tmp/cmezixbshurefmdsgmafaulooifjf4hu5ofsldijijfkuo5iwvvs.cpp: In member function ‚Äòvoid torch::aot_inductor::AOTInductorModel::run_impl(AtenTensorOpaque<strong>, AtenTensorOpaque</strong>, torch::aot_inductor::DeviceStreamType, AOTIProxyExecutorHandle)‚Äô:<br />
/tmp/cmezixbshurefmdsgmafaulooifjf4hu5ofsldijijfkuo5iwvvs.cpp:384:33: error: ‚Äòaoti_torch__embedding_bag‚Äô was not declared in this scope<br />
  384 |     AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch__embedding_bag(fn_weight, RAIIAtenTensorHandle(tmp_tensor_handle_0), buf0, 0, 1L, 0, 0, 0, -1L, &amp;buf2_handle, &amp;buf3_handle, &amp;buf4_handle, &amp;buf5_handle));<br />
      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~<br />
/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/include/torch/csrc/inductor/aoti_runtime/model.h:73:8: note: in definition of macro ‚ÄòAOTI_TORCH_ERROR_CODE_CHECK‚Äô<br />
   73 |   if ((call) != AOTI_TORCH_SUCCESS) {           \<br />
      |        ^~~~<br />
Traceback (most recent call last):<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/codecache.py", line 1500, in run_command_and_check<br />
    subprocess.check_call(cmd)<br />
  File "/usr/local/lib/python3.11/subprocess.py", line 413, in check_call<br />
    raise CalledProcessError(retcode, cmd)<br />
subprocess.CalledProcessError: Command '['g++', '/tmp/cmezixbshurefmdsgmafaulooifjf4hu5ofsldijijfkuo5iwvvs.cpp', '-shared', '-fPIC', '-Wall', '-std=c++17', '-Wno-unused-variable', '-Wno-unknown-pragmas', '-D_GLIBCXX_USE_CXX11_ABI=0', '-I/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/include', '-I/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include', '-I/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/include/TH', '-I/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/include/THC', '-I/usr/local/cuda-12.1/include', '-I/usr/local/include/python3.11', '-L/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/lib', '-L/usr/local/cuda-12.1/lib64', '-L/usr/local/lib', '-ltorch', '-ltorch_cpu', '-lgomp', '-lc10_cuda', '-lcuda', '-ltorch_cuda', '-mavx512f', '-mavx512dq', '-mavx512vl', '-mavx512bw', '-mfma', '-DCPU_CAPABILITY_AVX512', '-D', 'USE_CUDA', '-O3', '-DNDEBUG', '-ffast-math', '-fno-finite-math-only', '-fno-unsafe-math-optimizations', '-march=native', '-fopenmp', '-D', 'C10_USING_CUSTOM_GENERATED_MACROS', '-c', '-o', '/tmp/cmezixbshurefmdsgmafaulooifjf4hu5ofsldijijfkuo5iwvvs.o']' returned non-zero exit status 1.</p>
<p>The above exception was the direct cause of the following exception:</p>
<p>Traceback (most recent call last):<br />
  File "/home/xiang.liu/cpp/torch_inference/python/torch_issue/aot.py", line 17, in <module><br />
    so_path = torch.<em>export.aot_compile(<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_export/<strong>init</strong>.py", line 1153, in aot_compile<br />
    so_path = torch._inductor.aot_compile(gm, flat_example_inputs, options)  # type: ignore[arg-type]<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/<strong>init</strong>.py", line 78, in aot_compile<br />
    return compile_fx_aot(<br />
           ^^^^^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 847, in compile_fx_aot<br />
    compiled_lib_path = compile_fx(<br />
                        ^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 952, in compile_fx<br />
    return compile_fx(<br />
           ^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 986, in compile_fx<br />
    return compile_fx(<br />
           ^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1163, in compile_fx<br />
    return inference_compiler(unlifted_gm, example_inputs</em>)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(<em>args, </em><em>kwargs)<br />
        ^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1100, in fw_compiler_base<br />
    return inner_compile(<br />
           ^^^^^^^^^^^^^^<br />
  File "/usr/local/lib/python3.11/contextlib.py", line 81, in inner<br />
    return func(</em>args, <strong>kwds)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper<br />
    inner_compiled_fn = compiler_fn(gm, example_inputs)<br />
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/debug.py", line 305, in inner<br />
    return fn(*args, </strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/usr/local/lib/python3.11/contextlib.py", line 81, in inner<br />
    return func(<em>args, </em>*kwds)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 320, in compile_fx_inner<br />
    compiled_graph = fx_codegen_and_compile(<br />
                     ^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 550, in fx_codegen_and_compile<br />
    compiled_fn = graph.compile_to_fn()<br />
                  ^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/graph.py", line 1112, in compile_to_fn<br />
    return AotCodeCache.compile(<br />
           ^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/codecache.py", line 1622, in compile<br />
    run_command_and_check(cmd)<br />
  File "/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/_inductor/codecache.py", line 1502, in run_command_and_check<br />
    raise exc.CppCompileError(cmd, e.output) from e<br />
torch._inductor.exc.CppCompileError: C++ compile error</p>
<p>Command:<br />
g++ /tmp/cmezixbshurefmdsgmafaulooifjf4hu5ofsldijijfkuo5iwvvs.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=0 -I/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/include -I/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/include/TH -I/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/local/include/python3.11 -L/home/xiang.liu/pyenv/torch2.2/lib/python3.11/site-packages/torch/lib -L/usr/local/cuda-12.1/lib64 -L/usr/local/lib -ltorch -ltorch_cpu -lgomp -lc10_cuda -lcuda -ltorch_cuda -mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma -DCPU_CAPABILITY_AVX512 -D USE_CUDA -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -c -o /tmp/cmezixbshurefmdsgmafaulooifjf4hu5ofsldijijfkuo5iwvvs.o</p>
<p>Output:<br />
None</p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.2.0+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.5 LTS (x86_64)<br />
GCC version: (Ubuntu 10.3.0-1ubuntu1~20.04) 10.3.0<br />
Clang version: Could not collect<br />
CMake version: version 3.28.0-rc5<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.11.3 (main, Jan 11 2024, 03:38:59) [GCC 10.3.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-1029-gcp-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 12.1.105<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA L4<br />
GPU 1: NVIDIA L4</p>
<p>Nvidia driver version: 545.23.08<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
CPU(s):                          24<br />
On-line CPU(s) list:             0-23<br />
Thread(s) per core:              2<br />
Core(s) per socket:              12<br />
Socket(s):                       1<br />
NUMA node(s):                    1<br />
Vendor ID:                       GenuineIntel<br />
CPU family:                      6<br />
Model:                           85<br />
Model name:                      Intel(R) Xeon(R) CPU @ 2.20GHz<br />
Stepping:                        7<br />
CPU MHz:                         2200.212<br />
BogoMIPS:                        4400.42<br />
Hypervisor vendor:               KVM<br />
Virtualization type:             full<br />
L1d cache:                       384 KiB<br />
L1i cache:                       384 KiB<br />
L2 cache:                        12 MiB<br />
L3 cache:                        38.5 MiB<br />
NUMA node0 CPU(s):               0-23<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT Host state unknown<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown<br />
Vulnerability Retbleed:          Mitigation; Enhanced IBRS<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT Host state unknown<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.0<br />
[pip3] pytorch-lightning==2.2.0<br />
[pip3] torch==2.2.0<br />
[pip3] torchmetrics==0.11.0<br />
[pip3] torchvision==0.17.0<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @desertfire</p>]]></description>
      <pubDate>Thu, 22 Feb 2024 01:27:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120394</guid>
    </item>
    <item>
      <title>Torch compile does not work on python 3.12</title>
      <link>https://github.com/pytorch/pytorch/issues/120233</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Currently torch, as of 2.2.0 does not support torch compile with python 3.12</p>
<p>See following PR for example: https://github.com/pytorch/pytorch/pull/117853</p>
<ol>
<li>We need to be able to use python 3.12 with torch.compile feature. </li>
<li>Include triton with Linux 3.12 wheel </li>
<li>Enable Python 3.12 and torch.compile CI testsing</li>
</ol>
<p>cc: @albanD @malfet </p>
<h3>Versions</h3>
<p>2.2.0</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Tue, 20 Feb 2024 06:14:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120233</guid>
    </item>
    <item>
      <title>[inductor] add evict-first</title>
      <link>https://github.com/pytorch/pytorch/pull/119622</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #119622</p>
<p>In https://github.com/pytorch/pytorch/issues/119523 we discovered that in persistent reduction, adding evict_first hint to loads that are not used later can help perf quite a bit for small kernels. E.g. for the example i added in test_evict_first_for_persistent_reduction , the perf improves by 16% for that kernel. I'm not sure how this affect large kernels. But I'll run a perf test.</p>
<p>Our code seems to on purpose skip adding these hints for pointwise / persistent reduction. Please comment if there is reason why we do that. </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Fri, 09 Feb 2024 18:00:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119622</guid>
    </item>
    <item>
      <title>CUDA Memory leak w/ torch.compile in both stable and trunk</title>
      <link>https://github.com/pytorch/pytorch/issues/119607</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>models traced with torch.compile don't seem to be freeing CUDA memory</p>
<p>``` python<br />
import torch<br />
import gc</p>
<p>def main():<br />
    x = torch.randn(1000, 3000, device="cuda", requires_grad=True)<br />
    model = torch.nn.Sequential(<br />
        torch.nn.Linear(3000, 10000),<br />
        torch.nn.ReLU(),<br />
        torch.nn.Linear(10000, 50000),<br />
        torch.nn.ReLU(),<br />
        torch.nn.Linear(50000, 20000),<br />
        torch.nn.ReLU(),<br />
        torch.nn.Linear(20000, 1234),<br />
    ).to("cuda")<br />
    model = torch.compile(model, backend="eager")<br />
    model(x)</p>
<p>if <strong>name</strong> == "<strong>main</strong>":<br />
    main()</p>
<pre><code># tried clearing with a few ways
torch.cuda.synchronize()
torch.cuda.empty_cache()
torch._C._cuda_clearCublasWorkspaces()
gc.collect()

print(f"{torch.cuda.memory_allocated()/1e9} GB!!")  # 6.219729408 GB!!
</code></pre>
<p>```</p>
<p>one high priority use case to fix this is for compiled autograd, which calls torch.compile for compiled fw and once for compiled bw, leading to 2x memory use</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @Chillee </p>
<h3>Versions</h3>
<p>2.2.0<br />
trunk</p>]]></description>
      <pubDate>Fri, 09 Feb 2024 15:35:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/119607</guid>
    </item>
    <item>
      <title>inductor: log unique id to match output_code to aot graphs</title>
      <link>https://github.com/pytorch/pytorch/pull/118647</link>
      <description><![CDATA[<p>I found it helpful to be able to see, given some inductor output code, which AOT graph it came from. When you have large models with multiple graphs floating around this can be difficult, so I added the aot_config.aot_id to the printed inductor output.</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #118647<br />
* #118646<br />
* #118645<br />
* #118644<br />
* #118643<br />
* #118937</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 30 Jan 2024 08:59:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/118647</guid>
    </item>
    <item>
      <title>[AOTInductor] Support freezing model in aoti</title>
      <link>https://github.com/pytorch/pytorch/pull/114451</link>
      <description><![CDATA[<p>Fixes #114450</p>
<p>This commit supports aotinductor processing opaque tensor, so we can enable freezing in aoti. Need help from the community to review and see if this is a suitable way.</p>
<p>Test on cpu only device:<br />
<code>bash
python test_aot_inductor.py -k AOTInductorTestNonABICompatibleCpu.test_freezing_non_abi_compatible_cpu</code></p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 22 Nov 2023 23:45:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/114451</guid>
    </item>
    <item>
      <title>[AOTInductor] Add Runtime Constant-folding for AOTInductor</title>
      <link>https://github.com/pytorch/pytorch/pull/113243</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #113243</p>
<p>Summary:<br />
Add Runtime Constant-folding for AOTInductor.<br />
This also include the invocation of constant folding at load time.</p>
<p>The constant folding lowering is a 2-step process. <br />
First, we split the graph into 2 modules, one of it is the constant module, which doesn't depend on any input and the whole module could be inferred (constant-folded) one-time and be reused. The constant module, is lowered, and being codegen-ed as usual and cached (let's call this constant code). The constant code reuses the whole lowering/profiling/etc. process, only difference is that we do not generate any headers or initialization for the constant code.<br />
Second, after handling the constant module, we take care of the main module (which is the part that would depend on the user input.) For the main module, we take in one additional component, the constant code, compare with a normal lowering. Addition step we do here is that, we inject the constant code into the codegen-ed main module, and create the caller for the main module to consume the result of the constant module.</p>
<p>Test Plan:<br />
Included in commit.</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D52430653">D52430653</a></p>]]></description>
      <pubDate>Tue, 07 Nov 2023 21:20:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/113243</guid>
    </item>
    <item>
      <title>[compile] DDPOptimizer + activation checkpointing not supported</title>
      <link>https://github.com/pytorch/pytorch/issues/104674</link>
      <description><![CDATA[<p>Activation checkpointing is supported via higher order operators. However, DDP optimizer backend in Dynamo does not work with higher order operators yet. The workaround is to disable DDP optimizer using <code>torch._dynamo.config.optimize_ddp = False</code>. However, the tradeoff is bad performance because there will be just one bucket for the entire Dynamo graph.</p>
<p>No plan to support it yet. We will revisit if this is a common ask. </p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang @soulitzer @msaroufim @bdhirsh @kiukchung @lucasllc @d4l3k  </p>]]></description>
      <pubDate>Wed, 05 Jul 2023 15:12:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/104674</guid>
    </item>
    <item>
      <title>tacotron2 slow compile</title>
      <link>https://github.com/pytorch/pytorch/issues/98467</link>
      <description><![CDATA[<p>Repro:<br />
<code>python benchmarks/dynamo/torchbench.py --accuracy --inference --amp --backend inductor --disable-cudagraphs --device cuda --only tacotron2</code></p>
<p>ctrl+c gives this stack information, which looks like a problem in the fuser heuristic,<br />
<code>File "/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 636, in __init__
    self.fuse_nodes()
  File "/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 817, in fuse_nodes
    self.fuse_nodes_once()
  File "/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 833, in fuse_nodes_once
    if self.can_fuse(node1, node2) and not self.will_fusion_create_cycle(
  File "/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 907, in will_fusion_create_cycle
    return any(check(self.name_to_fused_node[n]) for n in combined_predecessors)
  File "/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 907, in &lt;genexpr&gt;
    return any(check(self.name_to_fused_node[n]) for n in combined_predecessors)
  File "/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 896, in check
    return bool(combined_names &amp; node.recursive_predecessors) or any(
KeyboardInterrupt</code></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @wconstab @soumith @ngimel @Xia-Weiwen</p>]]></description>
      <pubDate>Wed, 05 Apr 2023 16:47:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/98467</guid>
    </item>
    <item>
      <title>[Inductor] atomic_add does not support bf16</title>
      <link>https://github.com/pytorch/pytorch/issues/97016</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>This is may be known already, but triton does not support <code>atomic_add</code> with bf16, see https://github.com/openai/triton/blob/c9740f0870f6ae2480acd2a76a5fb4c920bc5ce5/python/triton/language/semantic.py#L904.</p>
<p>This is not a problem in eager mode, only with <code>torch.compile</code> as it works right now, ideally this op should not be currently selected?</p>
<p>I made a minified repro below, but there is probably an even easier way to replicate this, I'm just unsure how to exactly trigger <code>atomic_add</code>. </p>
<h3>Error logs</h3>
<p>```<br />
    raise ValueError("atomic_" + op + " does not support " + str(element_ty))<br />
ValueError: atomic_add does not support bf16</p>
<p>The above exception was the direct cause of the following exception:<br />
```</p>
<p><code>triton.compiler.CompilationError: at 11:85:
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 61440
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x1 = (xindex // 768)
    x2 = xindex
    x0 = xindex % 768
    tmp0 = tl.load(in_ptr0 + (x1), None)
    tmp1 = tl.load(in_ptr1 + (x2), None).to(tl.float32)
    tl.atomic_add(out_ptr0 + (x0 + (768*tmp0) + tl.zeros([XBLOCK], tl.int32)), tmp1, None)
                                                                                     ^</code></p>
<h3>Minified repro</h3>
<p>```<br />
import torch._inductor.overrides</p>
<p>import torch<br />
from torch import tensor, device<br />
import torch.fx as fx<br />
from torch._dynamo.testing import rand_strided<br />
from math import inf<br />
from torch.fx.experimental.proxy_tensor import make_fx</p>
<p>import torch._dynamo.config<br />
import torch._inductor.config<br />
import torch._functorch.config<br />
torch._dynamo.config.load_config(b'\x80\x02}q\x00(X\x0b\x00\x00\x00output_codeq\x01\x89X\r\x00\x00\x00log_file_nameq\x02NX\x07\x00\x00\x00verboseq\x03\x89X\x11\x00\x00\x00output_graph_codeq\x04\x89X\x12\x00\x00\x00verify_correctnessq\x05\x89X\x12\x00\x00\x00minimum_call_countq\x06K\x01X\x15\x00\x00\x00dead_code_eliminationq\x07\x88X\x10\x00\x00\x00cache_size_limitq\x08K@X\x0e\x00\x00\x00specialize_intq\t\x88X\x0e\x00\x00\x00dynamic_shapesq\n\x89X\x18\x00\x00\x00assume_static_by_defaultq\x0b\x89X\x10\x00\x00\x00guard_nn_modulesq\x0c\x89X\x1b\x00\x00\x00traceable_tensor_subclassesq\rc__builtin__\nset\nq\x0e]q\x0f\x85q\x10Rq\x11X\x0f\x00\x00\x00suppress_errorsq\x12\x89X\x15\x00\x00\x00replay_record_enabledq\x13\x89X \x00\x00\x00rewrite_assert_with_torch_assertq\x14\x88X\x12\x00\x00\x00print_graph_breaksq\x15\x89X\x07\x00\x00\x00disableq\x16\x89X<em>\x00\x00\x00allowed_functions_module_string_ignorelistq\x17h\x0e]q\x18(X\x0b\x00\x00\x00torch._refsq\x19X\x0c\x00\x00\x00torch._primsq\x1aX\x13\x00\x00\x00torch.distributionsq\x1bX\r\x00\x00\x00torch._decompq\x1cX\r\x00\x00\x00torch.testingq\x1de\x85q\x1eRq\x1fX\x12\x00\x00\x00repro_forward_onlyq \x89X\x0f\x00\x00\x00repro_toleranceq!G?PbM\xd2\xf1\xa9\xfcX\x16\x00\x00\x00capture_scalar_outputsq"\x89X \x00\x00\x00capture_dynamic_output_shape_opsq#\x89X\x19\x00\x00\x00enforce_cond_guards_matchq$\x88X\x0c\x00\x00\x00optimize_ddpq%\x88X\x1a\x00\x00\x00raise_on_ctx_manager_usageq&amp;\x88X\x1c\x00\x00\x00raise_on_unsafe_aot_autogradq\'\x89X\x17\x00\x00\x00raise_on_backend_changeq(\x89X\x18\x00\x00\x00error_on_nested_fx_traceq)\x88X\t\x00\x00\x00allow_rnnq</em>\x89X\x08\x00\x00\x00base_dirq+X;\x00\x00\x00/home/jonas/miniconda3/envs/dl/lib/python3.10/site-packagesq,X\x0e\x00\x00\x00debug_dir_rootq-XN\x00\x00\x00/home/jonas/Dropbox/Documents_Hyperion/Python/cramming-dev/torch_compile_debugq.X)\x00\x00\x00DO_NOT_USE_legacy_non_fake_example_inputsq/\x89X\x13\x00\x00\x00_save_config_ignoreq0h\x0e]q1(X!\x00\x00\x00skipfiles_inline_module_allowlistq2X\x12\x00\x00\x00constant_functionsq3X\x0b\x00\x00\x00repro_levelq4X\x0b\x00\x00\x00repro_afterq5e\x85q6Rq7u.')<br />
torch._inductor.config.load_config(b'\x80\x02}q\x00(X\x05\x00\x00\x00debugq\x01\x89X\x10\x00\x00\x00disable_progressq\x02\x88X\x10\x00\x00\x00verbose_progressq\x03\x89X\x0b\x00\x00\x00cpp_wrapperq\x04\x89X\x03\x00\x00\x00dceq\x05\x89X\x14\x00\x00\x00static_weight_shapesq\x06\x88X\x0c\x00\x00\x00size_assertsq\x07\x88X\x10\x00\x00\x00pick_loop_ordersq\x08\x88X\x0f\x00\x00\x00inplace_buffersq\t\x88X\x11\x00\x00\x00benchmark_harnessq\n\x88X\x0f\x00\x00\x00epilogue_fusionq\x0b\x89X\x15\x00\x00\x00epilogue_fusion_firstq\x0c\x89X\x0f\x00\x00\x00pattern_matcherq\r\x88X\n\x00\x00\x00reorderingq\x0e\x89X\x0c\x00\x00\x00max_autotuneq\x0f\x89X\x15\x00\x00\x00search_autotune_cacheq\x10\x89X\x17\x00\x00\x00realize_reads_thresholdq\x11K\x04X\x17\x00\x00\x00realize_bytes_thresholdq\x12M\xd0\x07X\x1b\x00\x00\x00realize_acc_reads_thresholdq\x13K\x08X\x0f\x00\x00\x00fallback_randomq\x14\x89X\x12\x00\x00\x00implicit_fallbacksq\x15\x88X\x0b\x00\x00\x00tune_layoutq\x16\x89X\x11\x00\x00\x00aggressive_fusionq\x17\x89X\x0f\x00\x00\x00max_fusion_sizeq\x18K@X\x1b\x00\x00\x00unroll_reductions_thresholdq\x19K\x08X\x0e\x00\x00\x00comment_originq\x1a\x89X\x10\x00\x00\x00benchmark_kernelq\x1b\x89X\x12\x00\x00\x00developer_warningsq\x1c\x89X\x0f\x00\x00\x00compile_threadsq\x1dK\x10X\x11\x00\x00\x00global_cache_pathq\x1eNX\x13\x00\x00\x00kernel_name_max_opsq\x1fK\nX\r\x00\x00\x00shape_paddingq \x89X\x0e\x00\x00\x00permute_fusionq!\x89X\x1a\x00\x00\x00profiler_mark_wrapper_callq"\x89X\x18\x00\x00\x00_raise_error_for_testingq#\x89X\x0c\x00\x00\x00_profile_varq$X\x00\x00\x00\x00q%X\x11\x00\x00\x00profile_bandwidthq&amp;\x89X\x17\x00\x00\x00profile_bandwidth_regexq\'h%X\x0b\x00\x00\x00cpp.threadsq(J\xff\xff\xff\xffX\x13\x00\x00\x00cpp.dynamic_threadsq)\x89X\x0b\x00\x00\x00cpp.simdlenq*NX\x12\x00\x00\x00cpp.min_chunk_sizeq+M\x00\x10X\x07\x00\x00\x00cpp.cxxq,NX\x03\x00\x00\x00g++q-\x86q.X\x19\x00\x00\x00cpp.enable_kernel_profileq/\x89X\x12\x00\x00\x00cpp.weight_prepackq0\x88X\x11\x00\x00\x00triton.cudagraphsq1\x89X\x17\x00\x00\x00triton.debug_sync_graphq2\x89X\x18\x00\x00\x00triton.debug_sync_kernelq3\x89X\x15\x00\x00\x00triton.dense_indexingq4\x89X\x10\x00\x00\x00triton.max_tilesq5K\x02X\x19\x00\x00\x00triton.autotune_pointwiseq6\x88X\'\x00\x00\x00triton.tiling_prevents_pointwise_fusionq7\x88X\'\x00\x00\x00triton.tiling_prevents_reduction_fusionq8\x88X\x1b\x00\x00\x00triton.ordered_kernel_namesq9\x89X\x1f\x00\x00\x00triton.descriptive_kernel_namesq:\x89X\x1c\x00\x00\x00triton.persistent_reductionsq;\x88X\x10\x00\x00\x00triton.max_blockq&lt;}q=(X\x01\x00\x00\x00Xq&gt;M\x00\x08X\x01\x00\x00\x00Yq?M\x00\x04X\x01\x00\x00\x00Zq@M\x00\x04uX\r\x00\x00\x00trace.enabledqA\x89X\x0f\x00\x00\x00trace.debug_logqB\x88X\x0e\x00\x00\x00trace.info_logqC\x89X\x0e\x00\x00\x00trace.fx_graphqD\x88X\x1a\x00\x00\x00trace.fx_graph_transformedqE\x88X\x13\x00\x00\x00trace.ir_pre_fusionqF\x88X\x14\x00\x00\x00trace.ir_post_fusionqG\x88X\x11\x00\x00\x00trace.output_codeqH\x88X\x13\x00\x00\x00trace.graph_diagramqI\x89X\x15\x00\x00\x00trace.compile_profileqJ\x89X\x10\x00\x00\x00trace.upload_tarqKNu.')<br />
torch._functorch.config.load_config(b'\x80\x02}q\x00(X\x11\x00\x00\x00use_functionalizeq\x01\x88X\x0f\x00\x00\x00use_fake_tensorq\x02\x88X\x16\x00\x00\x00fake_tensor_allow_metaq\x03\x88X\x0c\x00\x00\x00debug_assertq\x04\x88X\x14\x00\x00\x00debug_fake_cross_refq\x05\x89X\x11\x00\x00\x00debug_partitionerq\x06\x89X\x0c\x00\x00\x00debug_graphsq\x07\x89X\x0b\x00\x00\x00debug_jointq\x08\x89X\x14\x00\x00\x00static_weight_shapesq\t\x88X\x03\x00\x00\x00cseq\n\x88X\x10\x00\x00\x00max_dist_from_bwq\x0bK\x03X\t\x00\x00\x00log_levelq\x0cK\x14u.')</p>
<p>from torch.nn import *<br />
class Repro(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()</p>
<pre><code>def forward(self, arg27_1, mm_1, full):
    index_put = torch.ops.aten.index_put.default(full, [arg27_1], mm_1, True);  full = arg27_1 = mm_1 = None
    return (index_put,)
</code></pre>
<p>args = [((80,), (1,), torch.int64, 'cuda'), ((80, 768), (768, 1), torch.bfloat16, 'cuda'), ((512, 768), (768, 1), torch.bfloat16, 'cuda')]<br />
args = [rand_strided(sh, st, dt, dev) for (sh, st, dt, dev) in args]<br />
mod = make_fx(Repro(), tracing_mode='real')(*args)</p>
<p>from torch._inductor.compile_fx import compile_fx_inner<br />
from torch._dynamo.debug_utils import same_two_models</p>
<p>compiled = compile_fx_inner(mod, args)<br />
ref = compiled(args)<br />
torch.cuda.synchronize() # Ensures that segfaults are surfaced<br />
```</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.1.0.dev20230316<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Pop!_OS 22.04 LTS (x86_64)<br />
GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0<br />
Clang version: Could not collect<br />
CMake version: Could not collect<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-6.2.0-76060200-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU<br />
Nvidia driver version: 525.85.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.23.5<br />
[pip3] torch==2.1.0.dev20230316<br />
[pip3] torchaudio==2.0.0.dev20230317<br />
[pip3] torchvision==0.16.0.dev20230317<br />
[conda] blas                      1.0                         mkl<br />
[conda] lion-pytorch              0.0.7                    pypi_0    pypi<br />
[conda] mkl                       2021.4.0           h06a4308_640<br />
[conda] mkl-service               2.4.0           py310h7f8727e_0<br />
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0<br />
[conda] mkl_random                1.2.2           py310h00e6091_0<br />
[conda] numpy                     1.23.5          py310hd5efca6_0<br />
[conda] numpy-base                1.23.5          py310h8e6c178_0<br />
[conda] pytorch                   2.1.0.dev20230316 py3.10_cuda11.8_cudnn8.7.0_0    pytorch-nightly<br />
[conda] pytorch-cuda              11.8                 h7e8668a_3    pytorch-nightly<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly<br />
[conda] torchaudio                2.0.0.dev20230317     py310_cu118    pytorch-nightly<br />
[conda] torchtriton               2.1.0+2c32f43999           py310    pytorch-nightly<br />
[conda] torchvision               0.16.0.dev20230317     py310_cu118    pytorch-nightly</p>
<p>```</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @soumith @ngimel @Xia-Weiwen @desertfire</p>]]></description>
      <pubDate>Fri, 17 Mar 2023 04:45:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/97016</guid>
    </item>
    <item>
      <title>torch.compile breaks reproducibility</title>
      <link>https://github.com/pytorch/pytorch/issues/94855</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Adding torch.compile does not ensure deterministic results after setting a seed (and ensuring all the steps here: https://pytorch.org/docs/stable/notes/randomness.html#:~:text=Reproducibility%20Completely%20reproducible%20results%20are%20not%20guaranteed%20across,and%20GPU%20executions%2C%20even%20when%20using%20identical%20seeds.). </p>
<p>I've been stuck trying to debug why my results are non-deterministic across runs. Finally, removing torch.compile ensures that results across multiple runs are the same. This can be easily reproduced by having multiple runs of a model with torch.compile enabled. </p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>PyTorch version: 2.0.0.dev20230213+cu117<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.7<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.5 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.25.0<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.8.15 (default, Nov 24 2022, 15:19:38)  [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.4.0-1085-azure-x86_64-with-glibc2.17<br />
Is CUDA available: True<br />
CUDA runtime version: 11.7.99<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration:<br />
GPU 0: NVIDIA A100 80GB PCIe<br />
GPU 1: NVIDIA A100 80GB PCIe<br />
GPU 2: NVIDIA A100 80GB PCIe<br />
GPU 3: NVIDIA A100 80GB PCIe</p>
<p>Nvidia driver version: 510.73.08<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.5.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   48 bits physical, 48 bits virtual<br />
CPU(s):                          96<br />
On-line CPU(s) list:             0-95<br />
Thread(s) per core:              1<br />
Core(s) per socket:              48<br />
Socket(s):                       2<br />
NUMA node(s):                    4<br />
Vendor ID:                       AuthenticAMD<br />
CPU family:                      25<br />
Model:                           1<br />
Model name:                      AMD EPYC 7V13 64-Core Processor<br />
Stepping:                        1<br />
CPU MHz:                         2478.466<br />
BogoMIPS:                        4890.88<br />
Hypervisor vendor:               Microsoft<br />
Virtualization type:             full<br />
L1d cache:                       3 MiB<br />
L1i cache:                       3 MiB<br />
L2 cache:                        48 MiB<br />
L3 cache:                        384 MiB<br />
NUMA node0 CPU(s):               0-23<br />
NUMA node1 CPU(s):               24-47<br />
NUMA node2 CPU(s):               48-71<br />
NUMA node3 CPU(s):               72-95<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Not affected<br />
Vulnerability Spec store bypass: Vulnerable<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr arat umip vaes vpclmulqdq rdpid</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.23.5<br />
[pip3] pytorch-lightning==1.6.3<br />
[pip3] pytorch-triton==2.0.0+0d7e753227<br />
[pip3] torch==2.0.0.dev20230213+cu117<br />
[pip3] torch-nebula==0.15.9<br />
[pip3] torch-ort==1.13.1<br />
[pip3] torchaudio==2.0.0.dev20230214+cu117<br />
[pip3] torchmetrics==0.7.1<br />
[pip3] torchvision==0.15.0.dev20230214+cu117<br />
[conda] magma-cuda117             2.6.1                         1    pytorch<br />
[conda] mkl                       2021.4.0                 pypi_0    pypi<br />
[conda] mkl-include               2021.4.0                 pypi_0    pypi<br />
[conda] numpy                     1.23.5                   pypi_0    pypi<br />
[conda] pytorch-lightning         1.6.3                    pypi_0    pypi<br />
[conda] pytorch-triton            2.0.0+0d7e753227          pypi_0    pypi<br />
[conda] torch                     2.0.0.dev20230213+cu117          pypi_0    pypi<br />
[conda] torch-nebula              0.15.9                   pypi_0    pypi<br />
[conda] torch-ort                 1.13.1                   pypi_0    pypi<br />
[conda] torchaudio                2.0.0.dev20230214+cu117          pypi_0    pypi<br />
[conda] torchmetrics              0.7.1                    pypi_0    pypi<br />
[conda] torchvision               0.15.0.dev20230214+cu117          pypi_0    pypi</p>
<p>cc @pbelevich @mruberry @kurtamohler @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @soumith @wconstab @ngimel @yanboliang @Xia-Weiwen @desertfire</p>]]></description>
      <pubDate>Tue, 14 Feb 2023 13:47:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/94855</guid>
    </item>
    <item>
      <title>TorchInductor CPU Performance Dashboard</title>
      <link>https://github.com/pytorch/pytorch/issues/93531</link>
      <description><![CDATA[<p>Dashboard to track the performance of torchinductor on CPU.</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @soumith @ngimel @chauhang </p>]]></description>
      <pubDate>Wed, 12 Oct 2022 18:29:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/93531</guid>
    </item>
  </channel>
</rss>
