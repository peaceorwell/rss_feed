<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[export] Remove aot_compile in benchmarks</title>
      <link>https://github.com/pytorch/pytorch/pull/121643</link>
      <description><![CDATA[<p>Fixes #ISSUE_NUMBER</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 09:26:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121643</guid>
    </item>
    <item>
      <title>[draft][HOO][dynamo] Contextual hints for torch.compile backends</title>
      <link>https://github.com/pytorch/pytorch/pull/121639</link>
      <description><![CDATA[<p><strong>Idea:</strong><br />
This is a draft for a feature that allows end-user to add contextual hints to chunks of code via HOO mechanism. </p>
<p>Such feature allows user to provide more structure and allow better programmability for the topology he creates as he would be able to provide additional hints to the torch.compile backend. One trivial example would be allowing end user to specify streams he would like specific parts of the topology to be ran on, assuming the backend that later optimizes it is able to take that information into account.</p>
<p>This is not intended to be final implementation (I guess) but more like a discussion opener.</p>
<p><strong>High level:</strong><br />
Implemented <strong>hinted_context</strong> higher-order-op that allows to specify function, arguments and a <strong>hint</strong>.</p>
<p>For the purpose of the draft, the <strong>hint</strong> currently is a JSON string that is going to be propagated into all ops within specified function. Such hints can be nested to provide even more structure to the operations.</p>
<p>Hints can be specified in two manners: <br />
- <strong>hints only inside the FWD code</strong>, in that scenario hints are going to be automatically propagated into corresponding BWD nodes when joint graph is generated by AOT Autograd<br />
- <strong>hints inside autograd overridden ops</strong>, in that scenario automatic propagation shouldn't happen and user is able to provide hints that are different for FWD and BWD</p>
<p>Provided test shows various usages, including nested annotations for both cases above. Each test runs on dummy backend that just prints the hint metadata from the nodes:<br />
<img alt="image" src="https://github.com/pytorch/pytorch/assets/122799060/cce26bcf-9ffc-4b0b-ab92-524284e29e50" /></p>
<p>Fixes #118181</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 07:20:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121639</guid>
    </item>
    <item>
      <title>[inductor] Incorrect handle of `autocast` results in type mismatch</title>
      <link>https://github.com/pytorch/pytorch/issues/121631</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>...
  File "/home/yyc/.local/miniconda3/envs/zero_env/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 611, in run
    return model(new_inputs)
  File "/home/yyc/.local/miniconda3/envs/zero_env/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 892, in _run_from_cache
    return compiled_graph.compiled_artifact(inputs)
  File "/tmp/torchinductor_yyc/u6/cu6jjzq2vnxkdniqmnw3r7htiuocckvp7ou6qiiyn2ofrtdy7rz7.py", line 503, in call
    buf12 = aten._scaled_dot_product_efficient_attention(buf7, buf8, buf9, buf11, True)
  File "/home/yyc/.local/miniconda3/envs/zero_env/lib/python3.10/site-packages/torch/_ops.py", line 755, in __call__
    return self._op(*args, **(kwargs or {}))
RuntimeError: invalid dtype for bias - should match query's dtype</code></p>
<p>when using inductor backend.</p>
<p>I'm not able to create a single reproducer but I bisected to the triggering commit: https://github.com/huggingface/transformers/commit/20164cc2c67c07190893933e47c8f13dc1c1f1d7, it looks to me that the nested <code>autocast</code> is not being handled correctly.</p>
<p>Full reproducer:<br />
The issue occurs when training llama with <code>transformers==4.38.2</code> and <code>torch&gt;=2.2.0</code>, and can be reproduced with a (not very minimized) script as:<br />
<code>TORCHDYNAMO_DEBUG_FUNCTION='forward' TORCH_LOGS='+dynamo' torchrun train.py --model=meta-llama/Llama-2-7b-hf --cc=inductor</code></p>
<p>```<br />
import argparse<br />
import gc<br />
import json<br />
import os<br />
import time</p>
<p>import numpy as np<br />
import torch<br />
import torch.distributed as dist<br />
import transformers<br />
from accelerate import init_empty_weights<br />
from datasets import Dataset<br />
from torch.distributed.fsdp.fully_sharded_data_parallel import ShardingStrategy<br />
from transformers import (AutoConfig, AutoModelForCausalLM, LlamaConfig,<br />
                          PreTrainedModel, Trainer, TrainerCallback,<br />
                          TrainingArguments)</p>
<p>assert transformers.<strong>version</strong> &gt;= '4.36', 'requires transformers 4.36+ to enable sdpa by default'</p>
<p>parser = argparse.ArgumentParser()<br />
parser.add_argument('--model', type=str, default='gpt2')<br />
parser.add_argument('--mbs', type=int, default=1)<br />
parser.add_argument('--ga', type=int, default=1)<br />
parser.add_argument('--seq-len', type=int, default=512)<br />
parser.add_argument('--iters', type=int, default=100)<br />
parser.add_argument('--cc', default=None)<br />
parser.add_argument('--layers', type=int, default=2)<br />
parser.add_argument('--embd', type=int, default=-1)<br />
parser.add_argument('--vocab-size', type=int, default=-1)<br />
parser.add_argument('--zero-stage', '--zero', type=int, default=0)<br />
parser.add_argument('--overlap-comm', action="store_true", default=False)<br />
parser.add_argument('--local-rank', type=int, default=-1)<br />
parser.add_argument('--print-gc', action="store_true", default=False)</p>
<p>class ThrputCb(TrainerCallback):<br />
    def <strong>init</strong>(self, iters, seq_len):<br />
        self.t0 = time.perf_counter()<br />
        self.total_iters = iters<br />
        self.seq_len = seq_len<br />
        self.iter = 0<br />
        self.smooth_thrput = 0<br />
        self.smooth_toks_per_dev = 0</p>
<pre><code>def on_log(self, args, state, control, logs, **kwargs):
    _ = logs.pop("total_flos", None)
    self.iter += 1
    if not state.is_local_process_zero:
        return
    if self.iter &gt; self.total_iters:
        print(
            f'üéâ {self.smooth_thrput:.2f} samples/sec, {self.smooth_toks_per_dev:.2f} toks/GPU')
        return
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    now = time.perf_counter()
    elapsed = now - self.t0
    self.t0 = now
    thrput = world_size * args.per_device_train_batch_size * \
        args.gradient_accumulation_steps / elapsed
    self.smooth_thrput = self.smooth_thrput * 0.9 + thrput * 0.1
    toks_per_dev = thrput / world_size * self.seq_len
    self.smooth_toks_per_dev = self.smooth_toks_per_dev * 0.9 + toks_per_dev * 0.1
    print(f'üê£ Iter {self.iter}/{self.total_iters}: {elapsed*1000:.2f}ms, {thrput:.2f}({self.smooth_thrput:.2f})samples/sec, {toks_per_dev:.2f}({self.smooth_toks_per_dev:.2f})toks/GPU, gc={gc.get_count()}', end=' ')
</code></pre>
<p>def print_rank0(s):<br />
    if not dist.is_initialized() or dist.get_rank() == 0:<br />
        print(s)</p>
<p>def load_model(model_id: str, layers, embd, vocab_size, zero) -&gt; PreTrainedModel:<br />
    # Reduce dependence on the network environment for llama<br />
    if model_id == 'meta-llama/Llama-2-7b-hf':<br />
        config = LlamaConfig(hidden_size=4096, num_hidden_layers=32,<br />
                             num_attention_heads=32, num_key_value_heads=32, intermediate_size=11008)<br />
    elif model_id == 'meta-llama/Llama-2-13b-hf':<br />
        config = LlamaConfig(hidden_size=5120, num_hidden_layers=40,<br />
                             num_attention_heads=40, num_key_value_heads=40, intermediate_size=13824)<br />
    elif model_id == 'meta-llama/Llama-2-30b-hf':  # not official<br />
        config = LlamaConfig(hidden_size=6400, num_hidden_layers=60,<br />
                             num_attention_heads=40, num_key_value_heads=40, intermediate_size=17280)<br />
    elif model_id == 'meta-llama/Llama-2-70b-hf':<br />
        config = LlamaConfig(hidden_size=8192, num_hidden_layers=80,<br />
                             num_attention_heads=64, num_key_value_heads=8, intermediate_size=28672)<br />
    else:<br />
        if os.path.exists(f'./models/{model_id}'):<br />
            model_id = f'./models/{model_id}'<br />
            print_rank0(f'üíæ Loading model from local {model_id}')<br />
        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)</p>
<pre><code>if layers != -1: 
    if hasattr(config, 'num_hidden_layers'):
        config.num_hidden_layers = layers
    elif hasattr(config, 'num_layers'):
        config.num_layers = layers
if embd != -1:
    if hasattr(config, 'hidden_size'):
        config.hidden_size = embd
    elif hasattr(config, 'n_embd'):
        config.n_embd = embd
if vocab_size != -1:
    if hasattr(config, 'vocab_size'):
        config.vocab_size = vocab_size

if zero:
    import deepspeed
    with deepspeed.zero.Init():
        model = AutoModelForCausalLM.from_config(
            config, trust_remote_code=True)
else:
    with init_empty_weights():
        model = AutoModelForCausalLM.from_config(
            config, trust_remote_code=True).to_empty(device='cpu')
model.config.pad_token_id = model.config.eos_token_id
return model
</code></pre>
<p>def guess_transformer_layer_class(model: torch.nn.Module):<br />
    mlist = [m for m in model.modules() if isinstance(m, torch.nn.ModuleList)]<br />
    return mlist[0][0].<strong>class</strong></p>
<p>class CompileOptimizerTrainer(Trainer):<br />
    def <strong>init</strong>(self, <em>args, </em><em>kwargs):<br />
        super().<strong>init</strong>(</em>args, **kwargs)</p>
<pre><code>def create_optimizer_and_scheduler(self, num_training_steps: int):
    self.optimizer = torch.optim.AdamW(
        self.model.parameters(), lr=torch.scalar_tensor(1e-5), foreach=True, capturable=True)
    self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        self.optimizer, T_max=1000, eta_min=1e-6)
    self.optimizer.step = torch.compile(self.optimizer.step)
</code></pre>
<p>def train(model, args):<br />
    # deepspeed_dict = json.loads(DEEPSPEED_TEMPLATE)<br />
    # deepspeed_dict['zero_optimization']['stage'] = args.zero_stage<br />
    # deepspeed_dict['zero_optimization']['overlap_comm'] = args.overlap_comm<br />
    # deepspeed_dict['wall_clock_breakdown'] = True</p>
<pre><code>input_ids = np.random.randint(100, 30000, (1000, args.seq_len))
data_set = Dataset.from_dict({
    "input_ids": input_ids,
    "labels": input_ids
})

training_args = TrainingArguments(
    log_level='info',
    per_device_train_batch_size=args.mbs,
    gradient_accumulation_steps=args.ga,
    max_steps=args.iters,
    save_steps=10e9,
    logging_steps=1,
    output_dir='./tmp',
    disable_tqdm=True,
    bf16=True,
    torch_compile_backend=args.cc,
)

trainer_cls = CompileOptimizerTrainer if args.cc and args.zero_stage == 0 else Trainer
trainer = trainer_cls(model, args=training_args,
                      train_dataset=data_set,
                      callbacks=[ThrputCb(args.iters, args.seq_len)])
trainer.train()
</code></pre>
<p>if <strong>name</strong> == '<strong>main</strong>':<br />
    if 'RANK' in os.environ and not dist.is_initialized():<br />
        dist.init_process_group(backend='nccl', init_method='env://')<br />
    args = parser.parse_args()<br />
    print_rank0(f'args: {args}')<br />
    if args.zero_stage != 0 and args.fsdp:<br />
        raise ValueError('FSDP and ZeRO are mutually exclusive')<br />
    if args.print_gc:<br />
        gc.set_debug(gc.DEBUG_STATS)<br />
    model: PreTrainedModel = load_model(<br />
        args.model, args.layers, args.embd, args.vocab_size, args.zero_stage != 0)<br />
    print_rank0(f'{model}\nparameters: {model.num_parameters()/1e9:.3f} B')<br />
    train(model, args)<br />
```</p>
<p>Note that <code>--cc=eager</code> works fine.</p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.2.1+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Alibaba Cloud Linux release 3 (Soaring Falcon)  (x86_64)<br />
GCC version: (GCC) 10.2.1 20200825 (Alibaba 10.2.1-3.5 2.32)<br />
Clang version: Could not collect<br />
CMake version: version 3.20.2<br />
Libc version: glibc-2.32</p>
<p>Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.10.134-16.1.al8.x86_64-x86_64-with-glibc2.32<br />
Is CUDA available: True<br />
CUDA runtime version: 12.3.103<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A10<br />
GPU 1: NVIDIA A10</p>
<p>Nvidia driver version: 535.146.02<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:        x86_64<br />
CPU op-mode(s):      32-bit, 64-bit<br />
Byte Order:          Little Endian<br />
CPU(s):              64<br />
On-line CPU(s) list: 0-63<br />
Thread(s) per core:  2<br />
Core(s) per socket:  32<br />
Socket(s):           1<br />
NUMA node(s):        1<br />
Vendor ID:           GenuineIntel<br />
CPU family:          6<br />
Model:               106<br />
Model name:          Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz<br />
Stepping:            6<br />
CPU MHz:             2900.000<br />
BogoMIPS:            5800.00<br />
Hypervisor vendor:   KVM<br />
Virtualization type: full<br />
L1d cache:           48K<br />
L1i cache:           32K<br />
L2 cache:            1280K<br />
L3 cache:            49152K<br />
NUMA node0 CPU(s):   0-63<br />
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat avx512vbmi avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] torch==2.2.1<br />
[pip3] triton==2.2.0<br />
[conda] numpy                     1.26.4                   pypi_0    pypi<br />
[conda] torch                     2.2.1                    pypi_0    pypi<br />
[conda] triton                    2.2.0                    pypi_0    pypi</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 11 Mar 2024 04:41:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121631</guid>
    </item>
    <item>
      <title>[Inductor] [ReImplement] Outer Loop Fusion for CPP Backend</title>
      <link>https://github.com/pytorch/pytorch/pull/121625</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121625</p>
<p><strong>Summary</strong><br />
Re-implement of https://github.com/pytorch/pytorch/pull/121064</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_outer_loop_fusion</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 10 Mar 2024 23:44:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121625</guid>
    </item>
    <item>
      <title>[dynamo] Refactor TorchInGraphFunctionVariable for compile time</title>
      <link>https://github.com/pytorch/pytorch/pull/121616</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121616<br />
* #121615</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Sun, 10 Mar 2024 21:06:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121616</guid>
    </item>
    <item>
      <title>[dynamo] Minor compile time optimizations in torch.py</title>
      <link>https://github.com/pytorch/pytorch/pull/121615</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121616<br />
* <strong>-&gt;</strong> #121615</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Sun, 10 Mar 2024 21:06:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121615</guid>
    </item>
    <item>
      <title>DISABLED test_pynode_destruction_deadlock (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/121576</link>
      <description><![CDATA[<p>Platforms: asan</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_compiled_autograd.py%3A%3ATestAutogradWithCompiledAutograd%3A%3Atest_pynode_destruction_deadlock%22%5D">recent examples</a>).</p>
<p>This test starts to become flaky in trunk today (March 8th) but the root cause is unclear yet.</p>
<p>cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7 @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Sat, 09 Mar 2024 01:57:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121576</guid>
    </item>
    <item>
      <title>[AOTInductor] Reuse generated kernels between constant graph and main graph</title>
      <link>https://github.com/pytorch/pytorch/pull/121564</link>
      <description><![CDATA[<p>Summary: We copy the src_to_kernel from constant graph to main graph so that we could avoid generating duplicating kernels. And pass throught the name counter such that no duplicated names will be generated.</p>
<p>Test Plan: Included in commit</p>
<p>Differential Revision: D54706767</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 17:35:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121564</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Tolerate dynamic shapes</title>
      <link>https://github.com/pytorch/pytorch/pull/121497</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* <strong>-&gt;</strong> #121497<br />
* #121491<br />
* #121490<br />
* #121489</p>
<p>Previously, when the Cutlass backend was<br />
enabled, using dynamic shapes could lead<br />
to exceptions during JIT.</p>
<p>With this change, there are guards in place to just disable the Cutlass backend if dynamic dimensions are involved.</p>
<p>In addition, if no choices for a GEMM are available using the selected backends,<br />
then an ATen Kernel is used as fallback,<br />
even if the ATen backend is not enabled.</p>
<p>Test:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 07:55:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121497</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] DO NOT REVIEW - to be split up</title>
      <link>https://github.com/pytorch/pytorch/pull/121492</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121492<br />
* #121497<br />
* #121491<br />
* #121490<br />
* #121489</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121492</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] GEMM size threshold for Cutlass backend usage</title>
      <link>https://github.com/pytorch/pytorch/pull/121491</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121497<br />
* <strong>-&gt;</strong> #121491<br />
* #121490<br />
* #121489</p>
<ul>
<li>
<p>Adds a configurable GEMM size threshold for the usage of Cutlass GEMM Kernels <strong>_inductor.config.cutlass_backend_min_gemm_size</strong></p>
</li>
<li>
<p>During GEMM algorithm choice generation: <strong>if no viable choices can be generated using the configured backends, the ATen backend will be used as a fallback backend</strong>, even if it is not enabled in <strong>_inductor.config.max_autotune_gemm_backends</strong></p>
</li>
</ul>
<p>Test plan:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121491</guid>
    </item>
    <item>
      <title>[Inductor] Make codecache CUDA compilation more robust &amp; flexible</title>
      <link>https://github.com/pytorch/pytorch/pull/121490</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121497<br />
* #121491<br />
* <strong>-&gt;</strong> #121490<br />
* #121489</p>
<p>Minor changes which make the CUDA compilation within _inductor/codecache.py<br />
more robust and flexible.</p>
<p>Test plan:<br />
CI<br />
Additional test in test_codecache.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121490</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Move tests to separate file</title>
      <link>https://github.com/pytorch/pytorch/pull/121489</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121497<br />
* #121491<br />
* #121490<br />
* <strong>-&gt;</strong> #121489</p>
<p>Move Cutlass backend related tests to test/inductor/test_cutlass_backend.py - no changes to the tests themselves.</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121489</guid>
    </item>
    <item>
      <title>Setting static arguments in `torch.compile()`</title>
      <link>https://github.com/pytorch/pytorch/issues/121299</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>JAX's <code>jax.jit</code> function allows you to set <code>static_argnums</code> or <code>static_argnames</code>. These arguments will be compiled into the function.</p>
<p>https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html</p>
<p>There doesn't seem to be a way to do this in PyTorch.</p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 00:41:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121299</guid>
    </item>
    <item>
      <title>[Inductor] Add prologue fusion</title>
      <link>https://github.com/pytorch/pytorch/pull/121211</link>
      <description><![CDATA[<p>Hi, I am new in PyTorch, but trying to add prologue fusion in the Inductor-Triton path by following the implementation of epilogue fusion. </p>
<p>I mainly changed two parts: scheduler and codegen. Current scheduler gives up fusing prologues such as relu followed by mm due to the mismatch of their dependency types. I changed TemplateBuffer so that it also issues MemoryDep when prologue fusion is enabled in config to make the dependency types match.</p>
<p>Regarding codegen, as a first use case, I changed Triton mm kernel so that it can emit prologue code. Kernel is supposed to receive single type of activation function either as its first parameter, second parameter, or both.</p>
<p>Any comments or suggestions are appreciated.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 00:10:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121211</guid>
    </item>
    <item>
      <title>Add CUTLASS kernel as choice for _int_mm() Inductor autotuning</title>
      <link>https://github.com/pytorch/pytorch/pull/119685</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #119685</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 12 Feb 2024 07:05:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119685</guid>
    </item>
    <item>
      <title>Increased compile time max GPUs to 512. Switched to int16_t DeviceIndex.</title>
      <link>https://github.com/pytorch/pytorch/pull/119639</link>
      <description><![CDATA[<p>Fixes #115331.</p>
<p>This PR increases the number of valid GPU devices to 512 (from 64) in order to future-proof PyTorch for providers that offer <a href="https://www.tensorwave.com/">single nodes with a large device count</a>. Until now, <code>DeviceIndex</code> was an <code>int8_t</code>, thus multiple changes were necessary:</p>
<ul>
<li><code>DeviceIndex</code> changed to <code>int16_t</code>. Updated consumers that assume it to be an <code>int8_t</code>.</li>
<li>Updated bounds checking for <code>torch.device()</code> in the Python frontend. Right now, we allow funny things like <code>torch.device('cpu', 200).index == -56</code>, which is undefined behavior. I inserted some checks to only allow values between 0 and <code>c10::Device::MAX_NUM_DEVICES - 1</code>.</li>
<li>Updated the <code>ArgumentInfo</code> struct as it hardcodes the device index as 8 bit field [^1]. Might be a breaking change, not sure if users rely on this.</li>
<li>Introduced <code>c10::Device::MAX_NUM_DEVICES</code> as a replacement for the old <code>C10_COMPILE_TIME_MAX_GPUS</code></li>
</ul>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225</p>
<p>[^1]: This field was unsigned, so I guess this has also been undef behavior the whole time? Our default device index is -1, so this always wrapped around to 255 when written to the <code>ArgumentInfo</code> struct. When I switched the <code>DeviceIndex</code> to <code>int16_t</code>, it actually stayed 255 after unpacking from <code>ArgumentInfo</code> again, as the <code>DeviceIndex</code> was now wide enough that it didn't wrap back to -1.</p>]]></description>
      <pubDate>Sat, 10 Feb 2024 05:14:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119639</guid>
    </item>
    <item>
      <title>[HOO][torch.compile] Contexts and passing forward node meta to backward node</title>
      <link>https://github.com/pytorch/pytorch/issues/118181</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>Hello,<br />
I have implemented some feature that allows to wrap "contexts" of operations using Higher Order Ops. It basically implements wrap-example-like HOO and variable with some minor differences, namely one causing all ops generated within an HOO to have access to a stack of HOO's that wrapped it, effectively allowing to feed nodes with additional metadata specified by the user and passing this data through functionalization of FWD graph within aot_autograd.</p>
<p>For example, you can consider this pseudocode:</p>
<p>```<br />
def some_subcode_part1(in1, in2, hint):<br />
    tmp1 = in1 * in2 #node A<br />
    tmp2 = in1 / in2 #node B</p>
<pre><code>return tmp1, tmp2
</code></pre>
<p>def some_subcode_part2(tmp1, tmp2, hint):<br />
    out1 = tmp1 + tmp2 #node C<br />
    out2 = tmp1 - tmp2 #node D</p>
<pre><code>return out1, out2
</code></pre>
<p>def some_code(in1, in2, hint):<br />
    tmp1, tmp2 = torch.ops.higher_order.hinted_context(some_subcode_part1, in1, torch.max(in2), hint='{"mode": "1"}')<br />
    out1, out2 = torch.ops.higher_order.hinted_context(some_subcode_part2, tmp1, tmp2, hint='{"mode": "2"}')</p>
<pre><code>return out1, out2
</code></pre>
<p><em>(...somewhere within torch.compiled model...)</em><br />
out1, out2 = torch.ops.higher_order.hinted_context(some_code, out, x, hint='{"tune": "True"}')<br />
```<br />
Feature I have implemented causes nodes A and B to have meta of {tune: True, mode:1} and nodes C and D to have meta of {tune: True, mode:2} as visible in FX graph provided to user fw_compiler. This meta creation happens during creation of joint graph to be precise.</p>
<p>Now, I'd like to extend this feature so that it will also pass this metadata to backward graph created by autograd (BWD nodes should take this metadata from corresponding FWD nodes they were created from). Could you recommend the best approach here to pass it?</p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @ydwu4</p>]]></description>
      <pubDate>Wed, 24 Jan 2024 02:30:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/118181</guid>
    </item>
    <item>
      <title>Check for releasing GIL at compiletime</title>
      <link>https://github.com/pytorch/pytorch/pull/116695</link>
      <description><![CDATA[<p>Introduce <code>conditional_gil_scoped_release</code> and use it in <code>wrap_pybind_function*</code> to avoid a runtime branch making the code cleaner and faster.</p>
<p>@albanD This is the GIL change extracted from #112607 as discussed.</p>
<p>Also fixes a potential use of a moved-from object introduced in #116560:<br />
- <code>f</code> is captured by value in a lambda that may be used called times<br />
- After <code>std::move(f)</code> the lambda is not safe to call anymore</p>
<p>CC @cyyever for that change</p>]]></description>
      <pubDate>Wed, 03 Jan 2024 04:21:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116695</guid>
    </item>
    <item>
      <title>[WIP] Add a simple cache mechanism to accelerate torch.compile-for-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/116368</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to provide a cache mechanism to accelerate torch.compile-for-eager. </p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #116449<br />
* <strong>-&gt;</strong> #116368<br />
* #121387<br />
* #120595<br />
* #121296</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 03:21:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116368</guid>
    </item>
    <item>
      <title>[Quant][PT2E] Enable linear and linear-unary post-op gelu quant recipe for x86 inductor quantizer</title>
      <link>https://github.com/pytorch/pytorch/pull/114853</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #116004<br />
* #114854<br />
* <strong>-&gt;</strong> #114853<br />
* #114852</p>
<p><strong>Summary</strong><br />
Add Gelu for linear-unary post-op quantization recipe to x86 inductor quantizer. </p>
<p><strong>Test plan</strong><br />
python -m pytest test/quantization/pt2e/test_x86inductor_quantizer.py -k test_linear_unary_gelu<br />
python test/test_quantization.py -k test_linear_unary_with_quantizer_api</p>]]></description>
      <pubDate>Wed, 29 Nov 2023 23:53:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/114853</guid>
    </item>
    <item>
      <title>TorchInductor CPU Performance Dashboard</title>
      <link>https://github.com/pytorch/pytorch/issues/93531</link>
      <description><![CDATA[<p>Dashboard to track the performance of torchinductor on CPU.</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @soumith @ngimel @chauhang </p>]]></description>
      <pubDate>Wed, 12 Oct 2022 18:29:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/93531</guid>
    </item>
  </channel>
</rss>
