<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>DISABLED test_comprehensive_randn_like_cuda_float32 (__main__.TestInductorOpInfoCUDA)</title>
      <link>https://github.com/pytorch/pytorch/issues/124758</link>
      <description><![CDATA[<p>Platforms: linux</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_comprehensive_randn_like_cuda_float32&amp;suite=TestInductorOpInfoCUDA&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24160169921">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 3 failures and 3 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_comprehensive_randn_like_cuda_float32</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 970, in test_wrapper
    return test(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 1202, in only_fn
    return fn(self, *args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1937, in wrapper
    fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 1027, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 1027, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1358, in wrapper
    fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1307, in wrapper
    fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/unittest/mock.py", line 1379, in patched
    return func(*newargs, **newkeywargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py", line 443, in inner
    raise e
  File "/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py", line 435, in inner
    fn(self, device, dtype, op)
  File "/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py", line 676, in test_comprehensive
    raise e
  File "/var/lib/jenkins/workspace/test/inductor/test_torchinductor_opinfo.py", line 636, in test_comprehensive
    self.check_model_gpu(
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/var/lib/jenkins/workspace/test/inductor/test_torchinductor.py", line 595, in check_model_gpu
    check_model(
  File "/var/lib/jenkins/workspace/test/inductor/test_torchinductor.py", line 442, in check_model
    actual = run(*example_inputs, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert
    return _compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py", line 70, in wrapper_function
    return function(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform
    tracer.run()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2237, in run
    super().run()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 875, in run
    while self.step():
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 790, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2394, in RETURN_VALUE
    self._return(inst)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2379, in _return
    self.output.compile_subgraph(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1066, in compile_subgraph
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1283, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1374, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1355, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/var/lib/jenkins/workspace/test/inductor/test_torchinductor.py", line 434, in compile_fx_wrapper
    return compile_fx(model_, example_inputs_)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1416, in compile_fx
    return aot_autograd(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 149, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1320, in fw_compiler_base
    return inner_compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 473, in compile_fx_inner
    compiled_graph = FxGraphCache.load(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 887, in load
    compiled_graph = FxGraphCache._lookup_graph(key, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 776, in _lookup_graph
    write_atomic(artifact_path, graph.source_code)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 412, in write_atomic
    with tmp_path.open(write_mode) as f:
  File "/opt/conda/envs/py_3.10/lib/python3.10/pathlib.py", line 1119, in open
    return self._accessor.open(self, mode, buffering, encoding, errors,
torch._dynamo.exc.BackendCompilerFailed: backend='compile_fx_wrapper' raised:
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpe99jauzr/b2/.8748.140378729464448.tmp'

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 2757, in wrapper
    method(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 2757, in wrapper
    method(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 419, in instantiated_test
    result = test(self, **param_kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1358, in wrapper
    fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 976, in test_wrapper
    raise Exception(  # noqa: TRY002
Exception: Caused by sample input at index 0: SampleInput(input=Tensor[size=(), device="cuda:0", dtype=torch.float32], args=(), kwargs={}, broadcasts_input=False, name='')

To execute this test, run the following from the base repo dir:
     python test/inductor/test_torchinductor_opinfo.py -k test_comprehensive_randn_like_cuda_float32

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_torchinductor_opinfo.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 10:39:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124758</guid>
    </item>
    <item>
      <title>DISABLED test_saved_variable_packing_unpacking_did_not_save_original_with_hooks (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124757</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_saved_variable_packing_unpacking_did_not_save_original_with_hooks&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24162863753">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_saved_variable_packing_unpacking_did_not_save_original_with_hooks</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/var/lib/jenkins/pytorch/test/test_autograd.py", line 9151, in test_saved_variable_packing_unpacking_did_not_save_original_with_hooks
    y.sum().backward()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_tensor.py", line 523, in backward
    torch.autograd.backward(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/graph.py", line 767, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 735, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 315, in __call__
    raise e
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 302, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert
    return _compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_utils_internal.py", line 70, in wrapper_function
    return function(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform
    tracer.run()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2237, in run
    super().run()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 875, in run
    while self.step():
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 790, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2394, in RETURN_VALUE
    self._return(inst)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2379, in _return
    self.output.compile_subgraph(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1091, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1283, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1374, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1355, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/__init__.py", line 1781, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
  File "inductor/test_compiled_autograd.py", line 26, in inner_compiler
    return inductor.compile(gm_, example_inputs_)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/__init__.py", line 28, in compile
    return compile_fx(gm, example_inputs, config_patches=options)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1238, in compile_fx
    return flatten_graph_inputs(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 2665, in flatten_graph_inputs
    compiled_fn = compile_gm(GmWrapper(gm, spec), inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1416, in compile_fx
    return aot_autograd(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 155, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1320, in fw_compiler_base
    return inner_compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 477, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 752, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/graph.py", line 1541, in compile_to_fn
    return self.compile_to_module().call
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/graph.py", line 1488, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2405, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_jenkins/ha/chaiwoogobd5cfkipulkgctgiwhxpc43ere47n2n5ulcryagtdke.py", line 50, in <module>
    async_compile.wait(globals())
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 3007, in wait
    scope[key] = result.result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2816, in result
    return self.result_fn()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2285, in future
    result = get_result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2122, in load_fn
    future.result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/_base.py", line 444, in result
    return self.__get_result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2147, in _worker_compile_cpp
    compile_file(input_path, output_path, shlex.split(cmd))
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2018, in compile_file
    raise exc.CppCompileError(cmd, output) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inner_compiler' raised:
CppCompileError: C++ compile error

Command:
g++ /tmp/torchinductor_jenkins/jx/cjx4wsy4zstguukcsz4w26l3xawtfsnbmnfaegz242swwijtfpev.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=1 -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/envs/py_3.8/include/python3.8 -L/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/lib -L/opt/conda/envs/py_3.8/lib -L/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -mavx2 -mfma -DCPU_CAPABILITY_AVX2 -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_jenkins/jx/cjx4wsy4zstguukcsz4w26l3xawtfsnbmnfaegz242swwijtfpev.so

Output:
/tmp/torchinductor_jenkins/jx/cjx4wsy4zstguukcsz4w26l3xawtfsnbmnfaegz242swwijtfpev.cpp:2:10: fatal error: /tmp/tmpnb7gu59g/z2/cz2uvkefmshwlhxxsbghzvp6zv66yqdenm36rax6nft66odb4erj.h: No such file or directory
    2 | #include "/tmp/tmpnb7gu59g/z2/cz2uvkefmshwlhxxsbghzvp6zv66yqdenm36rax6nft66odb4erj.h"
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test/test_autograd.py -k test_saved_variable_packing_unpacking_did_not_save_original_with_hooks

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 10:39:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124757</guid>
    </item>
    <item>
      <title>[inductor] Keep inductor cache for tests when TORCH_COMPILE_DEBUG is specified</title>
      <link>https://github.com/pytorch/pytorch/pull/124755</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124755</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @chauhang</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 10:33:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124755</guid>
    </item>
    <item>
      <title>Improved unbacked SymInt input support in Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/124739</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124740<br />
* <strong>-&gt;</strong> #124739<br />
* #124658<br />
* #124394<br />
* #124316<br />
* #124314<br />
* #124310<br />
* #124297<br />
* #124290</p>
<p>This is a subset of changes extracted from https://github.com/pytorch/pytorch/pull/124683/</p>
<p>This PR contains modifications to make Inductor work with unbacked symbol inputs, which can occur when a data-dependent sized tensor is saved for backwards. The problems to be fixed:</p>
<ul>
<li>When binding initial symbols, we unconditionally bind unbacked symbols (instead of computing if they are needed, which only looks at backed symbols)</li>
<li>Benchmark generation code doesn't work with unbacked symints as we have no hints to actually feed in real values. So I pick a random number and you are expected to fix it if it doesn't work</li>
<li>Need to make sure we don't install dependencies on unbacked SymInt inputs, that puts us down the "promptly deallocate the input" path, but that's pointless for unbacked SymInt</li>
</ul>
<p>Fixes https://github.com/pytorch/pytorch/issues/124652</p>
<p>Test case is in the next PR in the stack which is split for CI purposes, I'll fold them together when CI is done.</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 08:33:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124739</guid>
    </item>
    <item>
      <title>DISABLED test_saved_variable_packing_unpacking_did_not_save_original_with_default_hooks (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124733</link>
      <description><![CDATA[<p>Platforms: linux, rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_saved_variable_packing_unpacking_did_not_save_original_with_default_hooks&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24144541139">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_saved_variable_packing_unpacking_did_not_save_original_with_default_hooks</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/var/lib/jenkins/pytorch/test/test_autograd.py", line 9215, in test_saved_variable_packing_unpacking_did_not_save_original_with_default_hooks
    y.sum().backward()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_tensor.py", line 523, in backward
    torch.autograd.backward(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/graph.py", line 767, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 735, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 315, in __call__
    raise e
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 302, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert
    return _compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_utils_internal.py", line 70, in wrapper_function
    return function(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform
    tracer.run()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2237, in run
    super().run()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 875, in run
    while self.step():
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 790, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2394, in RETURN_VALUE
    self._return(inst)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2379, in _return
    self.output.compile_subgraph(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1082, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1274, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1365, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1346, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/__init__.py", line 1781, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
  File "inductor/test_compiled_autograd.py", line 26, in inner_compiler
    return inductor.compile(gm_, example_inputs_)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/__init__.py", line 28, in compile
    return compile_fx(gm, example_inputs, config_patches=options)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1238, in compile_fx
    return flatten_graph_inputs(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 2665, in flatten_graph_inputs
    compiled_fn = compile_gm(GmWrapper(gm, spec), inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1416, in compile_fx
    return aot_autograd(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 149, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1320, in fw_compiler_base
    return inner_compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 477, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 752, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/graph.py", line 1549, in compile_to_fn
    return self.compile_to_module().call
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/graph.py", line 1496, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2405, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_jenkins/r3/cr3e2bunv7lmvn6gcgot2vb7jpnlpnoaz7ujk3hbc6dimabinc4q.py", line 50, in <module>
    async_compile.wait(globals())
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 3007, in wait
    scope[key] = result.result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2816, in result
    return self.result_fn()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2285, in future
    result = get_result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2122, in load_fn
    future.result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/_base.py", line 444, in result
    return self.__get_result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2147, in _worker_compile_cpp
    compile_file(input_path, output_path, shlex.split(cmd))
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2018, in compile_file
    raise exc.CppCompileError(cmd, output) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inner_compiler' raised:
CppCompileError: C++ compile error

Command:
g++ /tmp/torchinductor_jenkins/t4/ct42zcd4pysqmanytbe7plobzgm3tbi76jdfzp6wjbgev4puij7h.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=1 -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/envs/py_3.8/include/python3.8 -L/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/lib -L/opt/conda/envs/py_3.8/lib -L/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -mavx2 -mfma -DCPU_CAPABILITY_AVX2 -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_jenkins/t4/ct42zcd4pysqmanytbe7plobzgm3tbi76jdfzp6wjbgev4puij7h.so

Output:
/tmp/torchinductor_jenkins/t4/ct42zcd4pysqmanytbe7plobzgm3tbi76jdfzp6wjbgev4puij7h.cpp:2:10: fatal error: /tmp/tmpc1u04dg6/z2/cz2uvkefmshwlhxxsbghzvp6zv66yqdenm36rax6nft66odb4erj.h: No such file or directory
    2 | #include "/tmp/tmpc1u04dg6/z2/cz2uvkefmshwlhxxsbghzvp6zv66yqdenm36rax6nft66odb4erj.h"
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test/test_autograd.py -k test_saved_variable_packing_unpacking_did_not_save_original_with_default_hooks

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 07:39:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124733</guid>
    </item>
    <item>
      <title>DISABLED test_saved_tensor_hooks_custom_function_intermediates (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124723</link>
      <description><![CDATA[<p>Platforms: rocm, asan, linux, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_saved_tensor_hooks_custom_function_intermediates&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24136857563">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_saved_tensor_hooks_custom_function_intermediates</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/var/lib/jenkins/pytorch/test/test_autograd.py", line 9313, in test_saved_tensor_hooks_custom_function_intermediates
    out.backward()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_tensor.py", line 534, in backward
    torch.autograd.backward(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/graph.py", line 767, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 735, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 315, in __call__
    raise e
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 302, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert
    return _compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_utils_internal.py", line 70, in wrapper_function
    return function(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform
    tracer.run()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2237, in run
    super().run()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 875, in run
    while self.step():
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 790, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2394, in RETURN_VALUE
    self._return(inst)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2379, in _return
    self.output.compile_subgraph(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1082, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1274, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1365, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1346, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/__init__.py", line 1781, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
  File "inductor/test_compiled_autograd.py", line 26, in inner_compiler
    return inductor.compile(gm_, example_inputs_)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/__init__.py", line 28, in compile
    return compile_fx(gm, example_inputs, config_patches=options)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1238, in compile_fx
    return flatten_graph_inputs(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 2665, in flatten_graph_inputs
    compiled_fn = compile_gm(GmWrapper(gm, spec), inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1416, in compile_fx
    return aot_autograd(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 149, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1320, in fw_compiler_base
    return inner_compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 477, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 752, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/graph.py", line 1541, in compile_to_fn
    return self.compile_to_module().call
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/graph.py", line 1488, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2405, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_jenkins/nf/cnf5eck2imx7ln6exlkmmmyn4rogv7mz324qhqobrowm7aohewoo.py", line 46, in <module>
    async_compile.wait(globals())
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 3007, in wait
    scope[key] = result.result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2816, in result
    return self.result_fn()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2285, in future
    result = get_result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2122, in load_fn
    future.result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/_base.py", line 444, in result
    return self.__get_result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2147, in _worker_compile_cpp
    compile_file(input_path, output_path, shlex.split(cmd))
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2018, in compile_file
    raise exc.CppCompileError(cmd, output) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inner_compiler' raised:
CppCompileError: C++ compile error

Command:
g++ /tmp/torchinductor_jenkins/ev/cevghumy3koschb5qcj3orx73fv37tdp35vx72tbzf6w6e3rq27k.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=1 -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/envs/py_3.8/include/python3.8 -L/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/lib -L/opt/conda/envs/py_3.8/lib -L/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -mavx2 -mfma -DCPU_CAPABILITY_AVX2 -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_jenkins/ev/cevghumy3koschb5qcj3orx73fv37tdp35vx72tbzf6w6e3rq27k.so

Output:
/tmp/torchinductor_jenkins/ev/cevghumy3koschb5qcj3orx73fv37tdp35vx72tbzf6w6e3rq27k.cpp:2:10: fatal error: /tmp/tmp2xcjack_/z2/cz2uvkefmshwlhxxsbghzvp6zv66yqdenm36rax6nft66odb4erj.h: No such file or directory
    2 | #include "/tmp/tmp2xcjack_/z2/cz2uvkefmshwlhxxsbghzvp6zv66yqdenm36rax6nft66odb4erj.h"
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test/test_autograd.py -k test_saved_tensor_hooks_custom_function_intermediates

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 04:45:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124723</guid>
    </item>
    <item>
      <title>Ensure only builtins functions are wrapped in new frame for torch.compile</title>
      <link>https://github.com/pytorch/pytorch/pull/124720</link>
      <description><![CDATA[<p>Fixes #124269</p>
<p>I'm not sure which cases the existing check via <code>filename</code> is supposed to handle, but if the comment is right and <code>external_utils.wrap_inline</code> is supposed to be applied only on builtin functions, a check for <code>types.BuiltinFunctionType</code> should be sufficient.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 02:52:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124720</guid>
    </item>
    <item>
      <title>[CI] Add freezing for cpu inductor accuracy test in inductor CI</title>
      <link>https://github.com/pytorch/pytorch/pull/124715</link>
      <description><![CDATA[<p>This PR is to enable '--freezing' when running dynamo accuracy check in CI. <br />
Backgroud:<br />
ISSUES#124286 is not captured by CI since freezing is not enabled for cpu-inductor.</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 00:14:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124715</guid>
    </item>
    <item>
      <title>DISABLED test_save_on_cpu_and_checkpoint (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124706</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_save_on_cpu_and_checkpoint&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24130672204">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 21 failures and 7 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_save_on_cpu_and_checkpoint</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/var/lib/jenkins/pytorch/test/test_autograd.py", line 9319, in test_save_on_cpu_and_checkpoint
    b.sum().backward()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_tensor.py", line 534, in backward
    torch.autograd.backward(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/graph.py", line 767, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 735, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 315, in __call__
    raise e
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 302, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert
    return _compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_utils_internal.py", line 70, in wrapper_function
    return function(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform
    tracer.run()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2237, in run
    super().run()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 875, in run
    while self.step():
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 790, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2394, in RETURN_VALUE
    self._return(inst)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2379, in _return
    self.output.compile_subgraph(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1082, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1274, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1365, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1346, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/__init__.py", line 1781, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
  File "inductor/test_compiled_autograd.py", line 26, in inner_compiler
    return inductor.compile(gm_, example_inputs_)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/__init__.py", line 28, in compile
    return compile_fx(gm, example_inputs, config_patches=options)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1238, in compile_fx
    return flatten_graph_inputs(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 2665, in flatten_graph_inputs
    compiled_fn = compile_gm(GmWrapper(gm, spec), inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1416, in compile_fx
    return aot_autograd(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 149, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1320, in fw_compiler_base
    return inner_compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 477, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 752, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/graph.py", line 1541, in compile_to_fn
    return self.compile_to_module().call
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/graph.py", line 1488, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2405, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_jenkins/cq/ccqbo2v2fzggqm2mcork6qs4mubazscelegqext3q6j7igb5sfbn.py", line 64, in <module>
    async_compile.wait(globals())
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 3007, in wait
    scope[key] = result.result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2816, in result
    return self.result_fn()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2285, in future
    result = get_result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2122, in load_fn
    future.result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/_base.py", line 444, in result
    return self.__get_result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2147, in _worker_compile_cpp
    compile_file(input_path, output_path, shlex.split(cmd))
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2018, in compile_file
    raise exc.CppCompileError(cmd, output) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inner_compiler' raised:
CppCompileError: C++ compile error

Command:
g++ /tmp/torchinductor_jenkins/df/cdf7czvuqi3idj7frfalsuiok5mbky7rrnhudmjo3h4ugt23piae.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=1 -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/envs/py_3.8/include/python3.8 -L/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/lib -L/opt/conda/envs/py_3.8/lib -L/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -mavx2 -mfma -DCPU_CAPABILITY_AVX2 -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_jenkins/df/cdf7czvuqi3idj7frfalsuiok5mbky7rrnhudmjo3h4ugt23piae.so

Output:
/tmp/torchinductor_jenkins/df/cdf7czvuqi3idj7frfalsuiok5mbky7rrnhudmjo3h4ugt23piae.cpp:2:10: fatal error: /tmp/tmpkxa9d9lq/z2/cz2uvkefmshwlhxxsbghzvp6zv66yqdenm36rax6nft66odb4erj.h: No such file or directory
    2 | #include "/tmp/tmpkxa9d9lq/z2/cz2uvkefmshwlhxxsbghzvp6zv66yqdenm36rax6nft66odb4erj.h"
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test/test_autograd.py -k test_save_on_cpu_and_checkpoint

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 22:40:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124706</guid>
    </item>
    <item>
      <title> [WIP][Inductor Intel GPU backend Upstream] Reuse inductor test for Intel GPU (PART 3)</title>
      <link>https://github.com/pytorch/pytorch/pull/124702</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124702<br />
* #124147<br />
* #122866</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 22:10:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124702</guid>
    </item>
    <item>
      <title>[INDUCTOR] [CPU] [GPT-FAST-MOE] large perf regression with coordinate_descent_tuning disabled</title>
      <link>https://github.com/pytorch/pytorch/issues/124697</link>
      <description><![CDATA[<h3> Describe the bug</h3>
<p>When the flag <code>coordinate_descent_tuning</code> disabled, GPT-FAST-MOE encounters a large perf regression: 52s -&gt; 1049s. The impact of disabling it on CPU is to fallback bmm and mm in decomposition.</p>
<h3>Code snippet</h3>
<p>```python<br />
import torch<br />
from torch import nn, Tensor<br />
from torch.nn import functional as F<br />
import torch._inductor.config</p>
<p>torch._inductor.config.cpp.enable_kernel_profile = True<br />
torch._inductor.config.coordinate_descent_tuning = False # True</p>
<p>dim = 4096<br />
num_experts = 8<br />
num_activated_experts = 2<br />
intermediate_size = 14336</p>
<p>class ConditionalFeedForwardBit8(nn.Module):<br />
    def <strong>init</strong>(self, target_dtype):<br />
        super().<strong>init</strong>()</p>
<pre><code>    self.target_dtype = target_dtype

    self.register_buffer("w1", torch.empty(num_experts, intermediate_size, dim, dtype=target_dtype))
    self.register_buffer("w2", torch.empty(num_experts, dim, intermediate_size, dtype=target_dtype))
    self.register_buffer("w3", torch.empty(num_experts, intermediate_size, dim, dtype=target_dtype))

    self.register_buffer("scales1", torch.empty(num_experts, intermediate_size, dtype=torch.bfloat16))
    self.register_buffer("scales2", torch.empty(num_experts, dim, dtype=torch.bfloat16))
    self.register_buffer("scales3", torch.empty(num_experts, intermediate_size, dtype=torch.bfloat16))

def forward(self, x: Tensor, expert_indices: Tensor) -&gt; Tensor:
    w1_weights = self.w1.to(x.dtype)[expert_indices] # [T, A, D, D]
    w3_weights = self.w3.to(x.dtype)[expert_indices] # [T, A, D, D]
    w2_weights = self.w2.to(x.dtype)[expert_indices]
    x1 = F.silu(torch.einsum('ti,taoi -&gt; tao', x, w1_weights) * self.scales1[expert_indices].to(x.dtype))
    x3 = torch.einsum('ti, taoi -&gt; tao', x, w3_weights) * self.scales3[expert_indices].to(x.dtype)
    expert_outs = torch.einsum('tao, taio -&gt; tai', (x1 * x3), w2_weights) * self.scales2[expert_indices].to(x.dtype)  # [T, A, D, D]
    return expert_outs
</code></pre>
<p>def linear_forward_int8(x, weight_int8pack, scales, out_features):<br />
    origin_x_size = x.size()<br />
    x = x.reshape(-1, origin_x_size[-1])<br />
    c = torch.ops.aten._weight_int8pack_mm(x, weight_int8pack, scales)<br />
    new_shape = origin_x_size[:-1] + (out_features,)<br />
    c = c.reshape(new_shape)<br />
    return c</p>
<p>class WeightOnlyBit8Linear(torch.nn.Module):<br />
<strong>constants</strong> = ['in_features', 'out_features']<br />
    in_features: int<br />
    out_features: int<br />
    weight: torch.Tensor</p>
<pre><code>def __init__(self, in_features: int, out_features: int, bias: bool = True,
             device=None, dtype=None, target_dtype=None) -&gt; None:
    factory_kwargs = {'device': device, 'dtype': dtype}
    super().__init__()
    self.in_features = in_features
    self.out_features = out_features
    self.register_buffer("weight", torch.empty((out_features, in_features), dtype=target_dtype))
    self.register_buffer("scales", torch.ones(out_features, dtype=torch.bfloat16))

def forward(self, input: torch.Tensor) -&gt; torch.Tensor:
    return linear_forward_int8(
       input,
       self.weight, self.scales, self.out_features)
</code></pre>
<p>class MOEFeedForward(nn.Module):<br />
    def <strong>init</strong>(self) -&gt; None:<br />
        super().<strong>init</strong>()<br />
        self.gate = WeightOnlyBit8Linear(dim, num_experts, bias=False, target_dtype=torch.int8)<br />
        self.cond_ffn = ConditionalFeedForwardBit8(torch.int8)<br />
        self.dim = dim<br />
        self.num_activated_experts = num_activated_experts</p>
<pre><code>def forward(self, x: Tensor) -&gt; Tensor:
    x = x.view(-1, self.dim)
    # T = num_tokens, E = num_experts, D = hidden dim, A = activated experts
    # x: [T, D]
    scores = self.gate(x) # [T, E]
    expert_weights = F.softmax(scores, dim=-1)
    expert_weights, expert_indices = torch.topk(expert_weights, self.num_activated_experts, dim=-1) # [T, A], [T, A]
    expert_weights /= expert_weights.sum(dim=-1, keepdim=True) # [T, A]
    expert_outs = self.cond_ffn(x, expert_indices)
    return torch.einsum('tai,ta -&gt; ti', expert_outs, expert_weights)
</code></pre>
<p>mod = MOEFeedForward()<br />
input_shape = (1, 1, dim,)<br />
x = torch.randn(input_shape, dtype=torch.bfloat16)<br />
compiled_mod = torch.compile(mod)</p>
<p>for i in range(5):<br />
  y = compiled_mod(x)</p>
<p>prof = torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU])<br />
with prof:<br />
  y = compiled_mod(x) <br />
prof.export_chrome_trace("test_moe.json")<br />
print(prof.key_averages().table(sort_by="self_cpu_time_total"))<br />
```</p>
<h3>Profiling</h3>
<p>coordinate_descent_tuning=True<br />
```</p>
<hr />
<pre><code>                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
</code></pre>
<hr />
<pre><code>  graph_0_cpp_fused_bmm_sum_1        82.07%     884.479us        82.07%     884.479us     884.479us             1
        Torch-Compiled Region        11.72%     126.292us        98.82%       1.065ms       1.065ms             1
    aten::_weight_int8pack_mm         2.65%      28.601us         3.03%      32.667us      32.667us             1
                   aten::topk         1.33%      14.381us         1.33%      14.381us      14.381us             1
     TorchDynamo Cache Lookup         1.18%      12.767us         1.18%      12.767us      12.767us             1
inductor::_reinterpret_tensor         0.60%       6.493us         0.60%       6.493us       2.164us             3
                  aten::empty         0.38%       4.066us         0.38%       4.066us       4.066us             1
 graph_0_cpp_fused__softmax_0         0.06%       0.698us         0.06%       0.698us       0.698us             1
</code></pre>
<hr />
<p>Self CPU time total: 1.078ms<br />
```</p>
<p>coordinate_descent_tuning=False<br />
```</p>
<hr />
<pre><code>                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
</code></pre>
<hr />
<pre><code>   graph_0_cpp_fused__to_copy_index_1        61.29%      96.359ms        61.29%      96.359ms      96.359ms             1
                          aten::copy_        33.98%      53.422ms        33.98%      53.422ms      17.807ms             3
                            aten::bmm         4.16%       6.542ms        38.20%      60.056ms      15.014ms             4
                Torch-Compiled Region         0.35%     550.429us        99.98%     157.187ms     157.187ms             1
   graph_0_cpp_fused_index_mul_silu_2         0.07%     106.154us         0.07%     106.154us     106.154us             1
            aten::_weight_int8pack_mm         0.02%      37.146us         0.03%      42.080us      42.080us             1
        inductor::_reinterpret_tensor         0.02%      37.033us         0.02%      37.033us       3.086us            12
                          aten::clone         0.02%      33.289us        34.02%      53.486ms      17.829ms             3
             TorchDynamo Cache Lookup         0.02%      31.353us         0.02%      31.353us      31.353us             1
                           aten::topk         0.02%      24.819us         0.02%      24.819us      24.819us             1
                          aten::empty         0.01%      22.710us         0.01%      22.710us       5.678us             4
                     aten::contiguous         0.01%      16.971us        34.03%      53.503ms      17.834ms             3
                     aten::empty_like         0.01%      12.382us         0.02%      30.158us      10.053us             3
graph_0_cpp_fused_div_index_mul_sum_3         0.01%       9.925us         0.01%       9.925us       9.925us             1
                     aten::as_strided         0.00%       7.801us         0.00%       7.801us       3.900us             2
                   aten::resolve_conj         0.00%       3.070us         0.00%       3.070us       0.384us             8
         graph_0_cpp_fused__softmax_0         0.00%       1.890us         0.00%       1.890us       1.890us             1
</code></pre>
<hr />
<p>Self CPU time total: 157.218ms<br />
```</p>
<h3>Analysis</h3>
<p>According to the current analysis, there are two main reasons:<br />
- With flag disabled, bmm breaks the origin c++ kernel and some redundant calculations are generated to store uint8 to bf16 and then convert to fp32. The conversion from uint8 to bf16 takes a long time.<br />
- Many of the bmm kernels do additional contiguous in <a href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/mkldnn/Matmul.cpp#L298">Mkldnn Matmul</a> for non-contiguous and non-transposed format, which could not be solved by https://github.com/pytorch/pytorch/pull/122599).</p>
<h3>Versions</h3>
<p>PyTorch: 34bce27f0d12bf7226b37dfe365660aad456701a</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @jgong5 @leslie-fang-intel @yanbing-j @mingfeima </p>]]></description>
      <pubDate>Mon, 22 Apr 2024 19:04:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124697</guid>
    </item>
    <item>
      <title>DISABLED test_return_leaf_inplace (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124690</link>
      <description><![CDATA[<p>Platforms: rocm, asan, linux, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_return_leaf_inplace&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24121722822">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_return_leaf_inplace</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/var/lib/jenkins/pytorch/test/test_autograd.py", line 3757, in test_return_leaf_inplace
    q.sum().backward()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_tensor.py", line 534, in backward
    torch.autograd.backward(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/graph.py", line 767, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 735, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 315, in __call__
    raise e
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/fx/graph_module.py", line 302, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert
    return _compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_utils_internal.py", line 70, in wrapper_function
    return function(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform
    tracer.run()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2237, in run
    super().run()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 875, in run
    while self.step():
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 790, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2394, in RETURN_VALUE
    self._return(inst)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 2379, in _return
    self.output.compile_subgraph(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1082, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1274, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1365, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 1346, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/__init__.py", line 1781, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
  File "inductor/test_compiled_autograd.py", line 26, in inner_compiler
    return inductor.compile(gm_, example_inputs_)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/__init__.py", line 28, in compile
    return compile_fx(gm, example_inputs, config_patches=options)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1238, in compile_fx
    return flatten_graph_inputs(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 2665, in flatten_graph_inputs
    compiled_fn = compile_gm(GmWrapper(gm, spec), inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1416, in compile_fx
    return aot_autograd(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 149, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 1320, in fw_compiler_base
    return inner_compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 477, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/opt/conda/envs/py_3.8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 752, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/graph.py", line 1541, in compile_to_fn
    return self.compile_to_module().call
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/graph.py", line 1488, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2394, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/tmpgcspo91n/il/cildowh7mn45xj2qdipqs73kfp6pjysvcjhytpweznx6ppoiy5ki.py", line 53, in <module>
    async_compile.wait(globals())
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2996, in wait
    scope[key] = result.result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2805, in result
    return self.result_fn()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2274, in future
    result = get_result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2111, in load_fn
    future.result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/_base.py", line 444, in result
    return self.__get_result()
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/opt/conda/envs/py_3.8/lib/python3.8/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2136, in _worker_compile_cpp
    compile_file(input_path, output_path, shlex.split(cmd))
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 2007, in compile_file
    raise exc.CppCompileError(cmd, output) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inner_compiler' raised:
CppCompileError: C++ compile error

Command:
g++ /tmp/tmpgcspo91n/ku/ckum6m63swt74vcfevjty3o3ukihzvsi3cjzakuan3ovdxb44iwe.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=1 -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/envs/py_3.8/include/python3.8 -L/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/lib -L/opt/conda/envs/py_3.8/lib -L/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -mavx2 -mfma -DCPU_CAPABILITY_AVX2 -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/tmpgcspo91n/ku/ckum6m63swt74vcfevjty3o3ukihzvsi3cjzakuan3ovdxb44iwe.so

Output:
/tmp/tmpgcspo91n/ku/ckum6m63swt74vcfevjty3o3ukihzvsi3cjzakuan3ovdxb44iwe.cpp:2:10: fatal error: /tmp/tmpgcspo91n/z2/cz2uvkefmshwlhxxsbghzvp6zv66yqdenm36rax6nft66odb4erj.h: No such file or directory
    2 | #include "/tmp/tmpgcspo91n/z2/cz2uvkefmshwlhxxsbghzvp6zv66yqdenm36rax6nft66odb4erj.h"
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test/test_autograd.py -k test_return_leaf_inplace

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 16:57:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124690</guid>
    </item>
    <item>
      <title>Unbacked SymInts: Should backwards graph with unbacked SymInts be recompiled with hints</title>
      <link>https://github.com/pytorch/pytorch/issues/124686</link>
      <description><![CDATA[<h3> Describe the bug</h3>
<p>While working on https://github.com/pytorch/pytorch/issues/124652 I discovered that when we have a tensor with data-dependent size saved for backwards, we end up with symbolic sizes that are unbacked as inputs to the graph.</p>
<p>Now, if we ahead of time compile the backwards graph, there is no way to deal with these inputs. However, if we were willing to delay compilation (e.g., because backwards had to be delayed, or perhaps https://docs.google.com/document/d/1EHEMwSzBPOsmk0EDztRXlGwEo_TizVc5jkW_5Gp5eRs/edit#heading=h.moip2jp9ghoz ) we could give hints to all of these integers, as we'd waited long enough to actually see what their values are.</p>
<p>Should we do this?</p>
<h3>Versions</h3>
<p>main</p>
<p>cc @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 16:23:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124686</guid>
    </item>
    <item>
      <title>Initial implementation of Inductor FX Graph Remote Cache</title>
      <link>https://github.com/pytorch/pytorch/pull/124669</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* (to be filled)</p>
<p>This diff implements a remote caching strategy (memcache for internal and redis for external) for caching of Inductor FX Graph to Inductor generated wrapper file.</p>
<p>It uses the same idea with the autotuning result cache that is currently live.</p>
<p>This will land turned off and before turning this on by default, I will do more testing and including looking at the dynamic shape guards added by inductor.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D56441624/">D56441624</a></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 14:06:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124669</guid>
    </item>
    <item>
      <title>inductor creates unnecessary buffers</title>
      <link>https://github.com/pytorch/pytorch/issues/124653</link>
      <description><![CDATA[<h3> Describe the bug</h3>
<p>Following creates two triton kernels instead of one. In fact the <code>buf0</code> is unused.</p>
<p>```python<br />
import torch, itertools</p>
<p>n = 200<br />
a = torch.randn((n, n, n), device='cuda')</p>
<p>def fn(a):<br />
    t = (a, a + 1, a + 2)<br />
    shape = a.shape<br />
    m = 1<br />
    for dim in range(len(shape)):<br />
        view_shape = [1]*(dim + 1)<br />
        view_shape[dim] = -1<br />
        b = (torch.arange(shape[dim], device=a.device).view(view_shape))<br />
        m = torch.mul(m, b)<br />
    return sum(torch.mul(t1, m) for t1 in t)</p>
<p>torch.compile(fn)(a);<br />
```</p>
<details>

```
# AOT ID: ['0_inference']
from ctypes import c_void_p, c_long
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align

from torch import device, empty_strided
from torch._inductor.codecache import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
alloc_from_pool = torch.ops.inductor._alloc_from_pool
reinterpret_tensor = torch.ops.inductor._reinterpret_tensor
async_compile = AsyncCompile()


# kernel path: /tmp/torchinductor_isuruf/gw/cgwpfgkdk4yia3bpt37vqjdjda6rp635xory4heq5y2ea46iaqyt.py
# Source Nodes: [arange, b, m, m_1, m_2], Original ATen: [aten.arange, aten.mul, aten.view]
# arange => iota
# b => view
# m => mul
# m_1 => mul_1
# m_2 => mul_2
triton_poi_fused_arange_mul_view_0 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor

@triton_heuristics.pointwise(
    size_hints=[256], 
    filename=__file__,
    triton_meta={'signature': {0: '*i64', 1: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0,), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_arange_mul_view_0', 'mutated_arg_names': [], 'no_x_dim': False, 'backend_hash': '13d5379970a55f2f2c4bb8dbeb907c03d2af7e5fb1c9d1b1aa5bf5794d5f2277', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_(out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = x0*x0*x0
    tl.store(out_ptr0 + (x0), tmp0, xmask)
''', device_str='cuda')

import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream


# kernel path: /tmp/torchinductor_isuruf/vx/cvx4wnrm6ktqek4r6kmhrqqb3nlid4qanygazjhwmvx26svg4yl7.py
# Source Nodes: [add_2, add_3, add_4, mul_3, mul_4, mul_5, t1, t1_1], Original ATen: [aten.add, aten.mul]
# add_2 => add_2
# add_3 => add_3
# add_4 => add_4
# mul_3 => mul_3
# mul_4 => mul_4
# mul_5 => mul_5
# t1 => add
# t1_1 => add_1
triton_poi_fused_add_mul_1 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor

@triton_heuristics.pointwise(
    size_hints=[8388608], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_1', 'mutated_arg_names': [], 'no_x_dim': False, 'backend_hash': '13d5379970a55f2f2c4bb8dbeb907c03d2af7e5fb1c9d1b1aa5bf5794d5f2277', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 8000000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = xindex % 200
    tmp0 = tl.load(in_ptr0 + (x2), xmask)
    tmp1 = x0*x0*x0
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 * tmp2
    tmp4 = 0.0
    tmp5 = tmp3 + tmp4
    tmp6 = 1.0
    tmp7 = tmp0 + tmp6
    tmp8 = tmp7 * tmp2
    tmp9 = tmp5 + tmp8
    tmp10 = 2.0
    tmp11 = tmp0 + tmp10
    tmp12 = tmp11 * tmp2
    tmp13 = tmp9 + tmp12
    tl.store(out_ptr0 + (x2), tmp13, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (200, 200, 200), (40000, 200, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((1, 1, 200), (200, 200, 1), torch.int64)
        # Source Nodes: [arange, b, m, m_1, m_2], Original ATen: [aten.arange, aten.mul, aten.view]
        stream0 = get_raw_stream(0)
        triton_poi_fused_arange_mul_view_0.run(buf0, 200, grid=grid(200), stream=stream0)
        buf1 = empty_strided_cuda((200, 200, 200), (40000, 200, 1), torch.float32)
        # Source Nodes: [add_2, add_3, add_4, mul_3, mul_4, mul_5, t1, t1_1], Original ATen: [aten.add, aten.mul]
        triton_poi_fused_add_mul_1.run(arg0_1, buf1, 8000000, grid=grid(8000000), stream=stream0)
        del arg0_1
    return (buf0, buf1, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((200, 200, 200), (40000, 200, 1), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([arg0_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)

```

</details>

<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.4.0a0+gitf5ad149<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.2<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.2 LTS (x86_64)<br />
GCC version: (conda-forge gcc 12.3.0-2) 12.3.0<br />
Clang version: Could not collect<br />
CMake version: version 3.27.6<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.8.18 | packaged by conda-forge | (default, Oct 10 2023, 15:44:36)  [GCC 12.3.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-97-generic-x86_64-with-glibc2.10<br />
Is CUDA available: True<br />
CUDA runtime version: 12.2.140<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA GeForce RTX 2060<br />
GPU 1: NVIDIA GeForce RTX 2060</p>
<p>Nvidia driver version: 545.23.08<br />
cuDNN version: Probably one of the following:<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8<br />
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8<br />
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn.so.9<br />
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_adv.so.9<br />
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_cnn.so.9<br />
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_engines_precompiled.so.9<br />
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_engines_runtime_compiled.so.9<br />
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_graph.so.9<br />
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_heuristic.so.9<br />
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_ops.so.9<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: False</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      43 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             64<br />
On-line CPU(s) list:                0-63<br />
Vendor ID:                          AuthenticAMD<br />
Model name:                         AMD Ryzen Threadripper 3970X 32-Core Processor<br />
CPU family:                         23<br />
Model:                              49<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 32<br />
Socket(s):                          1<br />
Stepping:                           0<br />
Frequency boost:                    enabled<br />
CPU max MHz:                        3700.0000<br />
CPU min MHz:                        2200.0000<br />
BogoMIPS:                           7400.38<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es<br />
Virtualization:                     AMD-V<br />
L1d cache:                          1 MiB (32 instances)<br />
L1i cache:                          1 MiB (32 instances)<br />
L2 cache:                           16 MiB (32 instances)<br />
L3 cache:                           128 MiB (8 instances)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-63<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection<br />
Vulnerability Spec rstack overflow: Mitigation; safe RET<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==7.0.0<br />
[pip3] flake8-bugbear==23.3.23<br />
[pip3] flake8-comprehensions==3.12.0<br />
[pip3] flake8-executable==2.1.3<br />
[pip3] flake8-logging-format==0.9.0<br />
[pip3] flake8-pyi==23.3.1<br />
[pip3] flake8-simplify==0.19.3<br />
[pip3] mypy==1.9.0<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.24.3<br />
[pip3] onnx==1.15.0<br />
[pip3] onnxruntime==1.17.0<br />
[pip3] onnxscript==0.1.0.dev20240117<br />
[pip3] optree==0.11.0<br />
[pip3] pytorch-triton==3.0.0+989adb9a29<br />
[pip3] torch==2.3.0a0+gitf9f602f<br />
[pip3] torchvision==0.16.1+cf89794<br />
[conda] libmagma                  2.7.2                h173bb3b_0    conda-forge<br />
[conda] libmagma_sparse           2.7.2                h173bb3b_0    conda-forge<br />
[conda] libopenvino-pytorch-frontend 2023.1.0             h59595ed_1    conda-forge<br />
[conda] magma                     2.7.2                h51420fd_0    conda-forge<br />
[conda] mkl                       2022.2.1         h84fe81f_16997    conda-forge<br />
[conda] mkl-include               2023.2.0         h84fe81f_50495    conda-forge<br />
[conda] numpy                     1.24.3                   pypi_0    pypi<br />
[conda] optree                    0.11.0                   pypi_0    pypi<br />
[conda] pytorch-triton            3.0.0+989adb9a29          pypi_0    pypi<br />
[conda] torch                     2.4.0a0+giteae45c6           dev_0    <develop><br />
[conda] torchfix                  0.4.0                    pypi_0    pypi<br />
[conda] torchvision               0.16.1          cpu_py38h901811f_1    conda-forge<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 13:06:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124653</guid>
    </item>
    <item>
      <title>[inductor] Add option to create parent directory for write_atomic</title>
      <link>https://github.com/pytorch/pytorch/pull/124646</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124646</p>
<p>In #124640 I see the error</p>
<p><code>File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 887, in load
    compiled_graph = FxGraphCache._lookup_graph(key, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 776, in _lookup_graph
    write_atomic(artifact_path, graph.source_code)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 412, in write_atomic
    with tmp_path.open(write_mode) as f:
  File "/opt/conda/envs/py_3.10/lib/python3.10/pathlib.py", line 1119, in open
    return self._accessor.open(self, mode, buffering, encoding, errors,
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp02wlik2v/iu/.28383.139931139675904.tmp'</code></p>
<p>Which is fixed by creating the parent directory first. Since this is what you<br />
want to do in most cases, I add an argument to <code>write_atomic</code> to do so itself.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 11:52:34 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124646</guid>
    </item>
    <item>
      <title>DISABLED test_comprehensive_amax_cuda_float16 (__main__.TestInductorOpInfoCUDA)</title>
      <link>https://github.com/pytorch/pytorch/issues/124640</link>
      <description><![CDATA[<p>Platforms: inductor</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_comprehensive_amax_cuda_float16&amp;suite=TestInductorOpInfoCUDA&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24102915018">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_comprehensive_amax_cuda_float16</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_torchinductor_opinfo.py</code></p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @clee2000 @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 10:40:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124640</guid>
    </item>
    <item>
      <title>[inductor] allow clone cse cache during vectorized indirect load</title>
      <link>https://github.com/pytorch/pytorch/pull/124597</link>
      <description><![CDATA[<p>Fix https://github.com/pytorch/pytorch/issues/123502</p>
<p><code>swap_buffer</code> do not clone the <code>cse.cache</code> which will bring redundant computation.<br />
We may able to clone the <code>cse.cache</code> if there is no cse value in the <code>expr</code><br />
<code>auto tmp8 =
[&amp;]
{
    __at_align__ std::array&lt;int64_t, 16&gt; tmpbuf;
    tmp7.store(tmpbuf.data());
    return tmpbuf;
}
()
;
//
// other codes
//
// also store tmp7 here (redundant tmp16)
auto tmp16 =
[&amp;]
{
    __at_align__ std::array&lt;int64_t, 16&gt; tmpbuf;
    tmp7.store(tmpbuf.data());
    return tmpbuf;
}
()
;</code></p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124597</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 21 Apr 2024 22:47:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124597</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Improved GEMM template</title>
      <link>https://github.com/pytorch/pytorch/pull/124577</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* <strong>-&gt;</strong> #124577<br />
* #124576<br />
* #124575<br />
* #124574<br />
* #124107<br />
* #121734</p>
<p>Improves the Cutlass backend GEMM template:</p>
<ul>
<li>Adds code which allows to create stand-alone test runners for Cutlass GEMM Kernels, which allows (manual) debugging of, for example, CUDA IMA errors or similar problems which occur in practice. Includes some utility code and tests to actually compile and run these standalone tests.</li>
<li>Cleans up the GEMM template code through various refactorings</li>
<li>Eliminates code sections and options that are unneccessary now that epilogue fusions are being removed.</li>
<li>Puts some CPU runtime checks into #if / #endif blocks, such that it's possible to compile CUTLASS Kernels with lower CPU overhead.</li>
<li>Add documentation comments</li>
</ul>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 21 Apr 2024 14:58:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124577</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Improve GEMM op filtering</title>
      <link>https://github.com/pytorch/pytorch/pull/124576</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124577<br />
* <strong>-&gt;</strong> #124576<br />
* #124575<br />
* #124574<br />
* #124107<br />
* #121734</p>
<p>Add configurable allowlist / denylist regular expressions to make it possible to exclude certain<br />
CUTLASS GEMM implementations ( for example "pingpong" Kernels due to undesired numerical behavior ).</p>
<p>Remove usage of old 2.x Cutlass Kernels entirely.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 21 Apr 2024 14:55:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124576</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] clean up CUTLASSGemmTemplate.add_cutlass_gemm_choices</title>
      <link>https://github.com/pytorch/pytorch/pull/124575</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124577<br />
* #124576<br />
* <strong>-&gt;</strong> #124575<br />
* #124574<br />
* #124107<br />
* #121734</p>
<p>Clean up CUTLASSGemmTemplate.add_cutlass_gemm_choices, removing code that became unneccessary by removing EVT-based epilogue fusion.</p>
<p>Test Plan:<br />
Already covered by test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 21 Apr 2024 14:53:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124575</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Set INDUCTOR_TEST_DISABLE_FRESH_CACHE in test setup</title>
      <link>https://github.com/pytorch/pytorch/pull/124574</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124577<br />
* #124576<br />
* #124575<br />
* <strong>-&gt;</strong> #124574<br />
* #124107<br />
* #121734</p>
<p>The diff https://github.com/pytorch/pytorch/pull/122661 introduces a new automatic cache refresh mechanism during all inductor-derived test cases.</p>
<p>But this refresh mechanism seems not to work properly across process boundaries, specifically when using  autotune_in_subproc, which many tests in test_cutlass_backend.py rely on.</p>
<p>Solution: Set the env var INDUCTOR_TEST_DISABLE_FRESH_CACHE=1<br />
early during test setup within test_cutlass_backend.py</p>
<p>Test Plan:<br />
This is a change to unit tests only.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 21 Apr 2024 14:51:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124574</guid>
    </item>
    <item>
      <title>torch.compile does not work since 2.2.1 on MacOS for some models</title>
      <link>https://github.com/pytorch/pytorch/issues/124497</link>
      <description><![CDATA[<h3> Describe the bug</h3>
<p>The execution hangs when first calling the model to warm-up. <em>After</em> will never be printed. This is on an Apple M3 Max. Smaller models work sometimes.</p>
<p>```python<br />
import torch<br />
from transformers import AutoModelForImageClassification</p>
<p>neural_network = AutoModelForImageClassification.from_pretrained("microsoft/resnet-50")<br />
input_data = torch.randn(1, 3, 228, 228)<br />
print(neural_network(input_data))<br />
neural_network_c = torch.compile(neural_network, backend="inductor")<br />
print("Before")<br />
neural_network_c(input_data)<br />
print("After")<br />
```</p>
<p>The last version that worked was torch 2.2.0. </p>
<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.2.1<br />
Is debug build: False<br />
CUDA used to build PyTorch: None<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: macOS 14.4.1 (arm64)<br />
GCC version: Could not collect<br />
Clang version: 15.0.0 (clang-1500.3.9.4)<br />
CMake version: version 3.28.4<br />
Libc version: N/A</p>
<p>Python version: 3.11.8 (main, Feb  6 2024, 21:21:21) [Clang 15.0.0 (clang-1500.1.0.2.5)] (64-bit runtime)<br />
Python platform: macOS-14.4.1-arm64-arm-64bit<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Apple M3 Max</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] onnx==1.16.0<br />
[pip3] torch==2.2.1<br />
[conda] Could not collect<br />
```</p>
<p>Thanks for your help!</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @malfet @albanD @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 08:14:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124497</guid>
    </item>
    <item>
      <title>DISABLED test_inplace (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124446</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, slow, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_inplace&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23996309781">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_inplace</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 16:56:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124446</guid>
    </item>
    <item>
      <title>DISABLED test_indexing_duplicates (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124430</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_indexing_duplicates&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23990352147">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_indexing_duplicates</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 13:39:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124430</guid>
    </item>
    <item>
      <title>DISABLED test_indexing (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124420</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, slow, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_indexing&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23985486255">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_indexing</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 10:39:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124420</guid>
    </item>
    <item>
      <title>Reimplement unbacked symbol bindings in Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/124394</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124740<br />
* #124739<br />
* #124658<br />
* <strong>-&gt;</strong> #124394<br />
* #124316<br />
* #124314<br />
* #124310<br />
* #124297<br />
* #124290</p>
<p>This PR has a lot of "draw the rest of the fucking owl" energy. Here's how to break it down.</p>
<ol>
<li><strong>torch/_inductor/graph.py</strong> - We start by tightening unbacked symbol invariants. Specifically, as we lower FX nodes, we check whether or not every unbacked_binding recorded on the FX node meta, actually ends up getting bound (according to get_unbacked_symbol_defs) in all the buffers generated by the lowering. Hopefully this invariant is self evident. This leads to a lot of failures.</li>
<li><strong>torch/_inductor/ir.py</strong> - Problem 1: There is softness in how Inductor computes defs of unbacked symbols in IR node. Previously, we tried to infer it by looking at the output sizes/strides/etc and see if new unbacked symbols popped up that we hadn't seen in the inputs. I don't know exactly what was buggy about the old code, but sometimes we would fail to notice an unbacked symbol had been bound, or rebind an unbacked symbol multiple times. Fortunately, thanks to the earlier PRs in our stack, we now have a nice list of unbacked symbol bindings from FX, so we now just store it directly on ExternKernel and use it directly to report defs. This has to be done twice: once for FallbackKernel (e.g., nonzero) and once for DynamicScalar (e.g., item) (see also <strong>torch/_inductor/lowering.py</strong>, <strong>torch/_inductor/codegen/wrapper.py</strong> and  <strong>torch/_inductor/codegen/cpp_wrapper_cpu.py</strong> for the lowering and codegen changes for item)</li>
<li><strong>process_kernel</strong> - Sidequest! It turns out that Inductor lowering can reallocate unbacked symbols. This happens specifically when we repropagate fake tensors through the operator in <code>process_kernel</code>. This repropagation process is necessary because Inductor may have changed the strides of input tensors, and it must now recompute the strides so that it can continue to appropriately plan the rest of the lowering process. This is fine: we just make sure we do the rebind unbacked + compute_unbacked_bindings dance we've been doing previously in the PR stack. But instead of putting unbacked_bindings on a new FX node, they go straight into our unbacked_bindings on the Inductor IR node.<ul>
<li><strong>codegen_unbacked_symbol_defs</strong> - Sidequest! FallbackKernel lowering is done in two steps. First, you emit the FallbackKernel buffer. Then, you emit MultiOutput buffers which actually give access to the individual outputs of FallbackKernel, which may have been multi-output. There is a design decision here: does the FallbackKernel bind the unbacked symbols, or the MultiOutput buffer? Historically, we put the binding on MultiOutput buffer, because it's more convenient: the FallbackKernel buffer is fake, in fact, it doesn't even get a name in C++ codegen. But it's kind of inconsistent with the keypath model that we've been tracking unbacked bindings with: if you have a multi-output node, you'd expect a keypath like <code>[0].size()[0]</code> representing the first output's first dimension size. That suggests that it's the FallbackKernel that should define the things. So that was my first implementation. Unfortunately, the C++ codegen is too cursed and I could not understand how to make it work in that case. So now we just unsoundly assume you cannot have multi-output data dependent output, and do the codegen in MultiOutput. There are some comments explaining exactly what we are improperly assuming.</li>
</ul>
</li>
<li><strong>_rename_unbacked_to</strong> in <strong>torch/fx/experimental/symbolic_shapes.py</strong> - Previously, when we renamed unbacked symbols, we clobbered any facts we previously knew about them. So for example, if we had a replacement <code>u0 -&gt; s0</code> but then we renamed u0 to u1, we would now setup the replacement <code>u0 -&gt; u1</code>, clobbering the old replacement. This apparently didn't matter in earlier PRs in the stack, but with Inductor now on the ball, there were some tests that indicated this was a problem. The solution is easy: if u0 had a preexisting replacement, reapply it to u1. However...<ul>
<li><strong>torch/_functorch/_aot_autograd/collect_metadata_analysis.py</strong> - When we run forward analysis, this triggers fake tensor repropagation and fresh allocations. Previously, we just cleared out the pending symbols when finished the analysis. But with the change above, this would also migrate replacements to the new symbols... which are now dead. So now we explicitly suppress generation of these symbols with <code>ignore_fresh_unbacked_symbols</code> so that no rebinding happens at all.</li>
<li><strong>torch/_dynamo/eval_frame.py</strong> - same deal; I just searched for all sites we called clear() on pending</li>
</ul>
</li>
<li>The last step is fixing the long tail of extra problems that show up, now that unbacked_bindings are load bearing into Inductor<ul>
<li><strong>torch/_dynamo/eval_frame.py</strong> - Some of the exports are making copies of nodes without repropagating fake tensors, so in this case, it is important to also copy the <code>unbacked_bindings</code> (apparently this didn't matter before without the Inductor changes)</li>
<li><strong>torch/_export/pass_base.py</strong> - I discover that this is doing fake tensor repropagation via a test suite failure. Do the same playbook as AOTAutograd: PropagateUnbackedSymInts too!  Actually, they also have implemented their own tracer as well, so do the same playbook as proxy_tensor: record unbacked_bindings on the newly traced nodes. UGH code duplication.</li>
<li><strong>torch/_subclasses/fake_tensor.py</strong>, <strong>torch/_subclasses/fake_impls.py</strong> (with call site updates at  <strong>torch/_functorch/_aot_autograd/traced_function_transforms.py</strong> and <strong>torch/fx/passes/fake_tensor_prop.py</strong>) - What's this new epoch thing? I noticed that sometimes I would be retracing, call nonzero() on a fake tensor, and not allocate a new unbacked symbol. This is actually bad, because if I don't get a new unbacked symbol, I don't know there's a binding site, and <code>unbacked_bindings</code> is now missing a binding. The reason for this is memoization: if I reuse the exact same fake tensor on my retrace, it will already have an unbacked symint memoized on it and we will short circuit allocation. Well, that's no good. So I associate the memos with a fake tensor epoch, and every time you start a new fake tensor propagation from scratch, you bump the epoch so that I clear all the memos.</li>
<li><strong>torch/_inductor/scheduler.py</strong> - I notice in unit tests that V.current_node is not always set when we call process_kernel. So I save it into the IR node and restore it when we are running <code>get_estimated_runtime</code>.</li>
<li><strong>torch/fx/experimental/symbolic_shapes.py</strong> - A few things</li>
<li><strong>rebind_unbacked</strong> (re <strong>_tensor_version</strong>). Ordinarily, when you have an unbacked SymInt, you persistently hvae it all the way to the end of the program. <code>_tensor_version</code> violates this: this generates an unbacked SymInt (for reasons I don't quite understand?) and then gets rid of it later. This triggered an assert violation. I think this op is kind of misusing unbacked SymInt, but I didn't know how to refactor it, so it gets a special case.</li>
<li><strong>rebind_unbacked</strong> (re <strong>Simplify SymBool binding</strong>). Ugh, SymBool, what a pain in the butt. I have an assert that you can only rebind unbacked symbol to another unbacked symbol. This assert fails when a boolean is involved, because the result of running keypath on the result is not <code>u1</code>, it's <code>sympy.Piecewise(... sympy.Eq(u1, 1) ...)</code>. This is actually just <code>u1</code>, but Sympy doesn't know it because it doesn't know that <code>u1</code> value range is <code>[0, 1]</code>. So we manually implement the simplification needed to get the assert to pass.</li>
<li><strong>compute_unbacked_bindings</strong> (re <strong>This is pretty fragile</strong>). There is a really funny disaster involving memoization and Inductor process kernel. Ordinarily when I retrace, if there was a memo hit in the old trace, there will be a memo hit in the new trace. However, Inductor process kernel breaks this, because it recreates fake tensor inputs to the operator call from scratch (since they might have different strides), and obviously these tensor inputs don't have the memo from the old one. I tried a little bit to try to manually transplant the memo to the new fake tensor but it seemed hopeless, so I just let the fresh symbol ride, allocating a new unbacked symbol. However, in one of our tests, we rely on knowing that the first nonzero call is equal to the second (memoized) nonzero call. The equality test looked pretty easy to discharge, so I just went ahead and added a deferred runtime assert to this effect and it worked.</li>
</ul>
</li>
</ol>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 18 Apr 2024 06:23:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124394</guid>
    </item>
    <item>
      <title>[inductor] switch assume_aligned_inputs to False</title>
      <link>https://github.com/pytorch/pytorch/pull/124336</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124336<br />
* #123319</p>
<p>In #123319, we guard some behavior behind the <code>assume_aligned_inputs</code> config option. If we set this to <code>False</code>, then the behavior added in #123319 becomes the default behavior. See the referenced PR for more details about the behavior affected.</p>
<p>Side effects:<br />
* It's possible that this will hurt performance in some scenarios. For example, if an unaligned input is used in a matmul, it might be better to perform the clone to align it first.<br />
* This will occasionally cause recompiles. Specifically: the check we perform (<code>(storage_offset * get_dtype_size(dtype)) % ALIGNMENT == 0</code>) can be guarded on if the storage_offset becomes dynamic. storage_offset becomes dynamic during automatic_dynamic_shapes after a shape or stride changes. Previously, this was increasing graph breaks in cpu inductor torchbench tests (but is fixed by more carefully guarding checks on alignment, so that we don't run them and generate guards unless actually needed).</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 15:14:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124336</guid>
    </item>
    <item>
      <title>[torch.compile] torch._dynamo.exc.TorchRuntimeError: Failed running call_function &lt;method 'numpy' of 'torch._C.TensorBase' objects&gt;(*(FakeTensor(..., size=(32, 3, 64, 64)),), **{})</title>
      <link>https://github.com/pytorch/pytorch/issues/124247</link>
      <description><![CDATA[<h3> Describe the bug</h3>
<p>The following testcase can reproduce this issue:<br />
```<br />
import torch</p>
<p>def func(x):<br />
    # return torch.add(x, 1).numpy()<br />
    return torch.Tensor.numpy(torch.add(x, 1))</p>
<p>x = torch.randn(32, 3, 64, 64)<br />
compiled_func = torch.compile(func)</p>
<p>with torch.no_grad():<br />
    compiled_func(x)<br />
```</p>
<p>The error message:<br />
Traceback (most recent call last):<br />
  File "/home/jiayisun/pytorch/1.py", line 10, in <module><br />
    compiled_func(x)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/eval_frame.py", line 387, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/convert_frame.py", line 977, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state, skip=1)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/convert_frame.py", line 818, in _convert_frame<br />
    result = inner_convert(<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert<br />
    return _compile(<br />
  File "/home/jiayisun/pytorch/torch/_utils_internal.py", line 70, in wrapper_function<br />
    return function(</em>args, <strong>kwargs)<br />
  File "/home/jiayisun/anaconda3/envs/pt/lib/python3.9/contextlib.py", line 79, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/convert_frame.py", line 700, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/utils.py", line 266, in time_wrapper<br />
    r = func(<em>args, </em><em>kwargs)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/convert_frame.py", line 568, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/convert_frame.py", line 173, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/convert_frame.py", line 515, in transform<br />
    tracer.run()<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/symbolic_convert.py", line 2237, in run<br />
    super().run()<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/symbolic_convert.py", line 875, in run<br />
    while self.step():<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/symbolic_convert.py", line 790, in step<br />
    self.dispatch_table<a href="self, inst">inst.opcode</a><br />
  File "/home/jiayisun/pytorch/torch/_dynamo/symbolic_convert.py", line 492, in wrapper<br />
    return inner_fn(self, inst)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/symbolic_convert.py", line 1260, in CALL_FUNCTION<br />
    self.call_function(fn, args, {})<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/symbolic_convert.py", line 730, in call_function<br />
    self.push(fn.call_function(self, args, kwargs))<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/variables/torch.py", line 747, in call_function<br />
    tensor_variable = wrap_fx_proxy(<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/variables/builder.py", line 1425, in wrap_fx_proxy<br />
    return wrap_fx_proxy_cls(target_cls=TensorVariable, </strong>kwargs)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/variables/builder.py", line 1510, in wrap_fx_proxy_cls<br />
    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/utils.py", line 1804, in get_fake_value<br />
    raise TorchRuntimeError(str(e)).with_traceback(e.<strong>traceback</strong>) from None<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/utils.py", line 1736, in get_fake_value<br />
    ret_val = wrap_fake_exception(<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/utils.py", line 1251, in wrap_fake_exception<br />
    return fn()<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/utils.py", line 1737, in <lambda><br />
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/utils.py", line 1872, in run_node<br />
    raise RuntimeError(make_error_message(e)).with_traceback(<br />
  File "/home/jiayisun/pytorch/torch/_dynamo/utils.py", line 1854, in run_node<br />
    return node.target(<em>args, </em><em>kwargs)<br />
torch._dynamo.exc.TorchRuntimeError: Failed running call_function <method 'numpy' of 'torch._C.TensorBase' objects>(</em>(FakeTensor(..., size=(32, 3, 64, 64)),), **{}):<br />
.numpy() is not supported for tensor subclasses.</p>
<p>from user code:<br />
   File "/home/jiayisun/pytorch/1.py", line 5, in func<br />
    return torch.Tensor.numpy(torch.add(x, 1))</p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.4.0a0+git19f5033<br />
Is debug build: False<br />
CUDA used to build PyTorch: None<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 11.1.0-1ubuntu1~20.04) 11.1.0<br />
Clang version: 9.0.1-12<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.15 (main, Nov 11 2022, 13:58:57)  [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-102-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Byte Order:                         Little Endian<br />
Address sizes:                      52 bits physical, 57 bits virtual<br />
CPU(s):                             128<br />
On-line CPU(s) list:                0-127<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 32<br />
Socket(s):                          2<br />
NUMA node(s):                       2<br />
Vendor ID:                          GenuineIntel<br />
CPU family:                         6<br />
Model:                              106<br />
Model name:                         Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz<br />
Stepping:                           6<br />
CPU MHz:                            2600.000<br />
CPU max MHz:                        3400.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           5200.00<br />
L1d cache:                          3 MiB<br />
L1i cache:                          2 MiB<br />
L2 cache:                           80 MiB<br />
L3 cache:                           96 MiB<br />
NUMA node0 CPU(s):                  0-31,64-95<br />
NUMA node1 CPU(s):                  32-63,96-127<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] clip-anytorch==2.5.2<br />
[pip3] CoCa-pytorch==0.0.7<br />
[pip3] dalle2-pytorch==1.14.2<br />
[pip3] ema-pytorch==0.2.3<br />
[pip3] flake8==6.0.0<br />
[pip3] flake8-bugbear==23.3.23<br />
[pip3] flake8-comprehensions==3.12.0<br />
[pip3] flake8-executable==2.1.3<br />
[pip3] flake8-logging-format==0.9.0<br />
[pip3] flake8-pyi==23.3.1<br />
[pip3] flake8-simplify==0.19.3<br />
[pip3] intel-extension-for-pytorch==2.3.0+gitaf812a6<br />
[pip3] mypy==1.7.0<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.26.3<br />
[pip3] open-clip-torch==2.20.0<br />
[pip3] optree==0.9.1<br />
[pip3] pytorch-warmup==0.1.1<br />
[pip3] rotary-embedding-torch==0.2.7<br />
[pip3] torch==2.4.0a0+git1fd9e32<br />
[pip3] torch-fidelity==0.3.0<br />
[pip3] torch_geometric==2.4.0<br />
[pip3] torchaudio==2.1.0a0+5ee254e<br />
[pip3] torchmetrics==1.1.1<br />
[pip3] torchvision==0.16.0a0+6472a5c<br />
[pip3] vector-quantize-pytorch==1.7.0<br />
[conda] clip-anytorch             2.5.2                    pypi_0    pypi<br />
[conda] coca-pytorch              0.0.7                    pypi_0    pypi<br />
[conda] dalle2-pytorch            1.14.2                   pypi_0    pypi<br />
[conda] ema-pytorch               0.2.3                    pypi_0    pypi<br />
[conda] intel-extension-for-pytorch 2.3.0+gitaf812a6           dev_0    <develop><br />
[conda] mkl                       2023.0.0            intel_25398    intel<br />
[conda] mkl-include               2023.1.0                 pypi_0    pypi<br />
[conda] mkl-static                2023.1.0                 pypi_0    pypi<br />
[conda] numpy                     1.26.3                   pypi_0    pypi<br />
[conda] open-clip-torch           2.20.0                   pypi_0    pypi<br />
[conda] optree                    0.9.1                    pypi_0    pypi<br />
[conda] pytorch-warmup            0.1.1                    pypi_0    pypi<br />
[conda] rotary-embedding-torch    0.2.7                    pypi_0    pypi<br />
[conda] torch                     2.4.0a0+git1fd9e32           dev_0    <develop><br />
[conda] torch-fidelity            0.3.0                    pypi_0    pypi<br />
[conda] torch-geometric           2.4.0                    pypi_0    pypi<br />
[conda] torchaudio                2.1.0a0+5ee254e          pypi_0    pypi<br />
[conda] torchmetrics              1.1.1                    pypi_0    pypi<br />
[conda] torchvision               0.16.0a0+6472a5c          pypi_0    pypi<br />
[conda] vector-quantize-pytorch   1.7.0                    pypi_0    pypi</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @chenyang78 @kadeng</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 18:17:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124247</guid>
    </item>
    <item>
      <title>[torch.compile][FlopCounter] AssertionError: Global is not OptimizedModule._orig_mod</title>
      <link>https://github.com/pytorch/pytorch/issues/124196</link>
      <description><![CDATA[<h3> Describe the bug</h3>
<p>The error is coming from the <code>FlopCounterMode</code>. This seems like a tensor subclass. So, opening an issue.</p>
<p>The resulting flops probably needs to be cleaned up as well.</p>
<p>cc @Chillee @ezyang @zou3519 @albanD @samdow @msaroufim @bdhirsh @chauhang </p>
<p>~~~<br />
import torch<br />
from torch.utils.flop_counter import FlopCounterMode</p>
<p>class SimpleMod(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.linear = torch.nn.Linear(3, 3)</p>
<pre><code>def forward(self, x):
    return torch.sin(self.linear(x))
</code></pre>
<p>inp = torch.randn(3, 3, device='cuda', requires_grad=True)</p>
<h1>mod = torch.nn.Linear(3, 3).to(device="cuda")</h1>
<p>mod = SimpleMod().to(device="cuda")</p>
<p>mod = torch.compile(mod, backend="eager")</p>
<p>flop_counter = FlopCounterMode(mod)</p>
<p>with flop_counter:<br />
    for i in range(20):<br />
        mod(inp).sum().backward()<br />
~~~</p>
<h3>Error logs</h3>
<p>~~~<br />
Module                         FLOP    % Total</p>
<hr />
<p>Global                          216    100.00%<br />
 - aten.addmm                   108     50.00%<br />
 - aten.mm                      108     50.00%<br />
 OptimizedModule                162     75.00%<br />
  - aten.addmm                   54     25.00%<br />
  - aten.mm                     108     50.00%<br />
  OptimizedModule._orig_mod     162     75.00%<br />
   - aten.addmm                  54     25.00%<br />
   - aten.mm                    108     50.00%<br />
Traceback (most recent call last):<br />
  File "/data/users/anijain/pytorch2/examples/valerie.py", line 23, in <module><br />
    mod(inp).sum().backward()<br />
    ^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/nn/modules/module.py", line 1582, in _call_impl<br />
    result = forward_call(</em>args, <strong>kwargs)<br />
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/_dynamo/eval_frame.py", line 387, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/nn/modules/module.py", line 1595, in _call_impl<br />
    hook_result = hook(self, args, result)<br />
                  ^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/utils/flop_counter.py", line 412, in f<br />
    outputs = _pytreeify_preserve_structure(self._create_post_module(name))(outputs)<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/utils/flop_counter.py", line 330, in nf<br />
    out = f(</em>flat_args)<br />
          ^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/autograd/function.py", line 571, in apply<br />
    return super().apply(<em>args, </em>*kwargs)  # type: ignore[misc]<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/utils/flop_counter.py", line 420, in forward<br />
    assert self.parents[-1] == name, f"{self.parents[-1]} is not {name}"<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^<br />
AssertionError: Global is not OptimizedModule._orig_mod<br />
~~~</p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>N/A</p>
<p>cc @ezyang @msaroufim @bdhirsh @chauhang</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 10:08:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124196</guid>
    </item>
    <item>
      <title>[1/N] Scalar Support: Enable aot compile to support aten operations with scalar input like alpha</title>
      <link>https://github.com/pytorch/pytorch/pull/124177</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124070<br />
* <strong>-&gt;</strong> #124177<br />
* #116368<br />
* #121387<br />
* #123545</p>
<p>Some operations have a scalar input parameter, like <code>torch.add(a, b, alpha=2.0)</code>.  Currently, the aot compile does not support such a case because it requires the signature of the captured graph to align with the operation's signature. This means that some inputs in the captured graph may be scalar(float, int, bool, etc.). It breaks the assumption of <code>compile_fx_aot</code> as it assumes all the example inputs are tensor - https://github.com/pytorch/pytorch/blob/0f6ce45bcbd7026c00da43db0317ede10830378b/torch/_inductor/compile_fx.py#L1048</p>
<p>This PR intends to support such cases by allowing not-aligned signature and filtering out the non-Tensor parameters. </p>
<p>Captured graph for <code>torch.add(a, b, alpha=2.0)</code></p>
<p>```<br />
opcode         name      target           args              kwargs</p>
<hr />
<p>placeholder    arg0_1    arg0_1           ()                {}<br />
placeholder    arg1_1    arg1_1           ()                {}<br />
call_function  add       aten.add.Tensor  (arg0_1, arg1_1)  {'alpha': 2.0}<br />
output         output_1  output           ((add,),)         {}<br />
```</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 07:05:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124177</guid>
    </item>
    <item>
      <title>[WIP] [Inductor Intel GPU backend Upstream] Reuse inductor test for Intel GPU (PART 2)</title>
      <link>https://github.com/pytorch/pytorch/pull/124147</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124702<br />
* <strong>-&gt;</strong> #124147<br />
* #122866</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 21:37:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124147</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Disable epilogue fusions</title>
      <link>https://github.com/pytorch/pytorch/pull/124107</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124577<br />
* #124576<br />
* #124575<br />
* #124574<br />
* <strong>-&gt;</strong> #124107<br />
* #121734</p>
<p>This diff disables Cutlass backend EVT epilogue fusions. It does not yet contain the removal of most of the underlying implementation. </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 13:23:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124107</guid>
    </item>
    <item>
      <title>[2/N] Scalar Support: Add scalar to the cache for eager-through-torch.compile</title>
      <link>https://github.com/pytorch/pytorch/pull/124070</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124070<br />
* #124177<br />
* #116368<br />
* #121387<br />
* #123545</p>
<p>Add scalar information to the kernel configuration.</p>
<h4>Additional Context</h4>
<p>Currently, the input parameters are orchestrated by input order in the kernel configuration and loaded/mapped to the kernel at runtime. For example, the cache order of the input parameters of <code>torch.add(a, b, alpha=2.0)</code> is <code>a' first, followed by</code>b<code>and then</code>alpha`. The same order is for cache loading.</p>
<p>However, the orchestration mechanism does not support kwargs because the order of kwargs is useless. For example, the <code>out</code> of <code>aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -&gt; Tensor(a!)</code> may be before <code>approximate</code>. We will support it with subsequent PRs. </p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 07:04:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124070</guid>
    </item>
    <item>
      <title>[Inductor] Force the parallel depth as outer loop fusion depth</title>
      <link>https://github.com/pytorch/pytorch/pull/123899</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123899</p>
<p><strong>Summary</strong><br />
Fix issue: https://github.com/pytorch/pytorch/issues/123801 which brings performance regression of <code>pyhpc_turbulent_kinetic_energy</code> after outer loop fusion.</p>
<p><strong>Root Cause</strong></p>
<ul>
<li><a href="https://gist.github.com/leslie-fang-intel/54fe21ac8871fc63b9bf20fdb6edf209">Generated Kernel before Outer Loop Fusion</a></li>
<li>Taking below 2 kernels as example:<ul>
<li><a href="https://gist.github.com/leslie-fang-intel/54fe21ac8871fc63b9bf20fdb6edf209#file-pyhpc_turbulent_kinetic_energy-before-outer-loop-fusion-py-L255-L305">Kernel 0</a> has 2 loop levels with size [200, 200]. Parallelization is not feasible due to the inefficient number of elements determined by <a href="https://github.com/pytorch/pytorch/blob/aaec97a40364bb6ccfd968f28d309cfff8748d20/torch/_inductor/codegen/cpp.py#L2145-L2164"><code>decide_parallel_depth</code></a>. Therefore, the loop code will be generated with the <code>#pragma omp single</code> directive.</li>
<li><a href="https://gist.github.com/leslie-fang-intel/54fe21ac8871fc63b9bf20fdb6edf209#file-pyhpc_turbulent_kinetic_energy-before-outer-loop-fusion-py-L306-L316">Kernel 1</a> has 3 loop levels with size [200, 200, 26] which has enough number of elements to be parallelized.</li>
</ul>
</li>
<li><a href="https://gist.github.com/leslie-fang-intel/57a497b9d9c6aa82b1c6a686292fc887">Generated Kernel after Outer Loop Fusion</a></li>
<li>After outer loop fusion, <code>Kernel0</code> and <code>Kernel1</code> has been fused into one <a href="https://gist.github.com/leslie-fang-intel/57a497b9d9c6aa82b1c6a686292fc887#file-pyhpc_turbulent_kinetic_energy-after-outer-loop-fusion-py-L261-L497">OuterLoopFusedKernel</a>, the outer loop size is [200, 200] which does not contain enough number of elements to do parallelization.</li>
</ul>
<p>In this PR, we propose a fix for <code>loop_nest</code> involving <code>OuterLoopFusedKernel</code>. The fix entails adding a specific heuristic for <code>OuterLoopFusedKernel</code> to determine the parallel depth by combining <code>outer_loop_fusion_depth</code> with the internal kernels' parallel depth.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 11 Apr 2024 18:18:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123899</guid>
    </item>
    <item>
      <title>[inductor] Specialize on unguarded alignment of example inputs</title>
      <link>https://github.com/pytorch/pytorch/pull/123319</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124336<br />
* <strong>-&gt;</strong> #123319</p>
<p>When inductor generates triton code, the triton code can either assume that the inputs given to it are aligned or unaligned. If they are aligned, triton can use more efficient instructions (like vectorized loads or tensor cores). However, if we generate "aligned" code and pass in unaligned inputs, the triton code will error out; to fix this, we clone unaligned inputs that are passed to triton kernels that expect aligned inputs. This can lead to excessive clones if we have inputs that are not expected to be aligned.</p>
<p>In this PR, we use the example input to decide whether the generated triton code should assume alignment or not. If the example input is aligned, then we will generate triton code that assumes alignment; if at runtime we receive an unaligned input, we'll make a clone. Meanwhile, if the example input is not aligned, the generated triton code will not assume inputs are aligned and we won't ever need to clone.</p>
<p>Note that the alignment of the inputs is not guarded on; we found that adding guards on tensor offsets (a) was slow in cases where we do a lot of comparisons on tensor offsets, and (b) led to a lot of recompilations.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 18:06:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123319</guid>
    </item>
    <item>
      <title>[HOP][inductor] Support pytrees as associative_scan input</title>
      <link>https://github.com/pytorch/pytorch/pull/122137</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122137<br />
* #119430</p>
<p>This allows <code>associative_scan</code> to take an arbitrary pytree of tensors,<br />
which is flattened to their leaves before calling the <code>associative_scan</code><br />
higher order operator.</p>
<p>I also add support in inductor to generate code for scanning over sequences<br />
of tensors.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 18 Mar 2024 14:10:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122137</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Add bmm support</title>
      <link>https://github.com/pytorch/pytorch/pull/121734</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124577<br />
* #124576<br />
* #124575<br />
* #124574<br />
* #124107<br />
* <strong>-&gt;</strong> #121734</p>
<p>Add support for bmm ( batch matrix multiply ) op through the Cutlass backend.</p>
<p>Test Plan:<br />
 * CI<br />
 * Added test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 09:23:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121734</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] DO NOT REVIEW - to be split up</title>
      <link>https://github.com/pytorch/pytorch/pull/121492</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121492<br />
* #124577<br />
* #124576<br />
* #124575<br />
* #124574<br />
* #124107<br />
* #121734</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121492</guid>
    </item>
    <item>
      <title>Add registration API for torch.compile-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/121387</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we intend to provide a registration API dedicated to eager-through-torch.compile. The major workflow of this API will be as follows.</p>
<ul>
<li>Load cache</li>
<li>Check cache according to the input tensors</li>
<li>Cache Hit: Run the cached kernel directly</li>
<li>Cache Miss: Run the AOTI to produce kernel and run the produced kernel. If AOTI fails to produce the kernel, invoke the python fallback function.</li>
</ul>
<p>Currently, this PR always fallback to python kernel now and cache mechanism will be implemented in another PR - https://github.com/pytorch/pytorch/pull/116368</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124070<br />
* #124177<br />
* #116368<br />
* <strong>-&gt;</strong> #121387<br />
* #123545</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121387</guid>
    </item>
    <item>
      <title>[PT2D] Make the speedup benchmark works with DDP + CompiledAutograd</title>
      <link>https://github.com/pytorch/pytorch/pull/121315</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121315<br />
* #123424<br />
* #124422<br />
* #124421</p>
<p>https://github.com/pytorch/pytorch/pull/120454 is the original PR. It does not cover the optimizer part, which may be the root cause of breaking the dashboard. This PR save a new copy of optimizer as well.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54591562/">D54591562</a></p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang @d4l3k @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 09:35:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121315</guid>
    </item>
    <item>
      <title>In-place native functional collectives right after a custom torch op fail to codegen in inductor</title>
      <link>https://github.com/pytorch/pytorch/issues/121311</link>
      <description><![CDATA[<h3> Describe the bug</h3>
<p>When compiling one of my models using the new native functional collectives I hit the following error:</p>
<p>```<br />
<a href="AssertionError:">rank1</a>: Traceback (most recent call last):<br />
<a href="AssertionError:">rank1</a>:   File "/lustre/aviros/fms-oss/scripts/inference.py", line 234, in <module></p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/fms-oss/scripts/inference.py", line 215, in infer<br />
<a href="AssertionError:">rank1</a>:     result = generate(</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/fms-oss/fms/utils/generation.py", line 111, in generate<br />
<a href="AssertionError:">rank1</a>:     output = prefill_model(input_ids, **kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
<a href="AssertionError:">rank1</a>:     return self._call_impl(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1536, in _call_impl<br />
<a href="AssertionError:">rank1</a>:     return forward_call(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 437, in _fn<br />
<a href="AssertionError:">rank1</a>:     return fn(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
<a href="AssertionError:">rank1</a>:     return self._call_impl(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1536, in _call_impl<br />
<a href="AssertionError:">rank1</a>:     return forward_call(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 921, in catch_errors<br />
<a href="AssertionError:">rank1</a>:     return callback(frame, cache_entry, hooks, frame_state, skip=1)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert<br />
<a href="AssertionError:">rank1</a>:     return _compile(</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/contextlib.py", line 81, in inner<br />
<a href="AssertionError:">rank1</a>:     return func(<em>args, </em>*kwds)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 676, in _compile<br />
<a href="AssertionError:">rank1</a>:     guarded_code = compile_inner(code, one_graph, hooks, transform)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
<a href="AssertionError:">rank1</a>:     r = func(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner<br />
<a href="AssertionError:">rank1</a>:     out_code = transform_code_object(code, transform)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 165, in _fn<br />
<a href="AssertionError:">rank1</a>:     return fn(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 500, in transform</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2195, in run</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 850, in run<br />
<a href="AssertionError:">rank1</a>:     and self.step()</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 813, in step</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2314, in RETURN_VALUE</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/output_graph.py", line 984, in compile_subgraph</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/contextlib.py", line 81, in inner<br />
<a href="AssertionError:">rank1</a>:     return func(<em>args, </em>*kwds)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/output_graph.py", line 1161, in compile_and_call_fx_graph<br />
<a href="AssertionError:">rank1</a>:     compiled_fn = self.call_user_compiler(gm)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
<a href="AssertionError:">rank1</a>:     r = func(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/output_graph.py", line 1234, in call_user_compiler<br />
<a href="AssertionError:">rank1</a>:     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(<br />
<a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/output_graph.py", line 1215, in call_user_compiler<br />
<a href="AssertionError:">rank1</a>:     compiled_fn = compiler_fn(gm, self.example_inputs())</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper<br />
<a href="AssertionError:">rank1</a>:     compiled_gm = compiler_fn(gm, example_inputs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper<br />
<a href="AssertionError:">rank1</a>:     compiled_gm = compiler_fn(gm, example_inputs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/<strong>init</strong>.py", line 1729, in <strong>call</strong><br />
<a href="AssertionError:">rank1</a>:     return compile_fx(model_, inputs_, config_patches=self.config)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/contextlib.py", line 81, in inner<br />
<a href="AssertionError:">rank1</a>:     return func(<em>args, </em>*kwds)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1373, in compile_fx<br />
<a href="AssertionError:">rank1</a>:     return aot_autograd(</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn<br />
<a href="AssertionError:">rank1</a>:     cg = aot_module_simplified(gm, example_inputs, **kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py", line 878, in aot_module_simplified<br />
<a href="AssertionError:">rank1</a>:     compiled_fn = create_aot_dispatcher_function(</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
<a href="AssertionError:">rank1</a>:     r = func(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py", line 603, in create_aot_dispatcher_function<br />
<a href="AssertionError:">rank1</a>:     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 434, in aot_wrapper_dedupe<br />
<a href="AssertionError:">rank1</a>:     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 639, in aot_wrapper_synthetic_base<br />
<a href="AssertionError:">rank1</a>:     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base<br />
<a href="AssertionError:">rank1</a>:     compiled_fw = compiler(fw_module, updated_flat_args)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
<a href="AssertionError:">rank1</a>:     r = func(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1300, in fw_compiler_base<br />
<a href="AssertionError:">rank1</a>:     return inner_compile(</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper<br />
<a href="AssertionError:">rank1</a>:     inner_compiled_fn = compiler_fn(gm, example_inputs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/debug.py", line 304, in inner<br />
<a href="AssertionError:">rank1</a>:     return fn(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/contextlib.py", line 81, in inner<br />
<a href="AssertionError:">rank1</a>:     return func(<em>args, </em>*kwds)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/contextlib.py", line 81, in inner<br />
<a href="AssertionError:">rank1</a>:     return func(<em>args, </em>*kwds)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
<a href="AssertionError:">rank1</a>:     r = func(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 463, in compile_fx_inner<br />
<a href="AssertionError:">rank1</a>:     compiled_graph = fx_codegen_and_compile(</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 738, in fx_codegen_and_compile<br />
<a href="AssertionError:">rank1</a>:     compiled_fn = graph.compile_to_fn()</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/graph.py", line 1295, in compile_to_fn<br />
<a href="AssertionError:">rank1</a>:     return self.compile_to_module().call</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
<a href="AssertionError:">rank1</a>:     r = func(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/graph.py", line 1238, in compile_to_module</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/graph.py", line 1195, in codegen<br />
<a href="AssertionError:">rank1</a>:     self.scheduler = Scheduler(self.buffers)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
<a href="AssertionError:">rank1</a>:     r = func(<em>args, </em>*kwargs)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/scheduler.py", line 1260, in <strong>init</strong><br />
<a href="AssertionError:">rank1</a>:     self.nodes = [self.create_scheduler_node(n) for n in nodes]</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/scheduler.py", line 1260, in <listcomp><br />
<a href="AssertionError:">rank1</a>:     self.nodes = [self.create_scheduler_node(n) for n in nodes]</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/scheduler.py", line 1354, in create_scheduler_node<br />
<a href="AssertionError:">rank1</a>:     return ExternKernelSchedulerNode(self, node)</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/scheduler.py", line 166, in <strong>init</strong></p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/ir.py", line 7879, in get_read_writes<br />
<a href="AssertionError:">rank1</a>:     volatile_reads = self.get_volatile_reads()</p>
<p><a href="AssertionError:">rank1</a>:   File "/lustre/aviros/conda_envs/fms-dev/lib/python3.11/site-packages/torch/_inductor/ir.py", line 7848, in get_volatile_reads<br />
<a href="AssertionError:">rank1</a>:     assert isinstance(coll, _CollectiveKernel)<br />
<a href="AssertionError:">rank1</a>: torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:</p>
<p>```</p>
<p>When investigating why this is the case, I read through the <code>get_volatile_reads</code> function, and I noticed that the function expects that <code>inp</code> being a <code>MultiOutput</code> means that the collective is out-of-place. When running an in-place collective like all-reduce, whose input comes from a custom pytorch op (aka defined though <code>torch.library</code>), <code>inp</code> will also be a <code>MultiOutput</code> (as that's what a <code>FallbackKernel</code> will generate), but the collective will still be in-place. A pretty easy and self-contained fix I found is changing the <code>get_volatile_reads</code> function to the following:</p>
<p><code>def get_volatile_reads_fixed(self):
    inp = self.inputs[0]
    if isinstance(inp, ir._CollectiveKernel):
        # Out-of-place single-output
        return [inp.inputs[0]]
    elif isinstance(inp, ir.MultiOutput):
        # Out-of-place multi-output
        coll = inp.inputs[0]
        if isinstance(coll, ir._CollectiveKernel):
            _, idx = inp.indices[0]
            return [coll.inputs[idx]]
        return []  # e.g. regular FallbackKernel
    else:
        # In-place requires no additional deps handling for volatile
        # reads since the inputs are mutated.
        return []</code></p>
<p>I don't know if this is semantically correct, but I can make a PR if you think this is the correct fix.</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @yifuwang </p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.3.0.dev20240304+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.16.3<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) [GCC 12.3.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-1041-aws-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 12.1.105<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A100-SXM4-80GB<br />
GPU 1: NVIDIA A100-SXM4-80GB<br />
GPU 2: NVIDIA A100-SXM4-80GB<br />
GPU 3: NVIDIA A100-SXM4-80GB<br />
GPU 4: NVIDIA A100-SXM4-80GB<br />
GPU 5: NVIDIA A100-SXM4-80GB<br />
GPU 6: NVIDIA A100-SXM4-80GB<br />
GPU 7: NVIDIA A100-SXM4-80GB</p>
<p>Nvidia driver version: 535.54.03<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
CPU(s):                          96<br />
On-line CPU(s) list:             0-95<br />
Thread(s) per core:              2<br />
Core(s) per socket:              24<br />
Socket(s):                       2<br />
NUMA node(s):                    2<br />
Vendor ID:                       GenuineIntel<br />
CPU family:                      6<br />
Model:                           85<br />
Model name:                      Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz<br />
Stepping:                        7<br />
CPU MHz:                         3599.684<br />
BogoMIPS:                        5999.99<br />
Hypervisor vendor:               KVM<br />
Virtualization type:             full<br />
L1d cache:                       1.5 MiB<br />
L1i cache:                       1.5 MiB<br />
L2 cache:                        48 MiB<br />
L3 cache:                        71.5 MiB<br />
NUMA node0 CPU(s):               0-23,48-71<br />
NUMA node1 CPU(s):               24-47,72-95<br />
Vulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported<br />
Vulnerability L1tf:              Mitigation; PTE Inversion<br />
Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown<br />
Vulnerability Meltdown:          Mitigation; PTI<br />
Vulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown<br />
Vulnerability Retbleed:          Vulnerable<br />
Vulnerability Spec store bypass: Vulnerable<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke</p>
<p>Versions of relevant libraries:<br />
[pip3] mypy==1.7.1<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.26.2<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240304+cu121<br />
[pip3] torchaudio==2.2.0.dev20240304+cu121<br />
[pip3] torchvision==0.18.0.dev20240304+cu121<br />
[conda] libblas                   3.9.0            16_linux64_mkl    conda-forge<br />
[conda] libcblas                  3.9.0            16_linux64_mkl    conda-forge<br />
[conda] liblapack                 3.9.0            16_linux64_mkl    conda-forge<br />
[conda] mkl                       2022.1.0           h84fe81f_915    conda-forge<br />
[conda] numexpr                   2.8.3           mkl_py311h0b1cd97_1    conda-forge<br />
[conda] numpy                     1.26.2          py311h64a7726_0    conda-forge<br />
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch<br />
[conda] pytorch-triton            3.0.0+901819d2b6          pypi_0    pypi<br />
[conda] torch                     2.3.0.dev20240304+cu121          pypi_0    pypi<br />
[conda] torchaudio                2.2.0.dev20240304+cu121          pypi_0    pypi<br />
[conda] torchvision               0.18.0.dev20240304+cu121          pypi_0    pypi</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 07:29:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121311</guid>
    </item>
    <item>
      <title>DTensor + compile error's during backward when output is non-contiguous</title>
      <link>https://github.com/pytorch/pytorch/issues/118219</link>
      <description><![CDATA[<p>This was extracted from a larger internal repro. My smaller repro is below (I had it inside of <code>test_dtensor_compile.py</code> to handle DTensor initialization):<br />
```<br />
    def test_dynamo_dtensor_AAA(self):<br />
        mesh = DeviceMesh(self.device_type, torch.arange(self.world_size))</p>
<pre><code>    # test passing in DTensor as inputs/outputs and run some tensor computation
    def fn(x, y, z):
        tmp = x.permute(0, 2, 1).contiguous()
        out = torch._C._nn.linear(tmp, y, z)
        return out.permute(0, 2, 1)

    x = DTensor.from_local(torch.randn(4, 32, 4, requires_grad=True), mesh, [Shard(0)], run_check=False)
    y = DTensor.from_local(torch.randn(4, 32, requires_grad=True), mesh, [Shard(0)], run_check=False)
    z = DTensor.from_local(torch.randn(4, requires_grad=True), mesh, [Shard(0)], run_check=False)
    ref = fn(x, y, z)

    opt_fn = torch.compile(fn, backend="aot_eager", fullgraph=True)
    res = opt_fn(x, y, z)
    self.assertEqual(res, ref)
</code></pre>
<p>```</p>
<p>This fails while tracing out the backward, with:<br />
<code>return _reshape_view_helper(a, *shape, allow_copy=False)
  File "/data/users/hirsheybar/d/pytorch/torch/_refs/__init__.py", line 3643, in _reshape_view_helper
    raise ValueError(msg)
torch._dynamo.exc.BackendCompilerFailed: backend='aot_eager' raised:
ValueError: Cannot view a tensor with shape torch.Size([4, 4, 8]) and strides (32, 1, 4) as a tensor with shape (16, 8)!</code></p>
<p>There are a few interesting things on:</p>
<p>(1) When the output of a forward graph is not contiguous, we force the corresponding tangents to be contiguous so we can trace them properly</p>
<p>(2) this mismatch is coming from a <code>reshape()</code> call in the backward. I tried running the same test with <code>TwoTensor</code> and could not repro - it looks like with <code>TwoTensor</code>, we recognize in the backward that the <code>reshape()</code> requires a clone, but in this test DTensor, the strides are different between the outer DTensor and the inner tensor, so DTensor thinks that a clone is not necessary.</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @Chillee @albanD @samdow @msaroufim @anijain2305 @chauhang</p>]]></description>
      <pubDate>Wed, 24 Jan 2024 12:30:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/118219</guid>
    </item>
    <item>
      <title>Add `ciflow/inductor` for test only changes</title>
      <link>https://github.com/pytorch/pytorch/pull/118206</link>
      <description><![CDATA[<p>If dynamo or inductor tests are change, we want to run them, don't we?</p>
<p>Otherwise, see green signal on https://github.com/pytorch/pytorch/commit/a245abc34f3feeb51b4f3757e6cdcbb7621bc47b that is simply syntactically incorrect</p>]]></description>
      <pubDate>Wed, 24 Jan 2024 10:48:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/118206</guid>
    </item>
    <item>
      <title>Add a cache mechanism to accelerate torch.compile-for-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/116368</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124070<br />
* #124177<br />
* <strong>-&gt;</strong> #116368<br />
* #121387<br />
* #123545</p>
<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to enable a cache mechanism to accelerate <strong>eager-through-torch.compile</strong>. When <strong>eager-through-torch.compile</strong> is enabled, we will store a persistent config to cache the kernel information for the aten operation. </p>
<p>The persistent config consists of two parts - meta_info and kernel_path.</p>
<ul>
<li>meta_info: The input tensors' shape, stride, device type, data type, and symbolic flag.</li>
<li>kernel_path: The path of the kernel produced by Inductor.</li>
</ul>
<p>When an aten operation is registered, the <code>kernel_holder</code> will load the persistent config and parse it to build the cache map;  the meta_info is key, and the kernel library is the value.</p>
<p>Currently, this PR only supports static shape to guard the kernel.</p>
<p>Take a <code>mul</code> as an example.<br />
```python<br />
class MulKernel:<br />
  def <strong>init</strong>(self) -&gt; None:<br />
    pass</p>
<p>def <strong>call</strong>(self, <em>args: Any, </em><em>kwargs: Any) -&gt; Any:<br />
    with torch._C._SetExcludeDispatchKeyGuard(torch._C.DispatchKey.Python, False):<br />
      opt_fn = torch.compile(torch.ops.aten.mul, dynamic=False, options={<br />
          "aot_inductor.eager_mode": True,<br />
          "aot_inductor.eager_op_name": "mul_Tensor"<br />
        }<br />
      )<br />
      return opt_fn(</em>args, **kwargs)</p>
<p>torch_compile_op_lib_impl = torch.library.Library("aten", "IMPL")</p>
<p>_, overload_names = torch._C._jit_get_operation("aten::mul")<br />
schema = torch._C._get_schema("aten::mul", overload_name)<br />
reg_name = schema.name<br />
if schema.overload_name:<br />
  reg_name = f"{reg_name}.{schema.overload_name}"<br />
  torch_compile_op_lib_impl.impl(<br />
    reg_name,<br />
    MulKernel(),<br />
    "CUDA",<br />
    compile_mode=True)</p>
<p>a = torch.randn(1024, 1024, device=device)<br />
b = torch.randn(1024, 1024, device=device)<br />
warm_up_iter = 1000<br />
iter = 10000<br />
fn = torch.mul</p>
<h1>Warm up</h1>
<p>for _ in range(warm_up_iter):<br />
    fn(a, b)</p>
<h1>Collect performance</h1>
<p>beg = time.time()<br />
for _ in range(iter):<br />
    fn(a, b)<br />
end = time.time()<br />
print(f"E2E run: {end - beg}")<br />
```<br />
It will produce the config as follows.</p>
<p><code>json
[
    {
        "meta_info": [
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            },
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            }
        ],
        "kernel_path": "/tmp/torchinductor_eikan/e4/ce4jw46i5l2e7v3tvr2pyglpjmahnp7x3hxaqotrvxwoeh5t6qzc.so"
    }
]</code></p>
<p>Performance-wise, we collected mul.Tensor through torch.compile w/ 10000 runs(e2e). The data is as follows. And we will collect data when we support dynamic shape.</p>
<ul>
<li>Eager: ~266.11ms</li>
<li>W/O Cache: ~3455.54ms</li>
<li>W/ Cache and Cache Miss: ~3555.3ms</li>
<li>W/ Cache and Cache Hit: ~267.12ms</li>
</ul>
<p>Hardware:</p>
<ul>
<li>CPU: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz</li>
<li>GPU: CUDA A10</li>
</ul>
<p>Software:</p>
<ul>
<li>PyTorch Version: 39df084001c54cca5fe3174176f9b0206ddb7dcf</li>
<li>GPU Driver Version: 525.147.05</li>
<li>CUDA Version: 12.0</li>
</ul>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 03:21:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116368</guid>
    </item>
    <item>
      <title>`torch.func.functional_call` doesn't work with compiled models</title>
      <link>https://github.com/pytorch/pytorch/issues/97909</link>
      <description><![CDATA[<h3> Describe the bug</h3>
<p>When creating a custom model with <code>nn.Module</code> and compiling it with <code>torch.compile()</code>, the output of <code>torch.func.functional_call()</code> remains the same even if providing different <code>parameter_and_buffer_dicts</code>. Below is an example:<br />
```python<br />
import torch<br />
import torch.nn as nn</p>
<p>class LinearNet(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.linear = nn.Linear(1, 3)</p>
<pre><code>def forward(self, x):
    return self.linear(x)
</code></pre>
<p>inputs = torch.randn(1, 1)</p>
<p>print('instantiate w/ nn.Module + compile:')<br />
model = torch.compile(LinearNet())<br />
params = dict(model.named_parameters())<br />
print(torch.func.functional_call(model, params, inputs))</p>
<h1>set parameters to 0 so that outputs should also be 0</h1>
<p>for name, param in params.items():<br />
    params[name] = torch.zeros_like(param)<br />
print(torch.func.functional_call(model, params, inputs))</p>
<p>print('instantiate w/ nn.Linear + compile:')<br />
model = torch.compile(nn.Linear(1, 3))<br />
params = dict(model.named_parameters())<br />
print(torch.func.functional_call(model, params, inputs))<br />
for name, param in params.items():<br />
    params[name] = torch.zeros_like(param)<br />
print(torch.func.functional_call(model, params, inputs))</p>
<p>print('instantiate w/ nn.Module + no compile:')<br />
model = LinearNet()<br />
params = dict(model.named_parameters())<br />
print(torch.func.functional_call(model, params, inputs))<br />
for name, param in params.items():<br />
    params[name] = torch.zeros_like(param)<br />
print(torch.func.functional_call(model, params, inputs))</p>
<p><code>Outputs:</code><br />
instantiate w/ nn.Module + compile:<br />
tensor([[-0.2246,  0.4658,  0.3510]], grad_fn=<CompiledFunctionBackward>)<br />
tensor([[-0.2246,  0.4658,  0.3510]], grad_fn=<CompiledFunctionBackward>)<br />
instantiate w/ nn.Linear + compile:<br />
tensor([[ 0.3361,  0.3872, -0.6998]], grad_fn=<AddmmBackward0>)<br />
tensor([[0., 0., 0.]])<br />
instantiate w/ nn.Module + no compile:<br />
tensor([[-0.2540, -0.5778,  1.1222]], grad_fn=<AddmmBackward0>)<br />
tensor([[0., 0., 0.]])<br />
```<br />
Updates: In PyTorch 2.2, the 2nd result for "nn.Linear + compile" becomes non-zero.</p>
<details>
<summary>Versions</summary>

PyTorch version: 2.0.0
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.2 LTS (x86_64)
GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.19.0-38-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA RTX A5000
Nvidia driver version: 530.30.02
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   46 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          20
On-line CPU(s) list:             0-19
Vendor ID:                       GenuineIntel
Model name:                      12th Gen Intel(R) Core(TM) i7-12700
CPU family:                      6
Model:                           151
Thread(s) per core:              2
Core(s) per socket:              12
Socket(s):                       1
Stepping:                        2
CPU max MHz:                     4900.0000
CPU min MHz:                     800.0000
BogoMIPS:                        4224.00
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities
Virtualization:                  VT-x
L1d cache:                       512 KiB (12 instances)
L1i cache:                       512 KiB (12 instances)
L2 cache:                        12 MiB (9 instances)
L3 cache:                        25 MiB (1 instance)
NUMA node(s):                    1
NUMA node0 CPU(s):               0-19
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Not affected
Vulnerability Retbleed:          Not affected
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected

Versions of relevant libraries:
[pip3] numpy==1.23.5
[pip3] torch==2.0.0
[pip3] torchaudio==2.0.0
[pip3] torchvision==0.15.0
[pip3] triton==2.0.0
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0           py310h7f8727e_0  
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0  
[conda] mkl_random                1.2.2           py310h00e6091_0  
[conda] numpy                     1.23.5          py310hd5efca6_0  
[conda] numpy-base                1.23.5          py310h8e6c178_0  
[conda] pytorch                   2.0.0           py3.10_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_3    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.0.0               py310_cu117    pytorch
[conda] torchtriton               2.0.0                     py310    pytorch
[conda] torchvision               0.15.0              py310_cu117    pytorch
</details>

<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @soumith @wconstab @ngimel @Xia-Weiwen @desertfire</p>]]></description>
      <pubDate>Wed, 29 Mar 2023 12:54:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/97909</guid>
    </item>
  </channel>
</rss>
