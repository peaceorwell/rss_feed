<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>guard on grads being `None` in compiled optimizers</title>
      <link>https://github.com/pytorch/pytorch/pull/121291</link>
      <description><![CDATA[<p>Fixes #115607</p>
<p>We were missing guards when the grads were set to <code>None</code>. So if we compiled the optimizer with grads set to their proper value, and then with the grads set to <code>None</code> we'd continuously run the <code>None</code> version because all of the guards would pass and it would be ordered before the correct version in the cache.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 20:54:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121291</guid>
    </item>
    <item>
      <title>[aarch64 linux]  torch.compile() crashes on aarch64 linux with nightly torch wheel</title>
      <link>https://github.com/pytorch/pytorch/issues/121288</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>with the torch nightly wheels, torch.compile() mode for bert model crashes with the error added to the Error logs on aarch64 linux platform</p>
<p>confirmed that the issue doesn't happen on torch 2.2.1, it happens only on mainline, for e.g. torch wheel: torch==2.3.0.dev20240305</p>
<h3>Error logs</h3>
<p>```<br />
Traceback (most recent call last):<br />
  File "/home/ubuntu/torch_compile/bert_base_compile.py", line 21, in <module><br />
    model(<strong>encoded_input)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1536, in <em>call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 437, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 921, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state, skip=1)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame<br />
    result = inner_convert(<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert<br />
    return _compile(<br />
  File "/usr/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(</em>args, <strong>kwds)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 676, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 165, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 500, in transform<br />
    tracer.run()<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run<br />
    super().run()<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE<br />
    self.output.compile_subgraph(<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 984, in compile_subgraph<br />
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)<br />
  File "/usr/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(</em>args, <strong>kwds)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1161, in compile_and_call_fx_graph<br />
    compiled_fn = self.call_user_compiler(gm)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1234, in call_user_compiler<br />
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1215, in call_user_compiler<br />
    compiled_fn = compiler_fn(gm, self.example_inputs())<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper<br />
    compiled_gm = compiler_fn(gm, example_inputs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/<strong>init</strong>.py", line 1729, in <strong>call</strong><br />
    return compile_fx(model</em>, inputs_, config_patches=self.config)<br />
  File "/usr/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(<em>args, </em><em>kwds)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1373, in compile_fx<br />
    return aot_autograd(<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn<br />
    cg = aot_module_simplified(gm, example_inputs, </em><em>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 878, in aot_module_simplified<br />
    compiled_fn = create_aot_dispatcher_function(<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 603, in create_aot_dispatcher_function<br />
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 434, in aot_wrapper_dedupe<br />
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 639, in aot_wrapper_synthetic_base<br />
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base<br />
    compiled_fw = compiler(fw_module, updated_flat_args)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1300, in fw_compiler_base<br />
    return inner_compile(<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper<br />
    inner_compiled_fn = compiler_fn(gm, example_inputs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/debug.py", line 304, in inner<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(</em>args, <strong>kwds)<br />
  File "/usr/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(<em>args, </em><em>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 463, in compile_fx_inner<br />
    compiled_graph = fx_codegen_and_compile(<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 738, in fx_codegen_and_compile<br />
    compiled_fn = graph.compile_to_fn()<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1296, in compile_to_fn<br />
    return self.compile_to_module().call<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1243, in compile_to_module<br />
    mod = PyCodeCache.load_by_key_path(<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2103, in load_by_key_path<br />
    exec(code, mod.<strong>dict</strong>, mod.<strong>dict</strong>)<br />
  File "/tmp/torchinductor_ubuntu/lz/clzrjzvvoh7hpyv5irfu4w6okxqsrqexpvz3ibno6luc37ejdq3o.py", line 2376, in <module><br />
    async_compile.wait(globals())<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2658, in wait<br />
    scope[key] = result.result()<br />
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 451, in result<br />
    return self.__get_result()<br />
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result<br />
    raise self._exception<br />
  File "/usr/lib/python3.10/concurrent/futures/thread.py", line 58, in run<br />
    result = self.fn(*self.args, </strong>self.kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2015, in load_pybinding<br />
    result = cls.load(source_code + suffix, cuda)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1889, in load<br />
    compile_file(input_path, output_path, cmd)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(<em>args, </em>*kwargs)<br />
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1829, in compile_file<br />
    raise exc.CppCompileError(cmd, output) from e<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
CppCompileError: C++ compile error</p>
<p>Command:<br />
g++ /tmp/torchinductor_ubuntu/uq/cuqbpt5j5zi4s3xpnyg7rxhxsw7zq6nuc2oessjxj3mobho6xtvc.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=0 -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/include/python3.10 -L/home/ubuntu/.local/lib/python3.10/site-packages/torch/lib -L/usr/lib/aarch64-linux-gnu -L/home/ubuntu/.local/lib/python3.10/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -DCPU_CAPABILITY_NEON -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_ubuntu/uq/cuqbpt5j5zi4s3xpnyg7rxhxsw7zq6nuc2oessjxj3mobho6xtvc.so</p>
<p>Output:<br />
/tmp/torchinductor_ubuntu/uq/cuqbpt5j5zi4s3xpnyg7rxhxsw7zq6nuc2oessjxj3mobho6xtvc.cpp: In function ‚Äòvoid kernel(float<em>, float</em>, float<em>, const long int</em>, float*)‚Äô:<br />
/tmp/torchinductor_ubuntu/uq/cuqbpt5j5zi4s3xpnyg7rxhxsw7zq6nuc2oessjxj3mobho6xtvc.cpp:34:25: error: ‚Äòcvt_int64_to_fp32‚Äô was not declared in this scope<br />
   34 |             auto tmp1 = cvt_int64_to_fp32(tmp0);<br />
      |                         ^~~~~~~~~~~~~~~~~</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.3.0.dev20240305<br />
Is debug build: False<br />
CUDA used to build PyTorch: None<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (aarch64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.28.3<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-6.2.0-1017-aws-aarch64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       aarch64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Byte Order:                         Little Endian<br />
CPU(s):                             64<br />
On-line CPU(s) list:                0-63<br />
Vendor ID:                          ARM<br />
Model:                              1<br />
Thread(s) per core:                 1<br />
Core(s) per socket:                 64<br />
Socket(s):                          1<br />
Stepping:                           r1p1<br />
BogoMIPS:                           2100.00<br />
Flags:                              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop sha3 sm3 sm4 asimddp sha512 sve asimdfhm dit uscat ilrcpc flagm ssbs paca pacg dcpodp svei8mm svebf16 i8mm bf16 dgh rng<br />
L1d cache:                          4 MiB (64 instances)<br />
L1i cache:                          4 MiB (64 instances)<br />
L2 cache:                           64 MiB (64 instances)<br />
L3 cache:                           32 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-63<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; CSV2, BHB<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] bert-pytorch==0.0.1a4<br />
[pip3] clip-anytorch==2.6.0<br />
[pip3] CoCa-pytorch==0.1.0<br />
[pip3] dalle2-pytorch==1.14.2<br />
[pip3] ema-pytorch==0.4.2<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.23.5<br />
[pip3] onnx==1.15.0<br />
[pip3] open-clip-torch==2.24.0<br />
[pip3] pytorch-warmup==0.1.1<br />
[pip3] rotary-embedding-torch==0.3.3<br />
[pip3] torch==2.3.0.dev20240305<br />
[pip3] torch-fidelity==0.3.0<br />
[pip3] torch_geometric==2.4.0<br />
[pip3] torchaudio==2.2.1<br />
[pip3] torchmetrics==1.3.1<br />
[pip3] torchvision==0.17.1<br />
[pip3] vector_quantize_pytorch==1.14.1<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 19:54:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121288</guid>
    </item>
    <item>
      <title>[inductor] Update triton pin</title>
      <link>https://github.com/pytorch/pytorch/pull/121268</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121268<br />
* #121267</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 13:25:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121268</guid>
    </item>
    <item>
      <title>[inductor] Changes to support newer triton pin</title>
      <link>https://github.com/pytorch/pytorch/pull/121267</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121268<br />
* <strong>-&gt;</strong> #121267</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 13:25:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121267</guid>
    </item>
    <item>
      <title>Accuracy mismatch with torch.compile(backend="eager") for float16</title>
      <link>https://github.com/pytorch/pytorch/issues/121238</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>For float16 (the repro passes if dtype is torch.float32), the two implementations differ tangibly when passed through dynamo when they are the same in eager. Example results shown below.</p>
<p>One thing to note is that if we switch the following to not pass in -1 to addcdiv, the difference is okay again.<br />
<code>denom = exp_inf * bias_correction
    param.addcdiv_(exp_avg, denom, value=-1)</code><br />
to<br />
<code>denom = exp_inf * -bias_correction
    param.addcdiv_(exp_avg, denom)</code></p>
<h3>Error logs</h3>
<p><code>(pytorch-3.10) [janeyx@devgpu023.odn1 ~/local/pytorch (1e63ab5f)]$ python playground2.py 
The following are the same param=tensor([[ -0.8535,   0.1125,  -0.9385],
        [ -3.8652, -59.9062,  -6.5234]], device='cuda:0', dtype=torch.float16) and pc=tensor([[ -0.8535,   0.1125,  -0.9385],
        [ -3.8652, -59.9062,  -6.5234]], device='cuda:0', dtype=torch.float16)
/home/janeyx/.conda/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Traceback (most recent call last):
  File "/data/users/janeyx/pytorch/playground2.py", line 54, in &lt;module&gt;
    assert torch.allclose(param, pc), f"results are not the same! {param=} {pc=}"
AssertionError: results are not the same! param=tensor([[ -0.7939, -22.8125,  -4.2969],
        [ -1.7324,  -7.2891,  -4.1484]], device='cuda:0', dtype=torch.float16) pc=tensor([[ -0.7944, -22.8125,  -4.2969],
        [ -1.7314,  -7.2852,  -4.1445]], device='cuda:0', dtype=torch.float16)</code></p>
<h3>Minified repro</h3>
<p>```<br />
import torch<br />
dtype = torch.float16</p>
<p>beta1 = 0.9<br />
step_t = torch.tensor(2, dtype=dtype, device="cuda")<br />
g_step_t = [torch.tensor(2, dtype=dtype, device="cuda")]</p>
<p>param = torch.rand(2, 3, dtype=dtype, device="cuda") <br />
pc = param.clone()<br />
exp_inf = torch.rand(2, 3, dtype=dtype, device="cuda")<br />
exp_inf_c = exp_inf.clone()<br />
exp_avg = torch.rand(2, 3, dtype=dtype, device="cuda")<br />
exp_avg_c = exp_avg.clone()</p>
<p>def reset():<br />
    global beta1, step_t, g_step_t, param, pc, exp_inf, exp_inf_c, exp_avg, exp_avg_c<br />
    beta1 = 0.9<br />
    step_t = torch.tensor(2, dtype=dtype, device="cuda")<br />
    g_step_t = [torch.tensor(2, dtype=dtype, device="cuda")]</p>
<pre><code>param = torch.rand(2, 3, dtype=dtype, device="cuda") 
pc = param.clone()
exp_inf = torch.rand(2, 3, dtype=dtype, device="cuda")
exp_inf_c = exp_inf.clone()
exp_avg = torch.rand(2, 3, dtype=dtype, device="cuda")
exp_avg_c = exp_avg.clone()
</code></pre>
<p>def forloop_capturable():<br />
    bias_correction = 1 - beta1 ** step_t<br />
    denom = exp_inf * bias_correction<br />
    param.addcdiv_(exp_avg, denom, value=-1)</p>
<p>def foreach_capturable():<br />
    bias_corrections = torch.<em>foreach_pow(beta1, g_step_t)<br />
    # foreach_sub doesn't allow a scalar as the first arg<br />
    torch._foreach_sub</em>(bias_corrections, 1)</p>
<pre><code>denom = torch._foreach_mul([exp_inf_c], bias_corrections)
torch._foreach_addcdiv_([pc], [exp_avg_c], denom)
</code></pre>
<h1>these match in eager</h1>
<p>forloop_capturable()<br />
foreach_capturable()<br />
assert torch.allclose(param, pc)<br />
print(f"The following are the same {param=} and {pc=}")</p>
<p>reset()</p>
<h1>but not in compile</h1>
<p>torch.compile(forloop_capturable, backend="eager")()<br />
torch.compile(foreach_capturable, backend="eager")()<br />
assert torch.allclose(param, pc), f"results are not the same! {param=} {pc=}"<br />
```</p>
<h3>Versions</h3>
<p>main</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 08:43:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121238</guid>
    </item>
    <item>
      <title>[torch.compile] `addmm_fuse_pattern_second` raises error `Number of dimensions of tensors must match`</title>
      <link>https://github.com/pytorch/pytorch/issues/121231</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When using <code>torch.compile</code> and enabling <code>TORCHINDUCTOR_FREEZING=1</code>, <a href="https://github.com/pytorch/pytorch/blob/59d9f1e2275af96dfb6cc56af4f68b6cb9d4791a/torch/_inductor/fx_passes/freezing_patterns.py#L171"><code>addmm_fuse_pattern_second</code></a> raises error <code>Number of dimensions of tensors must match</code></p>
<p>```py<br />
import torch</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()
    self.w1 = torch.tensor([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], device='cuda')
    self.b1 = torch.zeros(3, device='cuda')
    self.w2 = torch.tensor([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0]], device='cuda')
    self.b2 = torch.tensor([[-1.0, -1.0, -1.0]], device='cuda')
    self.w3 = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]], device='cuda')
    self.b3 = torch.tensor([1.0, 2.0, 3.0], device='cuda')

def forward(self, x):
    out1 = torch.nn.functional.linear(x, self.w1, self.b1)
    out2 = torch.nn.functional.linear(x, self.w2, self.b2)
    out3 = torch.nn.functional.linear(x, self.w3, self.b3)
    return (out1, out2, out3)
</code></pre>
<p>func = Model().to('cuda').eval()</p>
<p>x = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]).cuda()</p>
<p>with torch.no_grad():<br />
    print(func(x.clone()))</p>
<pre><code>func1 = torch.compile(func)
print(func1(x.clone()))
</code></pre>
<p>```</p>
<p>Output</p>
<p><code>File "torch/_dynamo/eval_frame.py", line 437, in _fn
    return fn(*args, **kwargs)
  File "torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "torch/fx/_symbolic_trace.py", line 793, in trace
    (self.create_arg(fn(*args)),),
  File "torch/fx/experimental/proxy_tensor.py", line 584, in wrapped
    out = f(*tensors)
  File "torch/_inductor/fx_passes/freezing_patterns.py", line 180, in addmm_fuse_replacement_second
    cat_b = torch.cat((b1, b2, b3))
  File "torch/utils/_stats.py", line 20, in wrapper
    return fn(*args, **kwargs)
  File "torch/fx/experimental/proxy_tensor.py", line 663, in __torch_dispatch__
    return self.inner_torch_dispatch(func, types, args, kwargs)
  File "torch/fx/experimental/proxy_tensor.py", line 698, in inner_torch_dispatch
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)
  File "torch/fx/experimental/proxy_tensor.py", line 429, in proxy_call
    out = func(*args, **kwargs)
  File "torch/_ops.py", line 571, in __call__
    return self_._op(*args, **kwargs)
  File "torch/utils/_stats.py", line 20, in wrapper
    return fn(*args, **kwargs)
  File "torch/_subclasses/fake_tensor.py", line 892, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File "torch/_subclasses/fake_tensor.py", line 1231, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
  File "torch/_subclasses/fake_tensor.py", line 962, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
  File "torch/_subclasses/fake_tensor.py", line 1448, in _dispatch_impl
    r = func(*args, **kwargs)
  File "torch/_ops.py", line 571, in __call__
    return self_._op(*args, **kwargs)
  File "torch/_prims_common/wrappers.py", line 252, in _fn
    result = fn(*args, **kwargs)
  File "torch/_prims_common/wrappers.py", line 137, in _fn
    result = fn(**bound.arguments)
  File "torch/_refs/__init__.py", line 2764, in cat
    torch._check(
  File "torch/__init__.py", line 1138, in _check
    _check_with(RuntimeError, cond, message)
  File "torch/__init__.py", line 1121, in _check_with
    raise error_type(message_evaluated)
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
RuntimeError: Number of dimensions of tensors must match.  Expected 2-D tensors, but got 1-D for tensor number 0 in the list</code></p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 07:01:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121231</guid>
    </item>
    <item>
      <title>[torch.compile] `merge_unbind_stack` returns a tensor with WRONG shape</title>
      <link>https://github.com/pytorch/pytorch/issues/121228</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When using <code>torch.compile</code> and enabling <code>TORCHINDUCTOR_FREEZING=1</code>, <code>merge_unbind_stack</code> returns a tensor with <strong>WRONG shape</strong>.</p>
<p>```py<br />
import torch</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self, dim):
    super().__init__()
    self.dim = dim

def forward(self, x1):
    v1 = torch.unbind(x1, dim=self.dim)
    v2 = v1[1]
    v3 = torch.stack([v1[0], v2], dim=self.dim)
    return v3
</code></pre>
<p>func = Model(1).to('cpu')</p>
<p>x = torch.arange(12).view(3, 4).float()</p>
<p>with torch.no_grad():<br />
    print(func(x.clone()))<br />
    # tensor([[0., 1.],<br />
    #     [4., 5.],<br />
    #     [8., 9.]])</p>
<pre><code>func1 = torch.compile(func)
print(func1(x.clone()))
# tensor([[ 0.,  1.,  2.,  3.],
#     [ 4.,  5.,  6.,  7.],
#     [ 8.,  9., 10., 11.]])
</code></pre>
<p>```</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 06:47:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121228</guid>
    </item>
    <item>
      <title>[torch.compile] `merge_stack_tahn_unbind` returns a tensor with wrong values</title>
      <link>https://github.com/pytorch/pytorch/issues/121227</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When using <code>torch.compile</code> and enabling <code>TORCHINDUCTOR_FREEZING=1</code>, <code>merge_stack_tahn_unbind</code> returns a tensor with wrong values. More specifically, the tensor's value is mistakenly placed compared to the tensor computed by naive execution for the model with <code>split</code> + <code>stack</code> + <code>tanh</code></p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(42)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()

def forward(self, x1):
    v1 = torch.split(x1, [3, 3, 3], dim=-1)
    v2 = torch.stack(v1, dim=-1)
    v3 = torch.tanh(v2)
    return v3
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(1, 9)</p>
<p>with torch.no_grad():<br />
    print(func(x.clone()))<br />
    # tensor([[[ 0.3245,  0.2263,  0.9761],<br />
    #      [ 0.1281, -0.8086, -0.5635],<br />
    #      [ 0.2303, -0.1842,  0.4314]]])</p>
<pre><code>func1 = torch.compile(func)
print(func1(x.clone()))
# tensor([[[ 0.3245,  0.1281,  0.2303],
#      [ 0.2263, -0.8086, -0.1842],
#      [ 0.9761, -0.5635,  0.4314]]])
</code></pre>
<p>```</p>
<p>I think the root cause is <code>dim=-1</code>, that <code>merge_stack_tahn_unbind</code> doesn't consider the case when dim is negative. This is because when I replace <code>dim=-1</code> with <code>dim=2</code>, it will return the consistent result</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 06:19:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121227</guid>
    </item>
    <item>
      <title>[`torch.compile`] Inductor gets stuck, unbounded RAM usage</title>
      <link>https://github.com/pytorch/pytorch/issues/121197</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When I run a GNN workload with a specific dataset, the compilation process gets stuck on step 2.</p>
<h3>Error logs</h3>
<p><code>[2024-03-05 04:00:37,435] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env
[2024-03-05 04:00:37,668] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s0 = 341523 for L['x'].size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,669] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s1 = 128 for L['x'].size()[1] [2, 9223372036854775806]
[2024-03-05 04:00:37,853] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s2 = 471405 for L['subgraphs'][0].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,856] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s3 = 95082 for L['subgraphs'][0].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,884] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s3 - 1 &lt;= s0 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:37,951] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s3 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:37,951] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s3 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] set_replacement s1 = 128
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] set_replacement s1 = 128
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s1, 128) [guard added] at torch_geometric/nn/dense/linear.py:147 in forward (_meta_registrations.py:1980 in meta_mm)
[2024-03-05 04:00:38,017] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s4 = 124196 for L['subgraphs'][1].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,020] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s5 = 12758 for L['subgraphs'][1].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,039] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s5 - 1 &lt;= s3 - 1 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:38,098] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s5 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:38,099] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s5 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:38,143] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s6 = 12033 for L['subgraphs'][2].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,147] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s7 = 1025 for L['subgraphs'][2].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,165] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s7 - 1 &lt;= s5 - 1 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:38,229] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s7 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:38,230] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s7 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:38,277] [0/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2024-03-05 04:00:38,283] [0/0] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor</code></p>
<h3>Minified repro</h3>
<p>Running with the environment variable gets stuck too, producing no output.</p>
<p>The code in question is here: https://github.com/dmlc/dgl/pull/7183</p>
<p>You can install DGL nightly build and install torch_geometric via pip.</p>
<p><code>bash
pip install --pre dgl -f https://data.dgl.ai/wheels-test/cu121/repo.html
pip install torch_geometric
python dgl/examples/sampling/pyg/node_classification_2.py --dataset=ogbn-papers100M --torch-compile</code></p>
<h3>Versions</h3>
<details><summary>Details</summary>
<p>

Collecting environment information...
PyTorch version: 2.3.0a0+ebedce2
Is debug build: False
CUDA used to build PyTorch: 12.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.28.1
Libc version: glibc-2.35

Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.3.107
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100-SXM4-80GB
GPU 1: NVIDIA A100-SXM4-80GB
GPU 2: NVIDIA A100-SXM4-80GB
GPU 3: NVIDIA A100-SXM4-80GB

Nvidia driver version: 545.23.08
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      43 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             256
On-line CPU(s) list:                0-255
Vendor ID:                          AuthenticAMD
Model name:                         AMD EPYC 7702 64-Core Processor
CPU family:                         23
Model:                              49
Thread(s) per core:                 2
Core(s) per socket:                 64
Socket(s):                          2
Stepping:                           0
Frequency boost:                    enabled
CPU max MHz:                        2183.5930
CPU min MHz:                        1500.0000
BogoMIPS:                           4000.17
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es
Virtualization:                     AMD-V
L1d cache:                          4 MiB (128 instances)
L1i cache:                          4 MiB (128 instances)
L2 cache:                           64 MiB (128 instances)
L3 cache:                           512 MiB (32 instances)
NUMA node(s):                       2
NUMA node0 CPU(s):                  0-63,128-191
NUMA node1 CPU(s):                  64-127,192-255
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow: Mitigation; safe RET
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.24.4
[pip3] onnx==1.15.0rc2
[pip3] optree==0.10.0
[pip3] pytorch-quantization==2.1.2
[pip3] torch==2.3.0a0+ebedce2
[pip3] torch_geometric==2.5.0
[pip3] torch-tensorrt==2.3.0a0
[pip3] torchdata==0.7.1a0
[pip3] torchmetrics==1.3.1
[pip3] torchtext==0.17.0a0
[pip3] torchvision==0.18.0a0
[pip3] triton==2.2.0+e28a256
[conda] Could not collect


</p>
</details>

<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Mon, 04 Mar 2024 20:12:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121197</guid>
    </item>
    <item>
      <title>[torch.compile] `merge_stack_tahn_unbind` raises list index out of range error</title>
      <link>https://github.com/pytorch/pytorch/issues/121185</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When using <code>torch.compile</code>, the optimization <code>merge_stack_tahn_unbind</code> will raise <code>IndexError: list index out of range</code> </p>
<p><code>File torch/_inductor/fx_passes/split_cat.py", line 1459, in merge_stack_tahn_unbind
    unbind_user = find_next_users(user)[0]
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
IndexError: list index out of range</code></p>
<p>Here is the reproducible input:</p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(0)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()
    self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)

def forward(self, x1, split_sections, dim):
    t1 = torch.split(x1, split_sections, dim)
    targets = []
    for t in t1:
        targets.append(t)
    for target in targets:
        t2 = torch.stack(targets, dim)
        t3 = torch.tanh(t2)
    return t3
</code></pre>
<p>func = Model().to('cuda').eval()</p>
<p>x = torch.randn(1, 3, 64, 64).cuda()<br />
split_sections = 1<br />
dim = 1</p>
<p>with torch.no_grad():<br />
    print(func(x.clone(), split_sections, dim))</p>
<pre><code>func1 = torch.compile(func)
print(func1(x.clone(), split_sections, dim))
</code></pre>
<p>```</p>
<p>Found by our project: <a href="https://github.com/ise-uiuc/WhiteFox">WhiteFox</a></p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 04 Mar 2024 17:36:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121185</guid>
    </item>
    <item>
      <title>[torch.compile] Compiled Assumption Failed for `efficient_conv_bn_eval`</title>
      <link>https://github.com/pytorch/pytorch/issues/121184</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>torch.compile</code> raises an AssertionError <code>At compilation time, graph 4 was compiled under the assumption that parameter/buffer 4 would be a duplicate of parameter/buffer 2, but at runtime this was not the case.  This indicates a guard bug in AOTAutograd or Dynamo, please file a bug to PyTorch.</code></p>
<p>It seems that it doesn't check the consistency between <code>conv</code> and <code>bn</code> when fusing them.</p>
<p>```py<br />
import torch<br />
import math</p>
<p>torch.manual_seed(0)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()
    self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)
    self.bn = torch.nn.BatchNorm2d(8)

def forward(self, x):
    t1 = self.conv(x)
    self.bn.eval()
    self.bn.running_mean.zero_()
    self.bn.running_var.fill_(1)
    return self.bn(t1)
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(1, 3, 64, 64)<br />
func = Model()</p>
<p>with torch.no_grad():<br />
    print(func(x.clone()))</p>
<pre><code>func1 = torch.compile(func)
print(func1(x.clone()))
</code></pre>
<p>```</p>
<p><code>AssertionError: At compilation time, graph 4 was compiled under the assumption that parameter/buffer 4 would be a duplicate of parameter/buffer 2, but at runtime this was not the case.  This indicates a guard bug in AOTAutograd or Dynamo, please file a bug to PyTorch.</code></p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Mon, 04 Mar 2024 17:33:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121184</guid>
    </item>
    <item>
      <title>[torch.compile][freezing] reorder_linear_weight: weight's dtype should be float</title>
      <link>https://github.com/pytorch/pytorch/issues/121175</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When using <code>TORCHINDUCTOR_FREEZING=1</code>, <code>torch.compile</code> will raise the error <code>reorder_linear_weight: weight's dtype should be float</code> for the model</p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(42)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()
    self.weight1 = torch.nn.Parameter(torch.randn(10, 10, dtype=torch.float64))
    self.weight2 = torch.nn.Parameter(torch.randn(10, 10, dtype=torch.float64))
    self.bias = torch.nn.Parameter(torch.randn(10, dtype=torch.float64))

def forward(self, x1):
    v1 = torch.mm(x1, self.weight1)
    v2 = torch.addmm(self.bias, x1, self.weight2)
    return (v1, v2)
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(10, 10, dtype=torch.float64)</p>
<p>with torch.no_grad():<br />
    print(func(x.clone()))</p>
<pre><code>func1 = torch.compile(func)
print(func1(x.clone()))
</code></pre>
<p>```</p>
<p>Output<br />
```<br />
RuntimeError: reorder_linear_weight: weight's dtype should be float</p>
<p>While executing %_mkl_reorder_linear_weight_1 : [num_users=1] = call_function<a href="args = (%permute_default_1, 10), kwargs = {}">target=torch.ops.mkl._mkl_reorder_linear_weight</a><br />
Original traceback:<br />
None<br />
```</p>
<h3>Versions</h3>
<p>```<br />
Versions<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 04 Mar 2024 15:51:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121175</guid>
    </item>
    <item>
      <title>[torch.compile] `fuse_attention` returns inconsistent value for the model</title>
      <link>https://github.com/pytorch/pytorch/issues/121174</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>With <code>torch.compile</code>, <code>fuse_attention</code> returns inconsistent value for the model</p>
<p>```py<br />
import torch<br />
import math</p>
<p>torch.manual_seed(0)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()
    self.q_conv = torch.nn.Conv2d(4, 4, 1)
    self.k_conv = torch.nn.Conv2d(4, 4, 1)
    self.v_conv = torch.nn.Conv2d(4, 4, 1)

def forward(self, x):
    q = self.q_conv(x)
    k = self.k_conv(x)
    v = self.v_conv(x)
    q = q.permute(0, 2, 1, 3)
    k = k.permute(0, 2, 1, 3)
    v = v.permute(0, 2, 1, 3)
    div = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))
    attn_weight = torch.nn.functional.softmax(div, dim=-1)
    output = torch.matmul(attn_weight, v)
    return output
</code></pre>
<p>func = Model()</p>
<p>x = torch.randn(1, 4, 2, 2)</p>
<p>func = Model()</p>
<p>with torch.no_grad():<br />
    print(func(x.clone()))<br />
    # tensor([[[[ 0.1035, -0.2616],<br />
    #         [ 0.0461, -0.3923],<br />
    #         [ 0.0852, -0.4379],<br />
    #         [ 0.0538, -0.5618]],<br />
    #         [[-0.3332, -0.2985],<br />
    #         [-0.1777, -0.2978],<br />
    #         [-0.4235, -0.3401],<br />
    #         [-0.4361, -0.3477]]]])</p>
<pre><code>func1 = torch.compile(func)
print(func1(x.clone()))
# tensor([[[[ 0.1624,  0.2315],
#         [ 0.0282,  0.0822],
#         [ 0.2613,  0.0440],
#         [-0.0161, -0.0560]],

#         [[-0.3225, -0.2300],
#         [-0.2753, -0.3049],
#         [-0.3932, -0.3184],
#         [-0.3870, -0.2880]]]])

print(torch.equal(func(x.clone()), func1(x.clone()))) # False
print(torch.equal(func.k_conv.weight, func1.k_conv.weight)) # True
print(torch.equal(func.q_conv.weight, func1.q_conv.weight)) # True
print(torch.equal(func.v_conv.weight, func1.v_conv.weight)) # True
</code></pre>
<p>```</p>
<p>If I disable the <code>fuse_attention</code>, the compiled model will return the same value without optimization.</p>
<h3>Versions</h3>
<p>PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi</p>
<p>cc @jbschlosser @bhosmer @cpuhrsch @erichan1 @drisspg @mikaylagawarecki @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 04 Mar 2024 15:46:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121174</guid>
    </item>
    <item>
      <title>[torch.compile] `randperm_index_add_pattern` doesn't check the shape mismatch between self and source tensor </title>
      <link>https://github.com/pytorch/pytorch/issues/121135</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>randperm_index_add_pattern</code> (https://github.com/pytorch/pytorch/blob/main/torch/_inductor/fx_passes/misc_patterns.py) doesn't check the shape mismatch between self and source tensor</p>
<p>```py<br />
import torch</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()

def forward(self, x, y):
    index = torch.randperm(x.shape[0], device=x.device)[:y.shape[0]]
    result = torch.index_add(x, dim=0, source=y, index=index)
    return (result, index)
</code></pre>
<p>x = torch.randn(10, 5)<br />
y = torch.randn(5)</p>
<p>with torch.no_grad():<br />
    func = Model()<br />
    func = torch.compile(func)<br />
    print(func(x.clone(), y.clone()))</p>
<p>func = Model().to('cpu')<br />
print(func(x.clone(), y.clone()))<br />
```</p>
<p>The output is </p>
<p>```<br />
(tensor([[ 2.6314e+00, -1.6818e+00, -1.8673e+00,  4.0594e-02, -1.0124e+00],<br />
        [ 1.0112e+00,  2.6949e-01,  5.0703e-01,  4.7462e-01, -4.2823e-01],<br />
        [-1.8359e-01,  8.3147e-01, -8.7212e-01, -1.2075e+00, -2.2189e-01],<br />
        [ 9.4409e-01,  1.0321e+00,  1.8141e+00,  9.5272e-01, -1.3232e+00],<br />
        [ 1.5159e-01,  2.0597e+00, -1.8763e+00,  1.8238e+00,  1.8961e+00],<br />
        [ 5.4409e-01, -1.2991e+00, -3.4712e-02,  1.7646e+00, -2.9087e-01],<br />
        [ 2.6989e-01, -3.3603e-01, -9.5723e-01,  2.5158e-01, -7.3185e-01],<br />
        [-1.0166e+00, -8.1101e-02,  1.0997e-01,  1.7079e-03,  1.6978e+00],<br />
        [ 1.7775e+00, -6.5251e-01,  1.3048e+00,  3.8404e+00,  2.1692e-02],<br />
        [ 1.3554e+00, -1.8695e+00,  1.1919e+00,  7.9388e-02, -1.3319e+00]]), tensor([3, 8, 7, 9, 0]))</p>
<p>RuntimeError: source tensor shape must match self tensor shape, excluding the specified dimension. Got self.shape = [10, 5] source.shape = [5]<br />
```</p>
<p>In the naive execution, PyTorch will capture the inconsistency between <code>x</code> and <code>y</code> while in the compiled model, it will skip the check and return some value silently.</p>
<h3>Versions</h3>
<p>PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56)  [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             24<br />
On-line CPU(s) list:                0-23<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family:                         6<br />
Model:                              151<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 16<br />
Socket(s):                          1<br />
Stepping:                           2<br />
CPU max MHz:                        5200.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           6374.40<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          640 KiB (16 instances)<br />
L1i cache:                          768 KiB (16 instances)<br />
L2 cache:                           14 MiB (10 instances)<br />
L3 cache:                           30 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy                     1.26.4                   pypi_0    pypi<br />
[conda] pytorch-triton            3.0.0+901819d2b6          pypi_0    pypi<br />
[conda] torch                     2.3.0.dev20240301+cu118          pypi_0    pypi<br />
[conda] torchaudio                2.2.0.dev20240301+cu118          pypi_0    pypi<br />
[conda] torchvision               0.18.0.dev20240301+cu118          pypi_0    pypi</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 04 Mar 2024 07:32:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121135</guid>
    </item>
    <item>
      <title>[WIP] Add torch.cond support to AOT Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/121120</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121120</p>
<p>Summary: WIP: not ready for review (early feedback would be great, though!). Submitted to check the CI.</p>
<p>Test Plan:</p>
<p>```<br />
$ python test/inductor/test_aot_inductor.py -k test_conditional<br />
...</p>
<hr />
<p>Ran 12 tests in 167.212s</p>
<p>OK (skipped=4)<br />
```</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 03 Mar 2024 22:56:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121120</guid>
    </item>
    <item>
      <title>[FSDP][torch._dynamo.compiled_autograd] Final callbacks can only be installed during backward pass</title>
      <link>https://github.com/pytorch/pytorch/issues/121071</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Hi,</p>
<p>When using <code>torch.compile</code> and <code>torch._dynamo.compiled_autograd</code> to trace the FSDP model with the backward gradient hooks, the following error happened. According to the following error log, the error message is thrown from the code here: https://github.com/pytorch/pytorch/blob/bab4b5a341609aec3edd6f5cd3918d08e0cfb36c/torch/distributed/fsdp/_runtime_utils.py#L1515-L1519</p>
<p>Here is the error log:<br />
<code>Traceback (most recent call last):
  File "/home/bduser/zejun/pytorch/torch/multiprocessing/spawn.py", line 75, in _wrap
    fn(i, *args)
  File "/home/bduser/zejun/test_1_jiong_fsdp.py", line 48, in fsdp_main
    train(input, target, model, rank, world_size, optimizer)
  File "/home/bduser/zejun/test_1_jiong_fsdp.py", line 30, in train
    loss.backward()
  File "/home/bduser/zejun/pytorch/torch/_tensor.py", line 524, in backward
    torch.autograd.backward(
  File "/home/bduser/zejun/pytorch/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/bduser/zejun/pytorch/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/bduser/zejun/pytorch/torch/nn/modules/module.py", line 1529, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/bduser/zejun/pytorch/torch/nn/modules/module.py", line 1538, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/bduser/zejun/pytorch/torch/_dynamo/eval_frame.py", line 455, in _fn
    return fn(*args, **kwargs)
  File "/home/bduser/zejun/pytorch/torch/fx/graph_module.py", line 737, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/home/bduser/zejun/pytorch/torch/fx/graph_module.py", line 317, in __call__
    raise e
  File "/home/bduser/zejun/pytorch/torch/fx/graph_module.py", line 304, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/home/bduser/zejun/pytorch/torch/nn/modules/module.py", line 1529, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/bduser/zejun/pytorch/torch/nn/modules/module.py", line 1538, in _call_impl
    return forward_call(*args, **kwargs)
  File "&lt;eval_with_key&gt;.9", line 21, in forward
    call_hook = torch__dynamo_external_utils_call_hook(getitem_14, nll_loss_backward);  getitem_14 = nll_loss_backward = None
  File "/home/bduser/zejun/pytorch/torch/_dynamo/external_utils.py", line 34, in call_hook
    result = hook(*args)
  File "/home/bduser/zejun/pytorch/torch/distributed/fsdp/_runtime_utils.py", line 641, in _pre_backward_hook
    _register_post_backward_final_callback(state, module)
  File "/home/bduser/zejun/pytorch/torch/distributed/fsdp/_runtime_utils.py", line 1518, in _register_post_backward_final_callback
    Variable._execution_engine.queue_callback(
RuntimeError: Final callbacks can only be installed during backward pass.</code></p>
<p>I check the function https://github.com/pytorch/pytorch/blob/bab4b5a341609aec3edd6f5cd3918d08e0cfb36c/torch/distributed/fsdp/_runtime_utils.py#L1499 and find the <code>torch.distributed._functional_collectives.is_torchdynamo_compiling()</code> is wrongly <strong>False</strong> when running the reproducer. It should be True i guess because i am doing the trace with the context with <code>torch._dynamo.compiled_autograd.enable(torch.compile)</code>. When i manually set it to True, the expected graph with the backward gradient hook op i can get after the <code>torch.compile</code>.</p>
<p>Here is the reproducer script:<br />
```<br />
import os<br />
import torch<br />
import torch.nn as nn<br />
import torch.nn.functional as F<br />
import torch.optim as optim<br />
import torch.distributed as dist<br />
import torch.multiprocessing as mp<br />
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</p>
<p>class Net(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super(Net, self).<strong>init</strong>()<br />
        self.conv1 = nn.Conv2d(1, 4, 3, 1)<br />
        self.conv2 = nn.Conv2d(4, 4, 3, 1)<br />
        self.fc1 = nn.Linear(576, 10)</p>
<pre><code>def forward(self, x):
    x = self.conv1(x)
    x = self.conv2(x)
    x = F.max_pool2d(x, 2)
    x = torch.flatten(x, 1)
    x = self.fc1(x)
    output = F.log_softmax(x, dim=1)
    return output
</code></pre>
<p>def train(input, target, model, rank, world_size, optimizer):<br />
    output = model(input)<br />
    loss = F.nll_loss(output, target, reduction='sum')<br />
    optimizer.zero_grad()<br />
    loss.backward()<br />
    optimizer.step()<br />
    return loss</p>
<p>def fsdp_main(rank, world_size):<br />
    os.environ['MASTER_ADDR'] = 'localhost'<br />
    os.environ['MASTER_PORT'] = '12355'<br />
    dist.init_process_group("nccl", rank=rank, world_size=world_size)<br />
    torch.cuda.set_device("cuda:{}".format(rank))<br />
    model = Net().to("cuda:{}".format(rank))<br />
    model = FSDP(model, device_id="cuda:{}".format(rank), use_orig_params=True).train()<br />
    optimizer = optim.Adadelta(model.parameters(), lr=0.1)<br />
    model = torch.compile(model)</p>
<pre><code>input = torch.randn(4, 1, 28, 28).to("cuda:{}".format(rank))
target = torch.randint(1, 10, (4,)).to("cuda:{}".format(rank))

with torch._dynamo.compiled_autograd.enable(torch.compile):
    train(input, target, model, rank, world_size, optimizer)
dist.destroy_process_group()
</code></pre>
<p>if <strong>name</strong> == '<strong>main</strong>':<br />
    WORLD_SIZE = 2<br />
    mp.spawn(fsdp_main, args=(WORLD_SIZE,), nprocs=WORLD_SIZE, join=True)<br />
```</p>
<p>Here is the simplified expected graph i can get when the <code>torch.distributed._functional_collectives.is_torchdynamo_compiling()</code> is manually set to True in pytorch source code:<br />
<code>def forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1):
    alias = torch.ops.aten.alias.default(arg1_1);  arg1_1 = None
    alias_1 = torch.ops.aten.alias.default(alias);  alias = None
    alias_2 = torch.ops.aten.alias.default(alias_1);  alias_1 = None
    alias_3 = torch.ops.aten.alias.default(alias_2);  alias_2 = None
    exp = torch.ops.aten.exp.default(alias_3);  alias_3 = None
    sum_1 = torch.ops.aten.sum.dim_IntList(arg0_1, [1], True)
    mul = torch.ops.aten.mul.Tensor(exp, sum_1);  exp = sum_1 = None
    .........
    cat = torch.ops.aten.cat.default([view_7, view_6, view_5, view_4, view_3, view_2]);  view_7 = view_6 = view_5 = view_4 = view_3 = view_2 = None
    accumulate_grad_ = torch.ops.inductor.accumulate_grad_.default(arg10_1, cat);  arg10_1 = None
    return (cat,)</code></p>
<p>Thank you.</p>
<h3>Versions</h3>
<p>PyTorch version: 2.3.0a0+git2de7468<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.28.3<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.9.7 (default, Oct 27 2021, 01:23:21)  [GCC 9.3.0] (64-bit runtime)<br />
Python platform: Linux-5.4.0-149-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration:<br />
GPU 0: NVIDIA A100-SXM4-40GB<br />
GPU 1: NVIDIA A100-SXM4-40GB<br />
GPU 2: NVIDIA A100-SXM4-40GB<br />
GPU 3: NVIDIA A100-SXM4-40GB<br />
GPU 4: NVIDIA A100-SXM4-40GB<br />
GPU 5: NVIDIA A100-SXM4-40GB<br />
GPU 6: NVIDIA A100-SXM4-40GB<br />
GPU 7: NVIDIA A100-SXM4-40GB</p>
<p>Nvidia driver version: 525.105.17<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   43 bits physical, 48 bits virtual<br />
CPU(s):                          256<br />
On-line CPU(s) list:             0-255<br />
Thread(s) per core:              2<br />
Core(s) per socket:              64<br />
Socket(s):                       2<br />
NUMA node(s):                    8<br />
Vendor ID:                       AuthenticAMD<br />
CPU family:                      23<br />
Model:                           49<br />
Model name:                      AMD EPYC 7742 64-Core Processor<br />
Stepping:                        0<br />
Frequency boost:                 enabled<br />
CPU MHz:                         3396.848<br />
CPU max MHz:                     2250.0000<br />
CPU min MHz:                     1500.0000<br />
BogoMIPS:                        4491.63<br />
Virtualization:                  AMD-V<br />
L1d cache:                       4 MiB<br />
L1i cache:                       4 MiB<br />
L2 cache:                        64 MiB<br />
L3 cache:                        512 MiB<br />
NUMA node0 CPU(s):               0-15,128-143<br />
NUMA node1 CPU(s):               16-31,144-159<br />
NUMA node2 CPU(s):               32-47,160-175<br />
NUMA node3 CPU(s):               48-63,176-191<br />
NUMA node4 CPU(s):               64-79,192-207<br />
NUMA node5 CPU(s):               80-95,208-223<br />
NUMA node6 CPU(s):               96-111,224-239<br />
NUMA node7 CPU(s):               112-127,240-255<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Not affected<br />
Vulnerability Retbleed:          Vulnerable<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==3.8.2<br />
[pip3] flake8-bugbear==20.1.4<br />
[pip3] flake8-coding==1.3.3<br />
[pip3] flake8-comprehensions==3.3.0<br />
[pip3] flake8-executable==2.0.4<br />
[pip3] flake8-logging-format==0.9.0<br />
[pip3] flake8-pyi==20.5.0<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.24.1<br />
[pip3] optree==0.10.0<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0a0+git2de7468<br />
[pip3] torchaudio==2.1.2+cu118<br />
[pip3] torchvision==0.16.2+cu118<br />
[conda] numpy                     1.24.1                   pypi_0    pypi<br />
[conda] optree                    0.10.0                   pypi_0    pypi<br />
[conda] pytorch-triton            3.0.0+901819d2b6          pypi_0    pypi<br />
[conda] torch                     2.3.0a0+git2de7468           dev_0    <develop><br />
[conda] torchaudio                2.1.2+cu118              pypi_0    pypi<br />
[conda] torchfix                  0.4.0                    pypi_0    pypi<br />
[conda] torchvision               0.16.2+cu118             pypi_0    pypi</p>]]></description>
      <pubDate>Sat, 02 Mar 2024 04:07:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121071</guid>
    </item>
    <item>
      <title>[Inductor] Outer Loop Fusion for CPP Backend</title>
      <link>https://github.com/pytorch/pytorch/pull/121064</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121064</p>
<p><strong>Summary</strong><br />
* Cases to optimize <br />
<code>def fn(x):
      max = torch.amax(x, dim=-1, keepdim=True)
      return x - max</code><br />
  Because <code>torch.amax</code> is a reduction operator that can't be fully fused with the subsequent <code>sub</code> node, two independent loops will be generated in this case.<br />
<code>for x0 in range(outer_size):
    for x1 in range(inner_size):
      amax(x1)
  for x0 in range(outer_size):
    for x1 in range(inner_size):
      sub(x1)</code><br />
  However, because the reduction is along the last dimension, we can actually fuse these two nodes along the outer dimensions, resulting in code generation as:<br />
<code>for x0 in range(outer_size):
    for x1 in range(inner_size):
      amax(x1)
    for x1 in range(inner_size):
      sub(x1)</code><br />
  This approach should offer improved cache locality.</p>
<ul>
<li>
<p>Changes made in this PR</p>
</li>
<li>
<p>We changed the kernel finalization process to adopt a lazy behavior. This means we'll gather the <code>CppKernelProxy</code> capable of outer loop fusion into a list, and finalize this list of <code>CppKernelProxy</code> lazily.</p>
</li>
<li>In the CPP <code>codegen_nodes</code>, we will first generate code for the outer loops and then proceed to generate code for the inner loops one by one.</li>
</ul>
<p><strong>Test Case</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_outer_loop_fusion</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Fri, 01 Mar 2024 18:45:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121064</guid>
    </item>
    <item>
      <title>can't compile torchvision RPN with AOTInductor</title>
      <link>https://github.com/pytorch/pytorch/issues/121036</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I try to compile an object detection model that uses torchvison's rpn module. it fails when generating anchors with <code>AssertionError: Mutating module attribute cell_anchors during export.</code> I think this is related to https://github.com/pytorch/pytorch/issues/108805</p>
<h3>Error logs</h3>
<p>```python<br />
I0301 19:15:52.323000 140521716340544 torch/fx/experimental/symbolic_shapes.py:1869] [0/0] create_env<br />
I0301 19:15:52.435000 140521716340544 torch/fx/experimental/symbolic_shapes.py:2620] [0/0] create_symbol s0 = 2 for L['batch_tensor'].size()[0] [2, 13] (_dynamo/variables/builder.py:1791 in <lambda>)<br />
V0301 19:15:52.437000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval True == True [statically known]<br />
V0301 19:15:52.437000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval False == False [statically known]<br />
V0301 19:15:52.537000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(s0, 9223372036854775807) == False [statically known]<br />
V0301 19:15:52.538000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval False == False [statically known]<br />
V0301 19:15:52.539000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval s0 &lt; 9223372036854775807 == True [statically known]<br />
V0301 19:15:52.575000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Ne(s0, 1) == True [statically known]<br />
V0301 19:15:52.592000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(s0, 1) == False [statically known]<br />
V0301 19:15:52.659000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval True == True [statically known]<br />
V0301 19:16:11.974000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(3<em>s0, 3) == False [statically known]<br />
V0301 19:16:11.975000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Ne(3</em>s0, 3) == True [statically known]<br />
I0301 19:16:12.160000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3622] [0/0] set_replacement s0 = 2 (solve_backed) ValueRanges(lower=2, upper=2, is_bool=False)<br />
I0301 19:16:12.162000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3622] [0/0] set_replacement s0 = 2 (find) ValueRanges(lower=2, upper=2, is_bool=False)<br />
I0301 19:16:12.163000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3832] [0/0] eval Eq(s0, 2) [guard added] at torchvision/models/detection/transform.py:243 in batch_images (_dynamo/variables/tensor.py:892 in evaluate_expr)<br />
V0301 19:16:12.171000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3902] [0/0] eval 2 [trivial]</p>
<hr />
<p>AssertionError                            Traceback (most recent call last)<br />
Cell In[2], line 15<br />
     12 width_dim = torch.export.Dim("width")<br />
     14 if not os.path.exists(model_path):<br />
---&gt; 15     so_path = torch._export.aot_compile(<br />
     16         f = model,<br />
     17         args = (x, ),<br />
     18         # Specify the first dimension of the input x as dynamic<br />
     19         dynamic_shapes={"batch_tensor": {0: batch_dim}},<br />
     20         # Specify the generated shared library path<br />
     21         options={<br />
     22             "aot_inductor.output_path": model_path,<br />
     23             "max_autotune": True,<br />
     24         },<br />
     25     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_export/<strong>init</strong>.py:382, in aot_compile(f, args, kwargs, constraints, dynamic_shapes, options, remove_runtime_assertions, disable_constraint_solver)<br />
    378     gm = torch.export._trace._export(f, args, kwargs, constraints, pre_dispatch=True).module()<br />
    379 else:<br />
    380     # We want to export to Torch IR here to utilize the pre_grad passes in<br />
    381     # inductor, which run on Torch IR.<br />
--&gt; 382     gm = _export_to_torch_ir(<br />
    383         f,<br />
    384         args,<br />
    385         kwargs,<br />
    386         constraints,<br />
    387         disable_constraint_solver=disable_constraint_solver,<br />
    388         # Disabling this flag, because instead we can rely on the mapping<br />
    389         # dynamo_flat_name_to_original_fqn which is coming from Dynamo.<br />
    390         restore_fqn=False,<br />
    391     )<br />
    392 flat_example_inputs = pytree.arg_tree_leaves(<em>args, </em>*(kwargs or {}))<br />
    394 with torch.no_grad():</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/export/_trace.py:320, in _export_to_torch_ir(f, args, kwargs, constraints, preserve_module_call_signature, disable_constraint_solver, restore_fqn, _log_export_usage)<br />
    316     module_call_specs: Dict[str, Dict[str, pytree.TreeSpec]] = {}<br />
    317     with _wrap_submodules(<br />
    318         f, preserve_module_call_signature, module_call_specs<br />
    319     ), _ignore_backend_decomps():<br />
--&gt; 320         gm_torch_level, _ = torch._dynamo.export(<br />
    321             f,<br />
    322             constraints=constraints,<br />
    323             assume_static_by_default=True,<br />
    324             tracing_mode="symbolic",<br />
    325             disable_constraint_solver=disable_constraint_solver,<br />
    326             _log_export_usage=_log_export_usage,<br />
    327         )(<br />
    328             <em>args,<br />
    329             </em>*kwargs,<br />
    330         )<br />
    331 except (ConstraintViolationError, ValueRangeError) as e:<br />
    332     raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: TRY200</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1314, in export.<locals>.inner(<em>args, </em><em>kwargs)<br />
   1312 # TODO(voz): We may have instances of <code>f</code> that mutate inputs, we should track sideeffects and reject.<br />
   1313 try:<br />
-&gt; 1314     result_traced = opt_f(</em>args, **kwargs)<br />
   1315 except ConstraintViolationError as e:<br />
   1316     constraint_violation_error = e</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1529, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1527     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1528 else:<br />
-&gt; 1529     return self._call_impl(*args, </strong>kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1538, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1533 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1534 # this function, and just call forward.<br />
   1535 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1536         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1537         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1538     return forward_call(</em>args, **kwargs)<br />
   1540 try:<br />
   1541     result = None</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:455, in _TorchDynamoContext.<strong>call</strong>.<locals>._fn(<em>args, </em><em>kwargs)<br />
    453 prior = set_eval_frame(callback)<br />
    454 try:<br />
--&gt; 455     return fn(</em>args, **kwargs)<br />
    456 finally:<br />
    457     set_eval_frame(prior)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1529, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1527     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1528 else:<br />
-&gt; 1529     return self._call_impl(*args, </strong>kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1538, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1533 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1534 # this function, and just call forward.<br />
   1535 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1536         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1537         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1538     return forward_call(</em>args, **kwargs)<br />
   1540 try:<br />
   1541     result = None</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:912, in catch_errors_wrapper.<locals>.catch_errors(frame, cache_entry, frame_state)<br />
    908             return hijacked_callback(frame, cache_entry, hooks, frame_state)<br />
    910 with compile_lock, _disable_current_modes():<br />
    911     # skip=1: skip this frame<br />
--&gt; 912     return callback(frame, cache_entry, hooks, frame_state, skip=1)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:398, in convert_frame_assert.<locals>._convert_frame_assert(frame, cache_entry, hooks, frame_state, skip)<br />
    384 compile_id = CompileId(frame_id, frame_compile_id)<br />
    386 signpost_event(<br />
    387     "dynamo",<br />
    388     "_convert_frame_assert._compile",<br />
   (...)<br />
    395     },<br />
    396 )<br />
--&gt; 398 return _compile(<br />
    399     frame.f_code,<br />
    400     frame.f_globals,<br />
    401     frame.f_locals,<br />
    402     frame.f_builtins,<br />
    403     compiler_fn,<br />
    404     one_graph,<br />
    405     export,<br />
    406     export_constraints,<br />
    407     hooks,<br />
    408     cache_size,<br />
    409     frame,<br />
    410     frame_state=frame_state,<br />
    411     compile_id=compile_id,<br />
    412     skip=skip + 1,<br />
    413 )</p>
<p>File /opt/conda/lib/python3.10/contextlib.py:79, in ContextDecorator.<strong>call</strong>.<locals>.inner(<em>args, </em><em>kwds)<br />
     76 @wraps(func)<br />
     77 def inner(</em>args, <strong>kwds):<br />
     78     with self._recreate_cm():<br />
---&gt; 79         return func(*args, </strong>kwds)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:669, in _compile(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)<br />
    659 log.debug(<br />
    660     "torchdynamo start compiling %s %s:%s, stack (elided %s frames):\n%s",<br />
    661     code.co_name,<br />
   (...)<br />
    666     "".join(traceback.format_list(traceback.extract_stack()[: -2 - skip])),<br />
    667 )<br />
    668 try:<br />
--&gt; 669     guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
    670     return guarded_code<br />
    671 except (<br />
    672     Unsupported,<br />
    673     TorchRuntimeError,<br />
   (...)<br />
    680     BisectValidationException,<br />
    681 ) as e:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py:256, in dynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper(<em>args, </em><em>kwargs)<br />
    254 with torch.profiler.record_function(f"{key} (dynamo_timed)"):<br />
    255     t0 = time.time()<br />
--&gt; 256     r = func(</em>args, **kwargs)<br />
    257     time_spent = time.time() - t0<br />
    258 compilation_time_metrics[key].append(time_spent)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:542, in _compile.<locals>.compile_inner(code, one_graph, hooks, transform)<br />
    540 CompileContext.get().attempt = attempt<br />
    541 try:<br />
--&gt; 542     out_code = transform_code_object(code, transform)<br />
    543     break<br />
    544 except exc.RestartAnalysis as e:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1033, in transform_code_object(code, transformations, safe)<br />
   1030 instructions = cleaned_instructions(code, safe)<br />
   1031 propagate_line_nums(instructions)<br />
-&gt; 1033 transformations(instructions, code_options)<br />
   1034 return clean_and_assemble_instructions(instructions, keys, code_options)[1]</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:163, in preserve_global_state.<locals>._fn(<em>args, </em><em>kwargs)<br />
    161 cleanup = setup_compile_debug()<br />
    162 try:<br />
--&gt; 163     return fn(</em>args, **kwargs)<br />
    164 finally:<br />
    165     cleanup.close()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:507, in _compile.<locals>.transform(instructions, code_options)<br />
    505 try:<br />
    506     with tracing(tracer.output.tracing_context), tracer.set_current_tx():<br />
--&gt; 507         tracer.run()<br />
    508 except exc.UnspecializeRestartAnalysis:<br />
    509     speculation_log.clear()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2130, in InstructionTranslator.run(self)<br />
   2129 def run(self):<br />
-&gt; 2130     super().run()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1308, in InstructionTranslatorBase.STORE_ATTR(self, inst)<br />
   1303 val, obj = self.popn(2)<br />
   1305 if isinstance(obj, NNModuleVariable):<br />
   1306     # We don't allow side effects during export<br />
   1307     # https://github.com/pytorch/torchdynamo/issues/1475<br />
-&gt; 1308     assert (<br />
   1309         not self.export<br />
   1310     ), f"Mutating module attribute {inst.argval} during export."<br />
   1312 try:<br />
   1313     BuiltinVariable(setattr).call_function(<br />
   1314         self, [obj, ConstantVariable.create(inst.argval), val], {}<br />
   1315     )</p>
<p>AssertionError: Mutating module attribute cell_anchors during export.</p>
<p>from user code:<br />
   File "/opt/workspace/./satlas-src/satlas/model/model.py", line 841, in forward<br />
    cur_outputs, _ = head(batch_tensor, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/opt/workspace/<a href="http://127.0.0.1:8888/lab/tree/satlas-src/satlas/model/model.py#line=530">./satlas-src/satlas/model/model.py", line 531</a>, in forward<br />
    proposals, proposal_losses = self.rpn(images, features, targets)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/rpn.py", line 362, in forward<br />
    anchors = self.anchor_generator(images, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 126, in forward<br />
    self.set_cell_anchors(dtype, device)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 77, in set_cell_anchors<br />
    self.cell_anchors = [cell_anchor.to(dtype=dtype, device=device) for cell_anchor in self.cell_anchors]<br />
```</p>
<h3>Minified repro</h3>
<p>os.environ["TORCH_DYNAMO_REPRO_AFTER"] = "aot" doesn't seem to do anything. after I run the code, the traceback is the same.</p>
<p>a minimal repro is to do</p>
<p>```python<br />
import torch<br />
import torch._dynamo as torchdynamo<br />
from torchvision.models.detection import (<br />
    maskrcnn_resnet50_fpn,<br />
)</p>
<p>import torch._dynamo.config</p>
<h1>torch._dynamo.config.capture_scalar_outputs = True</h1>
<p>net = maskrcnn_resnet50_fpn()<br />
net.eval()</p>
<h1>Single input which is a list of images.</h1>
<p>images = [torch.rand(3, 16, 16)]</p>
<h1>Double-check that the inputs work for the normal net.</h1>
<p>net(images)<br />
torch._dynamo.export(net, images, tracing_mode="symbolic", aten_graph=True)</p>
<p>```</p>
<p>```python<br />
AssertionError: Mutating module attribute cell_anchors during export.</p>
<p>from user code:<br />
   File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py", line 104, in forward<br />
    proposals, proposal_losses = self.rpn(images, features, targets)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/rpn.py", line 362, in forward<br />
    anchors = self.anchor_generator(images, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(</em>args, **kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 126, in forward<br />
    self.set_cell_anchors(dtype, device)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 77, in set_cell_anchors<br />
    self.cell_anchors = [cell_anchor.to(dtype=dtype, device=device) for cell_anchor in self.cell_anchors]</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information<br />
```</p>
<p>from the test script here: https://docs.google.com/document/d/159NTQQhz8ovIBxbQvGQ-fZ10pF9e2RPXm1JZYqdEzt4/edit from this issue https://github.com/pytorch/pytorch/issues/108805</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.3.0.dev20240221<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.26.4<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.1.105<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090<br />
Nvidia driver version: 535.161.07<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      39 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             12<br />
On-line CPU(s) list:                0-11<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz<br />
CPU family:                         6<br />
Model:                              158<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 6<br />
Socket(s):                          1<br />
Stepping:                           10<br />
CPU max MHz:                        4600.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           6399.96<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp vnmi md_clear flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          192 KiB (6 instances)<br />
L1i cache:                          192 KiB (6 instances)<br />
L2 cache:                           1.5 MiB (6 instances)<br />
L3 cache:                           12 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-11<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable<br />
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Meltdown:             Mitigation; PTI<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Mitigation; IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Mitigation; Microcode<br />
Vulnerability Tsx async abort:      Mitigation; TSX disabled</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==5.0.4<br />
[pip3] mypy==1.0.1<br />
[pip3] mypy-extensions==0.4.4<br />
[pip3] numpy==1.26.4<br />
[pip3] optree==0.10.0<br />
[pip3] torch==2.3.0.dev20240221<br />
[pip3] torchaudio==2.2.0.dev20240221<br />
[pip3] torchelastic==0.2.2<br />
[pip3] torchvision==0.18.0.dev20240221<br />
[pip3] triton==2.2.0<br />
[pip3] vit-pytorch==1.6.5<br />
[conda] blas                      1.0                         mkl<br />
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch-nightly<br />
[conda] mkl                       2023.1.0         h213fc3f_46344<br />
[conda] mkl-service               2.4.0           py310h5eee18b_1<br />
[conda] mkl_fft                   1.3.8           py310h5eee18b_0<br />
[conda] mkl_random                1.2.4           py310hdb19cb5_0<br />
[conda] numpy                     1.26.4                   pypi_0    pypi<br />
[conda] optree                    0.10.0                   pypi_0    pypi<br />
[conda] pytorch                   2.3.0.dev20240221 py3.10_cuda12.1_cudnn8.9.2_0    pytorch-nightly<br />
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch-nightly<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly<br />
[conda] torchaudio                2.2.0.dev20240221     py310_cu121    pytorch-nightly<br />
[conda] torchelastic              0.2.2                    pypi_0    pypi<br />
[conda] torchvision               0.18.0.dev20240221     py310_cu121    pytorch-nightly<br />
[conda] triton                    2.2.0                    pypi_0    pypi<br />
[conda] vit-pytorch               1.6.5                    pypi_0    pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4</p>]]></description>
      <pubDate>Fri, 01 Mar 2024 11:34:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121036</guid>
    </item>
    <item>
      <title>[Quant][Inductor] Fix q/dq per channel lowering with 64-bit qparams</title>
      <link>https://github.com/pytorch/pytorch/pull/120984</link>
      <description><![CDATA[<p>Fixes #120869</p>
<p>Fix lowering of <code>quantize_per_channel</code> and <code>dequantize_per_channel</code> with float64 scale and int64 zero point.<br />
Generated codes are incorrect without explicit type conversion. Add type conversion to the lowering pass, i.e., float64 (double) -&gt; float32 and int64 -&gt; int32.</p>
<p><strong>Test plan</strong><br />
python test/inductor/test_cpu_repro.py -k test_per_channel_fake_quant_module_uint8</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Fri, 01 Mar 2024 01:29:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120984</guid>
    </item>
    <item>
      <title>torch.compile() and optimizer UX issues</title>
      <link>https://github.com/pytorch/pytorch/issues/120814</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I was trying to get some toy training loop going and there were a few sharp bits that we should probably document in 1 place</p>
<ol>
<li>Need to instantiate both <code>optimizer</code> and <code>criterion</code> outside of the compiled function</li>
<li>There are graph breaks on <code>optimizer.zero_grad()</code> and <code>optimizer.step()</code>, those graph breaks are deliberate since per @janeyx99 these would take too long to trace but users might still to try to fix them to make their models go faster</li>
<li><code>loss.backward()</code> fails in fullgraph with an <code>torch._dynamo.exc.Unsupported: Tensor.backward</code> error</li>
</ol>
<p>There might be more, will update this issue if I find them</p>
<h3>Error logs</h3>
<p>https://gist.github.com/msaroufim/15a4b97c3f45cead4b2feb90894ed8d3</p>
<h3>Minified repro</h3>
<p>n</p>
<h3>Versions</h3>
<p>Latest stable</p>
<p>cc @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar @ezyang @bdhirsh @anijain2305 @zou3519 @chauhang @mlazos</p>]]></description>
      <pubDate>Wed, 28 Feb 2024 11:24:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120814</guid>
    </item>
    <item>
      <title>Inductor INFO logs are kind of spammy</title>
      <link>https://github.com/pytorch/pytorch/issues/120774</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Internal xref: <a href="https://www.internalfb.com/mlhub/pipelines/runs/mast/f536225548-TrainingApplication">internalfb.com/mlhub/pipelines/runs/mast/f536225548-TrainingApplication</a></p>
<p>This job is a very simple job that @ckluk2 made which does a "hello world" pyper run where we just do a few matrix multiplies in the model body and are done. It can be launched with <a href="https://www.internalfb.com/intern/anp/view/?id=4942539">internalfb.com/intern/anp/view/?id=4942539</a></p>
<p>Inductor INFO logs seem spammy. https://github.com/pytorch/pytorch/issues/120771 is a particularly egregious case (for the WARNING) level, but the INFO logs are also trouble.</p>
<p><code>[rank0]:I0227 20:01:02.237000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] BatchLayernormFusion: key = ('batch_layernorm', 'torch.Size([3072, 128])', 'torch.Size([128])', 'torch.Size([128])', '(128,)', '1e-05'); subset size = 7 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:01:02.559000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] BatchSigmoidPreGradFusion: key = ('batch_sigmoid_pre_grad', 'torch.Size([3072, 1])', 'False'); subset size = 6 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:01:02.639000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] BatchSigmoidPreGradFusion: key = ('batch_sigmoid_pre_grad', 'torch.Size([3072, 128])', 'False'); subset size = 7 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:W0227 20:01:03.089000 139718474198016 torch/_inductor/fx_passes/split_cat.py:186] [0/0] example value absent for node: sigmoid_37 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/split_cat.py:186"
[rank0]:W0227 20:01:03.091000 139718474198016 torch/_inductor/fx_passes/split_cat.py:274] [0/0] example value absent for node: stack_13 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/split_cat.py:274"
[rank0]:W0227 20:01:03.095000 139718474198016 torch/_inductor/fx_passes/split_cat.py:186] [0/0] example value absent for node: sigmoid_38 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/split_cat.py:186"
[rank0]:W0227 20:01:03.095000 139718474198016 torch/_inductor/fx_passes/split_cat.py:274] [0/0] example value absent for node: stack_14 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/split_cat.py:274"
[rank0]:W0227 20:01:03.095000 139718474198016 torch/_inductor/fx_passes/split_cat.py:186] [0/0] example value absent for node: add_51 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/split_cat.py:186"
[rank0]:W0227 20:01:03.095000 139718474198016 torch/_inductor/fx_passes/split_cat.py:274] [0/0] example value absent for node: stack_10 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/split_cat.py:274"
[rank0]:W0227 20:01:03.097000 139718474198016 torch/_inductor/fx_passes/split_cat.py:274] [0/0] example value absent for node: stack_12 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/split_cat.py:274"
[rank0]:W0227 20:01:03.097000 139718474198016 torch/_inductor/fx_passes/split_cat.py:274] [0/0] example value absent for node: stack_11 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/split_cat.py:274"
[rank0]:I0227 20:01:50.542000 139718474198016 torch/_dynamo/logging.py:55] [0/0] Step 3: torchinductor compiling FORWARDS graph 0 #threadId="139718474198016" #callSite="torch/_dynamo/logging.py:55"
[rank0]:I0227 20:01:54.287000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072, 256]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 8 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:01:54.325000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072, 128]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 11 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:01:54.934000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] GroupLinearFusion: key = ('group_linear', False); subset size = 21 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:01:55.006000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] GroupLinearFusion: key = ('group_linear', False); subset size = 11 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:01:55.074000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] GroupLinearFusion: key = ('group_linear', False); subset size = 6 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:01:55.131000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] GroupLinearFusion: key = ('group_linear', False); subset size = 5 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:01:55.192000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] GroupLinearFusion: key = ('group_linear', False); subset size = 5 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0] Creating implicit fallback for: #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]   target: fb.to_dense_representation.default #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]   args[0]: TensorBox(StorageBox( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]     Pointwise( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       'cuda', #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       torch.float32, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       def inner_fn(index): #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           i0 = index #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp0 = ops.load(primals_401, i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp1 = ops.to_dtype(tmp0, torch.float32, src_dtype=torch.int32) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp2 = ops.load(primals_403, 0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp3 = tmp1 + tmp2 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           return tmp3 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       , #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       ranges=[3072], #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origin_node=add_163, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origins={add_163, convert_element_type} #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]     ) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]   )) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]   args[1]: TensorBox(StorageBox( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]     Pointwise( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       'cuda', #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       torch.int64, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       def inner_fn(index): #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           i0 = index #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp0 = ops.load(primals_402, i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp1 = ops.to_dtype(tmp0, torch.int64, src_dtype=torch.int32) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           return tmp1 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       , #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       ranges=[3072], #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origin_node=convert_element_type_1, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origins={convert_element_type_1} #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]     ) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]   )) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]   args[2]: TensorBox(StorageBox( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]     Pointwise( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       'cuda', #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       torch.int64, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       def inner_fn(index): #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           i0 = index #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp0 = ops.constant(0, torch.int64) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]           return tmp0 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       , #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       ranges=[3072], #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origin_node=full_default, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origins={full_default} #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]     ) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]   )) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.194000 139718474198016 torch/_inductor/graph.py:789] [0/0]   args[3]: 1 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0] Creating implicit fallback for: #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]   target: fb.conditional.default #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]   args[0]: TensorBox(StorageBox( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]     Pointwise( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       'cuda', #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       torch.bool, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       def inner_fn(index): #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           i0 = index #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp0 = ops.load(buf66, i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp1 = ops.to_dtype(tmp0, torch.int64, src_dtype=torch.float32) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp2 = ops.load(primals_404, 0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp3 = tmp1 &gt; tmp2 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           return tmp3 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       , #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       ranges=[3072], #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origin_node=gt, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origins={gt, convert_element_type_2} #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]     ) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]   )) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]   args[1]: TensorBox(StorageBox( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]     ComputedBuffer(name='buf64', layout=FixedLayout('cuda', torch.int64, size=[3072], stride=[1]), data=Pointwise( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       'cuda', #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       torch.int64, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       def inner_fn(index): #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           i0 = index #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp0 = ops.constant(0, torch.int64) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           return tmp0 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       , #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       ranges=[3072], #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origin_node=full_default, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origins={full_default} #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]     )) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]   )) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]   args[2]: TensorBox(StorageBox( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]     Pointwise( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       'cuda', #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       torch.int64, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       def inner_fn(index): #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           i0 = index #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp0 = ops.load(buf66, i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp1 = ops.to_dtype(tmp0, torch.int64, src_dtype=torch.float32) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]           return tmp1 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       , #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       ranges=[3072], #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origin_node=convert_element_type_2, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origins={convert_element_type_2} #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]     ) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:01:58.210000 139718474198016 torch/_inductor/graph.py:789] [0/0]   )) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0] Creating implicit fallback for: #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]   target: fb.scale_gradient.default #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]   args[0]: TensorBox(StorageBox( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]     ConcatKernel(name='buf918', layout=FixedLayout('cuda', torch.float32, size=[3072, 2049], stride=[2049, 1]), inputs=[ComputedBuffer(name='buf916', layout=AliasedLayout('cuda', torch.float32, size=[3072, 1], stride=[2049, 1]), data=Pointwise( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       'cuda', #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       torch.float32, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       def inner_fn(index): #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           i0, _ = index #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp0 = ops.load(buf61, 4 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp1 = ops.load(primals_191, 115840 + 121060 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp2 = tmp0 + tmp1 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp3 = ops.load(buf61, 1 + 4 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp4 = ops.load(primals_191, 115841 + 121060 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp5 = tmp3 + tmp4 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp6 = tmp2 + tmp5 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp7 = ops.load(buf61, 2 + 4 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp8 = ops.load(primals_191, 115842 + 121060 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp9 = tmp7 + tmp8 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp10 = tmp6 + tmp9 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp11 = ops.load(buf61, 3 + 4 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp12 = ops.load(primals_191, 115843 + 121060 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp13 = tmp11 + tmp12 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp14 = tmp10 + tmp13 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           return tmp14 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       , #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       ranges=[3072, 1], #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origin_node=sum_1, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origins={add_161, sum_1} #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]     )), ComputedBuffer(name='buf917', layout=AliasedLayout('cuda', torch.float32, size=[3072, 2048], stride=[2049, 1]), data=Pointwise( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       'cuda', #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       torch.float32, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       def inner_fn(index): #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           i0, i1 = index #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp0 = ops.load(buf892, i1 + 2048 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp1 = ops.load(buf904, i1 + 2048 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp2 = tmp0 + tmp1 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp3 = ops.load(buf911, i1 + 2048 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp4 = tmp2 + tmp3 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp5 = ops.load(buf892, i1 + 2048 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp6 = ops.load(buf904, i1 + 2048 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp7 = tmp5 + tmp6 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp8 = ops.load(buf911, i1 + 2048 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp9 = tmp7 + tmp8 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp10 = ops.load(buf912, i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp11 = tmp9 - tmp10 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp12 = ops.load(buf915, i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp13 = tmp11 * tmp12 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp14 = ops.load(primals_189, i1) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp15 = tmp13 * tmp14 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp16 = ops.load(primals_190, i1) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp17 = tmp15 + tmp16 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp18 = ops.sigmoid(tmp17) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           tmp19 = tmp4 * tmp18 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]           return tmp19 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       , #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       ranges=[3072, 2048], #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origin_node=mul_192, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origins={add_147, mul_190, add_152, mul_191, add_150, sigmoid... #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]     ))]) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]   )) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:11.895000 139718474198016 torch/_inductor/graph.py:789] [0/0]   args[1]: 1.0 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0] Creating implicit fallback for: #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]   target: fb.bucketize.default #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]   args[0]: TensorBox( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]     ReinterpretView( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]       StorageBox( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]         ComputedBuffer(name='buf950', layout=FixedLayout('cuda', torch.float32, size=[6, 3072, 1], stride=[3072, 1, 1]), data=Pointwise( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]           'cuda', #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]           torch.float32, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]           def inner_fn(index): #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]               i0, i1, _ = index #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]               tmp0 = ops.load(buf949, i1 + 3072 * i0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]               tmp1 = ops.sigmoid(tmp0) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]               return tmp1 #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]           , #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]           ranges=[6, 3072, 1], #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]           origin_node=sigmoid_47, #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]           origins={sigmoid_47} #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]         )) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]       ), #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]       FixedLayout('cuda', torch.float32, size=[3072], stride=[1], offset=12288), #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]       origins={view_62} #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]     ) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]   ) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]   args[1]: TensorBox(StorageBox( #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]     InputBuffer(name='primals_405', layout=FixedLayout('cuda', torch.float32, size=[4999], stride=[1])) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:02:12.576000 139718474198016 torch/_inductor/graph.py:789] [0/0]   )) #threadId="139718474198016" #callSite="torch/_inductor/graph.py:789"
[rank0]:I0227 20:05:29.510000 139718474198016 torch/_inductor/fb/utils.py:205] [0/0] [pt2]: log_module_code() is disabled #threadId="139718474198016" #callSite="torch/_inductor/fb/utils.py:205"
[rank0]:I0227 20:05:29.513000 139718474198016 torch/_dynamo/logging.py:55] [0/0] Step 3: torchinductor done compiling FORWARDS graph 0 #threadId="139718474198016" #callSite="torch/_dynamo/logging.py:55"
[rank0]:I0227 20:05:34.485000 139718474198016 torch/_dynamo/logging.py:55] [0/0] Step 3: torchinductor compiling FORWARDS graph 1 #threadId="139718474198016" #callSite="torch/_dynamo/logging.py:55"
[rank0]:I0227 20:05:35.102000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 8 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:05:35.116000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 5 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:05:35.140000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] BatchSubPostGradFusion: key = ('batch_sub.tensor_post_grad', '[3072]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 8 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:05:35.154000 139718474198016 torch/_inductor/fx_passes/group_batch_fusion.py:922] [0/0] BatchSubPostGradFusion: key = ('batch_sub.tensor_post_grad', '[3072]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 5 #threadId="139718474198016" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:06:01.877000 139718474198016 torch/_inductor/fb/utils.py:205] [0/0] [pt2]: log_module_code() is disabled #threadId="139718474198016" #callSite="torch/_inductor/fb/utils.py:205"
[rank0]:I0227 20:06:01.878000 139718474198016 torch/_dynamo/logging.py:55] [0/0] Step 3: torchinductor done compiling FORWARDS graph 1 #threadId="139718474198016" #callSite="torch/_dynamo/logging.py:55"
[rank0]:I0227 20:06:02.126000 139718474198016 torch/_dynamo/logging.py:55] [0/0] Step 2: done compiler function compile_fn #threadId="139718474198016" #callSite="torch/_dynamo/logging.py:55"
[rank0]:I0227 20:06:02.857000 139718474198016 torch/fx/experimental/symbolic_shapes.py:2683] [0/0] produce_guards #threadId="139718474198016" #callSite="torch/fx/experimental/symbolic_shapes.py:2683"
[rank0]:I0227 20:06:42.468000 139650725316160 torch/_dynamo/logging.py:55] Step 3: torchinductor compiling BACKWARDS graph 1 #threadId="139650725316160" #callSite="torch/_dynamo/logging.py:55"
[rank0]:I0227 20:06:42.824000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 7 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:06:42.902000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] GroupLinearFusion: key = ('group_linear', True); subset size = 6 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:25.075000 139650725316160 torch/_inductor/fb/utils.py:205] [pt2]: log_module_code() is disabled #threadId="139650725316160" #callSite="torch/_inductor/fb/utils.py:205"
[rank0]:I0227 20:07:25.076000 139650725316160 torch/_dynamo/logging.py:55] Step 3: torchinductor done compiling BACKWARDS graph 1 #threadId="139650725316160" #callSite="torch/_dynamo/logging.py:55"
[rank0]:I0227 20:07:30.949000 139650725316160 torch/_dynamo/logging.py:55] Step 3: torchinductor compiling BACKWARDS graph 0 #threadId="139650725316160" #callSite="torch/_dynamo/logging.py:55"
[rank0]:I0227 20:07:34.266000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] PostGradBatchLinearFusion: key = ('batch_linear', 512, 3072, 5120, False); subset size = 8 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:34.274000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] PostGradBatchLinearFusion: key = ('batch_linear', 512, 3072, 512, False); subset size = 8 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:34.281000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] PostGradBatchLinearFusion: key = ('batch_linear', 1024, 3072, 2048, False); subset size = 8 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:34.288000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] PostGradBatchLinearFusion: key = ('batch_linear', 2048, 3072, 1024, False); subset size = 8 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:34.290000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] PostGradBatchLinearFusion: key = ('batch_linear', 256, 3072, 128, False); subset size = 5 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:34.905000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchAddPostGradFusion: key = ('batch_add.tensor_post_grad', '[3072, 256]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 8 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:34.911000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchAddPostGradFusion: key = ('batch_add.tensor_post_grad', '[3072, 256]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 5 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:34.951000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchAddPostGradFusion: key = ('batch_add.tensor_post_grad', '[3072, 128]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 10 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:34.955000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchAddPostGradFusion: key = ('batch_add.tensor_post_grad', '[3072, 1]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 5 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.007000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchAddPostGradFusion: key = ('batch_add.tensor_post_grad', '[3072, 128]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 7 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.024000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchAddPostGradFusion: key = ('batch_add.tensor_post_grad', '[3072, 128]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 7 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.199000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchAddPostGradFusion: key = ('batch_add.tensor_post_grad', '[3072, 128]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 7 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.577000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072, 512]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 20 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.620000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072, 2048]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 16 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.645000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072, 1024]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 12 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.679000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072, 256]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 18 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.690000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072, 256]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 6 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.712000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072, 128]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 7 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.734000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072, 256]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 12 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.899000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072, 128]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 10 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:35.961000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchMulPostGradFusion: key = ('batch_mul.tensor_post_grad', '[3072, 128]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 7 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:37.650000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] BatchSubPostGradFusion: key = ('batch_sub.tensor_post_grad', '[3072, 256]', 'torch.float32', 'torch.float32', '1.0', 'None'); subset size = 6 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:38.781000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] GroupLinearFusion: key = ('group_linear', True); subset size = 91 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:38.889000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] GroupLinearFusion: key = ('group_linear', True); subset size = 6 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:39.008000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] GroupLinearFusion: key = ('group_linear', True); subset size = 18 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:07:39.010000 139650725316160 torch/_inductor/fx_passes/group_batch_fusion.py:922] GroupLinearFusion: key = ('group_linear', True); subset size = 6 #threadId="139650725316160" #callSite="torch/_inductor/fx_passes/group_batch_fusion.py:922"
[rank0]:I0227 20:19:20.697000 139650725316160 torch/_inductor/fb/utils.py:205] [pt2]: log_module_code() is disabled #threadId="139650725316160" #callSite="torch/_inductor/fb/utils.py:205"
[rank0]:I0227 20:19:20.699000 139650725316160 torch/_dynamo/logging.py:55] Step 3: torchinductor done compiling BACKWARDS graph 0 #threadId="139650725316160" #callSite="torch/_dynamo/logging.py:55"</code></p>
<p>There are two particular logs types which seem problematic:</p>
<ul>
<li>The fusion logs: BatchLayernormFusion, BatchSigmoidPreGradFusion, etc (@yanboliang @jackiexu1992 )</li>
<li>Creating implicit fallback for ... (@jansel)</li>
</ul>
<p>Do we need these at INFO level? Or can they go to DEBUG level?</p>
<p>cc @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>
<h3>Versions</h3>
<p>main</p>]]></description>
      <pubDate>Tue, 27 Feb 2024 20:57:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120774</guid>
    </item>
    <item>
      <title>[WIP][inductor] padding</title>
      <link>https://github.com/pytorch/pytorch/pull/120758</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120758</p>
<p>This is not fully ready for review yet. Sending it out in case people want to give early feedback. Also it can allow me run CI perf test earlier.</p>
<p>By testing BlenderbotSmallForConditionalGeneration I already see 2.5ms speedup. I feel the solution can be further improved to get more speedup.</p>
<p>cc @jansel @Chillee @eellison </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames</p>]]></description>
      <pubDate>Tue, 27 Feb 2024 17:00:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120758</guid>
    </item>
    <item>
      <title>[inductor] Difference between using `mode="reduce-overhead"` and directly recording the CUDAGraph for `mode="default"`</title>
      <link>https://github.com/pytorch/pytorch/issues/120733</link>
      <description><![CDATA[<h2>Issue description</h2>
<p>If I call <code>torch.compile(model, mode="reduce-overhead")</code> versus use with <code>torch.cuda.graph(self.graph, pool=memory_pool): compiled_model(*inputs)</code> to record the kernels invoked by <code>mode="default"</code>, should I expect a difference in performance?</p>
<p>Additionally, CUDAGraphs with <code>mode="reduce-overhead"</code> seems to not allow mutated inputs, whereas <code>torch.cuda.graph</code> allows these to be recorded. Is this a limitation of the reduce-overhead mode, and are there any workarounds, for instance if there is a custom in-place-only kernel in the graph?</p>
<p>https://github.com/pytorch/pytorch/blob/f36e00b8cef66153ee210dc0781674bebfad4448/torch/_inductor/cudagraph_utils.py#L48-L50</p>
<h2>Code example</h2>
<p>```python<br />
import torch<br />
import torchvision.models as models</p>
<p>model = models.resnet18(pretrained=True).eval().cuda().half()<br />
inputs = [torch.rand((1, 3, 224, 224), dtype=torch.half).cuda()]</p>
<p>"""<br />
Method 1<br />
"""<br />
reduce_overhead = torch.compile(model, mode="reduce-overhead")<br />
reduce_overhead(*inputs)</p>
<p>"""<br />
Method 2<br />
"""<br />
default = torch.compile(model, mode="default")<br />
out = default(*inputs)</p>
<p>graph = torch.cuda.CUDAGraph()<br />
with torch.cuda.graph(graph):<br />
    out = default(*inputs)</p>
<p>graph.replay()<br />
```</p>
<h2>System Info</h2>
<p><code>torch==2.3.0.dev20240222+cu121</code></p>
<p>cc @mcarilli @ezyang @eellison @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 27 Feb 2024 13:25:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120733</guid>
    </item>
    <item>
      <title>[compiled autograd] support custom ops backed by c++ autograd::Function</title>
      <link>https://github.com/pytorch/pytorch/pull/120681</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120681</p>
<ul>
<li>Adds support for custom ops backed by c++ custom autograd functions, e.g. fbgemm</li>
<li>Include files more granularly to avoid namespace pollution and circular imports</li>
</ul>
<p>limitations:<br />
- requires user to audit their code and opt-in their custom autograd::Function via autograd::Function::is_traceable and maybe additional compiled_args + apply_with_saved implementation. this was the only way I can think of for soundness<br />
- will throw if we can't hash the saved_data i.e. for any non implemented type other than list and dict in at::IValue::hash https://github.com/pytorch/pytorch/blob/b0cfa96e82d7fbd02f5dbcef2632714caf89615d/aten/src/ATen/core/ivalue.cpp#L364<br />
- can technically silently fail if both the typeid hash and the typeid string name of the custom autograd::Function collide at the same time, and an identical autograd graph containing a different custom autograd::Function, yet that has an identical implementation, is called. this case seems extremely unlikely, and the only alternative to hash collision i can think of is compiling with reflection<br />
- tensors not saved via save_variables are not lifted, and are specialized on TensorImpl*'s hash (treated as a memory address). if needed, we can lift them.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 26 Feb 2024 17:38:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120681</guid>
    </item>
    <item>
      <title>Inductor oneDNN Graph integration</title>
      <link>https://github.com/pytorch/pytorch/pull/120301</link>
      <description><![CDATA[<h2>Inductor-CPU integration with oneDNN Graph (base PR)</h2>
<p>Uses Inductor pattern-matcher to offload some compute to oneDNN Graph library, which has fusions for compute-intensive ops such as MHA &amp; MLP. This is the first PR that adds oneDNN Graph support, so that subsequent PRs can add more pattern replacements that use oneDNN Graph. This PR only adds a replacement for one pattern (GPT2 MHA/SDPA pattern, which is different for batch size 1 &amp; batch size &gt; 1). Currently, functionality is limited to inference &amp; only Linux is supported.</p>
<h4>Details</h4>
<p>In Inductor joint graph passes, if <code>config.onednn_graph</code> is enabled, or the env variable <code>TORCHINDUCTOR_ONEDNN_GRAPH=1</code> is used on a machine that supports some advanced ISAs, then if some specific patterns are matched, the corresponding ops are handled by oneDNN Graph. However, if oneDNN Graph is unable to match a pattern, then it's handled by the same ops that'd have handled it sans oneDNN Graph.</p>
<p>BF16 dtype requires a machine to support <code>AVX512_BF16</code> ISA.<br />
While FP32 dtype support should also work on Xeon SP generation 1 (SkyLake SP), which does not support <code>AVX512_VNNI</code> ISA, the lowest machine configuration for which this functionality can be enabled is Xeon SP generation 2 (Cascade Lake), but may be changed in a subsequent PR to include Skylake SP platform support as well.</p>
<h4>Summary of other changes</h4>
<ul>
<li>Adds an API to check if oneDNN Graph was built.</li>
<li>Serialization of SDPA patterns can now be done for some corner-cases, such as only inference.</li>
<li>Serialized attention patterns  can also have <code>getitem</code> in them, which should be replaced by <code>operator.getitem</code>.</li>
<li>Currently, oneDNN Graph is not used if dynamic shapes are provided at compile time.</li>
<li>In oneDNN v3.3.x, the GPT2 MHA pattern needs an attention mask, so in order to match cases of attention mask not being present, an attention mask of all zeros is being created. The performance is still quite high compared to the default implementation, which means it'd be even better with oneDNN v3.5, which would make attention mask optional for GPT2 MHA pattern.</li>
</ul>
<h3>Performance data</h3>
<p>These performance datapoints were collected for a single TorchBench run (TorchBench may have run the workload multiple times) of each configuration, and were not cherry-picked, so it's possible that performance may be even better with this feature enabled -</p>
<p>|MODEL |BATCH SIZE | DTYPE | BEFORE: Speedup over eager-mode with the default Inductor implementation | AFTER: Speedup over eager mode with the default Inductor implementation| Perf boost = (AFTER - BEFORE)/BEFORE * 100|<br />
|--------------------------|-------------|---------|-----------------------------|--------------------------|------------|<br />
|hf_GPT2| 1 | FP32 | 1.522x | 1.784x| 17.21%|<br />
|hf_GPT2| 1 | BF16 (AMP) | 1.795x | 2.602x| 44.91%|<br />
|hf_GPT2| 2 | FP32 |  1.313x |1.629x | 24.06%|<br />
|hf_GPT2|2| BF16 (AMP) | 1.556x | 2.107x | 35.41%|<br />
|hf_GPT2_large| 1 | FP32 | 1.380x |1.510x | 9.4%|<br />
|hf_GPT2_large| 1 | BF16 (AMP) | 1.208x | 1.653x | 36.83%|<br />
|hf_GPT2_large| 2 | FP32 | 1.188x | 1.508x | 26.93%|<br />
|hf_GPT2_large|2| BF16 (AMP) | 0.991x | 1.771x | 78.70%|</p>
<p>Config: 48 physical cores of one socket of Intel Xeon Platinum 8468H (Xeon SP 4th gen a.k.a. Sapphire Rapids) . Only one logical core of a physical core was used. Intel OpenMP &amp; tcmalloc were preloaded.</p>
<p>Example of running the workload:<br />
<code>TORCHINDUCTOR_ONEDNN_GRAPH=1 OMP_NUM_THREADS=48 MKL_NUM_THREADS=48 numactl --membind=0 -C 0-47 python benchmarks/dynamo/torchbench.py --performance --inference --inductor --float32 -dcpu --only hf_GPT2 --freezing --batch-size 2</code></p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @ashokei @jingxu10 @gujinghui @PenghuiCheng @jianyuh @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @voznesenskym @penguinwu @EikanWang @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames</p>]]></description>
      <pubDate>Wed, 21 Feb 2024 01:26:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120301</guid>
    </item>
    <item>
      <title>Compilation Failure with torch.nn.functional.rrelu(training=True) in Torch Compile</title>
      <link>https://github.com/pytorch/pytorch/issues/120292</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When attempting to use <code>torch.nn.functional.rrelu</code> with <code>training=True</code> in a model that is being compiled with <code>torch.compile</code>, the compilation fails. Feel free to close issue if this behavior is expected</p>
<h3>Error logs</h3>
<p>```<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
AssertionError: </p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>
<h3>Minified repro</h3>
<p>```python<br />
import torch<br />
import torch.nn as nn<br />
print("torch version: ",torch.<strong>version</strong>)<br />
class Model(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.p0 = torch.rand([9],dtype=torch.float32) # [9], torch.float32</p>
<pre><code>def forward(self):
    # v0_0: [], torch.int32
    v4_0 = torch.nn.functional.rrelu(input=self.p0, training=True)
    return v4_0
</code></pre>
<p>inputs = {}<br />
model = Model().to(torch.device("cpu"))<br />
ret_exported = torch.compile(model)(**inputs)<br />
```</p>
<h3>Versions</h3>
<p>PyTorch version: 2.3.0.dev20240220+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: Could not collect<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.8.17 (default, Jul  5 2023, 21:04:15)  [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-15-generic-x86_64-with-glibc2.17<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      48 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             256<br />
On-line CPU(s) list:                0-255<br />
Vendor ID:                          AuthenticAMD<br />
Model name:                         AMD EPYC 7763 64-Core Processor<br />
CPU family:                         25<br />
Model:                              1<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 64<br />
Socket(s):                          2<br />
Stepping:                           1<br />
Frequency boost:                    enabled<br />
CPU max MHz:                        3529.0520<br />
CPU min MHz:                        1500.0000<br />
BogoMIPS:                           4890.76<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm<br />
Virtualization:                     AMD-V<br />
L1d cache:                          4 MiB (128 instances)<br />
L1i cache:                          4 MiB (128 instances)<br />
L2 cache:                           64 MiB (128 instances)<br />
L3 cache:                           512 MiB (16 instances)<br />
NUMA node(s):                       2<br />
NUMA node0 CPU(s):                  0-63,128-191<br />
NUMA node1 CPU(s):                  64-127,192-255<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Mitigation; safe RET<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.23.5<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240220+cu118<br />
[pip3] torchaudio==2.2.0.dev20240220+cu118<br />
[pip3] torchvision==0.18.0.dev20240220+cu118<br />
[conda] numpy                     1.23.5                   pypi_0    pypi<br />
[conda] pytorch-triton            3.0.0+901819d2b6          pypi_0    pypi<br />
[conda] torch                     2.3.0.dev20240220+cu118          pypi_0    pypi<br />
[conda] torchaudio                2.2.0.dev20240220+cu118          pypi_0    pypi<br />
[conda] torchvision               0.18.0.dev20240220+cu118          pypi_0    pypi</p>
<p>cc @bdhirsh @ezyang @msaroufim @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Tue, 20 Feb 2024 21:42:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120292</guid>
    </item>
    <item>
      <title>Torch compile fx graph is not removing constant propagation</title>
      <link>https://github.com/pytorch/pytorch/issues/120057</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I am running GPTJ-6B model what i have input certain nodes got input from constants, it can be eliminated, but it didnot is there any why to eliminate.</p>
<p>```<br />
     # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:600, code: position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)<br />
        iota: "i64[19]" = torch.ops.prims.iota.default(19, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)</p>
<pre><code>    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:601, code: position_ids = position_ids.unsqueeze(0)
    unsqueeze: "i64[1, 19]" = torch.ops.aten.unsqueeze.default(iota, 0);  iota = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:630, code: inputs_embeds = self.wte(input_ids)
    embedding: "f32[1, 19, 4096]" = torch.ops.aten.embedding.default(arg0_1, view);  arg0_1 = view = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:638, code: hidden_states = self.drop(hidden_states)
    clone: "f32[1, 19, 4096]" = torch.ops.aten.clone.default(embedding);  embedding = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:310, code: hidden_states = self.ln_1(hidden_states)
    mean: "f32[1, 19, 1]" = torch.ops.aten.mean.dim(clone, [-1], True)
    sub: "f32[1, 19, 4096]" = torch.ops.aten.sub.Tensor(clone, mean);  mean = None
    pow_1: "f32[1, 19, 4096]" = torch.ops.aten.pow.Tensor_Scalar(sub, 2.0)
    mean_1: "f32[1, 19, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
    add: "f32[1, 19, 1]" = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
    rsqrt: "f32[1, 19, 1]" = torch.ops.aten.rsqrt.default(add);  add = None
    mul: "f32[1, 19, 4096]" = torch.ops.aten.mul.Tensor(sub, rsqrt);  sub = rsqrt = None
    mul_1: "f32[1, 19, 4096]" = torch.ops.aten.mul.Tensor(mul, arg1_1);  mul = arg1_1 = None
    add_1: "f32[1, 19, 4096]" = torch.ops.aten.add.Tensor(mul_1, arg2_1);  mul_1 = arg2_1 = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:207, code: query = self.q_proj(hidden_states)
    permute: "f32[4096, 4096]" = torch.ops.aten.permute.default(arg3_1, [1, 0]);  arg3_1 = None
    view_1: "f32[19, 4096]" = torch.ops.aten.view.default(add_1, [19, 4096])
    mm: "f32[19, 4096]" = torch.ops.aten.mm.default(view_1, permute);  view_1 = permute = None
    view_2: "f32[1, 19, 4096]" = torch.ops.aten.view.default(mm, [1, 19, 4096]);  mm = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:208, code: key = self.k_proj(hidden_states)
    permute_1: "f32[4096, 4096]" = torch.ops.aten.permute.default(arg4_1, [1, 0]);  arg4_1 = None
    view_3: "f32[19, 4096]" = torch.ops.aten.view.default(add_1, [19, 4096])
    mm_1: "f32[19, 4096]" = torch.ops.aten.mm.default(view_3, permute_1);  view_3 = permute_1 = None
    view_4: "f32[1, 19, 4096]" = torch.ops.aten.view.default(mm_1, [1, 19, 4096]);  mm_1 = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:209, code: value = self.v_proj(hidden_states)
    permute_2: "f32[4096, 4096]" = torch.ops.aten.permute.default(arg5_1, [1, 0]);  arg5_1 = None
    view_5: "f32[19, 4096]" = torch.ops.aten.view.default(add_1, [19, 4096])
    mm_2: "f32[19, 4096]" = torch.ops.aten.mm.default(view_5, permute_2);  view_5 = permute_2 = None
    view_6: "f32[1, 19, 4096]" = torch.ops.aten.view.default(mm_2, [1, 19, 4096]);  mm_2 = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:122, code: tensor = tensor.view(new_shape)
    view_7: "f32[1, 19, 16, 256]" = torch.ops.aten.view.default(view_2, [1, 19, 16, 256]);  view_2 = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:122, code: tensor = tensor.view(new_shape)
    view_8: "f32[1, 19, 16, 256]" = torch.ops.aten.view.default(view_4, [1, 19, 16, 256]);  view_4 = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:122, code: tensor = tensor.view(new_shape)
    view_9: "f32[1, 19, 16, 256]" = torch.ops.aten.view.default(view_6, [1, 19, 16, 256]);  view_6 = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:128, code: return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
    permute_3: "f32[1, 16, 19, 256]" = torch.ops.aten.permute.default(view_9, [0, 2, 1, 3]);  view_9 = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:192, code: return embed_positions.repeat(position_ids.shape[0], 1, 1)
    repeat: "f32[1, 2048, 64]" = torch.ops.aten.repeat.default(arg285_1, [1, 1, 1]);  arg285_1 = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:222, code: repeated_position_ids = position_ids.unsqueeze(-1).repeat(1, 1, embed_positions.shape[-1])
    unsqueeze_1: "i64[1, 19, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze, -1)
    repeat_1: "i64[1, 19, 64]" = torch.ops.aten.repeat.default(unsqueeze_1, [1, 1, 64]);  unsqueeze_1 = None

    # File: /home/tiru/work/vsim/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py:223, code: sincos = torch.gather(embed_positions, 1, repeated_position_ids)
    gather: "f32[1, 19, 64]" = torch.ops.aten.gather.default(repeat, 1, repeat_1);  repeat = repeat_1 = None
</code></pre>
<p>```</p>
<p>in the above graph input to gather both are repeat are constants , can be it eliminated ?</p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.2.0+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.27.0<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.8.10 (default, Nov 22 2023, 10:22:35)  [GCC 9.4.0] (64-bit runtime)<br />
Python platform: Linux-5.4.0-171-generic-x86_64-with-glibc2.29<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A100 80GB PCIe<br />
GPU 1: NVIDIA A100 80GB PCIe</p>
<p>Nvidia driver version: 545.23.08<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Byte Order:                         Little Endian<br />
Address sizes:                      46 bits physical, 57 bits virtual<br />
CPU(s):                             128<br />
On-line CPU(s) list:                0-127<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 32<br />
Socket(s):                          2<br />
NUMA node(s):                       2<br />
Vendor ID:                          GenuineIntel<br />
CPU family:                         6<br />
Model:                              106<br />
Model name:                         Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz<br />
Stepping:                           6<br />
CPU MHz:                            800.000<br />
CPU max MHz:                        3200.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           4000.00<br />
Virtualization:                     VT-x<br />
L1d cache:                          3 MiB<br />
L1i cache:                          2 MiB<br />
L2 cache:                           80 MiB<br />
L3 cache:                           96 MiB<br />
NUMA node0 CPU(s):                  0-31,64-95<br />
NUMA node1 CPU(s):                  32-63,96-127<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.4<br />
[pip3] onnx==1.14.0<br />
[pip3] onnx-graphsurgeon==0.3.27<br />
[pip3] onnx2pytorch==0.4.1<br />
[pip3] onnxruntime==1.15.1<br />
[pip3] optree==0.10.0<br />
[pip3] torch==2.2.0<br />
[pip3] torch-pac==0.0.1.dev0<br />
[pip3] torch-tb-profiler==0.4.3<br />
[pip3] torch-xla==1.0<br />
[pip3] torchvision==0.16.2<br />
[pip3] triton==2.2.0<br />
[conda] mkl                       2021.4.0           h06a4308_640<br />
[conda] mkl-include               2023.1.0         h06a4308_46344<br />
[conda] numpy                     1.26.3                   pypi_0    pypi<br />
[conda] torch                     2.1.2                    pypi_0    pypi<br />
[conda] triton                    2.1.0                    pypi_0    pypi</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Thu, 15 Feb 2024 21:20:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120057</guid>
    </item>
    <item>
      <title>Inductor pattern-matching seems to be sensitive to the presence/absence of clone nodes</title>
      <link>https://github.com/pytorch/pytorch/issues/119911</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<h3>Summary</h3>
<p>This issue had been discussed in https://github.com/pytorch/pytorch/pull/113004#discussion_r1383666731.<br />
The pattern-matching process seems to be sensitive to the presence or absence of <code>clone</code> nodes.<br />
A workaround may be possible. Creating this issue for tracking it, and for documenting this behavior (will add this issue's link in comments in the code when I'd add a new pattern).</p>
<h3>Problem</h3>
<p>As in the script below, if I add its corresponding pattern in <code>torch/_inductor/fx_passes/fuse_attention.py</code>, it's able to <em>match itself</em>, but not the <code>BERT_pytorch</code> SDPA pattern. </p>
<p>```python<br />
def BERT_pytorch_mha_pattern(query, key, value, attn_mask, dropout_p):<br />
    # From https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/attention/single.py#L14<br />
    scores = torch.matmul(query, key.transpose(-2, -1)).div(math.sqrt(query.size(-1)))<br />
    fill_value = torch.full((), -float(1e9), dtype=query.dtype, device=query.device)<br />
    scores = scores.masked_fill(attn_mask == 0, fill_value) <br />
    return torch.nn.functional.dropout(<br />
        scores.softmax(dim=-1), dropout_p<br />
    ).matmul(value)</p>
<p>with torch.no_grad():<br />
    q = torch.randn(1, 12, 256, 64)<br />
    k = torch.randn(1, 12, 256, 64)<br />
    v = torch.randn(1, 12, 256, 64)<br />
    attn_mask = torch.rand(1, 12, 256, 256).to(torch.bool)<br />
    BERT_pytorch_mha_model = torch.compile(BERT_pytorch_mha_pattern, fullgraph=True, options={"freezing": True})<br />
    BERT_pytorch_mha_model(q, k, v, attn_mask, 0.0)<br />
```</p>
<h3>Workaround</h3>
<p>However, if I <code>transpose</code>  <code>q</code>, <code>k</code> &amp; <code>v</code> in a <a href="https://github.com/pytorch/pytorch/compare/main...sanchitintel:pytorch:sanchitj/sdpa_pattern_example?expand=1">new SDPA pattern corresponding to the script below</a>, then Inductor is unable to match it <em>with itself</em> because the serialized pattern (generated by adding a pattern in <code>torch/_inductor/fx_passes/fuse_attention.py</code> corresponding to the script below has <code>clone</code> nodes after <code>permute</code> and <code>expand</code> of <code>query</code>, <code>key</code>, and <code>value</code>, but the <code>torch.fx.GraphModule</code> graph does not when <code>joint_graph_passes</code> are run.<br />
But I discovered that this new pattern is actually able to match the SDPA pattern in <code>BERT_pytorch</code> because the generated graph for the model also has the aforementioned <code>clone</code> nodes.</p>
<p>```python<br />
import torch</p>
<p>def BERT_pytorch_mha_pattern(q, k, v, attn_mask, dropout_p):<br />
    # attn_mask is already of bool dtype in this model <br />
    q = q.permute(0, 2, 1, 3)<br />
    k = k.permute(0, 2, 1, 3)<br />
    v = v.permute(0, 2, 1, 3)<br />
    # From https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/attention/single.py#L14<br />
    scores = torch.matmul(q, k.transpose(-2, -1)).div(math.sqrt(q.size(-1)))<br />
    fill_value = torch.full((), -float(1e9), dtype=q.dtype, device=q.device)<br />
    scores = scores.masked_fill(attn_mask == 0, fill_value) <br />
    return torch.nn.functional.dropout(<br />
        scores.softmax(dim=-1), dropout_p<br />
    ).matmul(v)</p>
<p>with torch.no_grad():<br />
    q = torch.randn(1, 256, 12, 64)<br />
    k = torch.randn(1, 256, 12, 64)<br />
    v = torch.randn(1, 256, 12, 64)<br />
    attn_mask = torch.rand(1, 12, 256, 256).to(torch.bool)<br />
    BERT_pytorch_mha_model = torch.compile(BERT_pytorch_mha_pattern, fullgraph=True, options={"freezing": True})<br />
    BERT_pytorch_mha_model(q, k, v, attn_mask, 0.0)<br />
```</p>
<p>So, the problem I had been facing has been resolved with this workaround, but I thought we should clearly document this behavior. Thanks!</p>
<h3>Versions</h3>
<p>Current main branch</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames</p>]]></description>
      <pubDate>Wed, 14 Feb 2024 11:41:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/119911</guid>
    </item>
    <item>
      <title>torch._inductor.triton_heuristics.cached_autotune is not thread safe / suited for multiprocessing via DDP/FSDP</title>
      <link>https://github.com/pytorch/pytorch/issues/119698</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>In multiprocessing mode (i.e. FSDP/DDP), there occur JSONDecodeErrors within torch._inductor.triton_heuristics.cached_autotune, if the filesystem does not lock the file itself.</p>
<h3>Error logs</h3>
<p>Excerpts from the stack trace:</p>
<p><code>...lib/python3.11/site-packages/torch/_inductor/triton_heuristics.py", line 740, in cached_autotune
    best_config = load_cached_autotuning(cache_filename, configs_hash, configs)</code></p>
<p><code>.../lib/python3.11/json/decoder.py", line 355, in raw_decode     
    raise JSONDecodeError("Expecting value", s, err.value) from None                                        
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:                                         
JSONDecodeError: Expecting value: line 1 column 1 (char 0)</code></p>
<h3>Minified repro</h3>
<p>```<br />
import torch<br />
import torch.nn as nn<br />
import torch.distributed as dist<br />
import torch.multiprocessing as mp<br />
from torch.nn.parallel import DistributedDataParallel as DDP<br />
from torch.distributed.optim import DistributedOptimizer<br />
from torch.optim import Adam</p>
<h1>Define a simple model</h1>
<p>class SimpleModel(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super(SimpleModel, self).<strong>init</strong>()<br />
        self.linear = nn.Linear(10, 10)</p>
<pre><code>def forward(self, x):
    return self.linear(x)
</code></pre>
<p>def example(rank, world_size):<br />
    # Initialize the distributed environment.<br />
    dist.init_process_group("nccl", rank=rank, world_size=world_size)</p>
<pre><code># Create the model and move it to GPU with the corresponding rank
model = SimpleModel().to(rank)

# Compile the model with torch.compile for optimization
model = torch.compile(model, example_inputs=torch.randn(1, 10).to(rank))

# Wrap the model with DDP
ddp_model = DDP(model, device_ids=[rank])

# Define an optimizer. Here we use Adam, but it can be any optimizer.
optimizer = Adam(ddp_model.parameters(), lr=0.001)

# Example training loop
for _ in range(100):  # Assuming 100 iterations
    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(64, 10).to(rank))  # Assuming a batch size of 64
    loss = outputs.mean()
    loss.backward()
    optimizer.step()
</code></pre>
<p>def main():<br />
    world_size = torch.cuda.device_count()<br />
    mp.spawn(example,<br />
             args=(world_size,),<br />
             nprocs=world_size,<br />
             join=True)</p>
<p>if <strong>name</strong> == "<strong>main</strong>":<br />
    main()</p>
<p>```</p>
<p>Code generated by ChatGPT, did not reproduce the error with this as the error will only occur occasionally, e.g. in shared filesystems, where latency is high.</p>
<h3>Versions</h3>
<p>Versions of relevant libraries:<br />
[pip3] lion-pytorch==0.1.2<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-lightning==2.2.0<br />
[pip3] torch==2.2.0<br />
[pip3] torchdata==0.7.1<br />
[pip3] torchmetrics==1.3.0.post0<br />
[pip3] torchtext==0.17.0<br />
[pip3] torchvision==0.17.0<br />
[pip3] triton==2.2.0<br />
[conda] blas                      1.0                         mkl    conda-forge<br />
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch<br />
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch<br />
[conda] mkl                       2023.1.0         h213fc3f_46344<br />
[conda] numpy                     1.26.4          py311h64a7726_0    conda-forge<br />
[conda] pytorch                   2.2.0           py3.11_cuda12.1_cudnn8.9.2_0    pytorch<br />
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch<br />
[conda] pytorch-lightning         2.2.0                    pypi_0    pypi<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch<br />
[conda] torchdata                 0.7.1                    pypi_0    pypi<br />
[conda] torchmetrics              1.3.0.post0              pypi_0    pypi<br />
[conda] torchtext                 0.17.0                   pypi_0    pypi<br />
[conda] torchtriton               2.2.0                     py311    pytorch<br />
[conda] torchvision               0.17.0              py311_cu121    pytorch</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 12 Feb 2024 10:10:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/119698</guid>
    </item>
    <item>
      <title>Add CUTLASS kernel as choice for _int_mm() Inductor autotuning</title>
      <link>https://github.com/pytorch/pytorch/pull/119685</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #119986<br />
* <strong>-&gt;</strong> #119685</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Mon, 12 Feb 2024 07:05:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119685</guid>
    </item>
    <item>
      <title> [Inductor Cutlass backend] 2 of 2 - Enabling flexible EVT-based pointwise fusions with additional tensor input</title>
      <link>https://github.com/pytorch/pytorch/pull/119601</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #119701<br />
* #119600<br />
* #119009<br />
* #119008<br />
* <strong>-&gt;</strong> #119601<br />
* #119007<br />
* #119598<br />
* #119597<br />
* #119006<br />
* #119004<br />
* #120620</p>
<p>This diff enables flexible EVT based Matmul fusions which may require one tensor input<br />
    in addition to the Matmul operands ( A and B ) via the same input operand (C) that is<br />
    also used for the Bias in the case of linear epilogues.</p>
<pre><code>Test Plan:
* Additional unit tests in test/inductor/test_max_autotune.py
* Manual inspection of the generated code
* CI

[Inductor max autotune] Robust Autotuning

This change provides many changes which make autotuning robust against failures,
e.g. Autotuning is not just for performance comparison anymore, but also to filter
out Kernels with bugs ( Segfaults / CUDA IMA, Infinite Loops ..).

Also, if no suitable Kernels are found, it allows to fall back to
alternative GEMM backends ( e.g. ATen )

[inductor max autotune] Flexible GEMM layout autotuning

This diff introduces memory layout autotuning and flexibilizes
memory layouts that are accepted and written by the Cutlass GEMM Kernels.

During autotuning, if Cutlass GEMM Kernels have inputs with flexible memory
layouts, all possible combinations of row-major or column major layouts are
tried during autotuning.

Test Plan:

* Additional Unit test(s)
* CI

[Inductor Cutlass backend] Improvements to CUTLASS Kernels

* Standalone runner ( CUTLASS Kernels may be compiled as standalone executables for debugging &amp; profiling )
* Retuning ( After Fusion, another round of Autotuning is done to determine what's the best Kernel to pick given the fusion)
* Support for auxiliary inputs ( additional GEMM inputs beyond operands A,B and C )
* Support for tanh and sigmoid in EVT epilogue expressions
* More unit tests

[Inductor Cutlass backend] prevent problems with symbolic shapes

Without this change, GEMM inputs with symbolic dimensions could lead
to runtime exceptions. This change makes sure that, while the CUTLASS
backend will not be picked, at least no exceptions occur.

[Inductor Cutlass backend] Broadcasting fixes &amp; Code quality improvements

Notable changes:

* Smaller refactorings ( extract method specifically to make code more readable )
* Several new unit tests
* Improved / more robust parsing of load index expressions when interpreting Pointwise.inner_fn node

[Inductor cutlass backend] Log CUDA compilation times

Log the time that CUDA compilations take in the debug log

[Inductor cutlass backend] Minor improvements to unit tests

As the title says, minor improvements / changes in the unit
tests of the Inductor CUTLASS backend

[Inductor cutlass backend] Add support for Aux Inputs without requiring shared memory

A common problem when fusing epilogues is that additional (auxiliary) inputs require
shared memory. But when all shared memory is already required by the GEMM op, like
is commonly the case for TMA ops, the compilation of the fused epilogue will fail.

This adds an Sm90AuxLoadDirect operator which loads auxiliary inputs without shared mem.

In experiments, this proved crucial to enable nontrivial epilogue fusions.

[Inductor cutlass backend] Support more edge cases involving strided memory layouts

This commit fixes several issues which were encountered when dealing with edge cases
from a real world model, where complex strided inputs with offsets were encountered.

[Inductor cutlass backend] Workaround for flaky tests caused by Pingpong Kernel nondeterministic numerical differences

Some tests of the Inductor Cutlass backend exhibited flakiness, e.g. were failing nondeterministically. Even when
ensuring identical inputs, zeroed-out buffers etc. the results could differ somewhat from invocation to invocation,
which could lead to test failures due to numerical differences.

This only happened when Cutlass' SM90 TMA Warpspecialized Pingpong Kernels were used.

This diff introduces config options that allow to whitelist/blacklist Cutlass Kernels,
and by default filters these "Pingpong" Kernels.

They can be easily re-enabled via a configuration setting if desired.
</code></pre>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Fri, 09 Feb 2024 13:13:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119601</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Disable Cutlass backend, split and add tests</title>
      <link>https://github.com/pytorch/pytorch/pull/119598</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #119701<br />
* #119600<br />
* #119009<br />
* #119008<br />
* #119601<br />
* #119007<br />
* <strong>-&gt;</strong> #119598<br />
* #119597<br />
* #119006<br />
* #119004<br />
* #120620</p>
<ul>
<li>Disables the Cutlass backend via _DISABLE_CUTLASS_BACKEND<br />
feature flag</li>
<li>Splits the Cutlass backend related unit tests out of test_max_autotune.py into<br />
 test_cutlass_backend.py</li>
<li>Adds a lot of new Cutlass backend specific tests, including those<br />
 covering enhanced epilogue fusion ( follows in the next PRs )</li>
</ul>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Fri, 09 Feb 2024 12:50:24 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119598</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Development feature flag</title>
      <link>https://github.com/pytorch/pytorch/pull/119597</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #119701<br />
* #119600<br />
* #119009<br />
* #119008<br />
* #119601<br />
* #119007<br />
* #119598<br />
* <strong>-&gt;</strong> #119597<br />
* #119006<br />
* #119004<br />
* #120620</p>
<p>Introducing a feature flag for development purposes, which allows<br />
to disable the CUTLASS backend entirely.</p>
<p>This flag is not intended to be used by users, it is for development<br />
purposes to ensure a working codebase when features are incrementally<br />
added over the course of multiple commits / PRs and it cannot<br />
be ensured that the cutlass backend is functional in-between<br />
incremental commits.</p>
<p>To disable the cutlass backend, set the private variable<br />
cutlass_utils._DISABLE_CUTLASS_BACKEND to True</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames</p>]]></description>
      <pubDate>Fri, 09 Feb 2024 12:50:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119597</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] 1 of 2 - Enabling flexible EVT-based pointwise fusions with additional tensor input</title>
      <link>https://github.com/pytorch/pytorch/pull/119007</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #119701<br />
* #119600<br />
* #119009<br />
* #119008<br />
* #119601<br />
* <strong>-&gt;</strong> #119007<br />
* #119598<br />
* #119597<br />
* #119006<br />
* #119004<br />
* #120620</p>
<p>This diff enables flexible EVT based Matmul fusions which may require one or more additional tensor inputs. It combines the changes from several original commits, whose commit messages are also found below.</p>
<p>Test Plan:<br />
* Additional unit tests in test/inductor/test_max_autotune.py<br />
* Manual inspection of the generated code<br />
* CI</p>
<p>[Inductor max autotune] Robust Autotuning</p>
<p>This change provides many changes which make autotuning robust against failures,<br />
e.g. Autotuning is not just for performance comparison anymore, but also to filter<br />
out Kernels with bugs ( Segfaults / CUDA IMA, Infinite Loops ..).</p>
<p>Also, if no suitable Kernels are found, it allows to fall back to<br />
alternative GEMM backends ( e.g. ATen )</p>
<p>[inductor max autotune] Flexible GEMM layout autotuning</p>
<p>This diff introduces memory layout autotuning and flexibilizes<br />
memory layouts that are accepted and written by the Cutlass GEMM Kernels.</p>
<p>During autotuning, if Cutlass GEMM Kernels have inputs with flexible memory<br />
layouts, all possible combinations of row-major or column major layouts are<br />
tried during autotuning.</p>
<p>Test Plan:</p>
<ul>
<li>Additional Unit test(s)</li>
<li>CI</li>
</ul>
<p>[Inductor Cutlass backend] Improvements to CUTLASS Kernels</p>
<ul>
<li>Standalone runner ( CUTLASS Kernels may be compiled as standalone executables for debugging &amp; profiling )</li>
<li>Retuning ( After Fusion, another round of Autotuning is done to determine what's the best Kernel to pick given the fusion)</li>
<li>Support for auxiliary inputs ( additional GEMM inputs beyond operands A,B and C )</li>
<li>Support for tanh and sigmoid in EVT epilogue expressions</li>
<li>More unit tests</li>
</ul>
<p>[Inductor Cutlass backend] prevent problems with symbolic shapes</p>
<p>Without this change, GEMM inputs with symbolic dimensions could lead<br />
to runtime exceptions. This change makes sure that, while the CUTLASS<br />
backend will not be picked, at least no exceptions occur.</p>
<p>[Inductor Cutlass backend] Broadcasting fixes &amp; Code quality improvements</p>
<p>Notable changes:</p>
<ul>
<li>Smaller refactorings ( extract method specifically to make code more readable )</li>
<li>Several new unit tests</li>
<li>Improved / more robust parsing of load index expressions when interpreting Pointwise.inner_fn node</li>
</ul>
<p>[Inductor cutlass backend] Log CUDA compilation times</p>
<p>Log the time that CUDA compilations take in the debug log</p>
<p>[Inductor cutlass backend] Minor improvements to unit tests</p>
<p>As the title says, minor improvements / changes in the unit<br />
tests of the Inductor CUTLASS backend</p>
<p>[Inductor cutlass backend] Add support for Aux Inputs without requiring shared memory</p>
<p>A common problem when fusing epilogues is that additional (auxiliary) inputs require<br />
shared memory. But when all shared memory is already required by the GEMM op, like<br />
is commonly the case for TMA ops, the compilation of the fused epilogue will fail.</p>
<p>This adds an Sm90AuxLoadDirect operator which loads auxiliary inputs without shared mem.</p>
<p>In experiments, this proved crucial to enable nontrivial epilogue fusions.</p>
<p>[Inductor cutlass backend] Support more edge cases involving strided memory layouts</p>
<p>This commit fixes several issues which were encountered when dealing with edge cases<br />
from a real world model, where complex strided inputs with offsets were encountered.</p>
<p>[Inductor cutlass backend] Workaround for flaky tests caused by Pingpong Kernel nondeterministic numerical differences</p>
<p>Some tests of the Inductor Cutlass backend exhibited flakiness, e.g. were failing nondeterministically. Even when<br />
ensuring identical inputs, zeroed-out buffers etc. the results could differ somewhat from invocation to invocation,<br />
which could lead to test failures due to numerical differences.</p>
<p>This only happened when Cutlass' SM90 TMA Warpspecialized Pingpong Kernels were used.</p>
<p>This diff introduces config options that allow to whitelist/blacklist Cutlass Kernels,<br />
and by default filters these "Pingpong" Kernels.</p>
<p>They can be easily re-enabled via a configuration setting if desired.</p>
<p>[Inductor cutlass backend] Enable StreamK and nonzero workspace sizes</p>
<p>This change introduces temporary workspace buffers, which can be used to<br />
provide workspace memory to Inductor ops. This is a required feature<br />
in order to enable the Cutlass StreamK tile scheduler, which should<br />
bring performance benefits and more reliable GEMM performance for<br />
many tile shapes.</p>
<p>See StreamK paper: https://arxiv.org/abs/2301.03598</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Fri, 02 Feb 2024 06:14:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119007</guid>
    </item>
    <item>
      <title>[inductor] Make some improvements to FX graph caching</title>
      <link>https://github.com/pytorch/pytorch/pull/117888</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #117888</p>
<p>Summary: This is in preparation to enable FX graph caching by default. First fix some bugs uncovered by running all unit tests under <code>test/inductor/</code>. I'll enable in a separate diff in case we need to revert. Summary of changes:<br />
* Turn off caching for tests that require a compilation, e.g., when checking that a relevant counter was incremented<br />
* Bypass caching when we see mkldnn tensors as constants (they currently don't serialize, so we can't save to disk)<br />
* Include various global settings that could affect compilation in the cache key calculation.<br />
* Handle a few config settings that break key calculation.<br />
* Handle code paths where no ShapeEnv is available (the cache impl requires a shape env as part of handling guards)<br />
* Skip caching when freezing is enabled (Freezing can embed constants that wouldn't be static across runs).<br />
* Fix the clear() method to not throw when the cache /tmp dir doesn't exist</p>
<p>Test Plan: Ran all tests under <code>test/inductor/</code> twice with TORCHINDUCTOR_FX_GRAPH_CACHE=1 to exercise any test that might be affected by caching.</p>]]></description>
      <pubDate>Fri, 19 Jan 2024 15:03:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/117888</guid>
    </item>
    <item>
      <title>[WIP] Add a dedicated registration API to support torch.compile-based aten implemantion</title>
      <link>https://github.com/pytorch/pytorch/pull/116368</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to provide a registration mode to implement a single aten operation on the top of <code>torch.compile</code> and then register to aten. </p>
<p>By now, the Python-based aten kernel implementation assumes the hermetic Python object. For <code>torch.compile</code>-based aten kernel implementation, the assumption will be broken. Because </p>
<blockquote>
<p>While HermeticPyObject was enabled, we attempted to create a tensor subclass with <strong>torch_dispatch</strong>.  This violates the invariant that operations in HermeticPyObject have equivalent C++ implementations.</p>
</blockquote>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #116449<br />
* <strong>-&gt;</strong> #116368<br />
* #120595<br />
* #121296</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 03:21:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116368</guid>
    </item>
    <item>
      <title>RFC: Integrating oneDNN Graph Compiler into Inductor C++/OpenMP Backend for Enhanced Graph Fusion and Performance</title>
      <link>https://github.com/pytorch/pytorch/issues/105582</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>Integrating oneDNN Graph Compiler into Inductor C++ Backend enables enhanced pattern fusion and performance for CPU. <br />
<img src="https://github.com/pytorch/pytorch/assets/19395079/47275ec4-f7ea-425b-9656-97a6a33b1844" alt="design" width="500"/></p>
<h3>Motivation</h3>
<p>Recent developments on the Inductor C++ backend have demonstrated promising performance on DL inference workloads with CPU, thanks to optimizations like Conv/GEMM + post-op fusions and vectorization (see <a href="https://dev-discuss.pytorch.org/t/Inductor-update-4-cpu-backend-started-to-show-promising-performance-boost/874">this</a> and <a href="https://dev-discuss.pytorch.org/t/torchinductor-update-5-cpu-backend-backend-performance-update-and-deep-dive-on-key-optimizations/1117">this</a>). </p>
<p><a href="https://spec.oneapi.io/onednn-graph/latest/introduction.html">oneDNN Graph API</a> (codename LLGA) extents oneDNN with a high-level graph API. It goes beyond Conv/GEMM post-op fusions and supports <a href="http://oneapi-src.github.io/oneDNN/dev_guide_graph_fusion_patterns.html#aggressive-fusion-patterns">aggressive fusion patterns</a> such as MultiheadAttention, MLP blocks, and more (with its <a href="http://oneapi-src.github.io/oneDNN/dev_guide_graph_compiler.html">graph compiler backend</a>). Other features include  <a href="http://oneapi-src.github.io/oneDNN/dev_guide_graph_low_precision.html">low precision</a>. Since PyTorch 1.12, this API has been added in <a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-onednn-graph-with-torchscript-for-inference">TorchScript JIT fuser path</a> showing promising performance <a href="https://github.com/pytorch/pytorch/issues/49444">#49444</a>.  </p>
<p>Integrating the oneDNN Graph Compiler with Inductor C++ backend offers further performance enhancements. Additionally, adopting the oneDNN Graph API simplifies design and development.</p>
<h3>Plan</h3>
<p>Our long-term goal is to use oneDNN Graph fusion by default, replacing post-op fusions. Starting as an experimental feature, we would add a <code>onednn_graph_fusion</code> pass into the Inductor post-graph passes, enabled by an inductor.cpp config option. We hope it will eventually become the default option after validation of no performance regression from Inductor C++ backend.</p>
<p>In PyTorch 2.3, oneDNN Graph Inductor pass would be experimental, and would support FP32 &amp; BF16 (with AMP).<br />
PR - #120301<br />
In PyTorch 2.4, we would add int8 support, as Quantization 2.0 would have matured by then.<br />
In PyTorch 2.5, we may add support for generic dynamic-shape-supporting kernels, if oneDNN would have added their support by then.</p>
<h3>Implementation</h3>
<h4>Inductor joint graph passes</h4>
<p>We introduce the <code>onednn_graph_fusion</code> pass directly takes the FX graph from AOTAutograd as input. The FX graph is used to construct an LLGA Graph by lowering aten/prims IR to LLGA IR and by lowering <code>FakeTensor</code> to LLGA <code>LogicalTensor</code>. LLGA identifies fusion opportunities in the Graph and returns a list of LLGA <code>partition</code>s, each represents a set of fused operations. </p>
<p>To enable the desired fusion, we use the Inductor pattern-matcher to replacing a pattern with a target fusion, which calls an ATen op, such as <code>torch.ops.mkldnn._graph_sdpa_pattern_1</code>:</p>
<p><img width="1069" alt="image" src="https://github.com/pytorch/pytorch/assets/90872293/7991c50c-29df-4a50-846d-1053196b9217"></p>
<h3>User interface:</h3>
<p><code>torch.compile(options={"cpp.onednn_graph": True})</code></p>]]></description>
      <pubDate>Wed, 19 Jul 2023 10:53:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/105582</guid>
    </item>
    <item>
      <title>[PT2.0][compile] torch._dynamo.config.log_level does not exist</title>
      <link>https://github.com/pytorch/pytorch/issues/104022</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>setting torch._dynamo.config.log_level=logging.DEBUG causes error in nightly build 20230622<br />
while this link mentiones that we can have it. https://pytorch.org/docs/master/compile/troubleshooting.html  </p>
<h3>Error logs</h3>
<p>torch_executor.py:21: in <module><br />
    torch._dynamo.config.log_level=logging.DEBUG<br />
/home/jthakur/.pt_2_0/lib/python3.8/site-packages/torch/_dynamo/config_utils.py:71: in <strong>setattr</strong><br />
    raise AttributeError(f"{self.<strong>name</strong>}.{name} does not exist")<br />
E   AttributeError: torch._dynamo.config.log_level does not exist</p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.1<br />
[pip3] torch==2.1.0.dev20230621+cpu<br />
[pip3] torchaudio==2.1.0.dev20230621+cpu<br />
[pip3] torchvision==0.16.0.dev20230621+cpu<br />
[pip3] triton==2.0.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305</p>]]></description>
      <pubDate>Wed, 21 Jun 2023 21:39:16 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/104022</guid>
    </item>
    <item>
      <title>TorchInductor CPU Performance Dashboard</title>
      <link>https://github.com/pytorch/pytorch/issues/93531</link>
      <description><![CDATA[<p>Dashboard to track the performance of torchinductor on CPU.</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @soumith @ngimel @chauhang </p>]]></description>
      <pubDate>Wed, 12 Oct 2022 18:29:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/93531</guid>
    </item>
  </channel>
</rss>
