<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[inductor] Refactor common triton imports into one function</title>
      <link>https://github.com/pytorch/pytorch/pull/121438</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121268<br />
* #121267<br />
* <strong>-&gt;</strong> #121438</p>
<p>This means when codegen depends on a particular import we only need to<br />
add it in one place and it's applied to all triton kernels.</p>
<p>This also changes codegen slightly so instead of generating<br />
<code>@pointwise</code> we now generate <code>@triton_heuristics.pointwise</code> just so<br />
the imports are the same for all kernel types.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 07 Mar 2024 11:42:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121438</guid>
    </item>
    <item>
      <title>[inductor] add decompostition for mm in backward (#120933)</title>
      <link>https://github.com/pytorch/pytorch/pull/121437</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* (to be filled)</p>
<p>Summary:<br />
1) As a follow up in D53602514. Found a new way to decompose mm in backward. Sum the permuted input and reduce along 0 dim. Some benchmark result P1190140001. 30x speedup<br />
Some explanations on why the original mm decomposition is slow. For mxkxn mm, when m is small and k is large, the stride for lhs is [m,1], hence it need to access memory k times to load all the data. As a result, decomposition will be effective with permute since the stride will be [k,1].</p>
<p>2) add another pattern for large k. benchmark result P1190596489 28x speedup</p>
<p>3) fix the value not found error in ig ctr. f536115499</p>
<p>Test Plan:<br />
pt2 decompose:</p>
<p>{F1462894821}<br />
decompose: f536159404<br />
baseline: f536282578<br />
705k vs 725k 4% for ig ctr</p>
<p>Differential Revision: D54294491<br />
Approved by: https://github.com/mengluy0125</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 07 Mar 2024 11:31:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121437</guid>
    </item>
    <item>
      <title>Revert "[PT2D] Make the speedup benchmark works with DDP + CompiledAutograd (#120454)"</title>
      <link>https://github.com/pytorch/pytorch/pull/121436</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* (to be filled)</p>
<p>This reverts commit 8c2e569928a200893fe971e615b82a2f9ce32630.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/120454 on behalf of https://github.com/desertfire due to breaks nightly dashboard cudagraphs run (<a href="https://github.com/pytorch/pytorch/pull/120454#issuecomment-1975001824">comment</a>)</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Thu, 07 Mar 2024 11:30:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121436</guid>
    </item>
    <item>
      <title>Revert "[PT2D] Make the speedup benchmark works with DDP + CompiledAutograd (#120454)"</title>
      <link>https://github.com/pytorch/pytorch/pull/121435</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* (to be filled)</p>
<p>This reverts commit 8c2e569928a200893fe971e615b82a2f9ce32630.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/120454 on behalf of https://github.com/desertfire due to breaks nightly dashboard cudagraphs run (<a href="https://github.com/pytorch/pytorch/pull/120454#issuecomment-1975001824">comment</a>)</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Thu, 07 Mar 2024 11:28:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121435</guid>
    </item>
    <item>
      <title>Revert "[PT2D] Make the speedup benchmark works with DDP + CompiledAutograd (#120454)"</title>
      <link>https://github.com/pytorch/pytorch/pull/121433</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* (to be filled)</p>
<p>This reverts commit 8c2e569928a200893fe971e615b82a2f9ce32630.</p>
<p>Reverted https://github.com/pytorch/pytorch/pull/120454 on behalf of https://github.com/desertfire due to breaks nightly dashboard cudagraphs run (<a href="https://github.com/pytorch/pytorch/pull/120454#issuecomment-1975001824">comment</a>)</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Thu, 07 Mar 2024 11:27:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121433</guid>
    </item>
    <item>
      <title>torch.compile + ring attention</title>
      <link>https://github.com/pytorch/pytorch/issues/121386</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I was trying to enable <a href="https://github.com/lucidrains/ring-attention-pytorch">ring attention</a> with torch.compile, here are the issues that I encountered:<br />
* einx triggers "unhashable type: non-nested SymInt"<br />
```<br />
Traceback (most recent call last):<br />
  File "/home/ybliang/local/pytorch/torch/multiprocessing/spawn.py", line 75, in _wrap<br />
    fn(i, <em>args)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/assert.py", line 85, in start<br />
    ring_out = ddp_ring_attention_net(seq)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/eval_frame.py", line 437, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/external_utils.py", line 36, in inner<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1593, in forward<br />
    else self._run_ddp_forward(</em>inputs, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward<br />
    return self.module(*inputs, </strong>kwargs)  # type: ignore[index]<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 548, in forward<br />
    (x, mask), batch_sizes, num_sharded_batches = sharded_batch_to_sharded_seq(x, mask, self.ring_seq_size)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 191, in sharded_batch_to_sharded_seq<br />
    x, sizes = all_gather(x)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 194, in torch_dynamo_resume_in_sharded_batch_to_sharded_seq_at_191<br />
    mask, _ = all_gather(mask)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 917, in catch_errors<br />
    return hijacked_callback(frame, cache_entry, hooks, frame_state)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 786, in _convert_frame<br />
    result = inner_convert(<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert<br />
    return _compile(<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 676, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 535, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 165, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 500, in transform<br />
    tracer.run()<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 2149, in run<br />
    super().run()<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 489, in wrapper<br />
    return inner_fn(self, inst)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 1272, in CALL_FUNCTION_KW<br />
    self.call_function(fn, args, kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
    self.push(fn.call_function(self, args, kwargs))<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/variables/torch.py", line 664, in call_function<br />
    tensor_variable = wrap_fx_proxy(<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/variables/builder.py", line 1325, in wrap_fx_proxy<br />
    return wrap_fx_proxy_cls(target_cls=TensorVariable, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/variables/builder.py", line 1410, in wrap_fx_proxy_cls<br />
    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1714, in get_fake_value<br />
    raise TorchRuntimeError(str(e)).with_traceback(e.<strong>traceback</strong>) from None<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1656, in get_fake_value<br />
    ret_val = wrap_fake_exception(<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1190, in wrap_fake_exception<br />
    return fn()<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1657, in <lambda><br />
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1782, in run_node<br />
    raise RuntimeError(make_error_message(e)).with_traceback(<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1764, in run_node<br />
    return node.target(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 70, in inner<br />
    graph = construct_graph(</em>args, backend=backend, <strong>kwargs)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 20, in inner<br />
    return func(*args, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/<strong>init</strong>.py", line 308, in <strong>hash</strong><br />
    raise TypeError("unhashable type: non-nested SymInt")<br />
torch._dynamo.exc.TorchRuntimeError: Failed running call_function <function rearrange at 0x7f7454380790>(<em>('(b s) n -&gt; b (s n)', FakeTensor(..., size=(16, 32), dtype=torch.int64)), </em>*{'s': 1}):<br />
unhashable type: non-nested SymInt</p>
<p>from user code:<br />
   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 206, in torch_dynamo_resume_in_sharded_batch_to_sharded_seq_at_194<br />
    x = rearrange('(b s) n -&gt; b (s n)', x, s = num_sharded_batches)<br />
<code>* einx rearrange issue</code><br />
Traceback (most recent call last):<br />
  File "/home/ybliang/local/pytorch/torch/multiprocessing/spawn.py", line 75, in _wrap<br />
    fn(i, <em>args)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/assert.py", line 85, in start<br />
    ring_out = ddp_ring_attention_net(seq)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/eval_frame.py", line 437, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/external_utils.py", line 36, in inner<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1593, in forward<br />
    else self._run_ddp_forward(</em>inputs, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward<br />
    return self.module(*inputs, </strong>kwargs)  # type: ignore[index]<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 548, in forward<br />
    (x, mask), batch_sizes, num_sharded_batches = sharded_batch_to_sharded_seq(x, mask, self.ring_seq_size)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 600, in torch_dynamo_resume_in_forward_at_548<br />
    logits = rearrange('b (i j) d -&gt; b (j i) d', logits, j = self.bucket_size)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 66, in inner<br />
    backend = einx.backend.get(input_tracer_values)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 70, in torch_dynamo_resume_in_inner_at_66<br />
    graph = construct_graph(*args, backend=backend, </strong>kwargs)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 20, in inner<br />
    return func(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 45, in construct_graph<br />
    output_tracers = func(</em>args, <strong>kwargs, backend=einx.backend.tracer)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/op/rearrange.py", line 118, in rearrange<br />
    exprs_in, exprs_out = parse(description, *[einx.param.get_shape(tensor) for tensor in tensors], cse=cse, </strong>parameters)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 20, in inner<br />
    return func(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/op/rearrange.py", line 56, in parse<br />
    exprs = einx.expr.solve(<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/util.py", line 108, in solve<br />
    exprs1, exprs2 = stage3.solve(exprs1, exprs2)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 333, in solve<br />
    exprs1 = [map(root) if not root is None else None for root in exprs1]<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 333, in <listcomp><br />
    exprs1 = [map(root) if not root is None else None for root in exprs1]<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 324, in map<br />
    return List([map(child) for child in expr.children])<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 324, in <listcomp><br />
    return List([map(child) for child in expr.children])<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 330, in map<br />
    return Composition.maybe(map(expr.inner))<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 324, in map<br />
    return List([map(child) for child in expr.children])<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 65, in <strong>init</strong><br />
    Expression.<strong>init</strong>(self, np.prod([c.value for c in children]).astype(int))<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 9, in <strong>init</strong><br />
    raise TypeError(f"Expected int, got {type(value)}")<br />
TypeError: Expected int, got <class 'numpy.ndarray'><br />
<code>* torch.distributed related graph break 1:</code><br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks] Graph break: from user code at:<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/external_utils.py", line 36, in inner<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     return fn(</em>args, <strong>kwargs)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     return forward_call(*args, </strong>kwargs)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1589, in forward<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     inputs, kwargs = self._pre_forward(<em>inputs, </em><em>kwargs)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [<strong>graph_breaks]   File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1464, in _pre_forward<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     self.reducer.prepare_for_forward()<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks] Traceback (most recent call last):<br />
......<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     self.push(fn.call_function(self, args, kwargs))<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/variables/user_defined.py", line 687, in call_function<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     return self.call_method(tx, "__call</strong>", args, kwargs)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [<strong>graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/variables/user_defined.py", line 579, in call_method<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     return super().call_method(tx, name, args, kwargs)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/variables/base.py", line 371, in call_method<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     raise unimplemented(f"call_method {self} {name} {args} {kwargs}")<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/exc.py", line 190, in unimplemented<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     raise Unsupported(msg)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks] torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(instancemethod) __call</strong> [] {}<br />
<code>Graph break 2:</code><br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks] Graph break: from user code at:<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 228, in sharded_seq_to_sharded_batch<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     logits, _ = all_gather(logits)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     return forward_call(</em>args, **kwargs)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/distributed.py", line 85, in forward<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     return AllGatherFunction.apply(x, self.dim, sizes)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/distributed.py", line 68, in forward<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     x, batch_sizes = all_gather_variable_dim(x, dim = dim, sizes = sizes)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/distributed.py", line 42, in all_gather_variable_dim<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     sizes = gather_sizes(t, dim = dim)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/distributed.py", line 32, in gather_sizes<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     sizes = all_gather_same_dim(size)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/distributed.py", line 27, in all_gather_same_dim<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     dist.all_gather(gathered_tensors, t)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/distributed/_functional_collectives.py", line 1036, in all_gather_inplace<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     output = all_gather_tensor(tensor, 0, group, tag)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/distributed/_functional_collectives.py", line 227, in all_gather_tensor<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     group_name = _resolve_group_name(group, tag)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/distributed/_functional_collectives.py", line 758, in _resolve_group_name<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     raise ValueError(f"Unsupported group type: {type(group)}, {group}")<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks] Traceback (most recent call last):<br />
......<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 1219, in CALL_FUNCTION<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]     self.call_function(fn, args, {})<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]     self.push(fn.call_function(self, args, kwargs))<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/variables/builtin.py", line 719, in call_function<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]     return super().call_function(tx, args, kwargs)<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/variables/base.py", line 352, in call_function<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]     unimplemented(f"call_function {self} {args} {kwargs}")<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/exc.py", line 190, in unimplemented<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]     raise Unsupported(msg)<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks] torch._dynamo.exc.Unsupported: call_function BuiltinVariable(ValueError) [ConstantVariable(str: "Unsupported group type: <class 'NoneType'>, None")] {}<br />
```</p>
<h3>Versions</h3>
<p>N/A</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:26:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121386</guid>
    </item>
    <item>
      <title>[inductor][cpu]AMP models regression and crash in 2024-03-04 nightly release</title>
      <link>https://github.com/pytorch/pytorch/issues/121377</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>AMP models regression and crash in 2024-03-04 nightly release.</p>
<p>AMP static shape default wrapper regression</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>llama</td>
      <td>multiple</td>
      <td>32</td>
      <td>1.372195</td>
      <td>0.014102689</td>
      <td>0.019351639332355</td>
      <td>19.282797</td>
      <td>32.0</td>
      <td>2.142721</td>
      <td>0.009058953000000002</td>
      <td>0.019410808831113003</td>
      <td>19.388413</td>
      <td>0.64</td>
      <td>1.0</td>
      <td>0.64</td>
      <td>1.01</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>multiple</td>
      <td>8</td>
      <td>0.975399</td>
      <td>0.09829916</td>
      <td>0.09588090236484</td>
      <td>17.287654</td>
      <td>8.0</td>
      <td>3.491892</td>
      <td>0.02860036</td>
      <td>0.09986936828111999</td>
      <td>21.98618</td>
      <td>0.28</td>
      <td>1.04</td>
      <td>0.29</td>
      <td>1.27</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>multiple</td>
      <td>64</td>
      <td>1.364183</td>
      <td>0.251519204</td>
      <td>0.34311822227033195</td>
      <td>36.793275</td>
      <td>64.0</td>
      <td>1.990777</td>
      <td>0.172512283</td>
      <td>0.343433485213891</td>
      <td>31.588458</td>
      <td>0.69</td>
      <td>1.0</td>
      <td>0.69</td>
      <td>0.86</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>llama</td>
      <td>single</td>
      <td>1</td>
      <td>0.68803</td>
      <td>0.061452501</td>
      <td>0.042281164263030004</td>
      <td>20.256173</td>
      <td>1.0</td>
      <td>3.788769</td>
      <td>0.01094602</td>
      <td>0.04147194124938</td>
      <td>19.422561</td>
      <td>0.18</td>
      <td>0.98</td>
      <td>0.18</td>
      <td>0.96</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>single</td>
      <td>1</td>
      <td>0.99071</td>
      <td>0.28245702899999997</td>
      <td>0.27983300320058996</td>
      <td>15.963345</td>
      <td>1.0</td>
      <td>3.867883</td>
      <td>0.07098655599999999</td>
      <td>0.274567693180948</td>
      <td>19.435037</td>
      <td>0.26</td>
      <td>0.98</td>
      <td>0.25</td>
      <td>1.22</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1</td>
      <td>1.220285</td>
      <td>0.116050149</td>
      <td>0.141614256072465</td>
      <td>29.88047</td>
      <td>1.0</td>
      <td>1.501315</td>
      <td>0.098143494</td>
      <td>0.14734429969461</td>
      <td>24.300461</td>
      <td>0.81</td>
      <td>1.04</td>
      <td>0.85</td>
      <td>0.81</td>
    </tr>
  </tbody>

</table>
<p>AMP static shape default wrapper failure</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>accuracy</th>
      <th>perf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>huggingface</td>
      <td>BlenderbotForCausalLM</td>
      <td>single</td>
      <td>‚àö</td>
      <td>X</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>stable_diffusion_text_encoder</td>
      <td>single</td>
      <td>X</td>
      <td>‚àö</td>
    </tr>

</table>

<p>AMP dynamic shape default wrapper regression</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>multiple</td>
      <td>8</td>
      <td>0.974551</td>
      <td>0.09871308399999999</td>
      <td>0.09620093472528399</td>
      <td>18.658825</td>
      <td>8.0</td>
      <td>2.825746</td>
      <td>0.034874982000000006</td>
      <td>0.09854784088657202</td>
      <td>26.072793</td>
      <td>0.34</td>
      <td>1.02</td>
      <td>0.35</td>
      <td>1.4</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>llama</td>
      <td>single</td>
      <td>1</td>
      <td>0.732906</td>
      <td>0.060439407</td>
      <td>0.044296404026741995</td>
      <td>20.219663</td>
      <td>1.0</td>
      <td>3.801087</td>
      <td>0.01084628</td>
      <td>0.04122765390636</td>
      <td>19.354458</td>
      <td>0.19</td>
      <td>0.93</td>
      <td>0.18</td>
      <td>0.96</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>single</td>
      <td>1</td>
      <td>0.989944</td>
      <td>0.281489406</td>
      <td>0.278658748533264</td>
      <td>15.981785</td>
      <td>1.0</td>
      <td>3.866154</td>
      <td>0.07068060400000001</td>
      <td>0.273262099877016</td>
      <td>19.418051</td>
      <td>0.26</td>
      <td>0.98</td>
      <td>0.25</td>
      <td>1.22</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1</td>
      <td>1.262171</td>
      <td>0.116195749</td>
      <td>0.14665890471107898</td>
      <td>29.754619</td>
      <td>1.0</td>
      <td>1.492518</td>
      <td>0.09633620799999999</td>
      <td>0.143783524491744</td>
      <td>24.215046</td>
      <td>0.85</td>
      <td>0.98</td>
      <td>0.83</td>
      <td>0.81</td>
    </tr>
  </tbody>

</table>

<p>AMP dynamic shape default wrapper failure</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>accuracy</th>
      <th>perf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>huggingface</td>
      <td>BlenderbotForCausalLM</td>
      <td>single</td>
      <td>‚àö</td>
      <td>X</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>stable_diffusion_text_encoder</td>
      <td>single</td>
      <td>X</td>
      <td>‚àö</td>
    </tr>
  </tbody>

</table>
<p>SW info</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>ff42d907</td>
      <td>main</td>
      <td>ff42d907</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>581fe26792849a80f04feaa7f8bdec69b7f41dd8</td>
      <td>main</td>
      <td>5c7b761f8e748fe45c8e2e29563df637ae651917</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
      <td>main</td>
      <td>0.18.0a0+a52607e</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
      <td>main</td>
      <td>2.2.0a0+5286f9f</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>

</table>

<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh <strong>thread</strong> inference performance/accuracy <strong>suite</strong> <strong>model</strong> amp first static/dynamic<br />
regression bisect log:<br />
<a href="https://github.com/pytorch/pytorch/files/14517983/torchbench-yolov3-inference-amp-dynamic-default-multiple-performance-drop_guilty_commit.log">torchbench-yolov3-inference-amp-dynamic-default-multiple-performance-drop_guilty_commit.log</a><br />
failure bisect log:<br />
<a href="https://github.com/pytorch/pytorch/files/14517991/huggingface-BlenderbotForCausalLM-inference-amp-dynamic-default-single-performance-crash_guilty_commit.log">huggingface-BlenderbotForCausalLM-inference-amp-dynamic-default-single-performance-crash_guilty_commit.log</a><br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/1104e0798c8206e0226f2d68f6bb065645e6276f<br />
cc @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 18:41:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121377</guid>
    </item>
    <item>
      <title>[inductor][cpu]fp32 models performance regression in 2024-03-04 nightly release</title>
      <link>https://github.com/pytorch/pytorch/issues/121376</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>fp32 models regression in 2024-03-04 nightly release.</p>
<p>fp32 static shape default wrapper</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>multiple</td>
      <td>8</td>
      <td>0.991369</td>
      <td>0.255836803</td>
      <td>0.253628675553307</td>
      <td>21.008619</td>
      <td>8</td>
      <td>1.591373</td>
      <td>0.156282283</td>
      <td>0.24870340554455897</td>
      <td>26.609133</td>
      <td>0.62</td>
      <td>0.98</td>
      <td>0.61</td>
      <td>1.27</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>multiple</td>
      <td>64</td>
      <td>1.269294</td>
      <td>0.8729793100000001</td>
      <td>1.10806740030714</td>
      <td>43.814319</td>
      <td>64</td>
      <td>1.540159</td>
      <td>0.7080052130000001</td>
      <td>1.0904406008488672</td>
      <td>38.130222</td>
      <td>0.82</td>
      <td>0.98</td>
      <td>0.81</td>
      <td>0.87</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>single</td>
      <td>1</td>
      <td>0.99786</td>
      <td>0.590719669</td>
      <td>0.58945552890834</td>
      <td>19.799038</td>
      <td>1</td>
      <td>1.387904</td>
      <td>0.428259948</td>
      <td>0.594383694868992</td>
      <td>24.260618</td>
      <td>0.72</td>
      <td>1.01</td>
      <td>0.72</td>
      <td>1.23</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1</td>
      <td>1.208036</td>
      <td>0.2238699</td>
      <td>0.2704428985164</td>
      <td>35.575396</td>
      <td>1</td>
      <td>1.372201</td>
      <td>0.196654595</td>
      <td>0.26984963191359496</td>
      <td>30.385211</td>
      <td>0.88</td>
      <td>1.0</td>
      <td>0.88</td>
      <td>0.85</td>
    </tr>
  </tbody>

</table>
<p>fp32 static shape cpp wrapper</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>multiple</td>
      <td>64.0</td>
      <td>1.284969</td>
      <td>0.844753783</td>
      <td>1.085482423787727</td>
      <td>33.745031</td>
      <td>64.0</td>
      <td>1.561684</td>
      <td>0.7009253529999999</td>
      <td>1.0946239089744518</td>
      <td>29.064628</td>
      <td>0.82</td>
      <td>1.01</td>
      <td>0.83</td>
      <td>0.86</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1.0</td>
      <td>1.213455</td>
      <td>0.221748512</td>
      <td>0.26908184062896</td>
      <td>30.388094</td>
      <td>1.0</td>
      <td>1.383058</td>
      <td>0.194475219</td>
      <td>0.26897050743970197</td>
      <td>25.875338</td>
      <td>0.88</td>
      <td>1.0</td>
      <td>0.88</td>
      <td>0.85</td>
    </tr>
  </tbody>

</table>
<p>fp32 dynamic shape default wrapper</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>multiple</td>
      <td>8</td>
      <td>0.983977</td>
      <td>0.25779631399999997</td>
      <td>0.253665643660778</td>
      <td>22.344479</td>
      <td>8.0</td>
      <td>1.503107</td>
      <td>0.16544836799999998</td>
      <td>0.24868660007937599</td>
      <td>30.578669</td>
      <td>0.65</td>
      <td>0.98</td>
      <td>0.64</td>
      <td>1.37</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>yolov3</td>
      <td>single</td>
      <td>1</td>
      <td>0.997312</td>
      <td>0.592453853</td>
      <td>0.5908613370431359</td>
      <td>20.022786</td>
      <td>1.0</td>
      <td>1.37728</td>
      <td>0.428594964</td>
      <td>0.5902952720179201</td>
      <td>24.479398</td>
      <td>0.72</td>
      <td>1.0</td>
      <td>0.72</td>
      <td>1.22</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1</td>
      <td>1.208458</td>
      <td>0.223794506</td>
      <td>0.270446261131748</td>
      <td>35.944872</td>
      <td>1.0</td>
      <td>1.374786</td>
      <td>0.196470478</td>
      <td>0.270104862567708</td>
      <td>30.699722</td>
      <td>0.88</td>
      <td>1.0</td>
      <td>0.88</td>
      <td>0.85</td>
    </tr>
  </tbody>

</table>
<p>fp32 dynamic shape cpp wrapper</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>convit_base</td>
      <td>single</td>
      <td>1.0</td>
      <td>1.222613</td>
      <td>0.221231601</td>
      <td>0.270480631393413</td>
      <td>30.374674</td>
      <td>1.0</td>
      <td>1.379473</td>
      <td>0.194968928</td>
      <td>0.268954372014944</td>
      <td>25.781443</td>
      <td>0.89</td>
      <td>0.99</td>
      <td>0.88</td>
      <td>0.85</td>
    </tr>
  </tbody>


</table>
<p>SW info</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>ff42d907</td>
      <td>main</td>
      <td>ff42d907</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>581fe26792849a80f04feaa7f8bdec69b7f41dd8</td>
      <td>main</td>
      <td>5c7b761f8e748fe45c8e2e29563df637ae651917</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
      <td>main</td>
      <td>0.18.0a0+a52607e</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
      <td>main</td>
      <td>2.2.0a0+5286f9f</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>

</table>

<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh <strong>thread</strong> inference performance <strong>suite</strong> <strong>model</strong> float32 first static/dynamic cpp/default<br />
<a href="https://github.com/pytorch/pytorch/files/14517683/torchbench-yolov3-inference-float32-static-default-multiple-performance-drop_guilty_commit.log">torchbench-yolov3-inference-float32-static-default-multiple-performance-drop_guilty_commit.log</a><br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/8c2e569928a200893fe971e615b82a2f9ce32630<br />
Seems the guilty commit pointed to PR is reverted, will double check in next test result.<br />
cc @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 18:04:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121376</guid>
    </item>
    <item>
      <title>`torch.compile` makes triton kernel slower in some cases</title>
      <link>https://github.com/pytorch/pytorch/issues/121367</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Hello, this is a follow-up issue of the previous https://github.com/pytorch/pytorch/issues/120478. The original issue was fixed in PR https://github.com/pytorch/pytorch/pull/120579 thanks to the awesome work of @aakhundov, @oulgen, etc. But I found that even after the fix, <code>torch.compile</code> still makes triton kernel slower in some cases. Let's still use the official trion matmul kernel https://github.com/openai/triton/blob/main/python/triton/ops/matmul.py as the testbed. </p>
<p>When inputA has shape <code>(4096,448)</code> and inputB has <code>(448, 2048)</code>. <code>torch.compile</code> works well:<br />
<code>Triton matmul:  0.006575107574462891
Compiling kernel
Triton matmul compiled:  0.002774953842163086</code></p>
<p>Yet if inputA has shape <code>(1, 6144)</code> and inputB has <code>(6144, 4096)</code><br />
<code>Triton matmul:  0.01115870475769043
Compiling kernel
Triton matmul compiled:  0.017619848251342773</code><br />
The compiled kernel runs slower than the original one.</p>
<p>I create this new issue because I think it might have a different cause than the previous one. The repro script is the same as https://github.com/pytorch/pytorch/issues/120478. I just changed the shapes of test tensors in it.</p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p>```<br />
import torch</p>
<p>from triton import Config, cdiv, heuristics, jit<br />
from triton import language as tl</p>
<p>import time</p>
<h1>the following are copied from https://github.com/openai/triton/blob/main/python/triton/ops/matmul.py</h1>
<p>_ordered_datatypes = [torch.int8, torch.float16, torch.bfloat16, torch.float32]</p>
<p>def upcast_if_fp8(a):<br />
    if "fp8" in str(a):<br />
        return torch.float16<br />
    return a</p>
<p>def get_higher_dtype(a, b):<br />
    a = upcast_if_fp8(a)<br />
    b = upcast_if_fp8(b)<br />
    if a is b:<br />
        return a</p>
<pre><code>assert a in _ordered_datatypes
assert b in _ordered_datatypes

for d in _ordered_datatypes:
    if a is d:
        return b
    if b is d:
        return a
</code></pre>
<p>def init_to_zero(name):<br />
    return lambda nargs: nargs[name].zero_()</p>
<p>def get_configs_io_bound():<br />
    configs = []<br />
    for num_stages in [2, 3, 4, 5, 6]:<br />
        for block_m in [16, 32]:<br />
            for block_k in [32, 64]:<br />
                for block_n in [32, 64, 128, 256]:<br />
                    configs.append(<br />
                        Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': 1},<br />
                               num_stages=num_stages, ))<br />
                    # split_k<br />
                    for split_k in [2, 4, 8, 16]:<br />
                        configs.append(<br />
                            Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': split_k},<br />
                                   num_stages=num_stages,  pre_hook=init_to_zero('C')))<br />
    return configs</p>
<p>@heuristics({<br />
    'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0,<br />
})<br />
@jit<br />
def _kernel(A, B, C, M, N, K,  #<br />
            stride_am, stride_ak,  #<br />
            stride_bk, stride_bn,  #<br />
            stride_cm, stride_cn,  #<br />
            acc_dtype: tl.constexpr,  #<br />
            allow_tf32: tl.constexpr,  #<br />
            fp8_fast_accum: tl.constexpr,  #<br />
            BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,  #<br />
            GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr, AB_DTYPE: tl.constexpr  #<br />
            ):<br />
    # matrix multiplication<br />
    pid = tl.program_id(0)<br />
    pid_z = tl.program_id(1)<br />
    grid_m = tl.cdiv(M, BLOCK_M)<br />
    grid_n = tl.cdiv(N, BLOCK_N)<br />
    # re-order program ID for better L2 performance<br />
    width = GROUP_M * grid_n<br />
    group_id = pid // width<br />
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)<br />
    pid_m = group_id * GROUP_M + (pid % group_size)<br />
    pid_n = (pid % width) // (group_size)<br />
    # do matrix multiplication<br />
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)<br />
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)<br />
    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)<br />
    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)<br />
    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)<br />
    # pointers<br />
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)<br />
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)<br />
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)<br />
    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):<br />
        if EVEN_K:<br />
            a = tl.load(A)<br />
            b = tl.load(B)<br />
        else:<br />
            k_remaining = K - k * (BLOCK_K * SPLIT_K)<br />
            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)<br />
            a = tl.load(A, mask=rk[None, :] &lt; k_remaining, other=_0)<br />
            b = tl.load(B, mask=rk[:, None] &lt; k_remaining, other=_0)<br />
        if AB_DTYPE is not None:<br />
            a = a.to(AB_DTYPE)<br />
            b = b.to(AB_DTYPE)<br />
        if fp8_fast_accum:<br />
            acc = tl.dot(a, b, acc, out_dtype=tl.float32, allow_tf32=allow_tf32)<br />
        else:<br />
            acc += tl.dot(a, b, out_dtype=tl.float32, allow_tf32=allow_tf32)<br />
        A += BLOCK_K * SPLIT_K * stride_ak<br />
        B += BLOCK_K * SPLIT_K * stride_bk<br />
    acc = acc.to(C.dtype.element_ty)<br />
    # rematerialize rm and rn to save registers<br />
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)<br />
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)<br />
    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)<br />
    mask = (rm &lt; M)[:, None] &amp; (rn &lt; N)[None, :]<br />
    # handles write-back with reduction-splitting<br />
    if SPLIT_K == 1:<br />
        tl.store(C, acc, mask=mask)<br />
    else:<br />
        tl.atomic_add(C, acc, mask=mask)</p>
<p>class _matmul(torch.autograd.Function):<br />
    kernel = _kernel</p>
<pre><code>_locks = {}

@staticmethod
def _call(a, b, acc_dtype=None, allow_tf32=True, fp8_fast_accum=True, output_dtype=None, use_torch_compile=False):
    device = a.device
    # handle non-contiguous inputs if necessary
    if a.stride(0) &gt; 1 and a.stride(1) &gt; 1:
        a = a.contiguous()
    if b.stride(0) &gt; 1 and b.stride(1) &gt; 1:
        b = b.contiguous()
    # checks constraints
    assert a.shape[1] == b.shape[0], "incompatible dimensions"
    M, K = a.shape
    _, N = b.shape

    # common type between a and b
    ab_dtype = get_higher_dtype(a.dtype, b.dtype)

    # allocates output
    if (output_dtype is None):
        output_dtype = ab_dtype

    c = torch.empty((M, N), device=device, dtype=output_dtype)

    # Allowed types for acc_type given the types of a and b.
    supported_acc_dtypes = {
        torch.float16: (torch.float32, torch.float16), torch.bfloat16: (torch.float32, torch.bfloat16),
        torch.float32: (torch.float32, ), torch.int8: (torch.int32, )
    }

    if acc_dtype is None:
        acc_dtype = supported_acc_dtypes[ab_dtype][0]
    else:
        assert isinstance(acc_dtype, torch.dtype), "acc_dtype must be a torch.dtype"
        assert acc_dtype in supported_acc_dtypes[a.dtype], "acc_dtype not compatible with the type of a"
        assert acc_dtype in supported_acc_dtypes[b.dtype], "acc_dtype not compatible with the type of b"

    def to_tl_type(ty):
        return getattr(tl, str(ty).split(".")[-1])

    acc_dtype = to_tl_type(acc_dtype)
    ab_dtype = to_tl_type(ab_dtype)
    output_dtype = to_tl_type(output_dtype)

    # Tensor cores support input with mixed float8 types.
    if a.dtype in [tl.float8e4nv, tl.float8e5] and b.dtype in [tl.float8e4nv, tl.float8e5]:
        ab_dtype = None
    # launch kernel
    def execute_kernel(a, b, c, M, N, K, allow_tf32, fp8_fast_accum):
        grid = lambda META: (cdiv(M, META['BLOCK_M']) * cdiv(N, META['BLOCK_N']), META['SPLIT_K'])
        _kernel[grid](
            a, b, c, M, N, K,  #
            a.stride(0), a.stride(1),  #
            b.stride(0), b.stride(1),  #
            c.stride(0), c.stride(1),  #
            acc_dtype=None,  #
            allow_tf32=allow_tf32,  #
            fp8_fast_accum=fp8_fast_accum,  #
            BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,  #
            SPLIT_K=1, 
            GROUP_M=8, AB_DTYPE=None)
    if not use_torch_compile:
        execute_kernel(a, b, c, M, N, K, allow_tf32, fp8_fast_accum)
    else:
        try:
            _matmul.execute_kernel_compiled(a, b, c, M, N, K, allow_tf32, fp8_fast_accum)
        except:
            print("Compiling kernel")
            _matmul.execute_kernel_compiled=torch.compile(execute_kernel)
            _matmul.execute_kernel_compiled(a, b, c, M, N, K, allow_tf32, fp8_fast_accum)
    return c

@staticmethod
def forward(ctx, a, b, acc_dtype=None, allow_tf32=True, fp8_fast_accum=True, output_dtype=None, use_torch_compile=False):
    return _matmul._call(a, b, acc_dtype=acc_dtype, allow_tf32=allow_tf32, fp8_fast_accum=fp8_fast_accum,
                         output_dtype=output_dtype, use_torch_compile=use_torch_compile)
</code></pre>
<p>matmul = _matmul._call</p>
<h1>benchmark starts</h1>
<h1>baseline</h1>
<p>test_tensor_a = torch.randn(1, 6144, dtype=torch.bfloat16, device='cuda')<br />
test_tensor_b = torch.randn(6144, 4096, dtype=torch.bfloat16, device='cuda')</p>
<h1>warmup</h1>
<p>for _ in range(10):<br />
    result = matmul(test_tensor_a, test_tensor_b, use_torch_compile=False)<br />
torch.cuda.synchronize()</p>
<h1>timing</h1>
<p>start_time = time.time()<br />
for _ in range(100):<br />
    result = matmul(test_tensor_a, test_tensor_b, use_torch_compile=False)<br />
torch.cuda.synchronize()<br />
print("Triton matmul: ", time.time() - start_time)</p>
<h1>torch.compile</h1>
<h1>warmup</h1>
<p>for _ in range(10):<br />
    result = matmul(test_tensor_a, test_tensor_b, use_torch_compile=True)<br />
torch.cuda.synchronize()</p>
<h1>timing</h1>
<p>start_time = time.time()<br />
for _ in range(100):<br />
    result = matmul(test_tensor_a, test_tensor_b, use_torch_compile=True)<br />
torch.cuda.synchronize()<br />
print("Triton matmul compiled: ", time.time() - start_time)<br />
```</p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.3.0a0+gitfff9d98<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.3<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.28.1<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-1057-azure-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 12.3.52<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA H100 NVL<br />
GPU 1: NVIDIA H100 NVL</p>
<p>Nvidia driver version: 535.129.03<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Byte Order:                         Little Endian<br />
Address sizes:                      48 bits physical, 48 bits virtual<br />
CPU(s):                             80<br />
On-line CPU(s) list:                0-79<br />
Thread(s) per core:                 1<br />
Core(s) per socket:                 80<br />
Socket(s):                          1<br />
NUMA node(s):                       2<br />
Vendor ID:                          AuthenticAMD<br />
CPU family:                         25<br />
Model:                              17<br />
Model name:                         AMD EPYC 9V84 96-Core Processor<br />
Stepping:                           1<br />
CPU MHz:                            2400.045<br />
BogoMIPS:                           4800.09<br />
Hypervisor vendor:                  Microsoft<br />
Virtualization type:                full<br />
L1d cache:                          2.5 MiB<br />
L1i cache:                          2.5 MiB<br />
L2 cache:                           80 MiB<br />
L3 cache:                           320 MiB<br />
NUMA node0 CPU(s):                  0-39<br />
NUMA node1 CPU(s):                  40-79<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Mitigation; safe RET, no microcode<br />
Vulnerability Spec store bypass:    Vulnerable<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr rdpru arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.3<br />
[pip3] qtorch==0.3.0<br />
[pip3] torch==2.3.0a0+git360761f<br />
[pip3] torchaudio==2.2.0.dev20240214<br />
[pip3] torchvision==0.18.0.dev20240214<br />
[pip3] triton==3.0.0<br />
[conda] blas                      1.0                         mkl<br />
[conda] brotlipy                  0.7.0           py311h9bf148f_1002    pytorch-nightly<br />
[conda] cffi                      1.15.1          py311h9bf148f_3    pytorch-nightly<br />
[conda] cryptography              38.0.4          py311h46ebde7_0    pytorch-nightly<br />
[conda] filelock                  3.9.0                   py311_0    pytorch-nightly<br />
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch-nightly<br />
[conda] magma-cuda121             2.6.1                         1    pytorch<br />
[conda] mkl                       2023.1.0         h213fc3f_46344<br />
[conda] mkl-service               2.4.0           py311h5eee18b_1<br />
[conda] mkl_fft                   1.3.8           py311h5eee18b_0<br />
[conda] mkl_random                1.2.4           py311hdb19cb5_0<br />
[conda] mpmath                    1.2.1                   py311_0    pytorch-nightly<br />
[conda] numpy                     1.26.3          py311h08b1b3b_0<br />
[conda] numpy-base                1.26.3          py311hf175353_0<br />
[conda] pillow                    9.3.0           py311h3fd9d12_2    pytorch-nightly<br />
[conda] pysocks                   1.7.1                   py311_0    pytorch-nightly<br />
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly<br />
[conda] qtorch                    0.3.0                    pypi_0    pypi<br />
[conda] requests                  2.28.1                  py311_0    pytorch-nightly<br />
[conda] torch                     2.3.0a0+git360761f           dev_0    <develop><br />
[conda] torchaudio                2.2.0.dev20240214     py311_cu121    pytorch-nightly<br />
[conda] torchvision               0.18.0.dev20240214     py311_cu121    pytorch-nightly<br />
[conda] triton                    3.0.0                    pypi_0    pypi<br />
[conda] urllib3                   1.26.14                 py311_0    pytorch-nightly</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @oulgen @aakhundov</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 16:15:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121367</guid>
    </item>
    <item>
      <title>Setting static arguments in `torch.compile()`</title>
      <link>https://github.com/pytorch/pytorch/issues/121299</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>JAX's <code>jax.jit</code> function allows you to set <code>static_argnums</code> or <code>static_argnames</code>. These arguments will be compiled into the function.</p>
<p>https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html</p>
<p>There doesn't seem to be a way to do this in PyTorch.</p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 00:41:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121299</guid>
    </item>
    <item>
      <title>[inductor] Update triton pin</title>
      <link>https://github.com/pytorch/pytorch/pull/121268</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121268<br />
* #121267<br />
* #121438</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 13:25:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121268</guid>
    </item>
    <item>
      <title>[inductor] Changes to support newer triton pin</title>
      <link>https://github.com/pytorch/pytorch/pull/121267</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121268<br />
* <strong>-&gt;</strong> #121267<br />
* #121438</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 13:25:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121267</guid>
    </item>
    <item>
      <title>[torch.compile] `index_select` out of bound read</title>
      <link>https://github.com/pytorch/pytorch/issues/121251</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When using <code>torch.compile</code>, <code>index_select</code> will out-of-bound read</p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(42)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()

def forward(self, x1):
    v = torch.index_select(x1, 0, torch.tensor([2]))
    return v
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(2, 2, 2, 2)</p>
<p>with torch.no_grad():<br />
    func1 = torch.compile(func)<br />
    print(func1(x.clone()))<br />
    # tensor([[[[1.0743e+23, 3.0618e-41],<br />
    #       [9.1084e-44, 0.0000e+00]],</p>
<pre><code>#      [[3.2230e-44, 0.0000e+00],
#       [3.2230e-44, 0.0000e+00]]]])

print(func(x.clone()))
# IndexError: index out of range in self
</code></pre>
<p>```</p>
<h3>Versions</h3>
<p>```<br />
Versions<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 10:35:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121251</guid>
    </item>
    <item>
      <title>[Inductor] Add prologue fusion</title>
      <link>https://github.com/pytorch/pytorch/pull/121211</link>
      <description><![CDATA[<p>Hi, I am new in PyTorch, but trying to add prologue fusion in the Inductor-Triton path by following the implementation of epilogue fusion. </p>
<p>I mainly changed two parts: scheduler and codegen. Current scheduler gives up fusing prologues such as relu followed by mm due to the mismatch of their dependency types. I changed TemplateBuffer so that it also issues MemoryDep when prologue fusion is enabled in config to make the dependency types match.</p>
<p>Regarding codegen, as a first use case, I changed Triton mm kernel so that it can emit prologue code. Kernel is supposed to receive single type of activation function either as its first parameter, second parameter, or both.</p>
<p>Any comments or suggestions are appreciated.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 00:10:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121211</guid>
    </item>
    <item>
      <title>[`torch.compile`] Inductor gets stuck, unbounded RAM usage</title>
      <link>https://github.com/pytorch/pytorch/issues/121197</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When I run a GNN workload with a specific dataset, the compilation process gets stuck on step 2.</p>
<h3>Error logs</h3>
<p><code>[2024-03-05 04:00:37,435] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env
[2024-03-05 04:00:37,668] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s0 = 341523 for L['x'].size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,669] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s1 = 128 for L['x'].size()[1] [2, 9223372036854775806]
[2024-03-05 04:00:37,853] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s2 = 471405 for L['subgraphs'][0].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,856] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s3 = 95082 for L['subgraphs'][0].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:37,884] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s3 - 1 &lt;= s0 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:37,951] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s3 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:37,951] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s3 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] set_replacement s1 = 128
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] set_replacement s1 = 128
[2024-03-05 04:00:37,996] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s1, 128) [guard added] at torch_geometric/nn/dense/linear.py:147 in forward (_meta_registrations.py:1980 in meta_mm)
[2024-03-05 04:00:38,017] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s4 = 124196 for L['subgraphs'][1].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,020] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s5 = 12758 for L['subgraphs'][1].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,039] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s5 - 1 &lt;= s3 - 1 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:38,098] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s5 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:38,099] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s5 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:38,143] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s6 = 12033 for L['subgraphs'][2].sampled_csc.indices.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,147] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s7 = 1025 for L['subgraphs'][2].sampled_csc.indptr.size()[0] [2, 9223372036854775806]
[2024-03-05 04:00:38,165] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval s7 - 1 &lt;= s5 - 1 [guard added] at ocalscratch/dgl-3/examples/sampling/graphbolt/lightning/../../pyg/node_classification_3.py:82 in convert_to_pyg (_decomp/decompositions.py:756 in slice_forward)
[2024-03-05 04:00:38,229] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s7 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:394 in _broadcast_shapes)
[2024-03-05 04:00:38,230] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Ne(s7 - 1, 1) [guard added] at torch_geometric/utils/_scatter.py:80 in scatter (_refs/__init__.py:400 in _broadcast_shapes)
[2024-03-05 04:00:38,277] [0/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2024-03-05 04:00:38,283] [0/0] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor</code></p>
<h3>Minified repro</h3>
<p>Running with the environment variable gets stuck too, producing no output.</p>
<p>The code in question is here: https://github.com/dmlc/dgl/pull/7183</p>
<p>You can install DGL nightly build and install torch_geometric via pip.</p>
<p><code>bash
pip install --pre dgl -f https://data.dgl.ai/wheels-test/cu121/repo.html
pip install torch_geometric
python dgl/examples/sampling/pyg/node_classification_2.py --dataset=ogbn-papers100M --torch-compile</code></p>
<h3>Versions</h3>
<details><summary>Details</summary>
<p>

Collecting environment information...
PyTorch version: 2.3.0a0+ebedce2
Is debug build: False
CUDA used to build PyTorch: 12.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.28.1
Libc version: glibc-2.35

Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.3.107
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100-SXM4-80GB
GPU 1: NVIDIA A100-SXM4-80GB
GPU 2: NVIDIA A100-SXM4-80GB
GPU 3: NVIDIA A100-SXM4-80GB

Nvidia driver version: 545.23.08
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      43 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             256
On-line CPU(s) list:                0-255
Vendor ID:                          AuthenticAMD
Model name:                         AMD EPYC 7702 64-Core Processor
CPU family:                         23
Model:                              49
Thread(s) per core:                 2
Core(s) per socket:                 64
Socket(s):                          2
Stepping:                           0
Frequency boost:                    enabled
CPU max MHz:                        2183.5930
CPU min MHz:                        1500.0000
BogoMIPS:                           4000.17
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es
Virtualization:                     AMD-V
L1d cache:                          4 MiB (128 instances)
L1i cache:                          4 MiB (128 instances)
L2 cache:                           64 MiB (128 instances)
L3 cache:                           512 MiB (32 instances)
NUMA node(s):                       2
NUMA node0 CPU(s):                  0-63,128-191
NUMA node1 CPU(s):                  64-127,192-255
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow: Mitigation; safe RET
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.24.4
[pip3] onnx==1.15.0rc2
[pip3] optree==0.10.0
[pip3] pytorch-quantization==2.1.2
[pip3] torch==2.3.0a0+ebedce2
[pip3] torch_geometric==2.5.0
[pip3] torch-tensorrt==2.3.0a0
[pip3] torchdata==0.7.1a0
[pip3] torchmetrics==1.3.1
[pip3] torchtext==0.17.0a0
[pip3] torchvision==0.18.0a0
[pip3] triton==2.2.0+e28a256
[conda] Could not collect


</p>
</details>

<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Mon, 04 Mar 2024 20:12:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121197</guid>
    </item>
    <item>
      <title>[Inductor] Outer Loop Fusion for CPP Backend</title>
      <link>https://github.com/pytorch/pytorch/pull/121064</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121064</p>
<p><strong>Summary</strong><br />
* Cases to optimize <br />
<code>def fn(x):
      max = torch.amax(x, dim=-1, keepdim=True)
      return x - max</code><br />
  Because <code>torch.amax</code> is a reduction operator that can't be fully fused with the subsequent <code>sub</code> node, two independent loops will be generated in this case.<br />
<code>for x0 in range(outer_size):
    for x1 in range(inner_size):
      amax(x1)
  for x0 in range(outer_size):
    for x1 in range(inner_size):
      sub(x1)</code><br />
  However, because the reduction is along the last dimension, we can actually fuse these two nodes along the outer dimensions, resulting in code generation as:<br />
<code>for x0 in range(outer_size):
    for x1 in range(inner_size):
      amax(x1)
    for x1 in range(inner_size):
      sub(x1)</code><br />
  This approach should offer improved cache locality.</p>
<ul>
<li>
<p>Changes made in this PR</p>
</li>
<li>
<p>We changed the kernel finalization process to adopt a lazy behavior. This means we'll gather the <code>CppKernelProxy</code> capable of outer loop fusion into a list, and finalize this list of <code>CppKernelProxy</code> lazily.</p>
</li>
<li>In the CPP <code>codegen_nodes</code>, we will first generate code for the outer loops and then proceed to generate code for the inner loops one by one.</li>
</ul>
<p><strong>Test Case</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_outer_loop_fusion</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Fri, 01 Mar 2024 18:45:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121064</guid>
    </item>
    <item>
      <title>can't compile torchvision RPN with AOTInductor</title>
      <link>https://github.com/pytorch/pytorch/issues/121036</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I try to compile an object detection model that uses torchvison's rpn module. it fails when generating anchors with <code>AssertionError: Mutating module attribute cell_anchors during export.</code> I think this is related to https://github.com/pytorch/pytorch/issues/108805</p>
<h3>Error logs</h3>
<p>```python<br />
I0301 19:15:52.323000 140521716340544 torch/fx/experimental/symbolic_shapes.py:1869] [0/0] create_env<br />
I0301 19:15:52.435000 140521716340544 torch/fx/experimental/symbolic_shapes.py:2620] [0/0] create_symbol s0 = 2 for L['batch_tensor'].size()[0] [2, 13] (_dynamo/variables/builder.py:1791 in <lambda>)<br />
V0301 19:15:52.437000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval True == True [statically known]<br />
V0301 19:15:52.437000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval False == False [statically known]<br />
V0301 19:15:52.537000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(s0, 9223372036854775807) == False [statically known]<br />
V0301 19:15:52.538000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval False == False [statically known]<br />
V0301 19:15:52.539000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval s0 &lt; 9223372036854775807 == True [statically known]<br />
V0301 19:15:52.575000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Ne(s0, 1) == True [statically known]<br />
V0301 19:15:52.592000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(s0, 1) == False [statically known]<br />
V0301 19:15:52.659000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval True == True [statically known]<br />
V0301 19:16:11.974000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Eq(3<em>s0, 3) == False [statically known]<br />
V0301 19:16:11.975000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3914] [0/0] eval Ne(3</em>s0, 3) == True [statically known]<br />
I0301 19:16:12.160000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3622] [0/0] set_replacement s0 = 2 (solve_backed) ValueRanges(lower=2, upper=2, is_bool=False)<br />
I0301 19:16:12.162000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3622] [0/0] set_replacement s0 = 2 (find) ValueRanges(lower=2, upper=2, is_bool=False)<br />
I0301 19:16:12.163000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3832] [0/0] eval Eq(s0, 2) [guard added] at torchvision/models/detection/transform.py:243 in batch_images (_dynamo/variables/tensor.py:892 in evaluate_expr)<br />
V0301 19:16:12.171000 140521716340544 torch/fx/experimental/symbolic_shapes.py:3902] [0/0] eval 2 [trivial]</p>
<hr />
<p>AssertionError                            Traceback (most recent call last)<br />
Cell In[2], line 15<br />
     12 width_dim = torch.export.Dim("width")<br />
     14 if not os.path.exists(model_path):<br />
---&gt; 15     so_path = torch._export.aot_compile(<br />
     16         f = model,<br />
     17         args = (x, ),<br />
     18         # Specify the first dimension of the input x as dynamic<br />
     19         dynamic_shapes={"batch_tensor": {0: batch_dim}},<br />
     20         # Specify the generated shared library path<br />
     21         options={<br />
     22             "aot_inductor.output_path": model_path,<br />
     23             "max_autotune": True,<br />
     24         },<br />
     25     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_export/<strong>init</strong>.py:382, in aot_compile(f, args, kwargs, constraints, dynamic_shapes, options, remove_runtime_assertions, disable_constraint_solver)<br />
    378     gm = torch.export._trace._export(f, args, kwargs, constraints, pre_dispatch=True).module()<br />
    379 else:<br />
    380     # We want to export to Torch IR here to utilize the pre_grad passes in<br />
    381     # inductor, which run on Torch IR.<br />
--&gt; 382     gm = _export_to_torch_ir(<br />
    383         f,<br />
    384         args,<br />
    385         kwargs,<br />
    386         constraints,<br />
    387         disable_constraint_solver=disable_constraint_solver,<br />
    388         # Disabling this flag, because instead we can rely on the mapping<br />
    389         # dynamo_flat_name_to_original_fqn which is coming from Dynamo.<br />
    390         restore_fqn=False,<br />
    391     )<br />
    392 flat_example_inputs = pytree.arg_tree_leaves(<em>args, </em>*(kwargs or {}))<br />
    394 with torch.no_grad():</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/export/_trace.py:320, in _export_to_torch_ir(f, args, kwargs, constraints, preserve_module_call_signature, disable_constraint_solver, restore_fqn, _log_export_usage)<br />
    316     module_call_specs: Dict[str, Dict[str, pytree.TreeSpec]] = {}<br />
    317     with _wrap_submodules(<br />
    318         f, preserve_module_call_signature, module_call_specs<br />
    319     ), _ignore_backend_decomps():<br />
--&gt; 320         gm_torch_level, _ = torch._dynamo.export(<br />
    321             f,<br />
    322             constraints=constraints,<br />
    323             assume_static_by_default=True,<br />
    324             tracing_mode="symbolic",<br />
    325             disable_constraint_solver=disable_constraint_solver,<br />
    326             _log_export_usage=_log_export_usage,<br />
    327         )(<br />
    328             <em>args,<br />
    329             </em>*kwargs,<br />
    330         )<br />
    331 except (ConstraintViolationError, ValueRangeError) as e:<br />
    332     raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: TRY200</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1314, in export.<locals>.inner(<em>args, </em><em>kwargs)<br />
   1312 # TODO(voz): We may have instances of <code>f</code> that mutate inputs, we should track sideeffects and reject.<br />
   1313 try:<br />
-&gt; 1314     result_traced = opt_f(</em>args, **kwargs)<br />
   1315 except ConstraintViolationError as e:<br />
   1316     constraint_violation_error = e</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1529, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1527     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1528 else:<br />
-&gt; 1529     return self._call_impl(*args, </strong>kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1538, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1533 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1534 # this function, and just call forward.<br />
   1535 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1536         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1537         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1538     return forward_call(</em>args, **kwargs)<br />
   1540 try:<br />
   1541     result = None</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:455, in _TorchDynamoContext.<strong>call</strong>.<locals>._fn(<em>args, </em><em>kwargs)<br />
    453 prior = set_eval_frame(callback)<br />
    454 try:<br />
--&gt; 455     return fn(</em>args, **kwargs)<br />
    456 finally:<br />
    457     set_eval_frame(prior)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1529, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1527     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1528 else:<br />
-&gt; 1529     return self._call_impl(*args, </strong>kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1538, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1533 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1534 # this function, and just call forward.<br />
   1535 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1536         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1537         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1538     return forward_call(</em>args, **kwargs)<br />
   1540 try:<br />
   1541     result = None</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:912, in catch_errors_wrapper.<locals>.catch_errors(frame, cache_entry, frame_state)<br />
    908             return hijacked_callback(frame, cache_entry, hooks, frame_state)<br />
    910 with compile_lock, _disable_current_modes():<br />
    911     # skip=1: skip this frame<br />
--&gt; 912     return callback(frame, cache_entry, hooks, frame_state, skip=1)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:398, in convert_frame_assert.<locals>._convert_frame_assert(frame, cache_entry, hooks, frame_state, skip)<br />
    384 compile_id = CompileId(frame_id, frame_compile_id)<br />
    386 signpost_event(<br />
    387     "dynamo",<br />
    388     "_convert_frame_assert._compile",<br />
   (...)<br />
    395     },<br />
    396 )<br />
--&gt; 398 return _compile(<br />
    399     frame.f_code,<br />
    400     frame.f_globals,<br />
    401     frame.f_locals,<br />
    402     frame.f_builtins,<br />
    403     compiler_fn,<br />
    404     one_graph,<br />
    405     export,<br />
    406     export_constraints,<br />
    407     hooks,<br />
    408     cache_size,<br />
    409     frame,<br />
    410     frame_state=frame_state,<br />
    411     compile_id=compile_id,<br />
    412     skip=skip + 1,<br />
    413 )</p>
<p>File /opt/conda/lib/python3.10/contextlib.py:79, in ContextDecorator.<strong>call</strong>.<locals>.inner(<em>args, </em><em>kwds)<br />
     76 @wraps(func)<br />
     77 def inner(</em>args, <strong>kwds):<br />
     78     with self._recreate_cm():<br />
---&gt; 79         return func(*args, </strong>kwds)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:669, in _compile(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)<br />
    659 log.debug(<br />
    660     "torchdynamo start compiling %s %s:%s, stack (elided %s frames):\n%s",<br />
    661     code.co_name,<br />
   (...)<br />
    666     "".join(traceback.format_list(traceback.extract_stack()[: -2 - skip])),<br />
    667 )<br />
    668 try:<br />
--&gt; 669     guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
    670     return guarded_code<br />
    671 except (<br />
    672     Unsupported,<br />
    673     TorchRuntimeError,<br />
   (...)<br />
    680     BisectValidationException,<br />
    681 ) as e:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py:256, in dynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper(<em>args, </em><em>kwargs)<br />
    254 with torch.profiler.record_function(f"{key} (dynamo_timed)"):<br />
    255     t0 = time.time()<br />
--&gt; 256     r = func(</em>args, **kwargs)<br />
    257     time_spent = time.time() - t0<br />
    258 compilation_time_metrics[key].append(time_spent)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:542, in _compile.<locals>.compile_inner(code, one_graph, hooks, transform)<br />
    540 CompileContext.get().attempt = attempt<br />
    541 try:<br />
--&gt; 542     out_code = transform_code_object(code, transform)<br />
    543     break<br />
    544 except exc.RestartAnalysis as e:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1033, in transform_code_object(code, transformations, safe)<br />
   1030 instructions = cleaned_instructions(code, safe)<br />
   1031 propagate_line_nums(instructions)<br />
-&gt; 1033 transformations(instructions, code_options)<br />
   1034 return clean_and_assemble_instructions(instructions, keys, code_options)[1]</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:163, in preserve_global_state.<locals>._fn(<em>args, </em><em>kwargs)<br />
    161 cleanup = setup_compile_debug()<br />
    162 try:<br />
--&gt; 163     return fn(</em>args, **kwargs)<br />
    164 finally:<br />
    165     cleanup.close()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:507, in _compile.<locals>.transform(instructions, code_options)<br />
    505 try:<br />
    506     with tracing(tracer.output.tracing_context), tracer.set_current_tx():<br />
--&gt; 507         tracer.run()<br />
    508 except exc.UnspecializeRestartAnalysis:<br />
    509     speculation_log.clear()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2130, in InstructionTranslator.run(self)<br />
   2129 def run(self):<br />
-&gt; 2130     super().run()</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336, in NNModuleVariable.call_function(self, tx, args, kwargs)<br />
    334 else:<br />
    335     assert istype(fn, types.FunctionType)<br />
--&gt; 336 return tx.inline_user_function_return(<br />
    337     variables.UserFunctionVariable(fn, source=fn_source),<br />
    338     args,<br />
    339     kwargs,<br />
    340 )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1243, in InstructionTranslatorBase.CALL_FUNCTION_EX(self, inst)<br />
   1241 # Map to a dictionary of str -&gt; VariableTracker<br />
   1242 kwargsvars = kwargsvars.keys_as_python_constant()<br />
-&gt; 1243 self.call_function(fn, argsvars.items, kwargsvars)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:470, in break_graph_if_unsupported.<locals>.decorator.<locals>.wrapper(self, inst)<br />
    466 try:<br />
    467     TracingContext.set_current_loc(<br />
    468         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    469     )<br />
--&gt; 470     return inner_fn(self, inst)<br />
    471 except Unsupported as excp:<br />
    472     if self.generic_context_manager_depth &gt; 0:<br />
    473         # We don't support graph break under GenericContextWrappingVariable,<br />
    474         # If there is, we roll back to the checkpoint and fall back.</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1202, in InstructionTranslatorBase.CALL_FUNCTION(self, inst)<br />
   1200 args = self.popn(inst.argval)<br />
   1201 fn = self.pop()<br />
-&gt; 1202 self.call_function(fn, args, {})</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:657, in InstructionTranslatorBase.call_function(self, fn, args, kwargs)<br />
    655 if inner_fn and callable(inner_fn) and is_forbidden(inner_fn):<br />
    656     raise AssertionError(f"Attempt to trace forbidden callable {inner_fn}")<br />
--&gt; 657 self.push(fn.call_function(self, args, kwargs))</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:334, in UserMethodVariable.call_function(self, tx, args, kwargs)<br />
    326     if (<br />
    327         module_attr is not None<br />
    328         and module_attr.startswith("torch.nn.")<br />
    329         or self.is_constant<br />
    330     ):<br />
    331         return self.obj.call_method(<br />
    332             tx, self.fn.<strong>name</strong>, args, kwargs, constant=self.is_constant<br />
    333         )<br />
--&gt; 334 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:288, in UserFunctionVariable.call_function(self, tx, args, kwargs)<br />
    283 if self.is_constant:<br />
    284     return invoke_and_store_as_constant(<br />
    285         tx, self.fn, self.get_name(), args, kwargs<br />
    286     )<br />
--&gt; 288 return super().call_function(tx, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:89, in BaseUserFunctionVariable.call_function(self, tx, args, kwargs)<br />
     86 def call_function(<br />
     87     self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"<br />
     88 ) -&gt; "VariableTracker":<br />
---&gt; 89     return tx.inline_user_function_return(<br />
     90         self, list(self.self_args()) + list(args), kwargs<br />
     91     )</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:663, in InstructionTranslatorBase.inline_user_function_return(self, fn, args, kwargs)<br />
    659 def inline_user_function_return(self, fn, args, kwargs):<br />
    660     """<br />
    661     A call to some user defined function by inlining it.<br />
    662     """<br />
--&gt; 663     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2266, in InliningInstructionTranslator.inline_call(cls, parent, func, args, kwargs)<br />
   2263 @classmethod<br />
   2264 def inline_call(cls, parent, func, args, kwargs):<br />
   2265     with patch.dict(counters, {"unimplemented": counters["inline_call"]}):<br />
-&gt; 2266         return cls.inline_call</em>(parent, func, args, kwargs)</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/<em>dynamo/symbolic_convert.py:2380, in InliningInstructionTranslator.inline_call</em>(parent, func, args, kwargs)<br />
   2378 try:<br />
   2379     with strict_ctx:<br />
-&gt; 2380         tracer.run()<br />
   2381 except exc.SkipFrame as e:<br />
   2382     msg = f"SKIPPED INLINING {code}: {e}"</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:793, in InstructionTranslatorBase.run(self)<br />
    788 try:<br />
    789     self.output.push_tx(self)<br />
    790     while (<br />
    791         self.instruction_pointer is not None<br />
    792         and not self.output.should_exit<br />
--&gt; 793         and self.step()<br />
    794     ):<br />
    795         pass<br />
    796 except BackendCompilerFailed:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:756, in InstructionTranslatorBase.step(self)<br />
    752         unimplemented(f"missing: {inst.opname}")<br />
    753     TracingContext.set_current_loc(<br />
    754         self.f_code.co_filename, self.lineno, self.f_code.co_name<br />
    755     )<br />
--&gt; 756     getattr(self, inst.opname)(inst)<br />
    758     return inst.opname != "RETURN_VALUE"<br />
    759 except Unsupported:</p>
<p>File /opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1308, in InstructionTranslatorBase.STORE_ATTR(self, inst)<br />
   1303 val, obj = self.popn(2)<br />
   1305 if isinstance(obj, NNModuleVariable):<br />
   1306     # We don't allow side effects during export<br />
   1307     # https://github.com/pytorch/torchdynamo/issues/1475<br />
-&gt; 1308     assert (<br />
   1309         not self.export<br />
   1310     ), f"Mutating module attribute {inst.argval} during export."<br />
   1312 try:<br />
   1313     BuiltinVariable(setattr).call_function(<br />
   1314         self, [obj, ConstantVariable.create(inst.argval), val], {}<br />
   1315     )</p>
<p>AssertionError: Mutating module attribute cell_anchors during export.</p>
<p>from user code:<br />
   File "/opt/workspace/./satlas-src/satlas/model/model.py", line 841, in forward<br />
    cur_outputs, _ = head(batch_tensor, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/opt/workspace/<a href="http://127.0.0.1:8888/lab/tree/satlas-src/satlas/model/model.py#line=530">./satlas-src/satlas/model/model.py", line 531</a>, in forward<br />
    proposals, proposal_losses = self.rpn(images, features, targets)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/rpn.py", line 362, in forward<br />
    anchors = self.anchor_generator(images, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 126, in forward<br />
    self.set_cell_anchors(dtype, device)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 77, in set_cell_anchors<br />
    self.cell_anchors = [cell_anchor.to(dtype=dtype, device=device) for cell_anchor in self.cell_anchors]<br />
```</p>
<h3>Minified repro</h3>
<p>os.environ["TORCH_DYNAMO_REPRO_AFTER"] = "aot" doesn't seem to do anything. after I run the code, the traceback is the same.</p>
<p>a minimal repro is to do</p>
<p>```python<br />
import torch<br />
import torch._dynamo as torchdynamo<br />
from torchvision.models.detection import (<br />
    maskrcnn_resnet50_fpn,<br />
)</p>
<p>import torch._dynamo.config</p>
<h1>torch._dynamo.config.capture_scalar_outputs = True</h1>
<p>net = maskrcnn_resnet50_fpn()<br />
net.eval()</p>
<h1>Single input which is a list of images.</h1>
<p>images = [torch.rand(3, 16, 16)]</p>
<h1>Double-check that the inputs work for the normal net.</h1>
<p>net(images)<br />
torch._dynamo.export(net, images, tracing_mode="symbolic", aten_graph=True)</p>
<p>```</p>
<p>```python<br />
AssertionError: Mutating module attribute cell_anchors during export.</p>
<p>from user code:<br />
   File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py", line 104, in forward<br />
    proposals, proposal_losses = self.rpn(images, features, targets)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/rpn.py", line 362, in forward<br />
    anchors = self.anchor_generator(images, features)<br />
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl<br />
    return forward_call(</em>args, **kwargs)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 126, in forward<br />
    self.set_cell_anchors(dtype, device)<br />
  File "/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py", line 77, in set_cell_anchors<br />
    self.cell_anchors = [cell_anchor.to(dtype=dtype, device=device) for cell_anchor in self.cell_anchors]</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information<br />
```</p>
<p>from the test script here: https://docs.google.com/document/d/159NTQQhz8ovIBxbQvGQ-fZ10pF9e2RPXm1JZYqdEzt4/edit from this issue https://github.com/pytorch/pytorch/issues/108805</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.3.0.dev20240221<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.26.4<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.1.105<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090<br />
Nvidia driver version: 535.161.07<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      39 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             12<br />
On-line CPU(s) list:                0-11<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz<br />
CPU family:                         6<br />
Model:                              158<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 6<br />
Socket(s):                          1<br />
Stepping:                           10<br />
CPU max MHz:                        4600.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           6399.96<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp vnmi md_clear flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          192 KiB (6 instances)<br />
L1i cache:                          192 KiB (6 instances)<br />
L2 cache:                           1.5 MiB (6 instances)<br />
L3 cache:                           12 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-11<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable<br />
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Meltdown:             Mitigation; PTI<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Mitigation; IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Mitigation; Microcode<br />
Vulnerability Tsx async abort:      Mitigation; TSX disabled</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==5.0.4<br />
[pip3] mypy==1.0.1<br />
[pip3] mypy-extensions==0.4.4<br />
[pip3] numpy==1.26.4<br />
[pip3] optree==0.10.0<br />
[pip3] torch==2.3.0.dev20240221<br />
[pip3] torchaudio==2.2.0.dev20240221<br />
[pip3] torchelastic==0.2.2<br />
[pip3] torchvision==0.18.0.dev20240221<br />
[pip3] triton==2.2.0<br />
[pip3] vit-pytorch==1.6.5<br />
[conda] blas                      1.0                         mkl<br />
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch-nightly<br />
[conda] mkl                       2023.1.0         h213fc3f_46344<br />
[conda] mkl-service               2.4.0           py310h5eee18b_1<br />
[conda] mkl_fft                   1.3.8           py310h5eee18b_0<br />
[conda] mkl_random                1.2.4           py310hdb19cb5_0<br />
[conda] numpy                     1.26.4                   pypi_0    pypi<br />
[conda] optree                    0.10.0                   pypi_0    pypi<br />
[conda] pytorch                   2.3.0.dev20240221 py3.10_cuda12.1_cudnn8.9.2_0    pytorch-nightly<br />
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch-nightly<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly<br />
[conda] torchaudio                2.2.0.dev20240221     py310_cu121    pytorch-nightly<br />
[conda] torchelastic              0.2.2                    pypi_0    pypi<br />
[conda] torchvision               0.18.0.dev20240221     py310_cu121    pytorch-nightly<br />
[conda] triton                    2.2.0                    pypi_0    pypi<br />
[conda] vit-pytorch               1.6.5                    pypi_0    pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4</p>]]></description>
      <pubDate>Fri, 01 Mar 2024 11:34:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121036</guid>
    </item>
    <item>
      <title>[inductor] Difference between using `mode="reduce-overhead"` and directly recording the CUDAGraph for `mode="default"`</title>
      <link>https://github.com/pytorch/pytorch/issues/120733</link>
      <description><![CDATA[<h2>Issue description</h2>
<p>If I call <code>torch.compile(model, mode="reduce-overhead")</code> versus use with <code>torch.cuda.graph(self.graph, pool=memory_pool): compiled_model(*inputs)</code> to record the kernels invoked by <code>mode="default"</code>, should I expect a difference in performance?</p>
<p>Additionally, CUDAGraphs with <code>mode="reduce-overhead"</code> seems to not allow mutated inputs, whereas <code>torch.cuda.graph</code> allows these to be recorded. Is this a limitation of the reduce-overhead mode, and are there any workarounds, for instance if there is a custom in-place-only kernel in the graph?</p>
<p>https://github.com/pytorch/pytorch/blob/f36e00b8cef66153ee210dc0781674bebfad4448/torch/_inductor/cudagraph_utils.py#L48-L50</p>
<h2>Code example</h2>
<p>```python<br />
import torch<br />
import torchvision.models as models</p>
<p>model = models.resnet18(pretrained=True).eval().cuda().half()<br />
inputs = [torch.rand((1, 3, 224, 224), dtype=torch.half).cuda()]</p>
<p>"""<br />
Method 1<br />
"""<br />
reduce_overhead = torch.compile(model, mode="reduce-overhead")<br />
reduce_overhead(*inputs)</p>
<p>"""<br />
Method 2<br />
"""<br />
default = torch.compile(model, mode="default")<br />
out = default(*inputs)</p>
<p>graph = torch.cuda.CUDAGraph()<br />
with torch.cuda.graph(graph):<br />
    out = default(*inputs)</p>
<p>graph.replay()<br />
```</p>
<h2>System Info</h2>
<p><code>torch==2.3.0.dev20240222+cu121</code></p>
<p>cc @mcarilli @ezyang @eellison @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 27 Feb 2024 13:25:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120733</guid>
    </item>
    <item>
      <title>[compiled autograd] support custom ops backed by c++ autograd::Function</title>
      <link>https://github.com/pytorch/pytorch/pull/120681</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120681</p>
<ul>
<li>Adds support for custom ops backed by c++ custom autograd functions, e.g. fbgemm</li>
<li>Include files more granularly to avoid namespace pollution and circular imports</li>
</ul>
<p>limitations:<br />
- requires user to audit their code and opt-in their custom autograd::Function via autograd::Function::is_traceable and maybe additional compiled_args + apply_with_saved implementation. this was the only way I can think of for soundness<br />
- will throw if we can't hash the saved_data i.e. for any non implemented type other than list and dict in at::IValue::hash https://github.com/pytorch/pytorch/blob/b0cfa96e82d7fbd02f5dbcef2632714caf89615d/aten/src/ATen/core/ivalue.cpp#L364<br />
- can technically silently fail if both the typeid hash and the typeid string name of the custom autograd::Function collide at the same time, and an identical autograd graph containing a different custom autograd::Function, yet that has an identical implementation, is called. this case seems extremely unlikely, and the only alternative to hash collision i can think of is compiling with reflection<br />
- tensors not saved via save_variables are not lifted, and are specialized on TensorImpl*'s hash (treated as a memory address). if needed, we can lift them.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 26 Feb 2024 17:38:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120681</guid>
    </item>
    <item>
      <title>Inefficient triton kernels generated by inductor</title>
      <link>https://github.com/pytorch/pytorch/issues/120423</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I gathered all the 10K triton kernels generated by inductor using stack of PR ( https://github.com/pytorch/pytorch/pull/120048 ). After deduping same kernels used by different fx.Graph, we get about 2K unique kernels: https://gist.github.com/shunting314/161aa8031c8cca6744a56999d74c2b47 </p>
<p>Note that, the data is augmented with ncu output. For each row, we have columns like ncu_duration_us, ncu_mem_bw_gbps for latency captured by ncu and memory bandwidth usage of the kernel captured by ncu.</p>
<p>We can do various analysis from the metadata file, but one thing that may be helpful is to run a query like<br />
<code>select count(*) from kernels where cast(ncu_duration_us as decimal) &gt; 100 and cast(ncu_mem_bw_gbps as decimal) &lt; 500 and cast(num_atomic_add as decimal) = 0;</code></p>
<p>This gives us super inefficient kernels (since membw &lt; 500GBPS) that still run long enough time (&gt;100us). We skip kernels using atomic_add since the benchmarking result for those kernel are bad because of contention in atomic_add.</p>
<p>For the kernels gathered for huggingface, there are only 14 such inefficient kernels: https://gist.github.com/shunting314/45b43cb6c03e93cd7865185a5b1d818a . </p>
<p>I picked the 4th kernel: </p>
<p>https://gist.github.com/shunting314/2a0bb5b1668f79caeb5b7d3df2ce5561</p>
<p><code>@triton.jit
def triton_poi_fused_native_dropout_16(in_ptr0, out_ptr1, load_seed_offset, xnumel, XBLOCK : tl.constexpr):
    xnumel = 25214976
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + load_seed_offset)
    tmp1 = x0
    tmp2 = tl.rand(tmp0, (tmp1).to(tl.uint32))
    tmp3 = tmp2.to(tl.float32)
    tmp4 = 0.1
    tmp5 = tmp3 &gt; tmp4
    tl.store(out_ptr1 + (x0), tmp5, None)</code><br />
This one is inefficient mainly because <code>tl.rand</code>...</p>
<p>We may check other kernels on the list to find other inefficiencies either in inductor or in triton.</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @htyu @bertmaher @jansel </p>
<h3>Error logs</h3>
<p>..</p>
<h3>Minified repro</h3>
<p>..</p>
<h3>Versions</h3>
<p>..</p>]]></description>
      <pubDate>Thu, 22 Feb 2024 11:15:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120423</guid>
    </item>
    <item>
      <title>Inductor oneDNN Graph integration</title>
      <link>https://github.com/pytorch/pytorch/pull/120301</link>
      <description><![CDATA[<h2>Inductor-CPU integration with oneDNN Graph (base PR)</h2>
<p>Uses Inductor pattern-matcher to offload some compute to oneDNN Graph library, which has fusions for compute-intensive ops such as MHA &amp; MLP. This is the first PR that adds oneDNN Graph support, so that subsequent PRs can add more pattern replacements that use oneDNN Graph. This PR only adds a replacement for one pattern (GPT2 MHA/SDPA pattern, which is different for batch size 1 &amp; batch size &gt; 1). Currently, functionality is limited to inference &amp; only Linux is supported.</p>
<h4>Details</h4>
<p>In Inductor joint graph passes, if <code>config.onednn_graph</code> is enabled, or the env variable <code>TORCHINDUCTOR_ONEDNN_GRAPH=1</code> is used on a machine that supports some advanced ISAs, then if some specific patterns are matched, the corresponding ops are handled by oneDNN Graph. However, if oneDNN Graph is unable to match a pattern, then it's handled by the same ops that'd have handled it sans oneDNN Graph.</p>
<p>As opposed to other oneDNN kernels (created with oneDNN primitives API) &amp; oneMKL kernels, these kernels are compiled at Inductor compilation-time (in case of shape-mismatch at runtime, compilation for those static input shapes will occur again).</p>
<p>BF16 dtype requires a machine to support at least <code>AVX512_BF16</code> ISA. <code>AMX</code> ISA is also used, if available.<br />
While FP32 dtype support may also work on Xeon SP generation 1 (SkyLake SP), which does not support <code>AVX512_VNNI</code> ISA, the lowest machine configuration for which this functionality will be enabled in this PR is Xeon SP generation 2 (Cascade Lake), but may be changed in a subsequent PR to include Skylake SP platform support as well. The reason to not support Skylake SP at this point is that some consumer-grade CPUs also support the same AVX512 ISAs as SkyLake SP, but this implementation has not been verified on consumer-grade CPUs.</p>
<h4>Summary of other changes</h4>
<ul>
<li>Builds the Graph Compiler backend of oneDNN Graph, which is included in <code>libdnnl.a</code></li>
<li>Adds an API to check if oneDNN Graph was built.</li>
<li>Serialization of SDPA patterns can now be done for some corner-cases, such as only inference.</li>
<li>Serialized attention patterns  can also have <code>getitem</code> in them, which should be replaced by <code>operator.getitem</code>.</li>
<li>Currently, oneDNN Graph is not used if dynamic shapes are provided at compile time.</li>
<li>In oneDNN v3.3.x, the GPT2 MHA pattern needs an attention mask, so in order to match cases of attention mask not being present, an attention mask of all zeros is being created in this implementation. The performance is still better compared to the default implementation, which means it'd be even better with oneDNN v3.5, which would make attention mask optional for GPT2 MHA pattern.</li>
</ul>
<h3>Performance data</h3>
<p>This PR accelerates inference of <code>hf_GPT2</code> by offloading its MHA computation to oneDNN Graph. Other models did not suffer any regression/breakage.</p>
<p>These performance datapoints were collected for a single TorchBench run (TorchBench may have run the workload multiple times) of each configuration, and were not cherry-picked, so it's possible that performance may be even better with this feature enabled -</p>
<p>|MODEL |BATCH SIZE | DTYPE | BEFORE: Speedup over eager-mode with the default Inductor implementation | AFTER: Speedup over eager mode with the default Inductor implementation| Perf boost = (AFTER - BEFORE)/BEFORE * 100|<br />
|--------------------------|-------------|---------|-----------------------------|--------------------------|------------|<br />
|hf_GPT2| 1 | FP32 | 1.522x | 1.784x| 17.21%|<br />
|hf_GPT2| 1 | BF16 (AMP) | 1.795x | 2.602x| 44.91%|<br />
|hf_GPT2| 2 | FP32 |  1.313x |1.629x | 24.06%|<br />
|hf_GPT2|2| BF16 (AMP) | 1.556x | 2.107x | 35.41%|<br />
|hf_GPT2_large| 1 | FP32 | 1.380x |1.510x | 9.4%|<br />
|hf_GPT2_large| 1 | BF16 (AMP) | 1.208x | 1.653x | 36.83%|<br />
|hf_GPT2_large| 2 | FP32 | 1.188x | 1.508x | 26.93%|<br />
|hf_GPT2_large|2| BF16 (AMP) | 0.991x | 1.771x | 78.70%|</p>
<p>Config: 48 physical cores of one socket of Intel Xeon Platinum 8468H (Xeon SP 4th gen a.k.a. Sapphire Rapids) . Only one logical core of a physical core was used. Intel OpenMP &amp; tcmalloc were preloaded.</p>
<p>Example of running the workload:<br />
<code>TORCHINDUCTOR_ONEDNN_GRAPH=1 OMP_NUM_THREADS=48 MKL_NUM_THREADS=48 numactl --membind=0 -C 0-47 python benchmarks/dynamo/torchbench.py --performance --inference --inductor --float32 -dcpu --only hf_GPT2 --freezing --batch-size 2</code></p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @ashokei @jingxu10 @gujinghui @PenghuiCheng @jianyuh @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @voznesenskym @penguinwu @EikanWang @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 21 Feb 2024 01:26:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120301</guid>
    </item>
    <item>
      <title>Increased compile time max GPUs to 512. Switched to int16_t DeviceIndex.</title>
      <link>https://github.com/pytorch/pytorch/pull/119639</link>
      <description><![CDATA[<p>Fixes #115331.</p>
<p>This PR increases the number of valid GPU devices to 512 (from 64) in order to future-proof PyTorch for providers that offer <a href="https://www.tensorwave.com/">single nodes with a large device count</a>. Until now, <code>DeviceIndex</code> was an <code>int8_t</code>, thus multiple changes were necessary:</p>
<ul>
<li><code>DeviceIndex</code> changed to <code>int16_t</code>. Updated consumers that assume it to be an <code>int8_t</code>.</li>
<li>Updated bounds checking for <code>torch.device()</code> in the Python frontend. Right now, we allow funny things like <code>torch.device('cpu', 200).index == -56</code>, which is undefined behavior. I inserted some checks to only allow values between 0 and <code>c10::Device::MAX_NUM_DEVICES - 1</code>.</li>
<li>Updated the <code>ArgumentInfo</code> struct as it hardcodes the device index as 8 bit field [^1]. Might be a breaking change, not sure if users rely on this.</li>
<li>Introduced <code>c10::Device::MAX_NUM_DEVICES</code> as a replacement for the old <code>C10_COMPILE_TIME_MAX_GPUS</code></li>
</ul>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225</p>
<p>[^1]: This field was unsigned, so I guess this has also been undef behavior the whole time? Our default device index is -1, so this always wrapped around to 255 when written to the <code>ArgumentInfo</code> struct. When I switched the <code>DeviceIndex</code> to <code>int16_t</code>, it actually stayed 255 after unpacking from <code>ArgumentInfo</code> again, as the <code>DeviceIndex</code> was now wide enough that it didn't wrap back to -1.</p>]]></description>
      <pubDate>Sat, 10 Feb 2024 05:14:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119639</guid>
    </item>
    <item>
      <title>Fixing logging errors uing TORCH_COMPILE_DEBUG</title>
      <link>https://github.com/pytorch/pytorch/pull/119124</link>
      <description><![CDATA[<p>Instead of (trying) to log None to an decimal integer value with the formattin, I checked with an inline if-statement if it's None, if so a default value -1 gets logged. <br />
Maybe an improvement could be to change the strings of the logs to the newer f-strings?x</p>
<p>It's my first ever contribution to any open-source projects. Feel free to give me critical feedback. </p>
<p>Fixes #118476 </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sat, 03 Feb 2024 02:32:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119124</guid>
    </item>
    <item>
      <title>[inductor] Make some improvements to FX graph caching</title>
      <link>https://github.com/pytorch/pytorch/pull/117888</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #117888</p>
<p>Summary: This is in preparation to enable FX graph caching by default. First fix some bugs uncovered by running all unit tests under <code>test/inductor/</code>. I'll enable in a separate diff in case we need to revert. Summary of changes:<br />
* Turn off caching for tests that require a compilation, e.g., when checking that a relevant counter was incremented<br />
* Bypass caching when we see mkldnn tensors as constants (they currently don't serialize, so we can't save to disk)<br />
* Include various global settings that could affect compilation in the cache key calculation.<br />
* Handle a few config settings that break key calculation.<br />
* Handle code paths where no ShapeEnv is available (the cache impl requires a shape env as part of handling guards)<br />
* Skip caching when freezing is enabled (Freezing can embed constants that wouldn't be static across runs).<br />
* Fix the clear() method to not throw when the cache /tmp dir doesn't exist</p>
<p>Test Plan: Ran all tests under <code>test/inductor/</code> twice with TORCHINDUCTOR_FX_GRAPH_CACHE=1 to exercise any test that might be affected by caching.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 19 Jan 2024 15:03:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/117888</guid>
    </item>
    <item>
      <title>`torch.compile()` makes loss `nan`</title>
      <link>https://github.com/pytorch/pytorch/issues/114109</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>torch.compile()</code> causes instability issue during training in my use case. Specifically, I observed that the loss goes <code>nan</code> with <code>torch.compile()</code> enabled.</p>
<p>To reproduce, I first tried below, but the minifier gave me <code>RuntimeError: Input graph did not fail the tester</code>:<br />
```console<br />
$ TORCHDYNAMO_REPRO_AFTER="aot" TORCHDYNAMO_REPRO_LEVEL=4 python my_script.py<br />
...<br />
torch._dynamo.debug_utils.AccuracyError: Bad accuracy detected<br />
$ python torch_compile_debug/run_2023_11_20_08_23_04_362462-pid_45740/minifier/minifier_launcher.py<br />
Loading inputs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:02&lt;00:00, 45.68it/s]</p>
<blockquote>
<blockquote>
<p>Loading inputs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53/53 [00:01&lt;00:00, 26.81it/s]<br />
 [2023-11-20 08:23:46,301] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph</p>
</blockquote>
</blockquote>
<p>Traceback (most recent call last):<br />
  File "/home/akihiro/work/github.com/kumo-ai/some-repo/torch_compile_debug/run_2023_11_20_08_23_04_362462-pid_45740/minifier/minifier_launcher.py", line 540, in <module><br />
    run_repro(mod, load_args, accuracy=True, command='minify', save_dir='/home/akihiro/work/github.com/kumo-ai/some-repo/torch_compile_debug/run_2023_11_20_08_23_04_362462-pid_45740/minifier/checkpoints', tracing_mode='real', check_str=None)<br />
  File "/home/akihiro/.conda/envs/kumo310/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 927, in run_repro<br />
    COMMAND_FNS<a href="options, mod, load_args">options.command</a><br />
  File "/home/akihiro/.conda/envs/kumo310/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 530, in repro_minify<br />
    minifier(<br />
  File "/home/akihiro/.conda/envs/kumo310/lib/python3.10/site-packages/torch/_functorch/fx_minifier.py", line 189, in minifier<br />
    raise RuntimeError("Input graph did not fail the tester")<br />
RuntimeError: Input graph did not fail the tester<br />
<code>and then, I tried below instead:</code>console<br />
$ TORCHDYNAMO_REPRO_AFTER="dynamo" TORCHDYNAMO_REPRO_LEVEL=4 python my_script.py<br />
...<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
E                   AccuracyError: Bad accuracy detected.<br />
$  python torch_compile_debug/run_2023_11_20_08_45_58_834796-pid_52789/minifier/minifier_launcher.py<br />
...<br />
Wrote minimal repro out to repro.py<br />
```</p>
<h3>Error logs</h3>
<p><code>console
$ python repro.py
Loading inputs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02&lt;00:00,  1.04s/it]
[2023-11-20 08:54:20,386] torch._dynamo.debug_utils: [WARNING] Could not generate fp64 outputs
[2023-11-20 08:54:20,388] torch._dynamo.debug_utils: [WARNING] Could not generate fp64 outputs
[2023-11-20 08:54:24,498] torch._dynamo.utils: [ERROR] Accuracy failed: allclose not within tol=0.001
Traceback (most recent call last):
  File "/home/akihiro/work/github.com/kumo-ai/some-repo/repro.py", line 44, in &lt;module&gt;
    run_repro(mod, load_args, accuracy=True, command='run',
  File "/home/akihiro/.conda/envs/kumo310/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 566, in run_repro
    COMMAND_FNS[options.command](options, mod, load_args)
  File "/home/akihiro/.conda/envs/kumo310/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 428, in repro_run
    raise AccuracyError("Dynamo failed")
torch._dynamo.debug_utils.AccuracyError: Dynamo failed</code></p>
<h3>Minified repro</h3>
<p>This is the minified <code>repro.py</code>:</p>
<p>```python<br />
from math import inf<br />
import torch<br />
from torch import tensor, device<br />
import torch.fx as fx<br />
import torch._dynamo<br />
from torch._dynamo.testing import rand_strided<br />
from torch._dynamo.debug_utils import run_fwd_maybe_bwd<br />
import torch._dynamo.config<br />
import torch._inductor.config<br />
import torch._functorch.config<br />
torch._dynamo.config.assume_static_by_default = False<br />
from torch.nn import *</p>
<p>class Repro(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
    def forward(self, view_22, getitem_51):<br />
        expand_as_16 = view_22.expand_as(getitem_51);  view_22 = getitem_51 = None<br />
        return (expand_as_16,)</p>
<p>mod = Repro()</p>
<p>def load_args(reader):<br />
    buf0 = reader.storage('d4bfc3c81078c64c193b38e7b2189c50a5687e7a', 16, device=device(type='cuda', index=0), dtype_hint=torch.int64)<br />
    reader.tensor(buf0, (1, 1), dtype=torch.int64, storage_offset=1, is_leaf=True)  # view_22<br />
    buf1 = reader.storage('549a91c2aa84c73c8e9be35307048f20aeb4fd99', 128, device=device(type='cuda', index=0))<br />
    reader.tensor(buf1, (1, 32), requires_grad=True)  # getitem_51<br />
load_args._version = 0</p>
<p>if <strong>name</strong> == '<strong>main</strong>':<br />
    from torch._dynamo.repro.after_dynamo import run_repro<br />
    run_repro(mod, load_args, accuracy=True, command='run',<br />
        save_dir='/home/akihiro/work/github.com/kumo-ai/some-repo/checkpoints', autocast=False, backend='inductor')<br />
```</p>
<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.1.0+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.22.5<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0] (64-bit runtime)<br />
Python platform: Linux-5.13.0-1031-aws-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: Tesla T4<br />
Nvidia driver version: 510.47.03<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
CPU(s):                          16<br />
On-line CPU(s) list:             0-15<br />
Thread(s) per core:              2<br />
Core(s) per socket:              8<br />
Socket(s):                       1<br />
NUMA node(s):                    1<br />
Vendor ID:                       GenuineIntel<br />
CPU family:                      6<br />
Model:                           85<br />
Model name:                      Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz<br />
Stepping:                        7<br />
CPU MHz:                         2499.996<br />
BogoMIPS:                        4999.99<br />
Hypervisor vendor:               KVM<br />
Virtualization type:             full<br />
L1d cache:                       256 KiB<br />
L1i cache:                       256 KiB<br />
L2 cache:                        8 MiB<br />
L3 cache:                        35.8 MiB<br />
NUMA node0 CPU(s):               0-15<br />
Vulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported<br />
Vulnerability L1tf:              Mitigation; PTE Inversion<br />
Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown<br />
Vulnerability Meltdown:          Mitigation; PTI<br />
Vulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown<br />
Vulnerability Spec store bypass: Vulnerable<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke avx512_vnni</p>
<p>Versions of relevant libraries:<br />
[pip3] flake8==6.1.0<br />
[pip3] mypy-boto3-s3==1.26.62<br />
[pip3] numpy==1.23.5<br />
[pip3] pytorch-lightning==2.1.0<br />
[pip3] pytorch-triton==2.1.0+6e4932cda8<br />
[pip3] torch==2.1.0+cu118<br />
[pip3] torch_geometric==2.4.0<br />
[pip3] torch-scatter==2.1.2+pt21cu118<br />
[pip3] torchdata==0.7.0<br />
[pip3] torchmetrics==0.11.4<br />
[pip3] torchtext==0.16.0<br />
[pip3] triton==2.1.0<br />
[conda] numpy                     1.23.5                   pypi_0    pypi<br />
[conda] pytorch-lightning         2.1.0                    pypi_0    pypi<br />
[conda] pytorch-triton            2.1.0+6e4932cda8          pypi_0    pypi<br />
[conda] torch                     2.1.0+cu118              pypi_0    pypi<br />
[conda] torch-geometric           2.4.0                    pypi_0    pypi<br />
[conda] torch-scatter             2.1.2+pt21cu118          pypi_0    pypi<br />
[conda] torchdata                 0.7.0                    pypi_0    pypi<br />
[conda] torchmetrics              0.11.4                   pypi_0    pypi<br />
[conda] torchtext                 0.16.0                   pypi_0    pypi<br />
[conda] triton                    2.1.0                    pypi_0    pypi<br />
```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @wconstab @bdhirsh @anijain2305</p>]]></description>
      <pubDate>Mon, 20 Nov 2023 01:15:40 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/114109</guid>
    </item>
    <item>
      <title>Torch compile: libcuda.so cannot found</title>
      <link>https://github.com/pytorch/pytorch/issues/107960</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Using <code>torch.compile</code> with a Colab T4 GPU fails and gives a very cryptic error running on nightly 2.1</p>
<details>

<summary> Error logs </summary>

```python
---------------------------------------------------------------------------
BackendCompilerFailed                     Traceback (most recent call last)
[<ipython-input-8-3e6b92348d53>](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in <cell line: 1>()
----> 1 audio = pipe("brazilian samba drums").audios[0]

52 frames
[/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in decorate_context(*args, **kwargs)
    113     def decorate_context(*args, **kwargs):
    114         with ctx_factory():
--> 115             return func(*args, **kwargs)
    116 
    117     return decorate_context

[/usr/local/lib/python3.10/dist-packages/diffusers/pipelines/audioldm2/pipeline_audioldm2.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in __call__(self, prompt, audio_length_in_s, num_inference_steps, guidance_scale, negative_prompt, num_waveforms_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, generated_prompt_embeds, negative_generated_prompt_embeds, attention_mask, negative_attention_mask, max_new_tokens, return_dict, callback, callback_steps, cross_attention_kwargs, output_type)
    925 
    926                 # predict the noise residual
--> 927                 noise_pred = self.unet(
    928                     latent_model_input,
    929                     t,

[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in _wrapped_call_impl(self, *args, **kwargs)
   1516             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517         else:
-> 1518             return self._call_impl(*args, **kwargs)
   1519 
   1520     def _call_impl(self, *args, **kwargs):

[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in _call_impl(self, *args, **kwargs)
   1525                 or _global_backward_pre_hooks or _global_backward_hooks
   1526                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1527             return forward_call(*args, **kwargs)
   1528 
   1529         try:

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in _fn(*args, **kwargs)
    326             dynamic_ctx.__enter__()
    327             try:
--> 328                 return fn(*args, **kwargs)
    329             finally:
    330                 set_eval_frame(prior)

[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in _wrapped_call_impl(self, *args, **kwargs)
   1516             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517         else:
-> 1518             return self._call_impl(*args, **kwargs)
   1519 
   1520     def _call_impl(self, *args, **kwargs):

[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in _call_impl(self, *args, **kwargs)
   1525                 or _global_backward_pre_hooks or _global_backward_hooks
   1526                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1527             return forward_call(*args, **kwargs)
   1528 
   1529         try:

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in catch_errors(frame, cache_entry, frame_state)
    486 
    487         with compile_lock, _disable_current_modes():
--> 488             return callback(frame, cache_entry, hooks, frame_state)
    489 
    490     catch_errors._torchdynamo_orig_callable = callback  # type: ignore[attr-defined]

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in _convert_frame(frame, cache_entry, hooks, frame_state)
    623         counters["frames"]["total"] += 1
    624         try:
--> 625             result = inner_convert(frame, cache_entry, hooks, frame_state)
    626             counters["frames"]["ok"] += 1
    627             return result

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in _fn(*args, **kwargs)
    137         cleanup = setup_compile_debug()
    138         try:
--> 139             return fn(*args, **kwargs)
    140         finally:
    141             cleanup.close()

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in _convert_frame_assert(frame, cache_entry, hooks, frame_state)
    378         )
    379 
--> 380         return _compile(
    381             frame.f_code,
    382             frame.f_globals,

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in _compile(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id)
    553     with compile_context(CompileContext(compile_id)):
    554         try:
--> 555             guarded_code = compile_inner(code, one_graph, hooks, transform)
    556             return guarded_code
    557         except (

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in time_wrapper(*args, **kwargs)
    187             with torch.profiler.record_function(f"{key} (dynamo_timed)"):
    188                 t0 = time.time()
--> 189                 r = func(*args, **kwargs)
    190                 time_spent = time.time() - t0
    191             compilation_time_metrics[key].append(time_spent)

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in compile_inner(code, one_graph, hooks, transform)
    475         for attempt in itertools.count():
    476             try:
--> 477                 out_code = transform_code_object(code, transform)
    478                 orig_code_map[out_code] = code
    479                 break

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in transform_code_object(code, transformations, safe)
   1026     propagate_line_nums(instructions)
   1027 
-> 1028     transformations(instructions, code_options)
   1029     return clean_and_assemble_instructions(instructions, keys, code_options)[1]
   1030 

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in transform(instructions, code_options)
    442         try:
    443             with tracing(tracer.output.tracing_context):
--> 444                 tracer.run()
    445         except (exc.RestartAnalysis, exc.SkipFrame):
    446             raise

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in run(self)
   2072 
   2073     def run(self):
-> 2074         super().run()
   2075 
   2076     def match_nested_cell(self, name, cell):

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in run(self)
    722                     self.instruction_pointer is not None
    723                     and not self.output.should_exit
--> 724                     and self.step()
    725                 ):
    726                     pass

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in step(self)
    686                 self.f_code.co_filename, self.lineno, self.f_code.co_name
    687             )
--> 688             getattr(self, inst.opname)(inst)
    689 
    690             return inst.opname != "RETURN_VALUE"

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in RETURN_VALUE(self, inst)
   2160         )
   2161         log.debug("RETURN_VALUE triggered compile")
-> 2162         self.output.compile_subgraph(
   2163             self,
   2164             reason=GraphCompileReason(

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in compile_subgraph(self, tx, partial_convert, reason)
    855             if count_calls(self.graph) != 0 or len(pass2.graph_outputs) != 0:
    856                 output.extend(
--> 857                     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
    858                 )
    859 

[/usr/lib/python3.10/contextlib.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in inner(*args, **kwds)
     77         def inner(*args, **kwds):
     78             with self._recreate_cm():
---> 79                 return func(*args, **kwds)
     80         return inner
     81 

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in compile_and_call_fx_graph(self, tx, rv, root)
    955         )
    956 
--> 957         compiled_fn = self.call_user_compiler(gm)
    958         compiled_fn = disable(compiled_fn)
    959 

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in time_wrapper(*args, **kwargs)
    187             with torch.profiler.record_function(f"{key} (dynamo_timed)"):
    188                 t0 = time.time()
--> 189                 r = func(*args, **kwargs)
    190                 time_spent = time.time() - t0
    191             compilation_time_metrics[key].append(time_spent)

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in call_user_compiler(self, gm)
   1022             unimplemented_with_warning(e, self.root_tx.f_code, msg)
   1023         except Exception as e:
-> 1024             raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
   1025                 e.__traceback__
   1026             ) from None

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in call_user_compiler(self, gm)
   1007             if config.verify_correctness:
   1008                 compiler_fn = WrapperBackend(compiler_fn)
-> 1009             compiled_fn = compiler_fn(gm, self.example_inputs())
   1010             _step_logger()(logging.INFO, f"done compiler function {name}")
   1011             assert callable(compiled_fn), "compiler_fn did not return callable"

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in debug_wrapper(gm, example_inputs, **kwargs)
    115                     raise
    116         else:
--> 117             compiled_gm = compiler_fn(gm, example_inputs)
    118 
    119         return compiled_gm

[/usr/local/lib/python3.10/dist-packages/torch/__init__.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in __call__(self, model_, inputs_)
   1566         from torch._inductor.compile_fx import compile_fx
   1567 
-> 1568         return compile_fx(model_, inputs_, config_patches=self.config)
   1569 
   1570     def get_compiler_config(self):

[/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in compile_fx(model_, example_inputs_, inner_compile, config_patches, decompositions)
   1148         tracing_context
   1149     ), compiled_autograd.disable():
-> 1150         return aot_autograd(
   1151             fw_compiler=fw_compiler,
   1152             bw_compiler=bw_compiler,

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in compiler_fn(gm, example_inputs)
     53             # NB: NOT cloned!
     54             with enable_aot_logging(), patch_config:
---> 55                 cg = aot_module_simplified(gm, example_inputs, **kwargs)
     56                 counters["aot_autograd"]["ok"] += 1
     57                 return disable(cg)

[/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in aot_module_simplified(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)
   3889 
   3890     with compiled_autograd.disable():
-> 3891         compiled_fn = create_aot_dispatcher_function(
   3892             functional_call,
   3893             full_args,

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in time_wrapper(*args, **kwargs)
    187             with torch.profiler.record_function(f"{key} (dynamo_timed)"):
    188                 t0 = time.time()
--> 189                 r = func(*args, **kwargs)
    190                 time_spent = time.time() - t0
    191             compilation_time_metrics[key].append(time_spent)

[/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in create_aot_dispatcher_function(flat_fn, flat_args, aot_config)
   3427         # You can put more passes here
   3428 
-> 3429         compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
   3430         if aot_config.is_export:
   3431 

[/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in aot_wrapper_dedupe(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)
   2210 
   2211     if ok:
-> 2212         return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
   2213 
   2214     # export path: ban duplicate inputs for now, add later if requested.

[/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in aot_wrapper_synthetic_base(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)
   2390     # Happy path: we don't need synthetic bases
   2391     if synthetic_base_info is None:
-> 2392         return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
   2393 
   2394     # export path: ban synthetic bases for now, add later if requested.

[/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in aot_dispatch_base(flat_fn, flat_args, aot_config, fw_metadata)
   1571         if torch._guards.TracingContext.get():
   1572             torch._guards.TracingContext.get().fw_metadata = fw_metadata
-> 1573         compiled_fw = compiler(fw_module, flat_args)
   1574 
   1575     # This boxed_call handling happens inside create_runtime_wrapper as well.

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in time_wrapper(*args, **kwargs)
    187             with torch.profiler.record_function(f"{key} (dynamo_timed)"):
    188                 t0 = time.time()
--> 189                 r = func(*args, **kwargs)
    190                 time_spent = time.time() - t0
    191             compilation_time_metrics[key].append(time_spent)

[/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in fw_compiler_base(model, example_inputs, is_inference)
   1090             }
   1091 
-> 1092         return inner_compile(
   1093             model,
   1094             example_inputs,

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in debug_wrapper(gm, example_inputs, **kwargs)
     78             # Call the compiler_fn - which is either aot_autograd or inductor
     79             # with fake inputs
---> 80             inner_compiled_fn = compiler_fn(gm, example_inputs)
     81         except Exception as e:
     82             # TODO: Failures here are troublesome because no real inputs,

[/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in inner(*args, **kwargs)
    226         def inner(*args, **kwargs):
    227             with DebugContext():
--> 228                 return fn(*args, **kwargs)
    229 
    230         return wrap_compiler_debug(inner, compiler_name="inductor")

[/usr/lib/python3.10/contextlib.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in inner(*args, **kwds)
     77         def inner(*args, **kwds):
     78             with self._recreate_cm():
---> 79                 return func(*args, **kwds)
     80         return inner
     81 

[/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in newFunction(*args, **kwargs)
     52             @wraps(old_func)
     53             def newFunction(*args, **kwargs):
---> 54                 return old_func(*args, **kwargs)
     55 
     56             return newFunction

[/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in compile_fx_inner(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt)
    339     }
    340 
--> 341     compiled_graph: CompiledFxGraph = fx_codegen_and_compile(
    342         *graph_args, **graph_kwargs  # type: ignore[arg-type]
    343     )

[/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in fx_codegen_and_compile(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt)
    563                     else:
    564                         context.output_strides.append(None)
--> 565             compiled_fn = graph.compile_to_fn()
    566 
    567             if graph.disable_cudagraphs:

[/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in compile_to_fn(self)
    965             return AotCodeCache.compile(self, code, cuda=self.cuda)
    966         else:
--> 967             return self.compile_to_module().call
    968 
    969     def get_output_names(self):

[/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in time_wrapper(*args, **kwargs)
    187             with torch.profiler.record_function(f"{key} (dynamo_timed)"):
    188                 t0 = time.time()
--> 189                 r = func(*args, **kwargs)
    190                 time_spent = time.time() - t0
    191             compilation_time_metrics[key].append(time_spent)

[/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in compile_to_module(self)
    936         linemap = [(line_no, node.stack_trace) for line_no, node in linemap]
    937         key, path = PyCodeCache.write(code)
--> 938         mod = PyCodeCache.load_by_key_path(key, path, linemap=linemap)
    939         self.cache_key = key
    940         self.cache_path = path

[/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in load_by_key_path(cls, key, path, linemap)
   1137                 mod.__file__ = path
   1138                 mod.key = key
-> 1139                 exec(code, mod.__dict__, mod.__dict__)
   1140                 sys.modules[mod.__name__] = mod
   1141                 # another thread might set this first

[/tmp/torchinductor_root/7n/c7nibse6wgoaypjbdhgrkgsgbfcxqvdo7jque3pr52vtf6vor5yi.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in <module>
   7289 
   7290 
-> 7291 async_compile.wait(globals())
   7292 del async_compile
   7293 

[/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in wait(self, scope)
   1416                     pbar.set_postfix_str(key)
   1417                 if isinstance(result, (Future, TritonFuture)):
-> 1418                     scope[key] = result.result()
   1419                     pbar.update(1)
   1420 

[/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in result(self)
   1275             return self.kernel
   1276         # If the worker failed this will throw an exception.
-> 1277         self.future.result()
   1278         kernel = self.kernel = _load_kernel(self.kernel_name, self.source_code)
   1279         latency = time() - t0

[/usr/lib/python3.10/concurrent/futures/_base.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in result(self, timeout)
    456                     raise CancelledError()
    457                 elif self._state == FINISHED:
--> 458                     return self.__get_result()
    459                 else:
    460                     raise TimeoutError()

[/usr/lib/python3.10/concurrent/futures/_base.py](https://f1ggesi86gr-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230823-060135-RC00_559378898#) in __get_result(self)
    401         if self._exception:
    402             try:
--> 403                 raise self._exception
    404             finally:
    405                 # Break a reference cycle with the exception in self._exception

BackendCompilerFailed: backend='inductor' raised:
AssertionError: libcuda.so cannot found!


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
```

</details>

<h3>Minified repro</h3>
<p>https://colab.research.google.com/drive/1XwD2UpPoi6RFLHA9tcXL7BdbOgkKvQ_7?usp=sharing</p>
<h3>Versions</h3>
<p>PyTorch version: 2.1.0.dev20230825+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.2 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.27.2<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.15.109+-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: Tesla T4<br />
Nvidia driver version: 525.105.17<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
Byte Order:                      Little Endian<br />
CPU(s):                          2<br />
On-line CPU(s) list:             0,1<br />
Vendor ID:                       GenuineIntel<br />
Model name:                      Intel(R) Xeon(R) CPU @ 2.20GHz<br />
CPU family:                      6<br />
Model:                           79<br />
Thread(s) per core:              2<br />
Core(s) per socket:              1<br />
Socket(s):                       1<br />
Stepping:                        0<br />
BogoMIPS:                        4399.99<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities<br />
Hypervisor vendor:               KVM<br />
Virtualization type:             full<br />
L1d cache:                       32 KiB (1 instance)<br />
L1i cache:                       32 KiB (1 instance)<br />
L2 cache:                        256 KiB (1 instance)<br />
L3 cache:                        55 MiB (1 instance)<br />
NUMA node(s):                    1<br />
NUMA node0 CPU(s):               0,1<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Mitigation; PTE Inversion<br />
Vulnerability Mds:               Vulnerable; SMT Host state unknown<br />
Vulnerability Meltdown:          Vulnerable<br />
Vulnerability Mmio stale data:   Vulnerable<br />
Vulnerability Retbleed:          Vulnerable<br />
Vulnerability Spec store bypass: Vulnerable<br />
Vulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers<br />
Vulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Vulnerable</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.23.5<br />
[pip3] pytorch-triton==2.1.0+e6216047b8<br />
[pip3] torch==2.1.0.dev20230825+cu121<br />
[pip3] torchaudio==2.1.0.dev20230825+cu121<br />
[pip3] torchdata==0.6.1<br />
[pip3] torchsummary==1.5.1<br />
[pip3] torchtext==0.15.2<br />
[pip3] torchvision==0.15.2+cu118<br />
[pip3] triton==2.0.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305</p>]]></description>
      <pubDate>Fri, 25 Aug 2023 07:01:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/107960</guid>
    </item>
    <item>
      <title>[Inductor] Add support for NEON ISA in the Inductor C++ backend</title>
      <link>https://github.com/pytorch/pytorch/pull/105590</link>
      <description><![CDATA[<p>Fixes #104729</p>
<p>As suggested in the <a href="https://dev-discuss.pytorch.org/t/torchinductor-update-5-cpu-backend-backend-performance-update-and-deep-dive-on-key-optimizations/1117#:~:text=It%20can%20be,sub%2Dclasses.">blog</a>, I subclassed the <code>VecISA</code> class and implemented a NEON version of the <code>vec_reduce_all()</code> function, to go along with the existing AVX2 and AVX512 versions. Any operation that calls <code>vec_reduce_all()</code> will also take the NEON path and benefit from its vectorization.</p>
<p>The <code>vec_reduce_all()</code> is invoked by Softmax and other operations like norms. Using the fast path results in 30% time savings for Softmax as compared to the previously taken slow path.</p>
<p>| Slow path | Fast path (NEON intrinsics)<br />
-- | -- | --<br />
Softmax (100 passes, 1024 dimension) | 623.706ms | 452.011ms</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang @Xia-Weiwen @ngimel @malfet </p>]]></description>
      <pubDate>Wed, 19 Jul 2023 11:39:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/105590</guid>
    </item>
  </channel>
</rss>
