<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>torch.compile x custom ops: op that accepts float also accepts Tensor in eager-mode</title>
      <link>https://github.com/pytorch/pytorch/issues/123470</link>
      <description><![CDATA[<p>ops that accept float will accept single-element Tensor in place of the float in eager-mode PyTorch. Underneath torch.compile, this errors:<br />
```<br />
import torch<br />
import torch.ao.quantization.fx._decomposed<br />
from torch._subclasses.fake_tensor import FakeTensorMode # W: 'torch.<br />
from torch.library import Library # W: 'torch.library.cust<br />
from torch import Tensor # W: 'torch.Tensor' imported but unused</p>
<p>lib = Library("mylib", "FRAGMENT") # W: Use `torch.library._scoped_li<br />
lib.define("op(Tensor x, float scale) -&gt; Tensor")<br />
lib.impl("op", lambda x, y: x.clone(), "CompositeExplicitAutograd")<br />
op = torch.ops.mylib.op.default</p>
<h1>@custom_op("mylib::op", mutates_args=())</h1>
<h1>def op(x: Tensor, scale: float) -&gt; Tensor:</h1>
<h1>return x.clone()</h1>
<p>x = torch.zeros(3, 16, 8, 8, dtype=torch.int8)<br />
scale = torch.tensor(0.01)</p>
<p>@torch.compile(backend="eager", fullgraph=True)<br />
def f(x, scale):<br />
    return op(x, scale)</p>
<p>f(x, scale)<br />
<code>gives:</code><br />
TorchRuntimeError: Failed running call_function mylib.op.default(<em>(FakeTensor(..., size=(3, 16, 8, 8), dtype=torch.int8), FakeTensor(..., size=<br />
())), </em>*{}):<br />
mylib::op() Expected a value of type 'float' for argument 'scale' but instead found type 'FakeTensor'.<br />
Position: 1<br />
Value: FakeTensor(..., size=())<br />
Declaration: mylib::op(Tensor x, float scale) -&gt; Tensor<br />
Cast error details: Unable to cast Python instance of type <class 'torch._subclasses.fake_tensor.FakeTensor'> to C++ type '?' (#define PYBIND11<br />
_DETAILED_ERROR_MESSAGES or compile in debug mode for details)</p>
<p>from user code:<br />
   File "<ipython-input-2-4388c01b4a06>", line 21, in f<br />
    return op(x, scale)</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>]]></description>
      <pubDate>Fri, 05 Apr 2024 11:43:57 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123470</guid>
    </item>
    <item>
      <title>AOTAutograd: add config to error when overlapping input checks would cause slow compile / runtimes</title>
      <link>https://github.com/pytorch/pytorch/pull/123455</link>
      <description><![CDATA[<p>We should eventually make the non-overlapping checks faster when dynamic shapes are enabled, but this is pretty difficult to do. So for now this PR adds a config that lets us fail fast when this situation happens, instead of causing compile times to secretly come to a crawl.</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123455</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Fri, 05 Apr 2024 08:31:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123455</guid>
    </item>
    <item>
      <title>[inductor][cpu]PT2E and QAT quantization regression in 2024-03-31 nightly release</title>
      <link>https://github.com/pytorch/pytorch/issues/123448</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>PT2E and QAT quantization regression in 2024-03-31 nightly release</p>
<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:x="urn:schemas-microsoft-com:office:excel"
xmlns="http://www.w3.org/TR/REC-html40">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content="Microsoft Excel 15">
<link id=Main-File rel=Main-File
href="file:///C:/Users/zengxian/AppData/Local/Temp/msohtmlclip1/01/clip.htm">
<link rel=File-List
href="file:///C:/Users/zengxian/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml">
<!--table
    {mso-displayed-decimal-separator:"\.";
    mso-displayed-thousand-separator:"\,";}
@page
    {margin:1.0in .75in 1.0in .75in;
    mso-header-margin:.5in;
    mso-footer-margin:.5in;}
tr
    {mso-height-source:auto;}
col
    {mso-width-source:auto;}
br
    {mso-data-placement:same-cell;}
td
    {padding-top:1px;
    padding-right:1px;
    padding-left:1px;
    mso-ignore:padding;
    color:black;
    font-size:11.0pt;
    font-weight:400;
    font-style:normal;
    text-decoration:none;
    font-family:Calibri, sans-serif;
    mso-font-charset:0;
    mso-number-format:General;
    text-align:general;
    vertical-align:bottom;
    border:none;
    mso-background-source:auto;
    mso-pattern:auto;
    mso-protection:locked visible;
    white-space:nowrap;
    mso-rotate:0;}
.xl66
    {mso-number-format:Scientific;}
-->
</head>

<body link=blue vlink=purple>


model_name | PT2E_new | PT2E_old
-- | -- | --
densenet121 | 534.6600164 | 1217.14198
mobilenet_v2 | 4760.843136 | 5860.936014
mobilenet_v3_large | 3652.993057 | 5596.587481
squeezenet1_1 | 7583.483788 | 9030.394544



</body>

</html>

<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:x="urn:schemas-microsoft-com:office:excel"
xmlns="http://www.w3.org/TR/REC-html40">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content="Microsoft Excel 15">
<link id=Main-File rel=Main-File
href="file:///C:/Users/zengxian/AppData/Local/Temp/msohtmlclip1/01/clip.htm">
<link rel=File-List
href="file:///C:/Users/zengxian/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml">

<!--table
    {mso-displayed-decimal-separator:"\.";
    mso-displayed-thousand-separator:"\,";}
@page
    {margin:1.0in .75in 1.0in .75in;
    mso-header-margin:.5in;
    mso-footer-margin:.5in;}
tr
    {mso-height-source:auto;}
col
    {mso-width-source:auto;}
br
    {mso-data-placement:same-cell;}
td
    {padding-top:1px;
    padding-right:1px;
    padding-left:1px;
    mso-ignore:padding;
    color:black;
    font-size:11.0pt;
    font-weight:400;
    font-style:normal;
    text-decoration:none;
    font-family:Calibri, sans-serif;
    mso-font-charset:0;
    mso-number-format:General;
    text-align:general;
    vertical-align:bottom;
    border:none;
    mso-background-source:auto;
    mso-pattern:auto;
    mso-protection:locked visible;
    white-space:nowrap;
    mso-rotate:0;}
.xl68
    {mso-number-format:Scientific;}
-->

</head>

<body link=blue vlink=purple>


model_name | QAT_new | QAT_old
-- | -- | --
densenet121 | 535.9700177 | 1234.085813
mobilenet_v2 | 5058.754669 | 6045.72933
squeezenet1_1 | 7979.238023 | 9213.892967
mobilenet_v3_large-eval_throughput | 4231.302988 | 6811.576935
shufflenet_v2_x1_0-eval_throughput | 5354.989357 | 6619.653679




</body>

</html>

<h3>Versions</h3>
<p></table><p>SW info</p><table border="1" class="dataframe table"><br />
<br />
<thead>
    <tr style="text-align: right;">
      <th>SW</th>
      <th>Nightly commit</th>
      <th>Main commit</th>
    </tr>
  </thead></p>
<tbody>
    <tr>
      <td>Pytorch</td>
      <td>4af23dd</td>
      <td></td>
    </tr>
    <tr>
      <td>Torchbench</td>
      <td>/</td>
      <td>ee35d764</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>ea437b3</td>
      <td></td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>b0ebddc</td>
      <td></td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>2c4665f</td>
      <td></td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>0790338</td>
      <td></td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>nightly</td>
      <td>/</td>
    </tr>
  </tbody>
<p></table><p>Reference SW info (nightly)</p><table border="1" class="dataframe table"><br />
<br />
<thead>
    <tr style="text-align: right;">
      <th>item</th>
      <th>commit</th>
    </tr>
  </thead></p>
<tbody>
    <tr>
      <td>torchbench</td>
      <td>ee35d764</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>2.4.0a0+git384cbf2</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>0.19.0a0+a0c79b3</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>2.2.0a0+17a7081</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>nightly</td>
    </tr>
  </tbody>
</table>
<p>Repro:<br />
```<br />
git clone -b chuanqiw/inductor_quant https://github.com/pytorch/benchmark.git<br />
cd benchmark<br />
pip install --no-deps -r requirements.txt<br />
pip install --no-cache Jinja2==3.1.2 markupsafe==2.0.1 beartype==0.15.0 &amp;&amp; pip install mpmath==1.3.0<br />
python install.py --continue_on_fail<br />
export LD_PRELOAD=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}/lib/libiomp5.so:${CONDA_PREFIX:-"$(dirname $(which conda))/../"}/lib/libjemalloc.so<br />
export MALLOC_CONF="oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:-1,muzzy_decay_ms:-1"</p>
<h1>PT2E</h1>
<p>TORCHINDUCTOR_FREEZING=1 python run_benchmark.py cpu -m ${models} --torchdynamo inductor --quantize --launcher --launcher-args="--throughput-mode" -b 128 --metrics throughputs<br />
mv .userbenchmark/cpu ptq<br />
cat ptq/metric* # to see the results</p>
<h1>QAT</h1>
<p>TORCHINDUCTOR_FREEZING=1 python run_benchmark.py cpu -m ${models} --torchdynamo inductor --quantize --is_qat --launcher --launcher-args="--throughput-mode" -b 128 --metrics throughputs<br />
mv .userbenchmark/cpu qat<br />
cat qat/metric* # to see the results<br />
```<br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/6f4ed57b8aaa980169f8ed7f6219afd808487bc5<br />
<a href="https://github.com/pytorch/pytorch/files/14887092/torchbench-densenet121-inference-ptq-performance-drop_guilty_commit.log">torchbench-densenet121-inference-ptq-performance-drop_guilty_commit.log</a><br />
cc @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Fri, 05 Apr 2024 07:32:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123448</guid>
    </item>
    <item>
      <title>[aotinductor] Add test case for outputs with views</title>
      <link>https://github.com/pytorch/pytorch/pull/123415</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123415</p>
<p>Also test views instead of .contiguous() for outputs with multiple aliases.</p>
<p><code>output_handles[0] = buf0.release();
        output_handles[1] = output_handles[0];
        output_handles[2] = wrap_with_raii_handle_if_needed(tmp_tensor_handle_0).release();</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 04 Apr 2024 19:08:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123415</guid>
    </item>
    <item>
      <title>[inductor] Write generated files from parent process</title>
      <link>https://github.com/pytorch/pytorch/pull/123409</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #123012<br />
* #123011<br />
* #122941<br />
* <strong>-&gt;</strong> #123409</p>
<p>Before this PR we would pass generated source code over a pipe to the compile worker then the compile worker would write out the file.  Doing it this way is faster and results in smaller messages to the workers (and lets us skip creating the workers in the warm start case).</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 04 Apr 2024 16:38:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123409</guid>
    </item>
    <item>
      <title>[Compile FSDP2][1/n] Support using user-defined object instance method as hook</title>
      <link>https://github.com/pytorch/pytorch/pull/123399</link>
      <description><![CDATA[<p>FSDP2 has this pattern of using user-defined object instance method as hook, and it will throw this error under compile:<br />
<code>torch._dynamo.exc.Unsupported: call_function UserDefinedObjectVariable(_pre_forward) [FSDPManagedNNModuleVariable(), TupleVariable(), ConstDictVariable()] {}</code></p>
<p>This PR adds support for it by always allowing to call into a UserDefinedObjectVariable that's an instance method (i.e. <code>MethodType</code>).</p>
<p>Supersedes https://github.com/pytorch/pytorch/pull/123320.</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @chauhang @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @LucasLLC</p>]]></description>
      <pubDate>Thu, 04 Apr 2024 15:28:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123399</guid>
    </item>
    <item>
      <title>[inductor] Optionally save inductor cache on benchmark CI failure</title>
      <link>https://github.com/pytorch/pytorch/pull/123389</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123389</p>
<p>Summary:</p>
<p>Test Plan:</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 04 Apr 2024 13:42:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123389</guid>
    </item>
    <item>
      <title>[compiled autograd][dynamo] Codegen aliases to keep grad mutated tensors alive</title>
      <link>https://github.com/pytorch/pytorch/pull/123359</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122535<br />
* <strong>-&gt;</strong> #123359<br />
* #122353<br />
* #123212<br />
* #123007<br />
* #122746<br />
* #122691</p>
<p>The current codegen is problematic if __compiled_fn_0 clears the inputs list, since we need it for assignment afterwards<br />
<code>python
def forward(inputs):
    __compiled_fn_0 = ...  # The actual function needs to be provided
    graph_out_0 = __compiled_fn_0(inputs)  # clears inputs
    temp_list = []
    temp_list.append(graph_out_0[0])
    inputs[4].grad = graph_out_0[1]  # inputs is empty, index error
    inputs[7].grad = graph_out_0[2]
    inputs[8].grad = graph_out_0[3]
    inputs[9].grad = graph_out_0[3]
    del graph_out_0
    return temp_list</code></p>
<p>With this fix, we use aliases to keep the tensors alive<br />
<code>python
def forward(inputs):
    __compiled_fn_0 = ...  # The actual function needs to be provided
    inputs_ref_1 = inputs[9]
    inputs_ref_2 = inputs[4]
    inputs_ref_3 = inputs[8]
    inputs_ref_4 = inputs[7]
    graph_out_0 = __compiled_fn_0(inputs)
    temp_list = []
    temp_list.append(graph_out_0[0])
    inputs_ref_2.grad = graph_out_0[1]
    inputs_ref_4.grad = graph_out_0[2]
    inputs_ref_3.grad = graph_out_0[3]
    inputs_ref_1.grad = graph_out_0[3]
    del graph_out_0
    return temp_list</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Thu, 04 Apr 2024 08:49:46 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123359</guid>
    </item>
    <item>
      <title>[Inductor] Add a device agnostic DeviceGuard class to inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/123338</link>
      <description><![CDATA[<p>Summary: Currently although only in one place in inductor, the <code>device</code> context manager from the device interface is used . This PR creates an inductor specific <code>DeviceGuard</code> class for use in these cases, which keeps a reference to the <code>DeviceInterface</code> class which is defined and added out of tree. This then offloads the device specific work to the device interface, instead of having to define this logic on the device class which isn't strictly necessary for inductor.</p>
<p>Ideally I would have used the existing <code>DeviceGuard</code> class, but these are defined per device and don't work well with inductor's device agnostic/ out of tree compatible design. With the existing classes in mind, I am happy to take suggestions on the renaming of this class. </p>
<p>Whilst I was there, I also took the opportunity to rename <code>gpu_device</code> to <code>device_interface</code> to clarify this is not necessarily a GPU. </p>
<p>Test Plan: None currently, happy to add some. </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 04 Apr 2024 03:16:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123338</guid>
    </item>
    <item>
      <title>[inductor] Bypass FX graph cache when we have HigherOrderOperators</title>
      <link>https://github.com/pytorch/pytorch/pull/123325</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123325</p>
<p>Summary: The initial motivation was to avoid caching when we have triton higher order ops, but it's probably safer to avoid the cache for all higher order ops and allow/implement if/when we find it necessary.</p>
<p>Test Plan: Unit test cribbed from: https://docs-preview.pytorch.org/pytorch/tutorials/2783/recipes/torch_compile_user_defined_triton_kernel_tutorial.html?highlight=triton</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 19:20:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123325</guid>
    </item>
    <item>
      <title>[testing][inductor] Specialize on unguarded alignment of example inputs</title>
      <link>https://github.com/pytorch/pytorch/pull/123319</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123319</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 18:06:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123319</guid>
    </item>
    <item>
      <title>profile pt2 compile time with strobelight</title>
      <link>https://github.com/pytorch/pytorch/pull/123311</link>
      <description><![CDATA[<p>For oss this diff adds a decorator @profile_sb_fbcode that is a nop for non meta workload.</p>
<p>Facebook:<br />
With this diff someone can generate a strobelight profile for pt2 compilation.<br />
users need to set the env variable TORCH_COMPILE_SL_PROFILE =TRUE .</p>
<p>For example:<br />
<code>TORCH_COMPILE_SL_PROFILE =TRUE buck2 run  @//mode/inplace  @//mode/opt  //caffe2/fb/strobelight:compiletime_profile_example</code><br />
see sample output bellow, at the end of summary. </p>
<p>The way this works, is that a unique id is generated and associated with all samples that are collected for functions that are decorated with profile_sb_fbcode. <br />
This id can then be used to combine different strobe light profile into one. (for example three compilation events happens in the code bellow). </p>
<p>Right now the following two functions are annotated with  profile_sb_fbcode.  bw_compiler and _compile. if two profile_sl_fbcode is called recursively, recursive invocations are ignored and a log is printed. </p>
<p>The output is:<br />
<code>Strobelight is enabled for pt2 compilation
Unique user-id for this run is: 2024-04-03-13:59:49147091devvm4561.ash0.facebook.com
You can use the following link to access the strobelight profile at the end of the run:
https://www.internalfb.com/intern/scuba/query/?dataset=pyperf_experimental%2Fon_demand&amp;drillstate=%7B%22purposes%22%3A[]%2C%22end%22%3A%22now%22%2C%22start%22%3A%22-30%20days%22%2C%22filterMode%22%3A%22DEFAULT%22%2C%22modifiers%22%3A[]%2C%22sampleCols%22%3A[]%2C%22cols%22%3A[%22namespace_id%22%2C%22namespace_process_id%22]%2C%22derivedCols%22%3A[]%2C%22mappedCols%22%3A[]%2C%22enumCols%22%3A[]%2C%22return_remainder%22%3Afalse%2C%22should_pivot%22%3Afalse%2C%22is_timeseries%22%3Afalse%2C%22hideEmptyColumns%22%3Afalse%2C%22timezone%22%3A%22America%2FLos_Angeles%22%2C%22compare%22%3A%22none%22%2C%22samplingRatio%22%3A%221%22%2C%22metric%22%3A%22count%22%2C%22aggregation_field%22%3A%22async_stack_complete%22%2C%22top%22%3A10000%2C%22aggregateList%22%3A[]%2C%22param_dimensions%22%3A[%7B%22dim%22%3A%22py_async_stack%22%2C%22op%22%3A%22edge%22%2C%22param%22%3A%220%22%2C%22anchor%22%3A%220%22%7D]%2C%22order%22%3A%22weight%22%2C%22order_desc%22%3Atrue%2C%22constraints%22%3A[[%7B%22column%22%3A%22run_user%22%2C%22op%22%3A%22eq%22%2C%22value%22%3A[%22[%5C%222024-04-03-13:59:49147091devvm4561.ash0.facebook.com%5C%22]%22]%7D]]%2C%22c_constraints%22%3A[[]]%2C%22b_constraints%22%3A[[]]%2C%22ignoreGroupByInComparison%22%3Afalse%7D&amp;view=GraphProfilerView&amp;&amp;pool=uber&amp;graphprofiler_filter=&amp;graphprofiler_column_to_sort_by=exclusive
the link below takes you to the collected strobelight profile
https://www.internalfb.com/intern/scuba/query/?dataset=pyperf_experimental%2Fon_demand&amp;drillstate=%7B%22dimensions%22%3A%5B%5D%2C%22param_dimensions%22%3A%5B%7B%22anchor%22%3A%220%22%2C%22param%22%3A%220%22%2C%22op%22%3A%22edge%22%2C%22dim%22%3A%22py_async_stack%22%7D%5D%2C%22constraints%22%3A%5B%5B%7B%22value%22%3A%5B%22%5B%5C%22-6800545191281321%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_id%22%7D%2C%7B%22value%22%3A%5B%22%5B%5C%222024-04-03-13%3A59%3A49147091devvm4561.ash0.facebook.com%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_user%22%7D%5D%5D%2C%22top%22%3A10000%2C%22end%22%3A%221712181610%22%2C%22start%22%3A%221712174410%22%7D&amp;view=GraphProfilerView&amp;
1 storbelight success runs out of 1 non-ignored runs.
strobelight run id is: 6181728288420687
the link below takes you to the collected strobelight profile
https://www.internalfb.com/intern/scuba/query/?dataset=pyperf_experimental%2Fon_demand&amp;drillstate=%7B%22dimensions%22%3A%5B%5D%2C%22param_dimensions%22%3A%5B%7B%22anchor%22%3A%220%22%2C%22param%22%3A%220%22%2C%22op%22%3A%22edge%22%2C%22dim%22%3A%22py_async_stack%22%7D%5D%2C%22constraints%22%3A%5B%5B%7B%22value%22%3A%5B%22%5B%5C%226181728288420687%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_id%22%7D%2C%7B%22value%22%3A%5B%22%5B%5C%222024-04-03-13%3A59%3A49147091devvm4561.ash0.facebook.com%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_user%22%7D%5D%5D%2C%22top%22%3A10000%2C%22end%22%3A%221712181621%22%2C%22start%22%3A%221712174421%22%7D&amp;view=GraphProfilerView&amp;
2 storbelight success runs out of 2 non-ignored runs.
strobelight run id is: -1026103682715688
the link below takes you to the collected strobelight profile
https://www.internalfb.com/intern/scuba/query/?dataset=pyperf_experimental%2Fon_demand&amp;drillstate=%7B%22dimensions%22%3A%5B%5D%2C%22param_dimensions%22%3A%5B%7B%22anchor%22%3A%220%22%2C%22param%22%3A%220%22%2C%22op%22%3A%22edge%22%2C%22dim%22%3A%22py_async_stack%22%7D%5D%2C%22constraints%22%3A%5B%5B%7B%22value%22%3A%5B%22%5B%5C%22-1026103682715688%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_id%22%7D%2C%7B%22value%22%3A%5B%22%5B%5C%222024-04-03-13%3A59%3A49147091devvm4561.ash0.facebook.com%5C%22%5D%22%5D%2C%22op%22%3A%22eq%22%2C%22column%22%3A%22run_user%22%7D%5D%5D%2C%22top%22%3A10000%2C%22end%22%3A%221712181647%22%2C%22start%22%3A%221712174447%22%7D&amp;view=GraphProfilerView&amp;
3 storbelight success runs out of 3 non-ignored runs.</code></p>
<p>Test Plan:<br />
Was tested on buck2 run  @//mode/inplace  @//mode/opt  //caffe2/fb/strobelight:compiletime_profile_example</p>
<p>This was also tested in one of the ads benchmarks<br />
<code>TORCH_COMPILE_SL_PROFILE =TRUE buck2 run mode/opt mode/inplace //pytorch/benchmark:run -- ads_mc_igctr_mc3_v0 -d cuda -t train --torchdynamo inductor</code><br />
The results matches the results reported in <br />
https://fb.workplace.com/groups/257735836456307/permalink/657458576484029</p>
<p>Differential Revision: D55672271</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 03 Apr 2024 16:17:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123311</guid>
    </item>
    <item>
      <title>[fix] inductor `split` lowering fails if `item()` is captured</title>
      <link>https://github.com/pytorch/pytorch/pull/123032</link>
      <description><![CDATA[<p>Fixes #122937</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sat, 30 Mar 2024 10:57:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123032</guid>
    </item>
    <item>
      <title>[minifier] Don't recompile for accuracy minification</title>
      <link>https://github.com/pytorch/pytorch/pull/123005</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #123006<br />
* <strong>-&gt;</strong> #123005</p>
<p><code>backend_aot_accuracy_fails</code> reruns <code>compile_fx_inner</code> on the real inputs which<br />
means the graph is recompiled with static shapes. This meant accuracy failures<br />
related to dynamic shapes would never be captured by <code>REPRO_AFTER=aot</code>.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Fri, 29 Mar 2024 15:48:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123005</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Reuse inductor test for Intel GPU (PART 1)</title>
      <link>https://github.com/pytorch/pytorch/pull/122866</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122866</p>
<p>backend.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 20:28:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122866</guid>
    </item>
    <item>
      <title>[Inductor] Pass device interface to the worker compile</title>
      <link>https://github.com/pytorch/pytorch/pull/122492</link>
      <description><![CDATA[<p>Summary: In <code>codecache.py</code> pass the device_interface directly to <code>_worker_compile()</code> instead of calling <code>get_device_interface()</code> from inside the function.</p>
<p>If the device_interface is registered by an out-of-tree module then it will only be registered inside the main process and not inside the worker process. This fixes this issue. Happy to add a test if required. </p>
<p>Test plan:<br />
No tests added</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 22 Mar 2024 04:05:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122492</guid>
    </item>
    <item>
      <title>[effects] Add inductor support for tokens</title>
      <link>https://github.com/pytorch/pytorch/pull/122347</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122348<br />
* <strong>-&gt;</strong> #122347</p>
<p>Given the following code/dynamo graph:<br />
<code>class GraphModule(torch.nn.Module):
    def forward(self, L_x_ : torch.Tensor):
        l_x_ = L_x_
        _print = torch.ops.aten._print('moo')
        res = l_x_ + l_x_;  l_x_ = None
        _print_1 = torch.ops.aten._print('moo')
        return (res,)</code></p>
<p>AOTAutograd will trace the following program, threading tokens from the inputs, through the effectful operator calls (torch.ops.aten._print), and as an output:<br />
<code>class &lt;lambda&gt;(torch.nn.Module):
    def forward(self, arg0_1: "f32[0]", arg1_1: "f32[2, 3]"):
        with_effects = torch._higher_order_ops.effects.with_effects(arg0_1, torch.ops.aten._print.default, 'moo');  arg0_1 = None
        getitem: "f32[0]" = with_effects[0];  with_effects = None
        add: "f32[2, 3]" = torch.ops.aten.add.Tensor(arg1_1, arg1_1);  arg1_1 = None
        with_effects_1 = torch._higher_order_ops.effects.with_effects(getitem, torch.ops.aten._print.default, 'moo');  getitem = None
        getitem_2: "f32[0]" = with_effects_1[0];  with_effects_1 = None
        return (getitem_2, add)</code><br />
However when we get to inductor, since we want the inductor generated code to not have any token inputs/outputs for better readability, we want to modify the aten graph by removing the tokens from inputs, and creating them through <code>torch.ops.aten._make_dep_token</code>, and sinking them through the <code>torch.ops.aten._sink_tokens</code> operators. <br />
This has to be done <em>after</em> the partitioner, otherwise the partitioner will add the make_token/sink_token operators to the backwards graph. <br />
<code>class &lt;lambda&gt;(torch.nn.Module):
   def forward(self, arg1_1: "f32[2, 3]"):
       _make_dep_token_default: "f32[0]" = torch.ops.aten._make_dep_token.default()
       with_effects = torch._higher_order_ops.effects.with_effects(_make_dep_token_default, torch.ops.aten._print.default, 'moo');  _make_dep_token_default = None
       getitem: "f32[0]" = with_effects[0];  with_effects = None
       add: "f32[2, 3]" = torch.ops.aten.add.Tensor(arg1_1, arg1_1);  arg1_1 = None
       with_effects_1 = torch._higher_order_ops.effects.with_effects(getitem, torch.ops.aten._print.default, 'moo');  getitem = None
       getitem_2: "f32[0]" = with_effects_1[0];  with_effects_1 = None
       _sink_tokens_default = torch.ops.aten._sink_tokens.default((getitem_2,));  getitem_2 = None
       return (add,)</code><br />
When doing inductor lowering, we convert <code>with_effects</code> calls to an <code>EffectfulKernel</code>, which just a <code>FallbackKernel</code> but with a pointer to previous effectful operator's call. During scheduling, we will create a <code>StarDep</code> between the EffectfulKernel and its previous EffectfulKernel so that they don't get reordered. The inductor generated python code looks like:<br />
<code>def call(args):
    arg1_1, = args
    args.clear()
    assert_size_stride(arg1_1, (2, 3), (3, 1))
    # Source Nodes: [_print], Original ATen: []
    buf2 = aten._print.default('moo')
    # Source Nodes: [_print_1], Original ATen: []
    buf3 = aten._print.default('moo')
    buf4 = empty_strided_cpu((2, 3), (3, 1), torch.float32)
    cpp_fused_add_0(arg1_1, buf4)
    del arg1_1
    return (buf4, )</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 14:10:17 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122347</guid>
    </item>
    <item>
      <title>[HOP][inductor] Support pytrees as associative_scan input</title>
      <link>https://github.com/pytorch/pytorch/pull/122137</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122137<br />
* #119430</p>
<p>This allows <code>associative_scan</code> to take an arbitrary pytree of tensors,<br />
which is flattened to their leaves before calling the <code>associative_scan</code><br />
higher order operator.</p>
<p>I also add support in inductor to generate code for scanning over sequences<br />
of tensors.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 18 Mar 2024 14:10:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122137</guid>
    </item>
    <item>
      <title>torch.compile() and optimizer UX issues</title>
      <link>https://github.com/pytorch/pytorch/issues/120814</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I was trying to get some toy training loop going and there were a few sharp bits that we should probably document in 1 place</p>
<ol>
<li>Need to instantiate both <code>optimizer</code> and <code>criterion</code> outside of the compiled function</li>
<li>There are graph breaks on <code>optimizer.zero_grad()</code> and <code>optimizer.step()</code>, those graph breaks are deliberate since per @janeyx99 these would take too long to trace but users might still to try to fix them to make their models go faster</li>
<li><code>loss.backward()</code> fails in fullgraph with an <code>torch._dynamo.exc.Unsupported: Tensor.backward</code> error</li>
</ol>
<p>There might be more, will update this issue if I find them</p>
<h3>Error logs</h3>
<p>https://gist.github.com/msaroufim/15a4b97c3f45cead4b2feb90894ed8d3</p>
<h3>Minified repro</h3>
<p>n</p>
<h3>Versions</h3>
<p>Latest stable</p>
<p>cc @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar @ezyang @bdhirsh @anijain2305 @zou3519 @chauhang @mlazos</p>]]></description>
      <pubDate>Wed, 28 Feb 2024 11:24:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120814</guid>
    </item>
    <item>
      <title>[WIP][inductor] padding</title>
      <link>https://github.com/pytorch/pytorch/pull/120758</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120758<br />
* #123327</p>
<p>This is not fully ready for review yet. Sending it out in case people want to give early feedback. Also it can allow me run CI perf test earlier.</p>
<p>By testing BlenderbotSmallForConditionalGeneration I already see 2.5ms speedup. I feel the solution can be further improved to get more speedup.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang @jansel @Chillee @eellison </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames</p>]]></description>
      <pubDate>Tue, 27 Feb 2024 17:00:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120758</guid>
    </item>
    <item>
      <title>[HOP][inductor] Add higher order associative scan operator</title>
      <link>https://github.com/pytorch/pytorch/pull/119430</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122137<br />
* <strong>-&gt;</strong> #119430</p>
<p>Currently only supports single tensor scans, e.g. <code>cumsum</code>, <code>cumprod</code>, <code>logcumsumexp</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Wed, 07 Feb 2024 17:00:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119430</guid>
    </item>
  </channel>
</rss>
