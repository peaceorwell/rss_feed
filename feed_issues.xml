<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[Reland][Inductor] Add support for NEON ISA For Mac OS</title>
      <link>https://github.com/pytorch/pytorch/pull/122217</link>
      <description><![CDATA[<p>This is a re-land of https://github.com/pytorch/pytorch/pull/105590 but<br />
this time enbaling it only for Darwin platform where those instructions<br />
are available by default</p>
<p>Fixes #ISSUE_NUMBER</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 10:42:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122217</guid>
    </item>
    <item>
      <title>Update torch.compile_faq w.r.t to functorch</title>
      <link>https://github.com/pytorch/pytorch/pull/122213</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122213<br />
* #122212<br />
* #122211</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 09:47:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122213</guid>
    </item>
    <item>
      <title>[inductor][testing] Disable fp fusion in triton kernels</title>
      <link>https://github.com/pytorch/pytorch/pull/122203</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122203<br />
* #115435<br />
* #121924<br />
* #122031</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 08:00:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122203</guid>
    </item>
    <item>
      <title>Need to FUNCTORCH_STACK_MATCH in the vmap(compile(f)) case</title>
      <link>https://github.com/pytorch/pytorch/issues/122201</link>
      <description><![CDATA[<p>When entering torch.compile, if the functorch state is already active, then we need to emit this guard</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @Chillee @samdow @kshitij12345 @janeyx99</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 07:41:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122201</guid>
    </item>
    <item>
      <title>Bug: `torch.compile` fails to trace jvp with raw python numbers.</title>
      <link>https://github.com/pytorch/pytorch/issues/122197</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p><code>torch.compile</code> fails to compile jvp transformation with native python numbers. Many tests ignored on <code>dynamo_expected_fail</code> fails for this same reason.</p>
<p>```python<br />
import torch<br />
torch._dynamo.config.capture_func_transforms = True<br />
jvp = torch.func.jvp</p>
<p>def f(x, y, z):<br />
    a, b = x<br />
    return a + 2 * b + 3 * y + 4 * z</p>
<p>@torch.compile(backend='aot_eager', fullgraph=True)<br />
def fn(x):<br />
    return jvp(f, ((x, x,), x, x), ((x, x), x, x))</p>
<p>x = torch.tensor(1.,)<br />
y = fn(x)<br />
print(y)<br />
```</p>
<p>```python<br />
Traceback (most recent call last):<br />
  File "/home/guilhermeleobas/git/pytorch/a.py", line 24, in <module><br />
    y = fn(x)<br />
        ^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/eval_frame.py", line 450, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/convert_frame.py", line 923, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state, skip=1)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert<br />
    return _compile(<br />
           ^^^^^^^^^<br />
  File "/home/guilhermeleobas/micromamba/envs/pytorch/lib/python3.11/contextlib.py", line 81, in inner<br />
    return func(</em>args, <strong>kwds)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/convert_frame.py", line 676, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
        ^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/convert_frame.py", line 535, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/convert_frame.py", line 165, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/convert_frame.py", line 500, in transform<br />
    tracer.run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 2150, in run<br />
    super().run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
        ^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 489, in wrapper<br />
    return inner_fn(self, inst)<br />
           ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 1802, in CALL<br />
    self.call_function(fn, args, kwargs)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
    self.push(fn.call_function(self, args, kwargs))<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 289, in call_function<br />
    return super().call_function(tx, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 90, in call_function<br />
    return tx.inline_user_function_return(<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 680, in inline_user_function_return<br />
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 2286, in inline_call<br />
    return cls.inline_call</em>(parent, func, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 2400, in inline_call</em><br />
    tracer.run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
        ^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 489, in wrapper<br />
    return inner_fn(self, inst)<br />
           ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 1802, in CALL<br />
    self.call_function(fn, args, kwargs)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
    self.push(fn.call_function(self, args, kwargs))<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 289, in call_function<br />
    return super().call_function(tx, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 90, in call_function<br />
    return tx.inline_user_function_return(<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 680, in inline_user_function_return<br />
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 2286, in inline_call<br />
    return cls.inline_call</em>(parent, func, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 2400, in inline_call</em><br />
    tracer.run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
        ^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 489, in wrapper<br />
    return inner_fn(self, inst)<br />
           ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 1260, in CALL_FUNCTION_EX<br />
    self.call_function(fn, argsvars.items, kwargsvars)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
    self.push(fn.call_function(self, args, kwargs))<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 289, in call_function<br />
    return super().call_function(tx, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 90, in call_function<br />
    return tx.inline_user_function_return(<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 680, in inline_user_function_return<br />
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 2286, in inline_call<br />
    return cls.inline_call</em>(parent, func, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 2400, in inline_call</em><br />
    tracer.run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
        ^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 489, in wrapper<br />
    return inner_fn(self, inst)<br />
           ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 1260, in CALL_FUNCTION_EX<br />
    self.call_function(fn, argsvars.items, kwargsvars)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
    self.push(fn.call_function(self, args, kwargs))<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 289, in call_function<br />
    return super().call_function(tx, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/functions.py", line 90, in call_function<br />
    return tx.inline_user_function_return(<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 680, in inline_user_function_return<br />
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 2286, in inline_call<br />
    return cls.inline_call</em>(parent, func, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 2400, in inline_call</em><br />
    tracer.run()<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
        ^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 1764, in BINARY_OP<br />
    return getattr(self, "BINARY</em>" + opname)(inst)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/symbolic_convert.py", line 242, in impl<br />
    self.push(fn_var.call_function(self, self.popn(nargs), {}))<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/builtin.py", line 641, in call_function<br />
    return wrap_fx_proxy(tx, proxy)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/builder.py", line 1338, in wrap_fx_proxy<br />
    return wrap_fx_proxy_cls(target_cls=TensorVariable, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/variables/builder.py", line 1423, in wrap_fx_proxy_cls<br />
    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)<br />
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1732, in get_fake_value<br />
    raise TorchRuntimeError(str(e)).with_traceback(e.<strong>traceback</strong>) from None<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1667, in get_fake_value<br />
    ret_val = wrap_fake_exception(<br />
              ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1198, in wrap_fake_exception<br />
    return fn()<br />
           ^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1668, in <lambda><br />
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)<br />
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1800, in run_node<br />
    raise RuntimeError(make_error_message(e)).with_traceback(<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_dynamo/utils.py", line 1782, in run_node<br />
    return node.target(</em>args, <strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/utils/_stats.py", line 20, in wrapper<br />
    return fn(*args, </strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_subclasses/fake_tensor.py", line 893, in <strong>torch_dispatch</strong><br />
    return self.dispatch(func, types, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_subclasses/fake_tensor.py", line 1238, in dispatch<br />
    return self._cached_dispatch_impl(func, types, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_subclasses/fake_tensor.py", line 963, in _cached_dispatch_impl<br />
    output = self._dispatch_impl(func, types, args, kwargs)<br />
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_subclasses/fake_tensor.py", line 1455, in _dispatch_impl<br />
    r = func(<em>args, </em><em>kwargs)<br />
        ^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/guilhermeleobas/git/pytorch/torch/<em>ops.py", line 600, in <strong>call</strong><br />
    return self</em>._op(</em>args, <strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function mul>(*(2, GradTrackingTensor(lvl=1, value=<br />
    FakeTensor(..., size=())<br />
)), </strong>{}):<br />
aten::alias() Expected a value of type 'Tensor' for argument 'self' but instead found type 'int'.<br />
Position: 0<br />
Value: 2<br />
Declaration: aten::alias(Tensor(a) self) -&gt; Tensor(a)<br />
Cast error details: Unable to cast 2 to Tensor</p>
<p>from user code:<br />
   File "/home/guilhermeleobas/git/pytorch/a.py", line 20, in fn<br />
    return jvp(f, ((x, x,), x, x), ((x, x), x, x))<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_functorch/eager_transforms.py", line 985, in jvp<br />
    return _jvp_with_argnums(func, primals, tangents, argnums=None, strict=strict, has_aux=has_aux)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_functorch/vmap.py", line 47, in fn<br />
    return f(<em>args, </em><em>kwargs)<br />
  File "/home/guilhermeleobas/git/pytorch/torch/_functorch/eager_transforms.py", line 1031, in _jvp_with_argnums<br />
    result_duals = func(</em>duals)<br />
  File "/home/guilhermeleobas/git/pytorch/a.py", line 16, in f<br />
    return a + 2 * b + 3 * y + 4 * z</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True<br />
```</p>
<h3>Versions</h3>
<p>main</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 07:09:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122197</guid>
    </item>
    <item>
      <title>[inductor] config to control whether we assume inputs are aligned</title>
      <link>https://github.com/pytorch/pytorch/pull/122158</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122159<br />
* <strong>-&gt;</strong> #122158</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D55079094">D55079094</a></p>]]></description>
      <pubDate>Mon, 18 Mar 2024 16:25:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122158</guid>
    </item>
    <item>
      <title>Inductor: Don't clamp views when the views come from split_with_sizes</title>
      <link>https://github.com/pytorch/pytorch/pull/122149</link>
      <description><![CDATA[<p>Summary:<br />
Fixes #122126</p>
<p><code>split_with_sizes</code> don't need clamping.</p>
<p>Test Plan: Added test + CI</p>
<p>Differential Revision: D55043320</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 18 Mar 2024 16:04:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122149</guid>
    </item>
    <item>
      <title>Error "module compiled against ABI version" when using a device on MacBook Pro M2 with MacOS Sonoma 14.4</title>
      <link>https://github.com/pytorch/pytorch/issues/122056</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<ul>
<li>I created a brand new virtual environment and Python script containing the following code.</li>
<li>I installed the beta / nightly version of Pytorch, using the <a href="https://developer.apple.com/metal/pytorch/">directions on Apple's website for MPS (M2) support</a>.</li>
</ul>
<p><code>pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu</code></p>
<p><code>python
import torch
mps = torch.device("mps")</code></p>
<p>When I run the code, I get this error:</p>
<p>```</p>
<p>A module that was compiled using NumPy 1.x cannot be run in<br />
NumPy 2.0.0b1 as it may crash. To support both 1.x and 2.x<br />
versions of NumPy, modules must be compiled against NumPy 2.0.</p>
<p>If you are a user of the module, the easiest solution will be to<br />
either downgrade NumPy or update the failing module (if available).</p>
<p>Traceback (most recent call last):  File "/Users/trevor.sullivan/git/pytorch-audio-classifier/main.py", line 7, in <module><br />
    mps = torch.device("cpu")<br />
/Users/trevor.sullivan/git/pytorch-audio-classifier/main.py:7: DeprecationWarning: numpy.core._multiarray_umath is deprecated and has been renamed to numpy._core._multiarray_umath. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core._multiarray_umath._ARRAY_API.<br />
  mps = torch.device("cpu")<br />
/Users/trevor.sullivan/git/pytorch-audio-classifier/main.py:7: UserWarning: Failed to initialize NumPy: module compiled against ABI version 0x1000009 but this version of numpy is 0x2000000 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)<br />
  mps = torch.device("cpu")<br />
```</p>
<p>How can I go about resolving this?</p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.4.0.dev20240317<br />
Is debug build: False<br />
CUDA used to build PyTorch: None<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: macOS 14.4 (arm64)<br />
GCC version: Could not collect<br />
Clang version: 15.0.0 (clang-1500.3.9.4)<br />
CMake version: Could not collect<br />
Libc version: N/A</p>
<p>Python version: 3.12.2 (main, Feb  6 2024, 20:19:44) [Clang 15.0.0 (clang-1500.1.0.2.5)] (64-bit runtime)<br />
Python platform: macOS-14.4-arm64-arm-64bit<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Apple M2 Pro</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==2.0.0b1<br />
[pip3] torch==2.4.0.dev20240317<br />
[pip3] torchaudio==2.2.0.dev20240317<br />
[pip3] torchvision==0.18.0.dev20240317<br />
[conda] Could not collect<br />
```</p>
<p>cc @seemethere @malfet @osalpekar @atalman @mruberry @rgommers @kulinseth @albanD @DenisVieriu97 @razarmehr</p>]]></description>
      <pubDate>Sun, 17 Mar 2024 14:11:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122056</guid>
    </item>
    <item>
      <title>[aot_inductor] added runtime checks for input/output tensors in debug compile mode</title>
      <link>https://github.com/pytorch/pytorch/pull/122047</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122047</p>
<p>This PR added runtime checks to guard the dtypes and shapes of input/output tensors.<br />
Currently, we enable these only for debug compilation<br />
(i.e. aot_inductor.debug_compile is True) in abi_compatible mode.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54993148">D54993148</a></p>]]></description>
      <pubDate>Sat, 16 Mar 2024 23:22:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122047</guid>
    </item>
    <item>
      <title>[inductor] Use python constants in IndexPropagation</title>
      <link>https://github.com/pytorch/pytorch/pull/122031</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122203<br />
* #115435<br />
* #121924<br />
* <strong>-&gt;</strong> #122031</p>
<p>In the next PR I have the IR <code>ops.neg(ops.constant(0.0, torch.float32))</code><br />
which should be folded to <code>ops.constant(-0.0, torch.float32)</code> but it seems that<br />
<code>sympy.Float(-0.0)</code> doesn't respect the sign of the zero and so we instead<br />
get a positive zero constant.</p>
<p>Here, I work around this by doing the constant folding with python arithmetic<br />
which does respect signed zeros.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sat, 16 Mar 2024 13:00:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122031</guid>
    </item>
    <item>
      <title>Precompile triton templates</title>
      <link>https://github.com/pytorch/pytorch/pull/121998</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120278<br />
* <strong>-&gt;</strong> #121998<br />
* #121997<br />
* #120275<br />
* #121996</p>
<p>Before this PR we were not precompiling triton templates in parallel. Compilation would occur during benchmarking.</p>
<p>Triton benchmarking templates were emitted as :</p>
<p><code>@triton.jit
def triton_mm(arg_A, arg_B, out_ptr0):</code></p>
<p>In order to precompile we need to give the full kernel specification, as we do when we emit the template in the final output code generation.</p>
<p><code>@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'backend_hash': 'cdeecfeccd31ad7810f96b5752194b1c2406d0a81e39a6ca09c8ee150baae183'},
)
@triton.jit
def triton_mm(arg_A, arg_B, out_ptr0):</code></p>]]></description>
      <pubDate>Fri, 15 Mar 2024 13:46:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121998</guid>
    </item>
    <item>
      <title>Precompile in background</title>
      <link>https://github.com/pytorch/pytorch/pull/121997</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120278<br />
* #121998<br />
* <strong>-&gt;</strong> #121997<br />
* #120275<br />
* #121996</p>
<p>Precompile benchmarking choices in parallel, and then wait on those choices prior to benchmarking. In the case of deferred templates, we only only wait only those choices in the scheduler to allow multiple separate lowerings to compile in parallel.</p>]]></description>
      <pubDate>Fri, 15 Mar 2024 13:46:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121997</guid>
    </item>
    <item>
      <title>Compiled model raises error "attn_bias is not correctly aligned" in pytorch 2.2</title>
      <link>https://github.com/pytorch/pytorch/issues/121943</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When running the following code, errors may occur in pytorch 2.2.0 or 2.2.1, but not in 2.1.0.</p>
<p>```</p>
<p>from einops import rearrange<br />
import torch,torch.nn as nn<br />
def rotate_half(x):<br />
    x = rearrange(x, '... (d r) -&gt; ... d r', r = 2)<br />
    x1, x2 = x.unbind(dim = -1)<br />
    x = torch.stack((-x2, x1), dim = -1)<br />
    return rearrange(x, '... d r -&gt; ... (d r)')</p>
<p>def random_masking_v4(mask_kernel, percent,loss_kernel, B, H, W, device='cpu', loss_weight_factor = 1.0):<br />
    """<br />
    Perform per-sample random masking by per-sample shuffling.<br />
    Per-sample shuffling is done by argsort random noise.<br />
    x: [N, L, D], sequence<br />
    """<br />
    k1, k2 = mask_kernel<br />
    pad = (loss_kernel -1) // 2<br />
    with torch.no_grad():<br />
        noise1 = torch.rand(B, 1, H + k1 - 1, W + k2 - 1, device=device) * 800<br />
        noise1 = torch.nn.functional.max_pool2d(noise1, kernel_size=(k1, k2), stride=1, padding=0, )<br />
        noise2 = torch.rand(B, 1, H + k2 - 1, W + k1 - 1, device=device) * 800<br />
        noise2 = torch.nn.functional.max_pool2d(noise2, kernel_size=(k2, k1), stride=1, padding=0, )</p>
<pre><code>    noise = (torch.maximum(noise1, noise2)).view(B, 1, H, W)
    noise = (torch.rand(B, 1, H, W, device=device) - noise).view(B, -1)

    ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove, shape:B,N
    ids_restore = torch.argsort(ids_shuffle, dim=1) # represents the order of each id
    ids_mask = ids_restore &lt; int(H*W*percent)

    rand_center = torch.cat([ids_shuffle[:, 0:1] // W, ids_shuffle[:, 0:1] % W], 1).unsqueeze(-1)

    cy, cx = torch.meshgrid(torch.arange(H, device=device),
                            torch.arange(W, device=device), indexing='ij')
    coords = torch.stack([cy, cx]).view(1, 2, H * W)
    distance = (((coords - rand_center + torch.rand(B, 2, 1, device=device)) ** 2).sum(1)) ** 0.5  + 1
    ids_order = (distance * 3).int() * ~ids_mask + -100 * ids_mask
    can_see_p1 = ids_order[:,:,None] &gt;= ids_order[:,None,:]
    attn_mask = can_see_p1.unsqueeze(1)

    patch_order = ids_order.view(B,1,H,W).float()
    loss_order = torch.nn.functional.unfold(patch_order,loss_kernel,dilation=1, padding=pad)

    if loss_kernel == 3:
        loss_weight = torch.as_tensor((2,1,2,1,1,1,2,1,2),dtype=torch.float32,device=device)
    elif loss_kernel == 5:
        loss_weight = torch.as_tensor(((8,5,2,5,8),(5,2,1,2,5),(2,1,1,1,2),
                                            (5,2,1,2,5),(8,5,2,5,8)), dtype=torch.float32, device=device)
    else:
        raise NotImplementedError

    loss_weight = 1.0 / loss_weight.view(1,-1,1) ** loss_weight_factor
    loss_mask = ((loss_order-1e-5) &gt; patch_order.view(B,1,H*W)).float()

return torch.where(attn_mask,0,-9999.0), loss_mask * loss_weight
</code></pre>
<p>class Attention(nn.Module):</p>
<pre><code>def __init__(
        self,
        dim,
        num_heads=8,
        qkv_bias=True,
        qk_norm=False,
        attn_drop=0.,
        proj_drop=0.,
        norm_layer=nn.LayerNorm,
):
    super().__init__()
    assert dim % num_heads == 0, 'dim should be divisible by num_heads'
    self.num_heads = num_heads
    self.head_dim = dim // num_heads
    self.scale = self.head_dim ** -0.5
    self.fused_attn = True

    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
    self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
    self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
    self.attn_drop = nn.Dropout(attn_drop)
    self.proj = nn.Linear(dim, dim)
    self.proj_drop = nn.Dropout(proj_drop)

#@torch.compile
def forward(self, x,mask=None):
    B, N, C = x.shape
    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
    q, k, v = qkv.unbind(0)

    q, k = self.q_norm(q), self.k_norm(k)

    if self.fused_attn:
        x = torch.nn.functional.scaled_dot_product_attention(
            q, k, v,attn_mask=mask,
            dropout_p=self.attn_drop.p,
        )
    else:
        q = q * self.scale
        attn = q @ k.transpose(-2, -1)
        attn = attn + mask
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        x = attn @ v

    x = x.transpose(1, 2).reshape(B, N, C)
    x = self.proj(x)
    x = self.proj_drop(x)
    return x
</code></pre>
<p>class CustomModel(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.attn = Attention(768,12,True)</p>
<pre><code>def forward(self,x):
    mask,_ = random_masking_v4((2,5),0.15,3,x.shape[0],
                               int(x.shape[1]**0.5),int(x.shape[1]**0.5),device=x.device,
                               )
    return self.attn(x,mask)
</code></pre>
<p>model = CustomModel().cuda()<br />
model_without_ddp = model<br />
x =torch.zeros(256,196,768).cuda()</p>
<p>optimizer = torch.optim.AdamW(model_without_ddp.parameters())<br />
model = torch.compile(model)</p>
<p>with torch.cuda.amp.autocast():<br />
    out = model(x)<br />
loss = out.sum()<br />
loss.backward()<br />
optimizer.step()<br />
optimizer.zero_grad(True)</p>
<p>with torch.cuda.amp.autocast():<br />
    out = model(x)<br />
loss = out.sum()<br />
loss.backward()<br />
optimizer.step()<br />
optimizer.zero_grad(True)</p>
<p>```</p>
<p>error messages:<br />
```</p>
<p>File "//test.py", line 130, in <module><br />
    out = model(x)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "//test.py", line 116, in forward<br />
    def forward(self,x):<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 901, in forward<br />
    return compiled_fn(full_args)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 81, in g<br />
    return f(</em>args)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 83, in runtime_wrapper<br />
    all_outs = call_func_at_runtime_with_args(<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 105, in call_func_at_runtime_with_args<br />
    out = normalize_as_list(f(args))<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 81, in g<br />
    return f(<em>args)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/autograd/function.py", line 553, in apply<br />
    return super().apply(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 408, in forward<br />
    fw_outs = call_func_at_runtime_with_args(<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 105, in call_func_at_runtime_with_args<br />
    out = normalize_as_list(f(args))<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 864, in <strong>call</strong><br />
    return self.get_current_callable()(inputs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 611, in run<br />
    return model(new_inputs)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 892, in _run_from_cache<br />
    return compiled_graph.compiled_artifact(inputs)<br />
  File "/tmp/torchinductor_root/hz/chzdqrbr5gisewqim47noe7zsijncibkeouw2ryw2no4ecybmvnj.py", line 641, in call<br />
    buf21 = aten._scaled_dot_product_efficient_attention(reinterpret_tensor(buf18, (256, 12, 196, 64), (451584, 64, 2304, 1), 0), reinterpret_tensor(buf18, (256, 12, 196, 64), (451584, 64, 2304, 1), 768), reinterpret_tensor(buf18, (256, 12, 196, 64), (451584, 64, 2304, 1), 1536), buf20, True)<br />
  File "/usr/local/lib/python3.10/site-packages/torch/_ops.py", line 755, in <strong>call</strong><br />
    return self._op(*args, </strong>(kwargs or {}))<br />
RuntimeError: attn_bias is not correctly aligned (strideM). attn_bias.stride(2) = 196, and should be a multiple of 8.</p>
<p>```</p>
<h3>Versions</h3>
<p>/usr/local/lib/python3.10/runpy.py:126: RuntimeWarning: 'torch.utils.collect_env' found in sys.modules after import of package 'torch.utils', but prior to execution of 'torch.utils.collect_env'; this may result in unpredictable behaviour<br />
  warn(RuntimeWarning(msg))<br />
Collecting environment information...<br />
PyTorch version: 2.2.0+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.16.3<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.10.13 (main, Dec 19 2023, 08:15:18) [GCC 9.4.0] (64-bit runtime)<br />
Python platform: Linux-4.15.0-189-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: Quadro RTX 6000<br />
GPU 1: Quadro RTX 6000<br />
GPU 2: Quadro RTX 6000<br />
GPU 3: Quadro RTX 6000<br />
GPU 4: Quadro RTX 6000<br />
GPU 5: Quadro RTX 6000<br />
GPU 6: Quadro RTX 6000<br />
GPU 7: Quadro RTX 6000</p>
<p>Nvidia driver version: 535.54.03<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
CPU(s):                          64<br />
On-line CPU(s) list:             0-63<br />
Thread(s) per core:              2<br />
Core(s) per socket:              16<br />
Socket(s):                       2<br />
NUMA node(s):                    2<br />
Vendor ID:                       GenuineIntel<br />
CPU family:                      6<br />
Model:                           85<br />
Model name:                      Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz<br />
Stepping:                        7<br />
CPU MHz:                         2651.207<br />
CPU max MHz:                     3900.0000<br />
CPU min MHz:                     1000.0000<br />
BogoMIPS:                        4600.00<br />
Virtualization:                  VT-x<br />
L1d cache:                       1 MiB<br />
L1i cache:                       1 MiB<br />
L2 cache:                        32 MiB<br />
L3 cache:                        44 MiB<br />
NUMA node0 CPU(s):               0-15,32-47<br />
NUMA node1 CPU(s):               16-31,48-63<br />
Vulnerability Itlb multihit:     KVM: Mitigation: Split huge pages<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Mitigation; TSX disabled<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.3<br />
[pip3] torch==2.2.0+cu118<br />
[pip3] torchlaunch==1.0<br />
[pip3] torchvision==0.17.0+cu118<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 18:01:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121943</guid>
    </item>
    <item>
      <title>[inductor] Lower divide by constant as multiplication by reciprocal</title>
      <link>https://github.com/pytorch/pytorch/pull/121924</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122203<br />
* #115435<br />
* <strong>-&gt;</strong> #121924<br />
* #122031</p>
<p>Fixes #101039</p>
<p>This lowers division by a constant value to be multipication by reciprocal.<br />
The same optimization is applied in eager mode on CUDA:</p>
<p>https://github.com/pytorch/pytorch/blob/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu#L36-L38</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 11:41:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121924</guid>
    </item>
    <item>
      <title>[inductor][cpu]llama_v2_7b_16h AMP dynamic shape single thread performance test will be killed </title>
      <link>https://github.com/pytorch/pytorch/issues/121887</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c<br />
/workspace/pytorch# bash inductor_single_run.sh single inference performance torchbench llama_v2_7b_16h amp first dynamic<br />
Testing with dynamic shapes.<br />
Testing with freezing on.<br />
single-thread testing....<br />
/opt/conda/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.<br />
  _torch_pytree._register_pytree_node(<br />
loading model: 0it [00:40, ?it/s]<br />
cpu  eval  llama_v2_7b_16h<br />
skipping cudagraphs due to skipping cudagraphs due to cpu device. Found from :<br />
   File "benchmarks/dynamo/torchbench.py", line 354, in forward_pass<br />
    return mod(<em>inputs)<br />
  File "/workspace/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1174, in forward<br />
    outputs = self.model(<br />
  File "/workspace/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1011, in forward<br />
    inputs_embeds = self.embed_tokens(input_ids)</p>
<p>running benchmark:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 39/50 [08:13&lt;02:21, 12.89s/it]inductor_single_run.sh: line 64: 142610 Killed                  numactl -C 0-0 --membind=0 python benchmarks/dynamo/${SUITE}.py --${SCENARIO} --${DT} -dcpu -n50 --no-skip --dashboard --batch-size 1 --threads 1 --only "${MODEL}" ${Channels_extra} ${Shape_extra} ${Mode_extra} ${Flag_extra} --timeout 9000 --backend=${BACKEND} --output=/tmp/inductor_single_test_st.csv</p>
<p>https://github.com/pytorch/pytorch/commit/26740f853e10e34a7e8d2b72b950875796350418<br />
/workspace/pytorch# bash inductor_single_run.sh single inference performance torchbench llama_v2_7b_16h amp first dynamic<br />
Testing with dynamic shapes.<br />
Testing with freezing on.<br />
single-thread testing....<br />
/opt/conda/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.<br />
  _torch_pytree._register_pytree_node(<br />
loading model: 0it [00:40, ?it/s]<br />
cpu  eval  llama_v2_7b_16h<br />
skipping cudagraphs due to skipping cudagraphs due to cpu device. Found from :<br />
   File "benchmarks/dynamo/torchbench.py", line 354, in forward_pass<br />
    return mod(<em>inputs)<br />
  File "/workspace/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1174, in forward<br />
    outputs = self.model(<br />
  File "/workspace/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1011, in forward<br />
    inputs_embeds = self.embed_tokens(input_ids)</p>
<p>running benchmark: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [09:52&lt;00:00, 11.85s/it]<br />
1.529x<br />
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles<br />
cpu,llama_v2_7b_16h,1,1.529177,4678.804700,4.166590,0.871345,4347.434189,4989.334733,959,1,0,0,0,0</p>
<h3>Versions</h3>
<p></table><p>SW info</p><table border="1" class="dataframe table"><br />
<br />
<thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead></p>
<tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>1ef0a39e</td>
      <td>main</td>
      <td>ff42d907</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>41286f1505ffb214d386d72e4b72ebd680a4a475</td>
      <td>main</td>
      <td>581fe26792849a80f04feaa7f8bdec69b7f41dd8</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>

</table>
<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh single inference performance torchbench llama_v2_7b_16h amp first dynamic<br />
<a href="https://github.com/pytorch/pytorch/files/14597204/torchbench-llama_v2_7b_16h-inference-amp-dynamic-default-single-performance-crash_guilty_commit.log">torchbench-llama_v2_7b_16h-inference-amp-dynamic-default-single-performance-crash_guilty_commit.log</a><br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c<br />
cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 21:57:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121887</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Register general runtime device for Intel GPU</title>
      <link>https://github.com/pytorch/pytorch/pull/121883</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121895<br />
* <strong>-&gt;</strong> #121883</p>
<p>interface for Intel GPU.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 18:20:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121883</guid>
    </item>
    <item>
      <title>[inductor][cpu]lennard_jones, pyhpc_equation_of_state and pyhpc_isoneutral_mixing performance regression </title>
      <link>https://github.com/pytorch/pytorch/issues/121882</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>fp32 static shape default wrapper regression in 2024-03-10</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>lennard_jones</td>
      <td>single</td>
      <td>1</td>
      <td>1.442252</td>
      <td>4.8045e-05</td>
      <td>6.929299734e-05</td>
      <td>5.983722</td>
      <td>1.0</td>
      <td>1.833953</td>
      <td>3.8604e-05</td>
      <td>7.079792161199999e-05</td>
      <td>4.163693</td>
      <td>0.79</td>
      <td>1.02</td>
      <td>0.8</td>
      <td>0.7</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>pyhpc_equation_of_state</td>
      <td>single</td>
      <td>1</td>
      <td>18.71236</td>
      <td>5.3968e-05</td>
      <td>0.00100986864448</td>
      <td>8.857691</td>
      <td>1.0</td>
      <td>23.188102</td>
      <td>4.51e-05</td>
      <td>0.0010457834002</td>
      <td>6.913662</td>
      <td>0.81</td>
      <td>1.04</td>
      <td>0.84</td>
      <td>0.78</td>
    </tr>
  </tbody>

</table>
<p>fp32 dynamic shape default wrapper regression in 2024-03-10</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>lennard_jones</td>
      <td>single</td>
      <td>1</td>
      <td>1.431658</td>
      <td>4.8693e-05</td>
      <td>6.9711722994e-05</td>
      <td>6.040358</td>
      <td>1.0</td>
      <td>1.775623</td>
      <td>3.9242e-05</td>
      <td>6.9678997766e-05</td>
      <td>4.229882</td>
      <td>0.81</td>
      <td>1.0</td>
      <td>0.81</td>
      <td>0.7</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>pyhpc_equation_of_state</td>
      <td>single</td>
      <td>1</td>
      <td>18.8031</td>
      <td>5.4025e-05</td>
      <td>0.0010158374775</td>
      <td>8.885931</td>
      <td>1.0</td>
      <td>23.240634</td>
      <td>4.4543e-05</td>
      <td>0.001035207560262</td>
      <td>6.99299</td>
      <td>0.81</td>
      <td>1.02</td>
      <td>0.82</td>
      <td>0.79</td>
    </tr>
  </tbody>

</table>
<p>fp32 static shape cpp wrapper regression in 2024-03-10</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>lennard_jones</td>
      <td>single</td>
      <td>1</td>
      <td>1.810503</td>
      <td>3.8294e-05</td>
      <td>6.9331401882e-05</td>
      <td>14.183242</td>
      <td>1.0</td>
      <td>2.379847</td>
      <td>2.8666e-05</td>
      <td>6.822069410199999e-05</td>
      <td>12.376875</td>
      <td>0.76</td>
      <td>0.98</td>
      <td>0.75</td>
      <td>0.87</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>pyhpc_equation_of_state</td>
      <td>single</td>
      <td>1</td>
      <td>23.197532</td>
      <td>4.4514e-05</td>
      <td>0.001032614939448</td>
      <td>17.09731</td>
      <td>1.0</td>
      <td>29.022945</td>
      <td>3.5698e-05</td>
      <td>0.00103606109061</td>
      <td>15.172577</td>
      <td>0.8</td>
      <td>1.0</td>
      <td>0.8</td>
      <td>0.89</td>
    </tr>
  </tbody>

</table>
<p>fp32 dynamic shape cpp wrapper regression in 2024-03-10</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>lennard_jones</td>
      <td>single</td>
      <td>1</td>
      <td>1.76331</td>
      <td>3.8149000000000004e-05</td>
      <td>6.726851319e-05</td>
      <td>14.189571</td>
      <td>1.0</td>
      <td>2.273742</td>
      <td>2.8757000000000003e-05</td>
      <td>6.5385998694e-05</td>
      <td>12.387977</td>
      <td>0.78</td>
      <td>0.97</td>
      <td>0.75</td>
      <td>0.87</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>pyhpc_equation_of_state</td>
      <td>single</td>
      <td>1</td>
      <td>22.763</td>
      <td>4.464e-05</td>
      <td>0.00101614032</td>
      <td>17.139039</td>
      <td>1.0</td>
      <td>31.649198</td>
      <td>3.4811e-05</td>
      <td>0.001101740231578</td>
      <td>15.198502</td>
      <td>0.72</td>
      <td>1.08</td>
      <td>0.78</td>
      <td>0.89</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>pyhpc_isoneutral_mixing</td>
      <td>single</td>
      <td>1</td>
      <td>47.525991</td>
      <td>5.6787e-05</td>
      <td>0.0026988584509169996</td>
      <td>20.419413</td>
      <td>1.0</td>
      <td>54.58171</td>
      <td>4.9508e-05</td>
      <td>0.0027022312986800003</td>
      <td>18.502454</td>
      <td>0.87</td>
      <td>1.0</td>
      <td>0.87</td>
      <td>0.91</td>
    </tr>
  </tbody>


</table>
<p>SW info</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>1ef0a39e</td>
      <td>main</td>
      <td>ff42d907</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>41286f1505ffb214d386d72e4b72ebd680a4a475</td>
      <td>main</td>
      <td>581fe26792849a80f04feaa7f8bdec69b7f41dd8</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>
</table>

<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh <strong>thread</strong> inference performance torchbench <strong>model</strong> float32/amp first static/dynamic default/cpp<br />
<a href="https://github.com/pytorch/pytorch/files/14595771/torchbench-lennard_jones-inference-float32-static-default-single-performance-drop_guilty_commit.log">torchbench-lennard_jones-inference-float32-static-default-single-performance-drop_guilty_commit.log</a><br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/a7e93c341fe0f51c2affca106053f4bc452fbfd2<br />
cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 18:03:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121882</guid>
    </item>
    <item>
      <title>AOT Inductor / Support dict[string][tensor] as the argument of the C++ AOTIModelContainerRunnerCpu::run functions (same for cuda)</title>
      <link>https://github.com/pytorch/pytorch/issues/121785</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>We use this interface for our inference methods, I think it's a classic and flexible one but I'm not too versed here ...<br />
I haven't read the python code but I wonder what it would take to support that, if it's a bad idea (beside more API surface, does it make dynamic batch size computation harder, etc...).</p>
<p>Apparently torch.compile does supports this, so some parts must be working.</p>
<h3>Alternatives</h3>
<p>We convert our dict to vectors but this require to change models, and make it harder to adopt AOT.</p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4 @desertfire @chenyang78</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 19:10:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121785</guid>
    </item>
    <item>
      <title>DISABLED test_pynode_destruction_deadlock (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/121576</link>
      <description><![CDATA[<p>Platforms: asan</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22inductor%2Ftest_compiled_autograd.py%3A%3ATestAutogradWithCompiledAutograd%3A%3Atest_pynode_destruction_deadlock%22%5D">recent examples</a>).</p>
<p>This test starts to become flaky in trunk today (March 8th) but the root cause is unclear yet.</p>
<p>cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7 @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Sat, 09 Mar 2024 01:57:47 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121576</guid>
    </item>
    <item>
      <title>[torch.compile] `index_select` out of bound read</title>
      <link>https://github.com/pytorch/pytorch/issues/121251</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When using <code>torch.compile</code>, <code>index_select</code> will out-of-bound read</p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(42)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()

def forward(self, x1):
    v = torch.index_select(x1, 0, torch.tensor([2]))
    return v
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(2, 2, 2, 2)</p>
<p>with torch.no_grad():<br />
    func1 = torch.compile(func)<br />
    print(func1(x.clone()))<br />
    # tensor([[[[1.0743e+23, 3.0618e-41],<br />
    #       [9.1084e-44, 0.0000e+00]],</p>
<pre><code>#      [[3.2230e-44, 0.0000e+00],
#       [3.2230e-44, 0.0000e+00]]]])

print(func(x.clone()))
# IndexError: index out of range in self
</code></pre>
<p>```</p>
<h3>Versions</h3>
<p>```<br />
Versions<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 10:35:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121251</guid>
    </item>
    <item>
      <title>[inductor] Remove identity from ops.scan</title>
      <link>https://github.com/pytorch/pytorch/pull/119727</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122137<br />
* #119430<br />
* <strong>-&gt;</strong> #119727<br />
* #122136</p>
<p>Currently scan has an <code>init</code> argument which must be the identity of the<br />
combine function. This isn't strictly necessary if we are more careful about<br />
keeping track of the first element and avoid combining it with anything.</p>
<p>This does additionally require that there are no active load masks, since we can't<br />
do the <code>where_cond</code> any more. However, this shouldn't be possible anyway since<br />
scans are always realized and only fused via the scheduler.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Mon, 12 Feb 2024 15:27:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119727</guid>
    </item>
    <item>
      <title>torch.compile accuracy issue when graph outputs are DTensors with _Partial placements</title>
      <link>https://github.com/pytorch/pytorch/issues/119485</link>
      <description><![CDATA[<p>This is a placeholder for an internal issue, no repro yet.</p>
<p>(1) When torch.compile'ing a model, some of inputs to some of the graphs are DTensors with the Partial placement, and the net result is bad accuracy during training</p>
<p>(2) When manually coercing these DTensors to the Replicate() placement when they are the output of a previous graph, this appears to fix accuracy issues.</p>
<p>I'm hoping I can try to create a self-contained repro of this at some point.</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang @msaroufim @albanD @anijain2305</p>]]></description>
      <pubDate>Thu, 08 Feb 2024 13:15:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/119485</guid>
    </item>
    <item>
      <title>different behaviors in cumsum between torch.compile mode and eager mode</title>
      <link>https://github.com/pytorch/pytorch/issues/117623</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>It's about function <code>torch.cumsum</code>, leading to different behaviours between torch.compile mode and eager mode. However, there is no mistake when <code>out</code> is set to default. I believe the error occurred due to <code>out</code> being set as a tensor of a smaller type than <code>input</code>, causing issues during the automatic conversion.<br />
```python<br />
import torch<br />
import torch.nn as nn</p>
<p>class Model(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
    def forward(self, out, input):<br />
        out = torch.cumsum(out=out, dim=0,input=input)      <br />
        return out</p>
<p>x = torch.randint(-32768, 32767, [2, 8, 8, 2, 1, 3, 4, 8], dtype=torch.int16)<br />
y = torch.rand([2, 8, 8, 2, 1, 3, 4, 8], dtype=torch.float32)<br />
z = x.clone()</p>
<p>model = Model().to(torch.device('cpu'))<br />
eag = model(x, y)<br />
opt = torch.compile(model.forward, mode='reduce-overhead')(z, y)</p>
<p>same_val = torch.allclose(eag.to('cpu'), <br />
                            opt.to('cpu'), <br />
                            rtol=1e-3, atol=1e-3, <br />
                            equal_nan=True)<br />
if same_val == False : <br />
        raise ValueError('diff value')<br />
<code>error stacktrace</code><br />
raise ValueError 'diff value'<br />
```</p>
<h3>Versions</h3>
<p>PyTorch version: 2.3.0.dev20240106+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.17 (main, Jul 5 2023, 20:41:20) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.1.105<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration:<br />
GPU 0: NVIDIA GeForce RTX 2070<br />
GPU 1: NVIDIA GeForce RTX 2070<br />
GPU 2: NVIDIA GeForce RTX 2070<br />
GPU 3: NVIDIA GeForce RTX 2070</p>
<p>Nvidia driver version: 535.104.12<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 32<br />
On-line CPU(s) list: 0-31<br />
Vendor ID: GenuineIntel<br />
Model name: Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz<br />
CPU family: 6<br />
Model: 63<br />
Thread(s) per core: 2<br />
Core(s) per socket: 8<br />
Socket(s): 2<br />
Stepping: 2<br />
CPU max MHz: 3200.0000<br />
CPU min MHz: 1200.0000<br />
BogoMIPS: 4794.64<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear flush_l1d<br />
Virtualization: VT-x<br />
L1d cache: 512 KiB (16 instances)<br />
L1i cache: 512 KiB (16 instances)<br />
L2 cache: 4 MiB (16 instances)<br />
L3 cache: 40 MiB (2 instances)<br />
NUMA node(s): 2<br />
NUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30<br />
NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf: Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable<br />
Vulnerability Mds: Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Meltdown: Mitigation; PTI<br />
Vulnerability Mmio stale data: Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.3<br />
[pip3] pytorch-triton==2.2.0+e28a256d71<br />
[pip3] torch==2.3.0.dev20240106+cu118<br />
[pip3] torchaudio==2.2.0.dev20240106+cu118<br />
[pip3] torchvision==0.18.0.dev20240106+cu118<br />
[conda] cudatoolkit 11.8.0 h6a678d5_0 defaults<br />
[conda] numpy 1.24.3 pypi_0 pypi<br />
[conda] pytorch-triton 2.2.0+e28a256d71 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240106+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240106+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240106+cu118 pypi_0 pypi</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Wed, 17 Jan 2024 02:06:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/117623</guid>
    </item>
    <item>
      <title>torch.compile silently incorrect with full_backward_pre_hook</title>
      <link>https://github.com/pytorch/pytorch/issues/117265</link>
      <description><![CDATA[<p>```python<br />
import torch<br />
import torch.nn as nn</p>
<p>layer = nn.Linear(3, 3, bias=False)<br />
layer.weight = nn.Parameter(torch.eye(3, dtype=torch.float))</p>
<p>def hook(module, grad_output):<br />
    return tuple(go * 2 for go in grad_output)</p>
<p>@torch.compile(backend="aot_eager", fullgraph=True)<br />
def f(x):<br />
    y = layer(x)<br />
    return y</p>
<p>x = torch.randn(3, requires_grad=True)<br />
f(x)<br />
layer.register_full_backward_pre_hook(hook)</p>
<p>x = torch.randn(3, requires_grad=True)<br />
y = f(x)<br />
y.sum().backward()<br />
print(x.grad)<br />
```<br />
Gives different results with or without torch.compile.</p>
<p>cc @ezyang @gchanan @kadeng @msaroufim @wconstab @bdhirsh @anijain2305 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov</p>]]></description>
      <pubDate>Thu, 11 Jan 2024 07:21:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/117265</guid>
    </item>
    <item>
      <title>WIP: [inductor] Force IEEE-compliant division in triton</title>
      <link>https://github.com/pytorch/pytorch/pull/115435</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122203<br />
* <strong>-&gt;</strong> #115435<br />
* #121924<br />
* #122031</p>
<p>Fixes #101039</p>
<p>In the issue we have something like:<br />
<code>python
(x / 1e-6) + (y / 1e-6)</code><br />
which triton turns into<br />
<code>python
fma(x, 1e6, y * 1e6)</code></p>
<p>Where we have one division calculated to float32 precision,<br />
and the other calculated to infinite precision as part of the <code>fma</code>.<br />
In the issue this leads to a bad normalization in softmax.</p>
<p>Here we explicity use the <code>div_rn</code> which forces IEEE compliant rounding.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Fri, 08 Dec 2023 10:25:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/115435</guid>
    </item>
    <item>
      <title>Compile pytorch for ppc64 redhat8</title>
      <link>https://github.com/pytorch/pytorch/issues/114298</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I am making process for compile pytorch in ppc64le.. but magma-cuda mkl an d mkl-include no are in Conda repos.. and same triton</p>
<p>```<br />
Install Dependencies<br />
Common</p>
<p>conda install cmake ninja</p>
<h1>Run this command from the PyTorch directory after cloning the source code using the ‚ÄúGet the PyTorch Source‚Äú section below</h1>
<p>pip install -r requirements.txt<br />
On Linux</p>
<p>conda install mkl mkl-include</p>
<h1>CUDA only: Add LAPACK support for the GPU if needed</h1>
<p>conda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo</p>
<h1>(optional) If using torch.compile with inductor/triton, install the matching version of triton</h1>
<h1>Run from the pytorch directory after cloning</h1>
<p>make triton<br />
On MacOS</p>
<h1>Add this package on intel x86 processor machines only</h1>
<p>conda install mkl mkl-include</p>
<h1>Add these packages if torch.distributed is needed</h1>
<p>conda install pkg-config libuv<br />
On Windows</p>
<p>conda install mkl mkl-include</p>
<h1>Add these packages if torch.distributed is needed.</h1>
<h1>Distributed package support on Windows is a prototype feature and is subject to changes.</h1>
<p>conda install -c conda-forge libuv=1.39<br />
Get the PyTorch Source<br />
git clone --recursive https://github.com/pytorch/pytorch<br />
cd pytorch</p>
<h1>if you are updating an existing checkout</h1>
<p>git submodule sync<br />
git submodule update --init --recursive<br />
Install PyTorch<br />
On Linux</p>
<p>If you would like to compile PyTorch with <a href="https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html">new C++ ABI</a> enabled, then first run this command:</p>
<p>export _GLIBCXX_USE_CXX11_ABI=1<br />
If you're compiling for AMD ROCm then first run this command:</p>
<h1>Only run this if you're compiling for ROCm</h1>
<p>python tools/amd_build/build_amd.py<br />
Install PyTorch</p>
<p>export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}<br />
python setup.py develop</p>
<p>```</p>
<h3>Versions</h3>
<p>v2.1.0 pytorch</p>
<p>cc @malfet @seemethere</p>]]></description>
      <pubDate>Tue, 21 Nov 2023 11:51:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/114298</guid>
    </item>
  </channel>
</rss>
