<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[inductor][cpu]GPT2ForSequenceClassification AMP static/dynamic shape default/cpp wrapper single thread accuracy crash</title>
      <link>https://github.com/pytorch/pytorch/issues/123503</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>```<br />
loading model: 0it [00:02, ?it/s]cpu  eval  GPT2ForSequenceClassification      </p>
<p>E0402 16:13:08.429653 134289029980032 torch/_dynamo/utils.py:1405] RMSE (res-fp64): 0.00568, (ref-fp64): 0.00100 and shape=torch.Size([1, 2]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.004000<br />
E0402 16:13:08.429873 134289029980032 torch/_dynamo/utils.py:1319] Accuracy failed for key name logits<br />
fail_accuracy<br />
```</p>
<h3>Versions</h3>
<p></table><p>SW info</p><table border="1" class="dataframe table"><br />
<br />
<thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead></p>
<tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>d6015d42</td>
      <td>main</td>
      <td>d6015d42</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>781e8d2201c1e2aaeccbbc7b7b13f9322b481bc9</td>
      <td>main</td>
      <td>f0d461beacded34abe196c72ec4bcdb55bf01793</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.19.0a0+2c4665f</td>
      <td>main</td>
      <td>0.19.0a0+a0c79b3</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+ea437b3</td>
      <td>main</td>
      <td>2.2.0a0+17a7081</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>
</table>
</table>
<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh single inference accuracy huggingface GPT2ForSequenceClassification amp first static/dynamic default/cpp<br />
<a href="https://github.com/pytorch/pytorch/files/14893899/huggingface-GPT2ForSequenceClassification-inference-amp-static-cpp-single-accuracy-crash_guilty_commit.log">huggingface-GPT2ForSequenceClassification-inference-amp-static-cpp-single-accuracy-crash_guilty_commit.log</a><br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/5152945441b2a1387f6e9fc8fe1531bfeb6cabe4<br />
cc @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Sat, 06 Apr 2024 06:45:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123503</guid>
    </item>
    <item>
      <title>[inductor][cpu]basic_gnn_gcn AMP static/dynamic shape default/cpp wrapper single thread performance regression</title>
      <link>https://github.com/pytorch/pytorch/issues/123502</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>AMP static shape default wrapper</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>basic_gnn_gcn</td>
      <td>single</td>
      <td>1</td>
      <td>2.840742</td>
      <td>0.02862003</td>
      <td>0.08130212126226001</td>
      <td>8.595758</td>
      <td>1</td>
      <td>3.233373</td>
      <td>0.025129724</td>
      <td>0.08125377107905199</td>
      <td>8.340146</td>
      <td>0.88</td>
      <td>1.0</td>
      <td>0.88</td>
      <td>0.97</td>
    </tr>
  </tbody>

</table>

<p>AMP dynamic shape default wrapper</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>basic_gnn_gcn</td>
      <td>single</td>
      <td>1</td>
      <td>2.856395</td>
      <td>0.028604007</td>
      <td>0.081704342574765</td>
      <td>8.626275</td>
      <td>1</td>
      <td>3.252178</td>
      <td>0.025021263999999998</td>
      <td>0.08137360431299198</td>
      <td>8.333838</td>
      <td>0.88</td>
      <td>1.0</td>
      <td>0.87</td>
      <td>0.97</td>
    </tr>
  </tbody>

</table>

<p>AMP dynamic shape CPP wrapper</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>basic_gnn_gcn</td>
      <td>single</td>
      <td>1</td>
      <td>2.854889</td>
      <td>0.028386648</td>
      <td>0.081040729122072</td>
      <td>50.862871</td>
      <td>1</td>
      <td>3.263137</td>
      <td>0.024769257</td>
      <td>0.080825478979209</td>
      <td>50.52748</td>
      <td>0.87</td>
      <td>1.0</td>
      <td>0.87</td>
      <td>0.99</td>
    </tr>
  </tbody>

</table>
<p>SW info</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>d6015d42</td>
      <td>main</td>
      <td>d6015d42</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>35c493f2cf9b623bfdc7e6b34dc1cb39690a7919</td>
      <td>main</td>
      <td>f0d461beacded34abe196c72ec4bcdb55bf01793</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.19.0a0+2c4665f</td>
      <td>main</td>
      <td>0.19.0a0+a0c79b3</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+ea437b3</td>
      <td>main</td>
      <td>2.2.0a0+17a7081</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>

</table>

<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh single inference performance torchbench basic_gnn_gcn amp first static/dynamic default/cpp<br />
<a href="https://github.com/pytorch/pytorch/files/14893819/torchbench-basic_gnn_gcn-inference-amp-static-cpp-single-performance-drop_guilty_commit.log">torchbench-basic_gnn_gcn-inference-amp-static-cpp-single-performance-drop_guilty_commit.log</a><br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/49121603ab2d9070b1b6182a90f01a15afe5b6fe<br />
cc @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Sat, 06 Apr 2024 06:13:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123502</guid>
    </item>
    <item>
      <title>[inductor][cpu] FP32/AMP Static/Dynamic shape default/CPP wrapper models regression in 2024-04-01 nightly release</title>
      <link>https://github.com/pytorch/pytorch/issues/123501</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>FP32 static shape default wrapper</p>
<p></p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>cait_m36_384</td>
      <td>multiple</td>
      <td>4</td>
      <td>1.140354</td>
      <td>1.086999586</td>
      <td>1.239564325893444</td>
      <td>64.791505</td>
      <td>1</td>
      <td>0.441219</td>
      <td>0.7911862190000001</td>
      <td>0.349086392360961</td>
      <td>56.837279</td>
      <td>2.58</td>
      <td>0.28</td>
      <td>0.73</td>
      <td>0.88</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>crossvit_9_240</td>
      <td>multiple</td>
      <td>256</td>
      <td>1.094877</td>
      <td>0.469700884</td>
      <td>0.514264694771268</td>
      <td>63.755352</td>
      <td>128</td>
      <td>1.099781</td>
      <td>0.229482125</td>
      <td>0.252380080914625</td>
      <td>60.156846</td>
      <td>1.0</td>
      <td>0.49</td>
      <td>0.49</td>
      <td>0.94</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>dpn107</td>
      <td>multiple</td>
      <td>64</td>
      <td>1.499244</td>
      <td>0.9116250810000001</td>
      <td>1.366748432938764</td>
      <td>79.184748</td>
      <td>32</td>
      <td>1.447863</td>
      <td>0.466092112</td>
      <td>0.6748375235566559</td>
      <td>73.004516</td>
      <td>1.04</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>0.92</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ese_vovnet19b_dw</td>
      <td>multiple</td>
      <td>256</td>
      <td>2.623067</td>
      <td>0.547788526</td>
      <td>1.436886005529242</td>
      <td>30.177733</td>
      <td>128</td>
      <td>2.564612</td>
      <td>0.276665743</td>
      <td>0.709540284486716</td>
      <td>27.396263</td>
      <td>1.02</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>0.91</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetc_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.908041</td>
      <td>0.304632115</td>
      <td>1.1905147953367148</td>
      <td>10.581803</td>
      <td>128</td>
      <td>3.522162</td>
      <td>0.074850656</td>
      <td>0.263636136238272</td>
      <td>11.104427</td>
      <td>1.11</td>
      <td>0.22</td>
      <td>0.25</td>
      <td>1.05</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetv3_b</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.402622</td>
      <td>0.244504909</td>
      <td>0.831957782471398</td>
      <td>32.215416</td>
      <td>128</td>
      <td>3.359477</td>
      <td>0.11345530100000001</td>
      <td>0.38115047423757703</td>
      <td>29.729025</td>
      <td>1.01</td>
      <td>0.46</td>
      <td>0.46</td>
      <td>0.92</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ghostnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>2.323921</td>
      <td>0.35814755400000003</td>
      <td>0.8323066218392341</td>
      <td>65.301836</td>
      <td>128</td>
      <td>2.349767</td>
      <td>0.080207288</td>
      <td>0.18846843850189599</td>
      <td>56.086852</td>
      <td>0.99</td>
      <td>0.23</td>
      <td>0.22</td>
      <td>0.86</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>multiple</td>
      <td>256</td>
      <td>2.330024</td>
      <td>0.790078336</td>
      <td>1.840901484760064</td>
      <td>42.833512</td>
      <td>128</td>
      <td>2.22785</td>
      <td>0.40335163799999996</td>
      <td>0.8986069467183</td>
      <td>40.063013</td>
      <td>1.05</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>0.94</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>lcnet_050</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.854873</td>
      <td>0.026713650999999998</td>
      <td>0.102977731971323</td>
      <td>15.180312</td>
      <td>128</td>
      <td>3.447</td>
      <td>0.013115617999999999</td>
      <td>0.045209535246</td>
      <td>14.673959</td>
      <td>1.12</td>
      <td>0.44</td>
      <td>0.49</td>
      <td>0.97</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>levit_128</td>
      <td>multiple</td>
      <td>1024</td>
      <td>1.471971</td>
      <td>0.560375403</td>
      <td>0.8248563423293129</td>
      <td>63.730857</td>
      <td>128</td>
      <td>1.592713</td>
      <td>0.055000695</td>
      <td>0.087600321935535</td>
      <td>53.768647</td>
      <td>0.92</td>
      <td>0.11</td>
      <td>0.1</td>
      <td>0.84</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mnasnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.84082</td>
      <td>0.257652914</td>
      <td>0.98959846514948</td>
      <td>10.205394</td>
      <td>128</td>
      <td>3.51966</td>
      <td>0.062306846</td>
      <td>0.21929891359236</td>
      <td>10.533623</td>
      <td>1.09</td>
      <td>0.22</td>
      <td>0.24</td>
      <td>1.03</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mobilenetv3_large_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.579837</td>
      <td>0.23102624</td>
      <td>0.8270362819228799</td>
      <td>27.695847</td>
      <td>128</td>
      <td>3.343894</td>
      <td>0.054719699</td>
      <td>0.182976873167906</td>
      <td>24.756512</td>
      <td>1.07</td>
      <td>0.22</td>
      <td>0.24</td>
      <td>0.89</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>regnety_002</td>
      <td>multiple</td>
      <td>1024</td>
      <td>3.268152</td>
      <td>0.281319321</td>
      <td>0.9193943015647921</td>
      <td>21.142692</td>
      <td>128</td>
      <td>2.760777</td>
      <td>0.034572704</td>
      <td>0.095447526031008</td>
      <td>19.602438</td>
      <td>1.18</td>
      <td>0.1</td>
      <td>0.12</td>
      <td>0.93</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>res2net101_26w_4s</td>
      <td>multiple</td>
      <td>128</td>
      <td>2.123275</td>
      <td>0.6357017530000001</td>
      <td>1.3497696396010752</td>
      <td>44.854486</td>
      <td>64</td>
      <td>2.033543</td>
      <td>0.326089306</td>
      <td>0.663116625591158</td>
      <td>42.407127</td>
      <td>1.04</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>0.95</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>rexnet_100</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.108521</td>
      <td>0.224596835</td>
      <td>0.698163978131035</td>
      <td>70.855954</td>
      <td>128</td>
      <td>3.009718</td>
      <td>0.11193924</td>
      <td>0.33690554553431995</td>
      <td>63.664349</td>
      <td>1.03</td>
      <td>0.48</td>
      <td>0.5</td>
      <td>0.9</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>twins_pcpvt_base</td>
      <td>multiple</td>
      <td>128</td>
      <td>1.248297</td>
      <td>0.712319752</td>
      <td>0.889186609462344</td>
      <td>72.087075</td>
      <td>64</td>
      <td>1.276892</td>
      <td>0.35436746799999996</td>
      <td>0.45248898494945594</td>
      <td>66.29411</td>
      <td>0.98</td>
      <td>0.51</td>
      <td>0.5</td>
      <td>0.92</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>xcit_large_24_p8_224</td>
      <td>multiple</td>
      <td>16</td>
      <td>1.348916</td>
      <td>1.275678435</td>
      <td>1.7207830518264602</td>
      <td>65.656872</td>
      <td>5</td>
      <td>1.186433</td>
      <td>0.445356691</td>
      <td>0.5283858749732031</td>
      <td>60.538357</td>
      <td>1.14</td>
      <td>0.31</td>
      <td>0.35</td>
      <td>0.92</td>
    </tr>
  </tbody>

</table>

<p>FP32 dynamic shape default wrapper</p>
<p></p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>cait_m36_384</td>
      <td>multiple</td>
      <td>4</td>
      <td>1.127514</td>
      <td>1.129254079</td>
      <td>1.273249783629606</td>
      <td>98.135861</td>
      <td>1</td>
      <td>0.431363</td>
      <td>0.828249059</td>
      <td>0.357275998837417</td>
      <td>56.947655</td>
      <td>2.61</td>
      <td>0.28</td>
      <td>0.73</td>
      <td>0.58</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>crossvit_9_240</td>
      <td>multiple</td>
      <td>256</td>
      <td>1.003197</td>
      <td>0.515077432</td>
      <td>0.5167241345501039</td>
      <td>82.254846</td>
      <td>128</td>
      <td>0.999027</td>
      <td>0.256604335</td>
      <td>0.256354658982045</td>
      <td>78.60409</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.96</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>dpn107</td>
      <td>multiple</td>
      <td>64</td>
      <td>1.486524</td>
      <td>0.927576092</td>
      <td>1.378864122584208</td>
      <td>88.428965</td>
      <td>32</td>
      <td>1.420098</td>
      <td>0.481905851</td>
      <td>0.684353535193398</td>
      <td>81.821581</td>
      <td>1.05</td>
      <td>0.5</td>
      <td>0.52</td>
      <td>0.93</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ese_vovnet19b_dw</td>
      <td>multiple</td>
      <td>256</td>
      <td>2.577743</td>
      <td>0.555744362</td>
      <td>1.432566138934966</td>
      <td>30.167536</td>
      <td>128</td>
      <td>2.509638</td>
      <td>0.28503200199999995</td>
      <td>0.7153271434352758</td>
      <td>26.808312</td>
      <td>1.03</td>
      <td>0.5</td>
      <td>0.51</td>
      <td>0.89</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetc_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.912113</td>
      <td>0.306918258</td>
      <td>1.200698907059154</td>
      <td>10.912849</td>
      <td>128</td>
      <td>3.552056</td>
      <td>0.075377171</td>
      <td>0.26774393251357603</td>
      <td>11.687522</td>
      <td>1.1</td>
      <td>0.22</td>
      <td>0.25</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetv3_b</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.372567</td>
      <td>0.24783138700000001</td>
      <td>0.835827957360429</td>
      <td>42.956149</td>
      <td>128</td>
      <td>3.268059</td>
      <td>0.117503787</td>
      <td>0.384009308639433</td>
      <td>40.290453</td>
      <td>1.03</td>
      <td>0.46</td>
      <td>0.47</td>
      <td>0.94</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ghostnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>2.306817</td>
      <td>0.361580243</td>
      <td>0.8340994514165311</td>
      <td>68.797782</td>
      <td>128</td>
      <td>2.278216</td>
      <td>0.083493075</td>
      <td>0.1902152593542</td>
      <td>59.574652</td>
      <td>1.01</td>
      <td>0.23</td>
      <td>0.23</td>
      <td>0.87</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>multiple</td>
      <td>256</td>
      <td>2.273479</td>
      <td>0.8149635580000001</td>
      <td>1.8528025348782822</td>
      <td>44.670439</td>
      <td>128</td>
      <td>2.176267</td>
      <td>0.41677702299999997</td>
      <td>0.907018081513141</td>
      <td>41.837864</td>
      <td>1.04</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>0.94</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>lcnet_050</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.758119</td>
      <td>0.027559328</td>
      <td>0.10357123418403201</td>
      <td>17.764059</td>
      <td>128</td>
      <td>3.20641</td>
      <td>0.013902173</td>
      <td>0.04457606652893</td>
      <td>17.228544</td>
      <td>1.17</td>
      <td>0.43</td>
      <td>0.5</td>
      <td>0.97</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>levit_128</td>
      <td>multiple</td>
      <td>1024</td>
      <td>1.455365</td>
      <td>0.568456226</td>
      <td>0.82731129535249</td>
      <td>76.998127</td>
      <td>128</td>
      <td>1.636161</td>
      <td>0.054639514</td>
      <td>0.089399041865754</td>
      <td>67.343735</td>
      <td>0.89</td>
      <td>0.11</td>
      <td>0.1</td>
      <td>0.87</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mnasnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.761106</td>
      <td>0.26455574000000004</td>
      <td>0.9950221810484401</td>
      <td>10.14067</td>
      <td>128</td>
      <td>3.422151</td>
      <td>0.06402520099999999</td>
      <td>0.21910390562735096</td>
      <td>10.779666</td>
      <td>1.1</td>
      <td>0.22</td>
      <td>0.24</td>
      <td>1.06</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mobilenetv3_large_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.549333</td>
      <td>0.23375215600000002</td>
      <td>0.829664241111948</td>
      <td>30.574509</td>
      <td>128</td>
      <td>3.221498</td>
      <td>0.057299103000000004</td>
      <td>0.184588945716294</td>
      <td>27.759706</td>
      <td>1.1</td>
      <td>0.22</td>
      <td>0.25</td>
      <td>0.91</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>regnety_002</td>
      <td>multiple</td>
      <td>1024</td>
      <td>3.28244</td>
      <td>0.281324749</td>
      <td>0.92343160910756</td>
      <td>22.597179</td>
      <td>128</td>
      <td>2.62434</td>
      <td>0.035817132</td>
      <td>0.09399633219288</td>
      <td>21.302106</td>
      <td>1.25</td>
      <td>0.1</td>
      <td>0.13</td>
      <td>0.94</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>res2net101_26w_4s</td>
      <td>multiple</td>
      <td>128</td>
      <td>2.111449</td>
      <td>0.646725466</td>
      <td>1.365527838460234</td>
      <td>53.761084</td>
      <td>64</td>
      <td>2.024896</td>
      <td>0.33263768</td>
      <td>0.67355670768128</td>
      <td>51.192373</td>
      <td>1.04</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>0.95</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>rexnet_100</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.03076</td>
      <td>0.229566711</td>
      <td>0.69576160503036</td>
      <td>74.350244</td>
      <td>128</td>
      <td>2.986474</td>
      <td>0.113670111</td>
      <td>0.339472831078614</td>
      <td>66.975391</td>
      <td>1.01</td>
      <td>0.49</td>
      <td>0.5</td>
      <td>0.9</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>twins_pcpvt_base</td>
      <td>multiple</td>
      <td>128</td>
      <td>1.043975</td>
      <td>0.860415781</td>
      <td>0.8982525649694751</td>
      <td>95.133136</td>
      <td>64</td>
      <td>1.053523</td>
      <td>0.43921092</td>
      <td>0.46271880607116</td>
      <td>88.80695</td>
      <td>0.99</td>
      <td>0.52</td>
      <td>0.51</td>
      <td>0.93</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>xcit_large_24_p8_224</td>
      <td>multiple</td>
      <td>16</td>
      <td>1.156888</td>
      <td>1.504368146</td>
      <td>1.7403854556896479</td>
      <td>95.897711</td>
      <td>5</td>
      <td>1.092471</td>
      <td>0.493731496</td>
      <td>0.539387341166616</td>
      <td>91.285955</td>
      <td>1.06</td>
      <td>0.31</td>
      <td>0.33</td>
      <td>0.95</td>
    </tr>
  </tbody>

</table>

<p>Fp32 static shape CPP wrapper</p>
<p></p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>cait_m36_384</td>
      <td>multiple</td>
      <td>4</td>
      <td>1.158979</td>
      <td>1.0886943180000002</td>
      <td>1.2617738519813222</td>
      <td>79.461534</td>
      <td>1</td>
      <td>0.446001</td>
      <td>0.773658453</td>
      <td>0.345052443696453</td>
      <td>75.319831</td>
      <td>2.6</td>
      <td>0.27</td>
      <td>0.71</td>
      <td>0.95</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>crossvit_9_240</td>
      <td>multiple</td>
      <td>256</td>
      <td>1.106066</td>
      <td>0.468049084</td>
      <td>0.517693178143544</td>
      <td>38.302621</td>
      <td>128</td>
      <td>1.124618</td>
      <td>0.227634492</td>
      <td>0.256001847124056</td>
      <td>38.136845</td>
      <td>0.98</td>
      <td>0.49</td>
      <td>0.49</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>dpn107</td>
      <td>multiple</td>
      <td>64</td>
      <td>1.499578</td>
      <td>0.916745354</td>
      <td>1.3747311644606122</td>
      <td>55.470531</td>
      <td>32</td>
      <td>1.465319</td>
      <td>0.46425532500000005</td>
      <td>0.6802821485736751</td>
      <td>56.811867</td>
      <td>1.02</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>1.02</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ese_vovnet19b_dw</td>
      <td>multiple</td>
      <td>256</td>
      <td>2.601046</td>
      <td>0.548337072</td>
      <td>1.4262499477773123</td>
      <td>21.467738</td>
      <td>128</td>
      <td>2.581418</td>
      <td>0.276282195</td>
      <td>0.7131998312525101</td>
      <td>21.798501</td>
      <td>1.01</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>1.02</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetc_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.927466</td>
      <td>0.302957494</td>
      <td>1.189855257130204</td>
      <td>21.757132</td>
      <td>128</td>
      <td>3.606708</td>
      <td>0.07294885499999999</td>
      <td>0.26310521891933997</td>
      <td>23.112068</td>
      <td>1.09</td>
      <td>0.22</td>
      <td>0.24</td>
      <td>1.06</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetv3_b</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.470922</td>
      <td>0.240767978</td>
      <td>0.835686871735716</td>
      <td>42.285806</td>
      <td>128</td>
      <td>3.407856</td>
      <td>0.112002755</td>
      <td>0.38168926064328</td>
      <td>43.197343</td>
      <td>1.02</td>
      <td>0.46</td>
      <td>0.47</td>
      <td>1.02</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ghostnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>2.362552</td>
      <td>0.353897254</td>
      <td>0.8361006652322079</td>
      <td>34.979681</td>
      <td>128</td>
      <td>2.424974</td>
      <td>0.078061797</td>
      <td>0.18929782811827803</td>
      <td>34.890128</td>
      <td>0.97</td>
      <td>0.23</td>
      <td>0.22</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>multiple</td>
      <td>256</td>
      <td>2.313839</td>
      <td>0.794449976</td>
      <td>1.8382293380178643</td>
      <td>35.851439</td>
      <td>128</td>
      <td>2.207067</td>
      <td>0.40841779100000003</td>
      <td>0.901405428728997</td>
      <td>37.18825</td>
      <td>1.05</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>1.04</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>lcnet_050</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.953986</td>
      <td>0.026050794</td>
      <td>0.10300447476488399</td>
      <td>18.774874</td>
      <td>128</td>
      <td>3.511033</td>
      <td>0.01255185</td>
      <td>0.04406995956105</td>
      <td>18.618444</td>
      <td>1.13</td>
      <td>0.43</td>
      <td>0.48</td>
      <td>0.99</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>levit_128</td>
      <td>multiple</td>
      <td>1024</td>
      <td>1.476753</td>
      <td>0.557822047</td>
      <td>0.8237653813733911</td>
      <td>37.606742</td>
      <td>128</td>
      <td>1.664077</td>
      <td>0.053327407</td>
      <td>0.088740911458339</td>
      <td>36.708729</td>
      <td>0.89</td>
      <td>0.11</td>
      <td>0.1</td>
      <td>0.98</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mnasnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.870201</td>
      <td>0.256346177</td>
      <td>0.992111230571577</td>
      <td>19.645081</td>
      <td>128</td>
      <td>3.571366</td>
      <td>0.061437903</td>
      <td>0.219417237885498</td>
      <td>20.88773</td>
      <td>1.08</td>
      <td>0.22</td>
      <td>0.24</td>
      <td>1.06</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mobilenetv3_large_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.600386</td>
      <td>0.229944858</td>
      <td>0.827890247515188</td>
      <td>23.333964</td>
      <td>128</td>
      <td>3.42882</td>
      <td>0.053433641999999996</td>
      <td>0.18321434036244</td>
      <td>23.99561</td>
      <td>1.05</td>
      <td>0.22</td>
      <td>0.23</td>
      <td>1.03</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>regnety_002</td>
      <td>multiple</td>
      <td>1024</td>
      <td>3.337162</td>
      <td>0.275695935</td>
      <td>0.9200419978364701</td>
      <td>24.775651</td>
      <td>128</td>
      <td>2.87205</td>
      <td>0.033542149</td>
      <td>0.09633472903545001</td>
      <td>26.010073</td>
      <td>1.16</td>
      <td>0.1</td>
      <td>0.12</td>
      <td>1.05</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>res2net101_26w_4s</td>
      <td>multiple</td>
      <td>128</td>
      <td>2.151179</td>
      <td>0.6315934519999999</td>
      <td>1.3586705704799078</td>
      <td>78.403243</td>
      <td>64</td>
      <td>2.060897</td>
      <td>0.323493541</td>
      <td>0.6666868681662771</td>
      <td>82.334831</td>
      <td>1.04</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>1.05</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>rexnet_100</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.137162</td>
      <td>0.221542551</td>
      <td>0.695014872380262</td>
      <td>32.335564</td>
      <td>128</td>
      <td>3.131406</td>
      <td>0.108230756</td>
      <td>0.33891443872293603</td>
      <td>32.677039</td>
      <td>1.0</td>
      <td>0.49</td>
      <td>0.49</td>
      <td>1.01</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>twins_pcpvt_base</td>
      <td>multiple</td>
      <td>128</td>
      <td>1.2633</td>
      <td>0.709629395</td>
      <td>0.8964748147035001</td>
      <td>65.485718</td>
      <td>64</td>
      <td>1.288747</td>
      <td>0.356618178</td>
      <td>0.45959060704296606</td>
      <td>65.559322</td>
      <td>0.98</td>
      <td>0.51</td>
      <td>0.5</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>xcit_large_24_p8_224</td>
      <td>multiple</td>
      <td>16</td>
      <td>1.357646</td>
      <td>1.2815927790000001</td>
      <td>1.7399493100382342</td>
      <td>66.321275</td>
      <td>5</td>
      <td>1.18502</td>
      <td>0.449534658</td>
      <td>0.5327075604231599</td>
      <td>66.760574</td>
      <td>1.15</td>
      <td>0.31</td>
      <td>0.35</td>
      <td>1.01</td>
    </tr>
  </tbody>

</table>

<p>FP32 dynamic shape CPP wrapper</p>
<p></p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>cait_m36_384</td>
      <td>multiple</td>
      <td>4</td>
      <td>1.157724</td>
      <td>1.08832395</td>
      <td>1.2599787566898</td>
      <td>108.196251</td>
      <td>1</td>
      <td>0.433281</td>
      <td>0.805933086</td>
      <td>0.34919549343516604</td>
      <td>74.706499</td>
      <td>2.67</td>
      <td>0.28</td>
      <td>0.74</td>
      <td>0.69</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>crossvit_9_240</td>
      <td>multiple</td>
      <td>256</td>
      <td>1.020213</td>
      <td>0.503902617</td>
      <td>0.5140880005974211</td>
      <td>48.179545</td>
      <td>128</td>
      <td>1.019328</td>
      <td>0.250655377</td>
      <td>0.255500044126656</td>
      <td>48.275248</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>dpn107</td>
      <td>multiple</td>
      <td>64</td>
      <td>1.501824</td>
      <td>0.911563019</td>
      <td>1.369007219446656</td>
      <td>64.989948</td>
      <td>32</td>
      <td>1.45016</td>
      <td>0.46692544500000005</td>
      <td>0.6771166033212</td>
      <td>64.958384</td>
      <td>1.04</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ese_vovnet19b_dw</td>
      <td>multiple</td>
      <td>256</td>
      <td>2.614024</td>
      <td>0.5479070580000001</td>
      <td>1.4322421993813923</td>
      <td>22.780164</td>
      <td>128</td>
      <td>2.562044</td>
      <td>0.27894298500000003</td>
      <td>0.7146642010613401</td>
      <td>22.593701</td>
      <td>1.02</td>
      <td>0.5</td>
      <td>0.51</td>
      <td>0.99</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetc_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.932486</td>
      <td>0.30318026800000003</td>
      <td>1.192252159386248</td>
      <td>23.432252</td>
      <td>128</td>
      <td>3.594164</td>
      <td>0.073815911</td>
      <td>0.265306489943404</td>
      <td>24.56948</td>
      <td>1.09</td>
      <td>0.22</td>
      <td>0.24</td>
      <td>1.05</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetv3_b</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.447331</td>
      <td>0.241845338</td>
      <td>0.833720930892878</td>
      <td>54.596958</td>
      <td>128</td>
      <td>3.37908</td>
      <td>0.11299371700000001</td>
      <td>0.38181480924036004</td>
      <td>54.772247</td>
      <td>1.02</td>
      <td>0.46</td>
      <td>0.47</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ghostnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>2.340159</td>
      <td>0.356206858</td>
      <td>0.833580684610422</td>
      <td>39.319907</td>
      <td>128</td>
      <td>2.364657</td>
      <td>0.080146603</td>
      <td>0.18951922581017097</td>
      <td>39.397476</td>
      <td>0.99</td>
      <td>0.23</td>
      <td>0.23</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>multiple</td>
      <td>256</td>
      <td>2.309044</td>
      <td>0.797007512</td>
      <td>1.840325413538528</td>
      <td>39.111115</td>
      <td>128</td>
      <td>2.21477</td>
      <td>0.406210417</td>
      <td>0.89966264525909</td>
      <td>40.403285</td>
      <td>1.04</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>1.03</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>lcnet_050</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.854295</td>
      <td>0.026758655</td>
      <td>0.103135750173225</td>
      <td>22.347079</td>
      <td>128</td>
      <td>3.347588</td>
      <td>0.013192424000000001</td>
      <td>0.04416280027331201</td>
      <td>21.985946</td>
      <td>1.15</td>
      <td>0.43</td>
      <td>0.49</td>
      <td>0.98</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>levit_128</td>
      <td>multiple</td>
      <td>1024</td>
      <td>1.468738</td>
      <td>0.560645295</td>
      <td>0.8234410492877101</td>
      <td>49.20347</td>
      <td>128</td>
      <td>1.707832</td>
      <td>0.052439506000000004</td>
      <td>0.089557866410992</td>
      <td>48.104791</td>
      <td>0.86</td>
      <td>0.11</td>
      <td>0.09</td>
      <td>0.98</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mnasnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.859596</td>
      <td>0.256945002</td>
      <td>0.9917039019391919</td>
      <td>20.998321</td>
      <td>128</td>
      <td>3.512091</td>
      <td>0.062282559999999994</td>
      <td>0.21874201843295996</td>
      <td>21.878707</td>
      <td>1.1</td>
      <td>0.22</td>
      <td>0.24</td>
      <td>1.04</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mobilenetv3_large_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>3.581561</td>
      <td>0.230518948</td>
      <td>0.825617673917828</td>
      <td>27.524943</td>
      <td>128</td>
      <td>3.391753</td>
      <td>0.054429278</td>
      <td>0.184610666944334</td>
      <td>28.104456</td>
      <td>1.06</td>
      <td>0.22</td>
      <td>0.24</td>
      <td>1.02</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>regnety_002</td>
      <td>multiple</td>
      <td>1024</td>
      <td>3.319031</td>
      <td>0.276788221</td>
      <td>0.918668685933851</td>
      <td>27.631068</td>
      <td>128</td>
      <td>2.748546</td>
      <td>0.034065364</td>
      <td>0.09363021996074401</td>
      <td>28.449124</td>
      <td>1.21</td>
      <td>0.1</td>
      <td>0.12</td>
      <td>1.03</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>res2net101_26w_4s</td>
      <td>multiple</td>
      <td>128</td>
      <td>2.145738</td>
      <td>0.630896683</td>
      <td>1.3537389867870542</td>
      <td>88.546493</td>
      <td>64</td>
      <td>2.066828</td>
      <td>0.323112637</td>
      <td>0.667818245305436</td>
      <td>90.038044</td>
      <td>1.04</td>
      <td>0.49</td>
      <td>0.51</td>
      <td>1.02</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>rexnet_100</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.049068</td>
      <td>0.22717784700000002</td>
      <td>0.6926807035965961</td>
      <td>36.96708</td>
      <td>128</td>
      <td>3.068273</td>
      <td>0.10972670200000001</td>
      <td>0.33667147712564605</td>
      <td>36.616721</td>
      <td>0.99</td>
      <td>0.49</td>
      <td>0.48</td>
      <td>0.99</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>twins_pcpvt_base</td>
      <td>multiple</td>
      <td>128</td>
      <td>1.059609</td>
      <td>0.839628043</td>
      <td>0.889677431015187</td>
      <td>68.731034</td>
      <td>64</td>
      <td>1.078374</td>
      <td>0.425106899</td>
      <td>0.45842422710222597</td>
      <td>68.219985</td>
      <td>0.98</td>
      <td>0.52</td>
      <td>0.51</td>
      <td>0.99</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>xcit_large_24_p8_224</td>
      <td>multiple</td>
      <td>16</td>
      <td>1.165339</td>
      <td>1.47412525</td>
      <td>1.7178556447097497</td>
      <td>100.969271</td>
      <td>5</td>
      <td>1.120266</td>
      <td>0.478249328</td>
      <td>0.535766461681248</td>
      <td>100.115393</td>
      <td>1.04</td>
      <td>0.31</td>
      <td>0.32</td>
      <td>0.99</td>
    </tr>
  </tbody>

</table>

<p>AMP static shape default wrapper</p>
<p></p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>cait_m36_384</td>
      <td>multiple</td>
      <td>4.0</td>
      <td>0.821234</td>
      <td>0.467824574</td>
      <td>0.384193446204316</td>
      <td>46.319364</td>
      <td>1</td>
      <td>1.175352</td>
      <td>0.155457499</td>
      <td>0.182717282364648</td>
      <td>39.783606</td>
      <td>0.7</td>
      <td>0.48</td>
      <td>0.33</td>
      <td>0.86</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>crossvit_9_240</td>
      <td>multiple</td>
      <td>256.0</td>
      <td>1.227089</td>
      <td>0.14619261700000002</td>
      <td>0.17939135220191305</td>
      <td>56.177141</td>
      <td>128</td>
      <td>1.162999</td>
      <td>0.06934895899999999</td>
      <td>0.08065276996804098</td>
      <td>54.113906</td>
      <td>1.06</td>
      <td>0.45</td>
      <td>0.47</td>
      <td>0.96</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>dpn107</td>
      <td>multiple</td>
      <td>64.0</td>
      <td>1.904526</td>
      <td>0.222362896</td>
      <td>0.423495916867296</td>
      <td>61.104693</td>
      <td>32</td>
      <td>1.865618</td>
      <td>0.10538095800000001</td>
      <td>0.19660061210204402</td>
      <td>57.684216</td>
      <td>1.02</td>
      <td>0.46</td>
      <td>0.47</td>
      <td>0.94</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ese_vovnet19b_dw</td>
      <td>multiple</td>
      <td>256.0</td>
      <td>4.098944</td>
      <td>0.172709349</td>
      <td>0.7079259498274562</td>
      <td>20.88358</td>
      <td>128</td>
      <td>4.022234</td>
      <td>0.084977797</td>
      <td>0.34180058433849797</td>
      <td>19.989066</td>
      <td>1.02</td>
      <td>0.48</td>
      <td>0.49</td>
      <td>0.96</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetc_100</td>
      <td>multiple</td>
      <td>512.0</td>
      <td>4.686703</td>
      <td>0.097461478</td>
      <td>0.45677300132703397</td>
      <td>8.06404</td>
      <td>128</td>
      <td>4.042179</td>
      <td>0.021221615</td>
      <td>0.085781566499085</td>
      <td>8.217601</td>
      <td>1.16</td>
      <td>0.19</td>
      <td>0.22</td>
      <td>1.02</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetv3_b</td>
      <td>multiple</td>
      <td>256.0</td>
      <td>3.553484</td>
      <td>0.08159478099999999</td>
      <td>0.28994574876700396</td>
      <td>24.109637</td>
      <td>128</td>
      <td>3.32511</td>
      <td>0.040153064999999995</td>
      <td>0.13351335796214997</td>
      <td>22.754499</td>
      <td>1.07</td>
      <td>0.46</td>
      <td>0.49</td>
      <td>0.94</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ghostnet_100</td>
      <td>multiple</td>
      <td>512.0</td>
      <td>2.386965</td>
      <td>0.124847255</td>
      <td>0.298006028031075</td>
      <td>47.99194</td>
      <td>128</td>
      <td>2.428614</td>
      <td>0.024986687</td>
      <td>0.060683017861818005</td>
      <td>43.380463</td>
      <td>0.98</td>
      <td>0.2</td>
      <td>0.2</td>
      <td>0.9</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>multiple</td>
      <td>256.0</td>
      <td>3.714062</td>
      <td>0.179554818</td>
      <td>0.6668777264507161</td>
      <td>31.751746</td>
      <td>128</td>
      <td>3.69302</td>
      <td>0.087065555</td>
      <td>0.32153483592610005</td>
      <td>30.346457</td>
      <td>1.01</td>
      <td>0.48</td>
      <td>0.48</td>
      <td>0.96</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>lcnet_050</td>
      <td>multiple</td>
      <td>256.0</td>
      <td>4.492463</td>
      <td>0.007604695999999999</td>
      <td>0.034163815406247994</td>
      <td>11.200165</td>
      <td>128</td>
      <td>3.784486</td>
      <td>0.00459462</td>
      <td>0.017388275065319998</td>
      <td>10.888034</td>
      <td>1.19</td>
      <td>0.51</td>
      <td>0.6</td>
      <td>0.97</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>levit_128</td>
      <td>multiple</td>
      <td>1024.0</td>
      <td>1.713187</td>
      <td>0.232147052</td>
      <td>0.397711311574724</td>
      <td>50.051491</td>
      <td>128</td>
      <td>1.962821</td>
      <td>0.023311037</td>
      <td>0.045755392955377</td>
      <td>43.080736</td>
      <td>0.87</td>
      <td>0.12</td>
      <td>0.1</td>
      <td>0.86</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mnasnet_100</td>
      <td>multiple</td>
      <td>512.0</td>
      <td>4.661632</td>
      <td>0.081703139</td>
      <td>0.380869967262848</td>
      <td>7.739705</td>
      <td>128</td>
      <td>4.062481</td>
      <td>0.016934334</td>
      <td>0.068795410122654</td>
      <td>7.76239</td>
      <td>1.15</td>
      <td>0.18</td>
      <td>0.21</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mobilenetv3_large_100</td>
      <td>multiple</td>
      <td>512.0</td>
      <td>4.217876</td>
      <td>0.081061436</td>
      <td>0.34190708542993603</td>
      <td>20.582085</td>
      <td>128</td>
      <td>3.543356</td>
      <td>0.018187107</td>
      <td>0.064443394711092</td>
      <td>18.926005</td>
      <td>1.19</td>
      <td>0.19</td>
      <td>0.22</td>
      <td>0.92</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>regnety_002</td>
      <td>multiple</td>
      <td>1024.0</td>
      <td>4.247371</td>
      <td>0.08683193</td>
      <td>0.36880742135603</td>
      <td>16.077293</td>
      <td>128</td>
      <td>3.679262</td>
      <td>0.01135338</td>
      <td>0.04177205960556</td>
      <td>14.948273</td>
      <td>1.15</td>
      <td>0.11</td>
      <td>0.13</td>
      <td>0.93</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>res2net101_26w_4s</td>
      <td>multiple</td>
      <td>128.0</td>
      <td>3.853771</td>
      <td>0.100842208</td>
      <td>0.388622776766368</td>
      <td>33.616911</td>
      <td>64</td>
      <td>3.692029</td>
      <td>0.050396554</td>
      <td>0.186065538868066</td>
      <td>32.486645</td>
      <td>1.04</td>
      <td>0.48</td>
      <td>0.5</td>
      <td>0.97</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>rexnet_100</td>
      <td>multiple</td>
      <td>256.0</td>
      <td>3.557627</td>
      <td>0.081857427</td>
      <td>0.291218192445729</td>
      <td>53.379488</td>
      <td>128</td>
      <td>3.074343</td>
      <td>0.043169334999999996</td>
      <td>0.13271734287190498</td>
      <td>49.903826</td>
      <td>1.16</td>
      <td>0.46</td>
      <td>0.53</td>
      <td>0.93</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>twins_pcpvt_base</td>
      <td>multiple</td>
      <td>128.0</td>
      <td>1.697609</td>
      <td>0.139341013</td>
      <td>0.23654655773791702</td>
      <td>43.042323</td>
      <td>64</td>
      <td>1.64525</td>
      <td>0.072752314</td>
      <td>0.11969574460850001</td>
      <td>40.515721</td>
      <td>1.03</td>
      <td>0.51</td>
      <td>0.52</td>
      <td>0.94</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>xcit_large_24_p8_224</td>
      <td>multiple</td>
      <td>16.0</td>
      <td>1.702702</td>
      <td>0.282584614</td>
      <td>0.481157387427028</td>
      <td>44.870611</td>
      <td>5</td>
      <td>1.048677</td>
      <td>0.15425357299999998</td>
      <td>0.161762174172921</td>
      <td>41.760633</td>
      <td>1.62</td>
      <td>0.34</td>
      <td>0.55</td>
      <td>0.93</td>
    </tr>
  </tbody>

</table>

<p>AMP dynamic shape default wrapper</p>
<p></p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>cait_m36_384</td>
      <td>multiple</td>
      <td>4</td>
      <td>0.814857</td>
      <td>0.470797991</td>
      <td>0.38363303855228703</td>
      <td>78.952506</td>
      <td>1</td>
      <td>1.159468</td>
      <td>0.158862256</td>
      <td>0.184195702239808</td>
      <td>39.935301</td>
      <td>0.7</td>
      <td>0.48</td>
      <td>0.34</td>
      <td>0.51</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>crossvit_9_240</td>
      <td>multiple</td>
      <td>256</td>
      <td>1.233108</td>
      <td>0.146299583</td>
      <td>0.18040318619396403</td>
      <td>68.084426</td>
      <td>128</td>
      <td>1.180556</td>
      <td>0.06873586799999999</td>
      <td>0.08114654138260799</td>
      <td>64.843927</td>
      <td>1.04</td>
      <td>0.45</td>
      <td>0.47</td>
      <td>0.95</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>dpn107</td>
      <td>multiple</td>
      <td>64</td>
      <td>1.888547</td>
      <td>0.223828912</td>
      <td>0.422711420270864</td>
      <td>70.893526</td>
      <td>32</td>
      <td>1.727838</td>
      <td>0.113122292</td>
      <td>0.195456994764696</td>
      <td>67.1781</td>
      <td>1.09</td>
      <td>0.46</td>
      <td>0.51</td>
      <td>0.95</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ese_vovnet19b_dw</td>
      <td>multiple</td>
      <td>256</td>
      <td>4.083546</td>
      <td>0.17387914699999998</td>
      <td>0.710043495215262</td>
      <td>22.907931</td>
      <td>128</td>
      <td>3.598216</td>
      <td>0.09497522</td>
      <td>0.34174135620752</td>
      <td>21.078805</td>
      <td>1.13</td>
      <td>0.48</td>
      <td>0.55</td>
      <td>0.92</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetc_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>4.60871</td>
      <td>0.09882548199999999</td>
      <td>0.45545798714822</td>
      <td>10.232784</td>
      <td>128</td>
      <td>3.741382</td>
      <td>0.022623818</td>
      <td>0.084644345436476</td>
      <td>10.163726</td>
      <td>1.23</td>
      <td>0.19</td>
      <td>0.23</td>
      <td>0.99</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetv3_b</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.500109</td>
      <td>0.083322921</td>
      <td>0.291639305698389</td>
      <td>35.34553</td>
      <td>128</td>
      <td>3.204496</td>
      <td>0.041291156999999995</td>
      <td>0.13231734744187199</td>
      <td>33.365988</td>
      <td>1.09</td>
      <td>0.45</td>
      <td>0.5</td>
      <td>0.94</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ghostnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>2.378432</td>
      <td>0.12515369699999998</td>
      <td>0.29766955786310395</td>
      <td>53.469505</td>
      <td>128</td>
      <td>2.346709</td>
      <td>0.02637799</td>
      <td>0.06190146653491001</td>
      <td>47.52734</td>
      <td>1.01</td>
      <td>0.21</td>
      <td>0.21</td>
      <td>0.89</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.705405</td>
      <td>0.18047686799999998</td>
      <td>0.6687398890715399</td>
      <td>35.796922</td>
      <td>128</td>
      <td>3.638109</td>
      <td>0.088754713</td>
      <td>0.322899320157717</td>
      <td>33.871431</td>
      <td>1.02</td>
      <td>0.48</td>
      <td>0.49</td>
      <td>0.95</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>lcnet_050</td>
      <td>multiple</td>
      <td>256</td>
      <td>4.24778</td>
      <td>0.008053703</td>
      <td>0.03421035852934</td>
      <td>14.452111</td>
      <td>128</td>
      <td>3.500319</td>
      <td>0.004971924</td>
      <td>0.017403320043756002</td>
      <td>14.062838</td>
      <td>1.21</td>
      <td>0.51</td>
      <td>0.62</td>
      <td>0.97</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>levit_128</td>
      <td>multiple</td>
      <td>1024</td>
      <td>1.688824</td>
      <td>0.232931864</td>
      <td>0.39338092228793603</td>
      <td>61.938459</td>
      <td>128</td>
      <td>1.954829</td>
      <td>0.023472032</td>
      <td>0.045883808842528</td>
      <td>54.096936</td>
      <td>0.86</td>
      <td>0.12</td>
      <td>0.1</td>
      <td>0.87</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mnasnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>4.579905</td>
      <td>0.082785813</td>
      <td>0.379151158887765</td>
      <td>9.337362</td>
      <td>128</td>
      <td>3.964008</td>
      <td>0.017403708</td>
      <td>0.06898843774166401</td>
      <td>9.281249</td>
      <td>1.16</td>
      <td>0.18</td>
      <td>0.21</td>
      <td>0.99</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mobilenetv3_large_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>4.183641</td>
      <td>0.08144458</td>
      <td>0.34073488411578</td>
      <td>24.692048</td>
      <td>128</td>
      <td>3.341868</td>
      <td>0.019268153000000003</td>
      <td>0.064391623929804</td>
      <td>23.024572</td>
      <td>1.25</td>
      <td>0.19</td>
      <td>0.24</td>
      <td>0.93</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>regnety_002</td>
      <td>multiple</td>
      <td>1024</td>
      <td>4.20362</td>
      <td>0.08756928700000001</td>
      <td>0.36810800621894</td>
      <td>19.164196</td>
      <td>128</td>
      <td>3.250118</td>
      <td>0.012904351</td>
      <td>0.041940663463418</td>
      <td>17.896246</td>
      <td>1.29</td>
      <td>0.11</td>
      <td>0.15</td>
      <td>0.93</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>res2net101_26w_4s</td>
      <td>multiple</td>
      <td>128</td>
      <td>3.659522</td>
      <td>0.10625728100000001</td>
      <td>0.388850857479682</td>
      <td>44.590782</td>
      <td>64</td>
      <td>3.22483</td>
      <td>0.056892767999999996</td>
      <td>0.18346950502943998</td>
      <td>42.567328</td>
      <td>1.13</td>
      <td>0.47</td>
      <td>0.54</td>
      <td>0.95</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>rexnet_100</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.470168</td>
      <td>0.083745035</td>
      <td>0.29060934061588</td>
      <td>58.445286</td>
      <td>128</td>
      <td>2.930129</td>
      <td>0.044972733</td>
      <td>0.131775909172557</td>
      <td>54.638342</td>
      <td>1.18</td>
      <td>0.45</td>
      <td>0.54</td>
      <td>0.93</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>twins_pcpvt_base</td>
      <td>multiple</td>
      <td>128</td>
      <td>1.662596</td>
      <td>0.142323634</td>
      <td>0.236626704593864</td>
      <td>60.942311</td>
      <td>64</td>
      <td>1.620109</td>
      <td>0.073954289</td>
      <td>0.11981400919750101</td>
      <td>58.413977</td>
      <td>1.03</td>
      <td>0.51</td>
      <td>0.52</td>
      <td>0.96</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>xcit_large_24_p8_224</td>
      <td>multiple</td>
      <td>16</td>
      <td>1.731394</td>
      <td>0.278880742</td>
      <td>0.4828524434143481</td>
      <td>75.099117</td>
      <td>5</td>
      <td>1.062819</td>
      <td>0.15503407000000002</td>
      <td>0.16477315524333003</td>
      <td>72.124756</td>
      <td>1.63</td>
      <td>0.34</td>
      <td>0.56</td>
      <td>0.96</td>
    </tr>
  </tbody>

</table>

<p>AMP static shape CPP wrapper</p>
<p></p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>cait_m36_384</td>
      <td>multiple</td>
      <td>4</td>
      <td>0.830928</td>
      <td>0.46368191200000003</td>
      <td>0.38528628377433605</td>
      <td>67.494156</td>
      <td>1</td>
      <td>1.167289</td>
      <td>0.154926808</td>
      <td>0.18084435878351202</td>
      <td>59.008888</td>
      <td>0.71</td>
      <td>0.47</td>
      <td>0.33</td>
      <td>0.87</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>crossvit_9_240</td>
      <td>multiple</td>
      <td>256</td>
      <td>1.247981</td>
      <td>0.144943929</td>
      <td>0.180887269457349</td>
      <td>33.159835</td>
      <td>128</td>
      <td>1.20794</td>
      <td>0.067025233</td>
      <td>0.08096245995002001</td>
      <td>33.204855</td>
      <td>1.03</td>
      <td>0.45</td>
      <td>0.46</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>dpn107</td>
      <td>multiple</td>
      <td>64</td>
      <td>1.938903</td>
      <td>0.21847377599999998</td>
      <td>0.423599459707728</td>
      <td>47.766946</td>
      <td>32</td>
      <td>1.894499</td>
      <td>0.10402574099999999</td>
      <td>0.19707666229875898</td>
      <td>47.574879</td>
      <td>1.02</td>
      <td>0.47</td>
      <td>0.48</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ese_vovnet19b_dw</td>
      <td>multiple</td>
      <td>256</td>
      <td>4.135093</td>
      <td>0.172211974</td>
      <td>0.7121125282035821</td>
      <td>17.319972</td>
      <td>128</td>
      <td>4.050032</td>
      <td>0.084228177</td>
      <td>0.341126812151664</td>
      <td>17.723525</td>
      <td>1.02</td>
      <td>0.48</td>
      <td>0.49</td>
      <td>1.02</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetc_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>4.720744</td>
      <td>0.09660041300000001</td>
      <td>0.45602582006727205</td>
      <td>18.957905</td>
      <td>128</td>
      <td>4.096619</td>
      <td>0.020683105</td>
      <td>0.08473080092199499</td>
      <td>19.093041</td>
      <td>1.15</td>
      <td>0.19</td>
      <td>0.21</td>
      <td>1.01</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>fbnetv3_b</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.646554</td>
      <td>0.08008029</td>
      <td>0.29201710182066</td>
      <td>38.044669</td>
      <td>128</td>
      <td>3.43269</td>
      <td>0.038996093</td>
      <td>0.13386149848017</td>
      <td>37.762824</td>
      <td>1.06</td>
      <td>0.46</td>
      <td>0.49</td>
      <td>0.99</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>ghostnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>2.453529</td>
      <td>0.121961917</td>
      <td>0.29923710025509304</td>
      <td>29.809605</td>
      <td>128</td>
      <td>2.515739</td>
      <td>0.024097907</td>
      <td>0.06062404445827299</td>
      <td>29.731969</td>
      <td>0.98</td>
      <td>0.2</td>
      <td>0.2</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.737612</td>
      <td>0.17912178499999998</td>
      <td>0.6694877330774199</td>
      <td>31.249321</td>
      <td>128</td>
      <td>3.766203</td>
      <td>0.085030145</td>
      <td>0.320240787189435</td>
      <td>31.649777</td>
      <td>0.99</td>
      <td>0.48</td>
      <td>0.47</td>
      <td>1.01</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>lcnet_050</td>
      <td>multiple</td>
      <td>256</td>
      <td>4.561036</td>
      <td>0.007467409</td>
      <td>0.034059121275724</td>
      <td>15.31665</td>
      <td>128</td>
      <td>3.954882</td>
      <td>0.0044049959999999996</td>
      <td>0.017421239390472</td>
      <td>15.306443</td>
      <td>1.15</td>
      <td>0.51</td>
      <td>0.59</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>levit_128</td>
      <td>multiple</td>
      <td>1024</td>
      <td>1.709958</td>
      <td>0.230717214</td>
      <td>0.394516745817012</td>
      <td>31.956928</td>
      <td>128</td>
      <td>2.029729</td>
      <td>0.02218751</td>
      <td>0.045034632484790005</td>
      <td>31.132125</td>
      <td>0.84</td>
      <td>0.11</td>
      <td>0.1</td>
      <td>0.97</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mnasnet_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>4.669209</td>
      <td>0.081243729</td>
      <td>0.379343950640361</td>
      <td>16.982208</td>
      <td>128</td>
      <td>4.176039</td>
      <td>0.016410076</td>
      <td>0.068529117368964</td>
      <td>17.167433</td>
      <td>1.12</td>
      <td>0.18</td>
      <td>0.2</td>
      <td>1.01</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>mobilenetv3_large_100</td>
      <td>multiple</td>
      <td>512</td>
      <td>4.260008</td>
      <td>0.08015232500000001</td>
      <td>0.34144954571860003</td>
      <td>20.023572</td>
      <td>128</td>
      <td>3.633328</td>
      <td>0.017696648</td>
      <td>0.064297726684544</td>
      <td>20.037371</td>
      <td>1.17</td>
      <td>0.19</td>
      <td>0.22</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>regnety_002</td>
      <td>multiple</td>
      <td>1024</td>
      <td>4.29803</td>
      <td>0.085610023</td>
      <td>0.36795444715468995</td>
      <td>21.475541</td>
      <td>128</td>
      <td>3.811496</td>
      <td>0.010886469999999999</td>
      <td>0.041493736859119994</td>
      <td>21.651017</td>
      <td>1.13</td>
      <td>0.11</td>
      <td>0.13</td>
      <td>1.01</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>res2net101_26w_4s</td>
      <td>multiple</td>
      <td>128</td>
      <td>3.909864</td>
      <td>0.099102483</td>
      <td>0.387477230592312</td>
      <td>70.954566</td>
      <td>64</td>
      <td>3.827453</td>
      <td>0.048228105</td>
      <td>0.184590805166565</td>
      <td>71.423707</td>
      <td>1.02</td>
      <td>0.48</td>
      <td>0.49</td>
      <td>1.01</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>rexnet_100</td>
      <td>multiple</td>
      <td>256</td>
      <td>3.343283</td>
      <td>0.086991776</td>
      <td>0.29083812584060803</td>
      <td>28.392625</td>
      <td>128</td>
      <td>2.958854</td>
      <td>0.044547482</td>
      <td>0.131809495305628</td>
      <td>28.160011</td>
      <td>1.13</td>
      <td>0.45</td>
      <td>0.51</td>
      <td>0.99</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>tf_efficientnet_b0</td>
      <td>multiple</td>
      <td>128</td>
      <td>2.162988</td>
      <td>0.061583674000000005</td>
      <td>0.133204747857912</td>
      <td>25.918048</td>
      <td>128</td>
      <td>2.487402</td>
      <td>0.054038459000000004</td>
      <td>0.134415370993518</td>
      <td>25.934764</td>
      <td>0.87</td>
      <td>1.01</td>
      <td>0.88</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>tinynet_a</td>
      <td>multiple</td>
      <td>128</td>
      <td>2.301942</td>
      <td>0.039336965999999994</td>
      <td>0.09055141418797198</td>
      <td>28.503671</td>
      <td>128</td>
      <td>2.612417</td>
      <td>0.034847244000000006</td>
      <td>0.09103553262874803</td>
      <td>28.447873</td>
      <td>0.88</td>
      <td>1.01</td>
      <td>0.89</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>twins_pcpvt_base</td>
      <td>multiple</td>
      <td>128</td>
      <td>1.686005</td>
      <td>0.14037095600000002</td>
      <td>0.23666613367078002</td>
      <td>47.824959</td>
      <td>64</td>
      <td>1.667016</td>
      <td>0.072305913</td>
      <td>0.120535113865608</td>
      <td>47.400144</td>
      <td>1.01</td>
      <td>0.51</td>
      <td>0.52</td>
      <td>0.99</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>xcit_large_24_p8_224</td>
      <td>multiple</td>
      <td>16</td>
      <td>1.722335</td>
      <td>0.28213780000000005</td>
      <td>0.48593580776300005</td>
      <td>54.101319</td>
      <td>5</td>
      <td>1.066934</td>
      <td>0.15119244699999998</td>
      <td>0.16131236224749798</td>
      <td>52.793868</td>
      <td>1.61</td>
      <td>0.33</td>
      <td>0.54</td>
      <td>0.98</td>
    </tr>
  </tbody>


</table>
<p>SW info</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>d6015d42</td>
      <td>main</td>
      <td>d6015d42</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>35c493f2cf9b623bfdc7e6b34dc1cb39690a7919</td>
      <td>main</td>
      <td>f0d461beacded34abe196c72ec4bcdb55bf01793</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.19.0a0+2c4665f</td>
      <td>main</td>
      <td>0.19.0a0+a0c79b3</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+ea437b3</td>
      <td>main</td>
      <td>2.2.0a0+17a7081</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>

</table>

<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh <strong>thread</strong> inference performance timm_models <strong>model</strong> amp/float32 first static/dynamic default/cpp<br />
<a href="https://github.com/pytorch/pytorch/files/14893694/timm_models-crossvit_9_240-inference-amp-static-default-multiple-performance-drop_guilty_commit.log">timm_models-crossvit_9_240-inference-amp-static-default-multiple-performance-drop_guilty_commit.log</a><br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/52b1d2a73d49b53fae232ce1d1141c1b59024266<br />
cc @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Sat, 06 Apr 2024 05:32:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123501</guid>
    </item>
    <item>
      <title>[inductor][cpu]PT2E and QAT quantization regression in 2024-03-31 nightly release</title>
      <link>https://github.com/pytorch/pytorch/issues/123448</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>PT2E and QAT quantization regression in 2024-03-31 nightly release</p>
<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:x="urn:schemas-microsoft-com:office:excel"
xmlns="http://www.w3.org/TR/REC-html40">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content="Microsoft Excel 15">
<link id=Main-File rel=Main-File
href="file:///C:/Users/zengxian/AppData/Local/Temp/msohtmlclip1/01/clip.htm">
<link rel=File-List
href="file:///C:/Users/zengxian/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml">
<!--table
    {mso-displayed-decimal-separator:"\.";
    mso-displayed-thousand-separator:"\,";}
@page
    {margin:1.0in .75in 1.0in .75in;
    mso-header-margin:.5in;
    mso-footer-margin:.5in;}
tr
    {mso-height-source:auto;}
col
    {mso-width-source:auto;}
br
    {mso-data-placement:same-cell;}
td
    {padding-top:1px;
    padding-right:1px;
    padding-left:1px;
    mso-ignore:padding;
    color:black;
    font-size:11.0pt;
    font-weight:400;
    font-style:normal;
    text-decoration:none;
    font-family:Calibri, sans-serif;
    mso-font-charset:0;
    mso-number-format:General;
    text-align:general;
    vertical-align:bottom;
    border:none;
    mso-background-source:auto;
    mso-pattern:auto;
    mso-protection:locked visible;
    white-space:nowrap;
    mso-rotate:0;}
.xl66
    {mso-number-format:Scientific;}
-->
</head>

<body link=blue vlink=purple>


model_name | PT2E_new | PT2E_old
-- | -- | --
densenet121 | 534.6600164 | 1217.14198
mobilenet_v2 | 4760.843136 | 5860.936014
mobilenet_v3_large | 3652.993057 | 5596.587481
squeezenet1_1 | 7583.483788 | 9030.394544



</body>

</html>

<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:x="urn:schemas-microsoft-com:office:excel"
xmlns="http://www.w3.org/TR/REC-html40">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content="Microsoft Excel 15">
<link id=Main-File rel=Main-File
href="file:///C:/Users/zengxian/AppData/Local/Temp/msohtmlclip1/01/clip.htm">
<link rel=File-List
href="file:///C:/Users/zengxian/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml">

<!--table
    {mso-displayed-decimal-separator:"\.";
    mso-displayed-thousand-separator:"\,";}
@page
    {margin:1.0in .75in 1.0in .75in;
    mso-header-margin:.5in;
    mso-footer-margin:.5in;}
tr
    {mso-height-source:auto;}
col
    {mso-width-source:auto;}
br
    {mso-data-placement:same-cell;}
td
    {padding-top:1px;
    padding-right:1px;
    padding-left:1px;
    mso-ignore:padding;
    color:black;
    font-size:11.0pt;
    font-weight:400;
    font-style:normal;
    text-decoration:none;
    font-family:Calibri, sans-serif;
    mso-font-charset:0;
    mso-number-format:General;
    text-align:general;
    vertical-align:bottom;
    border:none;
    mso-background-source:auto;
    mso-pattern:auto;
    mso-protection:locked visible;
    white-space:nowrap;
    mso-rotate:0;}
.xl68
    {mso-number-format:Scientific;}
-->

</head>

<body link=blue vlink=purple>


model_name | QAT_new | QAT_old
-- | -- | --
densenet121 | 535.9700177 | 1234.085813
mobilenet_v2 | 5058.754669 | 6045.72933
squeezenet1_1 | 7979.238023 | 9213.892967
mobilenet_v3_large-eval_throughput | 4231.302988 | 6811.576935
shufflenet_v2_x1_0-eval_throughput | 5354.989357 | 6619.653679




</body>

</html>

<h3>Versions</h3>
<p></table><p>SW info</p><table border="1" class="dataframe table"><br />
<br />
<thead>
    <tr style="text-align: right;">
      <th>SW</th>
      <th>Nightly commit</th>
      <th>Main commit</th>
    </tr>
  </thead></p>
<tbody>
    <tr>
      <td>Pytorch</td>
      <td>4af23dd</td>
      <td></td>
    </tr>
    <tr>
      <td>Torchbench</td>
      <td>/</td>
      <td>ee35d764</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>ea437b3</td>
      <td></td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>b0ebddc</td>
      <td></td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>2c4665f</td>
      <td></td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>0790338</td>
      <td></td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>nightly</td>
      <td>/</td>
    </tr>
  </tbody>
<p></table><p>Reference SW info (nightly)</p><table border="1" class="dataframe table"><br />
<br />
<thead>
    <tr style="text-align: right;">
      <th>item</th>
      <th>commit</th>
    </tr>
  </thead></p>
<tbody>
    <tr>
      <td>torchbench</td>
      <td>ee35d764</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>2.4.0a0+git384cbf2</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>0.19.0a0+a0c79b3</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>2.2.0a0+17a7081</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>nightly</td>
    </tr>
  </tbody>
</table>
<p>Repro:<br />
```<br />
git clone -b chuanqiw/inductor_quant https://github.com/pytorch/benchmark.git<br />
cd benchmark<br />
pip install --no-deps -r requirements.txt<br />
pip install --no-cache Jinja2==3.1.2 markupsafe==2.0.1 beartype==0.15.0 &amp;&amp; pip install mpmath==1.3.0<br />
python install.py --continue_on_fail<br />
export LD_PRELOAD=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}/lib/libiomp5.so:${CONDA_PREFIX:-"$(dirname $(which conda))/../"}/lib/libjemalloc.so<br />
export MALLOC_CONF="oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:-1,muzzy_decay_ms:-1"</p>
<h1>PT2E</h1>
<p>TORCHINDUCTOR_FREEZING=1 python run_benchmark.py cpu -m ${models} --torchdynamo inductor --quantize --launcher --launcher-args="--throughput-mode" -b 128 --metrics throughputs<br />
mv .userbenchmark/cpu ptq<br />
cat ptq/metric* # to see the results</p>
<h1>QAT</h1>
<p>TORCHINDUCTOR_FREEZING=1 python run_benchmark.py cpu -m ${models} --torchdynamo inductor --quantize --is_qat --launcher --launcher-args="--throughput-mode" -b 128 --metrics throughputs<br />
mv .userbenchmark/cpu qat<br />
cat qat/metric* # to see the results<br />
```<br />
Suspected guilty commit: https://github.com/pytorch/pytorch/commit/6f4ed57b8aaa980169f8ed7f6219afd808487bc5<br />
<a href="https://github.com/pytorch/pytorch/files/14887092/torchbench-densenet121-inference-ptq-performance-drop_guilty_commit.log">torchbench-densenet121-inference-ptq-performance-drop_guilty_commit.log</a><br />
cc @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Fri, 05 Apr 2024 07:32:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123448</guid>
    </item>
    <item>
      <title>[Inductor] Add a device agnostic DeviceGuard class to inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/123338</link>
      <description><![CDATA[<p>Summary: Currently although only in one place in inductor, the <code>device</code> context manager from the device interface is used . This PR creates an inductor specific <code>DeviceGuard</code> class for use in these cases, which keeps a reference to the <code>DeviceInterface</code> class which is defined and added out of tree. This then offloads the device specific work to the device interface, instead of having to define this logic on the device class which isn't strictly necessary for inductor.</p>
<p>Ideally I would have used the existing <code>DeviceGuard</code> class, but these are defined per device and don't work well with inductor's device agnostic/ out of tree compatible design. With the existing classes in mind, I am happy to take suggestions on the renaming of this class. </p>
<p>Whilst I was there, I also took the opportunity to rename <code>gpu_device</code> to <code>device_interface</code> to clarify this is not necessarily a GPU. </p>
<p>Test Plan: None currently, happy to add some. </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 04 Apr 2024 03:16:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123338</guid>
    </item>
    <item>
      <title>[inductor][cpu]DebertaV2ForQuestionAnswering AMP static shape multiple thread default wrapper regression in 2024-03-23</title>
      <link>https://github.com/pytorch/pytorch/issues/122862</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>new_perf_regression in 2024-03-23</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>huggingface</td>
      <td>DebertaV2ForQuestionAnswering</td>
      <td>multiple</td>
      <td>1.0</td>
      <td>1.012825</td>
      <td>0.118098464</td>
      <td>0.11961307680080001</td>
      <td>26.196844</td>
      <td>1.0</td>
      <td>2.290567</td>
      <td>0.068807462</td>
      <td>0.157608101810954</td>
      <td>27.15124</td>
      <td>0.44</td>
      <td>1.32</td>
      <td>0.58</td>
      <td>1.04</td>
    </tr>
  </tbody>

</table>

<p>42624bc<br />
/workspace/pytorch# bash inductor_single_run.sh multiple inference performance huggingface DebertaV2ForQuestionAnswering amp first static<br />
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles<br />
cpu,DebertaV2ForQuestionAnswering,1,1.287133,95.115025,30.050526,0.930185,1250.272051,1344.110592,1088,1,0,0,0,0</p>
<p>e26280a<br />
/workspace/pytorch# bash inductor_single_run.sh multiple inference performance huggingface DebertaV2ForQuestionAnswering amp first static<br />
dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles<br />
cpu,DebertaV2ForQuestionAnswering,1,1.989292,62.880099,19.385791,0.930492,1270.559949,1365.470413,1088,1,0,0,0,0</p>
<h3>Versions</h3>
<p></table><p>SW info</p><table border="1" class="dataframe table"><br />
<br />
<thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead></p>
<tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>d6015d42</td>
      <td>main</td>
      <td>d6015d42</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>f0d461beacded34abe196c72ec4bcdb55bf01793</td>
      <td>main</td>
      <td>5bc7f7f97760d2d485621f9f30d0316e1f2440c6</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.19.0a0+a0c79b3</td>
      <td>main</td>
      <td>0.18.0a0+0325175</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+17a7081</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>

</table>
<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh multiple inference performance huggingface DebertaV2ForQuestionAnswering amp first static</p>
<p><a href="https://github.com/pytorch/pytorch/files/14783477/huggingface-DebertaV2ForQuestionAnswering-inference-amp-static-default-multiple-performance-drop_guilty_commit.1.log">huggingface-DebertaV2ForQuestionAnswering-inference-amp-static-default-multiple-performance-drop_guilty_commit (1).log</a></p>
<p>Suspected guilty commit: https://github.com/pytorch/pytorch/commit/42624bceb6856a8934a1fcacb8a621a418f1a2c3<br />
cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 19:11:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122862</guid>
    </item>
    <item>
      <title>torch.compile + ring attention</title>
      <link>https://github.com/pytorch/pytorch/issues/121386</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I was trying to enable <a href="https://github.com/lucidrains/ring-attention-pytorch">ring attention</a> with torch.compile, here are the issues that I encountered:<br />
* einx triggers "unhashable type: non-nested SymInt"<br />
```<br />
Traceback (most recent call last):<br />
  File "/home/ybliang/local/pytorch/torch/multiprocessing/spawn.py", line 75, in _wrap<br />
    fn(i, <em>args)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/assert.py", line 85, in start<br />
    ring_out = ddp_ring_attention_net(seq)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/eval_frame.py", line 437, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/external_utils.py", line 36, in inner<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1593, in forward<br />
    else self._run_ddp_forward(</em>inputs, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward<br />
    return self.module(*inputs, </strong>kwargs)  # type: ignore[index]<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 548, in forward<br />
    (x, mask), batch_sizes, num_sharded_batches = sharded_batch_to_sharded_seq(x, mask, self.ring_seq_size)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 191, in sharded_batch_to_sharded_seq<br />
    x, sizes = all_gather(x)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 194, in torch_dynamo_resume_in_sharded_batch_to_sharded_seq_at_191<br />
    mask, _ = all_gather(mask)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 917, in catch_errors<br />
    return hijacked_callback(frame, cache_entry, hooks, frame_state)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 786, in _convert_frame<br />
    result = inner_convert(<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert<br />
    return _compile(<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 676, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 535, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 165, in _fn<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/convert_frame.py", line 500, in transform<br />
    tracer.run()<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 2149, in run<br />
    super().run()<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 489, in wrapper<br />
    return inner_fn(self, inst)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 1272, in CALL_FUNCTION_KW<br />
    self.call_function(fn, args, kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
    self.push(fn.call_function(self, args, kwargs))<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/variables/torch.py", line 664, in call_function<br />
    tensor_variable = wrap_fx_proxy(<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/variables/builder.py", line 1325, in wrap_fx_proxy<br />
    return wrap_fx_proxy_cls(target_cls=TensorVariable, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/variables/builder.py", line 1410, in wrap_fx_proxy_cls<br />
    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1714, in get_fake_value<br />
    raise TorchRuntimeError(str(e)).with_traceback(e.<strong>traceback</strong>) from None<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1656, in get_fake_value<br />
    ret_val = wrap_fake_exception(<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1190, in wrap_fake_exception<br />
    return fn()<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1657, in <lambda><br />
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1782, in run_node<br />
    raise RuntimeError(make_error_message(e)).with_traceback(<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/utils.py", line 1764, in run_node<br />
    return node.target(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 70, in inner<br />
    graph = construct_graph(</em>args, backend=backend, <strong>kwargs)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 20, in inner<br />
    return func(*args, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/<strong>init</strong>.py", line 308, in <strong>hash</strong><br />
    raise TypeError("unhashable type: non-nested SymInt")<br />
torch._dynamo.exc.TorchRuntimeError: Failed running call_function <function rearrange at 0x7f7454380790>(<em>('(b s) n -&gt; b (s n)', FakeTensor(..., size=(16, 32), dtype=torch.int64)), </em>*{'s': 1}):<br />
unhashable type: non-nested SymInt</p>
<p>from user code:<br />
   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 206, in torch_dynamo_resume_in_sharded_batch_to_sharded_seq_at_194<br />
    x = rearrange('(b s) n -&gt; b (s n)', x, s = num_sharded_batches)<br />
<code>* einx rearrange issue</code><br />
Traceback (most recent call last):<br />
  File "/home/ybliang/local/pytorch/torch/multiprocessing/spawn.py", line 75, in _wrap<br />
    fn(i, <em>args)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/assert.py", line 85, in start<br />
    ring_out = ddp_ring_attention_net(seq)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/eval_frame.py", line 437, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/_dynamo/external_utils.py", line 36, in inner<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1593, in forward<br />
    else self._run_ddp_forward(</em>inputs, <strong>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward<br />
    return self.module(*inputs, </strong>kwargs)  # type: ignore[index]<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 548, in forward<br />
    (x, mask), batch_sizes, num_sharded_batches = sharded_batch_to_sharded_seq(x, mask, self.ring_seq_size)<br />
  File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 600, in torch_dynamo_resume_in_forward_at_548<br />
    logits = rearrange('b (i j) d -&gt; b (j i) d', logits, j = self.bucket_size)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 66, in inner<br />
    backend = einx.backend.get(input_tracer_values)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 70, in torch_dynamo_resume_in_inner_at_66<br />
    graph = construct_graph(*args, backend=backend, </strong>kwargs)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 20, in inner<br />
    return func(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 45, in construct_graph<br />
    output_tracers = func(</em>args, <strong>kwargs, backend=einx.backend.tracer)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/op/rearrange.py", line 118, in rearrange<br />
    exprs_in, exprs_out = parse(description, *[einx.param.get_shape(tensor) for tensor in tensors], cse=cse, </strong>parameters)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/lru_cache.py", line 20, in inner<br />
    return func(<em>args, </em><em>kwargs)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/op/rearrange.py", line 56, in parse<br />
    exprs = einx.expr.solve(<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/util.py", line 108, in solve<br />
    exprs1, exprs2 = stage3.solve(exprs1, exprs2)<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 333, in solve<br />
    exprs1 = [map(root) if not root is None else None for root in exprs1]<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 333, in <listcomp><br />
    exprs1 = [map(root) if not root is None else None for root in exprs1]<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 324, in map<br />
    return List([map(child) for child in expr.children])<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 324, in <listcomp><br />
    return List([map(child) for child in expr.children])<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 330, in map<br />
    return Composition.maybe(map(expr.inner))<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 324, in map<br />
    return List([map(child) for child in expr.children])<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 65, in <strong>init</strong><br />
    Expression.<strong>init</strong>(self, np.prod([c.value for c in children]).astype(int))<br />
  File "/home/ybliang/local/miniconda3/envs/pt/lib/python3.10/site-packages/einx/expr/stage3.py", line 9, in <strong>init</strong><br />
    raise TypeError(f"Expected int, got {type(value)}")<br />
TypeError: Expected int, got <class 'numpy.ndarray'><br />
<code>* torch.distributed related graph break 1:</code><br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks] Graph break: from user code at:<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/external_utils.py", line 36, in inner<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     return fn(</em>args, <strong>kwargs)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     return forward_call(*args, </strong>kwargs)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1589, in forward<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     inputs, kwargs = self._pre_forward(<em>inputs, </em><em>kwargs)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [<strong>graph_breaks]   File "/home/ybliang/local/pytorch/torch/nn/parallel/distributed.py", line 1464, in _pre_forward<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     self.reducer.prepare_for_forward()<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks] Traceback (most recent call last):<br />
......<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     self.push(fn.call_function(self, args, kwargs))<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/variables/user_defined.py", line 687, in call_function<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     return self.call_method(tx, "__call</strong>", args, kwargs)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [<strong>graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/variables/user_defined.py", line 579, in call_method<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     return super().call_method(tx, name, args, kwargs)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/variables/base.py", line 371, in call_method<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     raise unimplemented(f"call_method {self} {name} {args} {kwargs}")<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/exc.py", line 190, in unimplemented<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks]     raise Unsupported(msg)<br />
[rank3]:V0306 22:17:59.702000 139967781041280 torch/_dynamo/symbolic_convert.py:516] [0/0] [__graph_breaks] torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(instancemethod) __call</strong> [] {}<br />
<code>Graph break 2:</code><br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks] Graph break: from user code at:<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/ring_attention.py", line 228, in sharded_seq_to_sharded_batch<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     logits, _ = all_gather(logits)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/nn/modules/module.py", line 1536, in _call_impl<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     return forward_call(</em>args, **kwargs)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/distributed.py", line 85, in forward<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     return AllGatherFunction.apply(x, self.dim, sizes)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/distributed.py", line 68, in forward<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     x, batch_sizes = all_gather_variable_dim(x, dim = dim, sizes = sizes)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/distributed.py", line 42, in all_gather_variable_dim<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     sizes = gather_sizes(t, dim = dim)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/distributed.py", line 32, in gather_sizes<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     sizes = all_gather_same_dim(size)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/data/users/ybliang/debug/empathy/ring-attention-pytorch/ring_attention_pytorch/distributed.py", line 27, in all_gather_same_dim<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     dist.all_gather(gathered_tensors, t)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/distributed/_functional_collectives.py", line 1036, in all_gather_inplace<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     output = all_gather_tensor(tensor, 0, group, tag)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/distributed/_functional_collectives.py", line 227, in all_gather_tensor<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     group_name = _resolve_group_name(group, tag)<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/distributed/_functional_collectives.py", line 758, in _resolve_group_name<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks]     raise ValueError(f"Unsupported group type: {type(group)}, {group}")<br />
[rank4]:V0306 22:18:13.253000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [32/0] [__graph_breaks] Traceback (most recent call last):<br />
......<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 1219, in CALL_FUNCTION<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]     self.call_function(fn, args, {})<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/symbolic_convert.py", line 674, in call_function<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]     self.push(fn.call_function(self, args, kwargs))<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/variables/builtin.py", line 719, in call_function<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]     return super().call_function(tx, args, kwargs)<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/variables/base.py", line 352, in call_function<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]     unimplemented(f"call_function {self} {args} {kwargs}")<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]   File "/home/ybliang/local/pytorch/torch/_dynamo/exc.py", line 190, in unimplemented<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks]     raise Unsupported(msg)<br />
[rank4]:V0306 22:17:59.812000 139785056863360 torch/_dynamo/symbolic_convert.py:516] [1/0] [__graph_breaks] torch._dynamo.exc.Unsupported: call_function BuiltinVariable(ValueError) [ConstantVariable(str: "Unsupported group type: <class 'NoneType'>, None")] {}<br />
```</p>
<h3>Versions</h3>
<p>N/A</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:26:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/121386</guid>
    </item>
    <item>
      <title>[WIP][inductor] padding</title>
      <link>https://github.com/pytorch/pytorch/pull/120758</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #120758<br />
* #123327</p>
<p>This is not fully ready for review yet. Sending it out in case people want to give early feedback. Also it can allow me run CI perf test earlier.</p>
<p>By testing BlenderbotSmallForConditionalGeneration I already see 2.5ms speedup. I feel the solution can be further improved to get more speedup.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang @jansel @Chillee @eellison </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames</p>]]></description>
      <pubDate>Tue, 27 Feb 2024 17:00:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120758</guid>
    </item>
    <item>
      <title>Torch 2.1 compile + FSDP (mixed precision) + LlamaForCausalLM: `RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'.`</title>
      <link>https://github.com/pytorch/pytorch/issues/111317</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I am getting the following error when training LlamaForCausalLM with torch 2.1 and FSDP (mixed precision) and torch.compile. Same exact code works when torch.compile disabled or when torch 2.0.1 is used. I also tried enabling and disabling amp autocast, it doesn't matter and the same error happens.</p>
<p><code>RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'.
Please ensure that the gradient and the tensor have the same dtype</code></p>
<p>I am using a docker image, error happens in <strong>Environment 2</strong> which is provided in the <strong>Versions</strong> section.</p>
<h3>Error logs</h3>
<p>```<br />
  0%|          | 0/5 [00:00&lt;?, ?it/s]Traceback (most recent call last):<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
Traceback (most recent call last):<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
    main()<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    outputs = model(<strong>batch)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    main()<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    outputs = model(</strong>batch)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
Traceback (most recent call last):<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
Traceback (most recent call last):<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
    Traceback (most recent call last):<br />
return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
Traceback (most recent call last):<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
    main()<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        return forward_call(*args, </strong>kwargs)outputs = model(**batch)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    main()  <br />
main()<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    outputs = model(</strong>batch)<br />
Traceback (most recent call last):<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        outputs = model(<strong>batch)<br />
return fn(*args, </strong>kwargs)  <br />
main()  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl</p>
<p>File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 511, in <module><br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    outputs = model(</em><em>batch)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
        return self._call_impl(<em>args, </em><em>kwargs)return self._call_impl(</em>args, **kwargs)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    main()<br />
  File "/workspace/workdir/models/pretraining/huggingface/llama/pretrain_fsdp_torch2.1_minimal.py", line 387, in main<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
    output = self._fsdp_wrapped_module(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return self._call_impl(*args, </strong>kwargs)<br />
      File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
outputs = model(<strong>batch)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
        return forward_call(<em>args, </em><em>kwargs)<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
return forward_call(*args, </strong>kwargs)  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        return fn(</em>args, <strong>kwargs)  <br />
return fn(*args, </strong>kwargs)return self._call_impl(<em>args, </em>*kwargs)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
        return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
return fn(</em>args, <strong>kwargs)  <br />
output = self._fsdp_wrapped_module(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return self._call_impl(*args, </strong>kwargs)  <br />
return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
        return forward_call(</em>args, <strong>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
return fn(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        return self._call_impl(<em>args, </em><em>kwargs)<br />
      File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
return self._call_impl(</em>args, <strong>kwargs)return self._call_impl(*args, </strong>kwargs)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 328, in _fn<br />
    return model_forward(</em>args, <strong>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py", line 17, in inner<br />
        return self._call_impl(</em>args, <strong>kwargs)<br />
return fn(*args, </strong>kwargs)  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return convert_to_fp32(self.model_forward(<em>args, </em><em>kwargs))<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    output = self._fsdp_wrapped_module(</em>args, <strong>kwargs)<br />
      File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
return func(*args, </strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
    return model_forward(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
    output = self._fsdp_wrapped_module(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    output = self._fsdp_wrapped_module(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    outputs = self.model(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return convert_to_fp32(self.model_forward(*args, </strong>kwargs))<br />
    return self._call_impl(<em>args, </em>*kwargs)  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return func(<em>args, </em><em>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
        return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
output = self._fsdp_wrapped_module(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 839, in forward<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    outputs = self.model(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        return forward_call(</em>args, <strong>kwargs)output = self._fsdp_wrapped_module(*args, </strong>kwargs)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return model_forward(*args, </strong>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
        return self._call_impl(</em>args, <strong>kwargs)    return model_forward(*args, </strong>kwargs)<br />
return model_forward(<em>args, </em>*kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong></p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
    return convert_to_fp32(self.model_forward(<em>args, </em><em>kwargs))<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast<br />
    return func(</em>args, <strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    layer_outputs = decoder_layer(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return convert_to_fp32(self.model_forward(</em>args, <strong>kwargs))<br />
    return convert_to_fp32(self.model_forward(*args, </strong>kwargs))  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast<br />
        return func(<em>args, </em><em>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
return func(</em>args, <strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
    return model_forward(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
    outputs = self.model(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 659, in forward<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
        outputs = self.model(<br />
outputs = self.model(  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        return convert_to_fp32(self.model_forward(<em>args, </em>*kwargs))layer_outputs = decoder_layer(</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast<br />
        return model_forward(<em>args, </em><em>kwargs)<br />
  File "/workspace/accelerate/src/accelerate/utils/operations.py", line 647, in <strong>call</strong><br />
return func(</em>args, <strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
    return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
    return convert_to_fp32(self.model_forward(</em>args, <strong>kwargs))<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast<br />
        return self._call_impl(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return func(*args, </strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 1034, in forward<br />
    outputs = self.model(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    args, kwargs = _pre_forward(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
    unshard_fn(state, handle)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
        return forward_call(</em>args, <strong>kwargs)<br />
return forward_call(*args, </strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
      File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
outputs = self.model(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
        layer_outputs = decoder_layer(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
return self._call_impl(</em>args, **kwargs)<br />
      File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
_unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard<br />
    args, kwargs = _pre_forward(<br />
        layer_outputs = decoder_layer(layer_outputs = decoder_layer(  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    ran_pre_unshard = handle.pre_unshard()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    unshard_fn(state, handle)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
    return self._call_impl(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
    return forward_call(*args, </strong>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
    _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
    ret = self._writeback_orig_params()  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard</p>
<pre><code>    return self._call_impl(*args, **kwargs)return self._call_impl(*args, **kwargs)
</code></pre>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
        return func(<em>args, </em>*kwargs)ran_pre_unshard = handle.pre_unshard()</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 2202, in _writeback_orig_params<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/workspace/transformers/src/transformers/models/llama/modeling_llama.py", line 921, in forward<br />
    layer_outputs = decoder_layer(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
        return forward_call(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
    layer_outputs = decoder_layer(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl<br />
    ret = self._writeback_orig_params()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context<br />
    args, kwargs = _pre_forward(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward<br />
        return self._call_impl(</em>args, <strong>kwargs)return func(*args, </strong>kwargs)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 2202, in _writeback_orig_params<br />
    args, kwargs = _pre_forward(<br />
    args, kwargs = _pre_forward(  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward<br />
    unshard_fn(state, handle)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
    flat_param.grad = flat_param_grad<br />
RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'. Please ensure that the gradient and the tensor have the same dtype<br />
Traceback (most recent call last):<br />
        unshard_fn(state, handle)unshard_fn(state, handle)</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
    _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard<br />
    return self._call_impl(<em>args, </em>*kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl<br />
        _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
    ran_pre_unshard = handle.pre_unshard()  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard</p>
<p>_unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard<br />
    return forward_call(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
    ran_pre_unshard = handle.pre_unshard()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    ran_pre_unshard = handle.pre_unshard()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    flat_param.grad = flat_param_grad<br />
RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'. Please ensure that the gradient and the tensor have the same dtype<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 825, in forward<br />
    args, kwargs = _pre_forward(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward<br />
    ret = self._writeback_orig_params()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context<br />
    return func(*args, </strong>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 2202, in _writeback_orig_params<br />
    unshard_fn(state, handle)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
        ret = self._writeback_orig_params()ret = self._writeback_orig_params()</p>
<p>File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context<br />
    args, kwargs = _pre_forward(<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 429, in _pre_forward<br />
        return func(<em>args, </em><em>kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 2202, in _writeback_orig_params<br />
return func(</em>args, **kwargs)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 2202, in _writeback_orig_params<br />
    _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard<br />
    unshard_fn(state, handle)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 464, in _pre_forward_unshard<br />
    ran_pre_unshard = handle.pre_unshard()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 336, in _unshard<br />
    ran_pre_unshard = handle.pre_unshard()<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/flat_param.py", line 1194, in pre_unshard<br />
    flat_param.grad = flat_param_grad<br />
RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'. Please ensure that the gradient and the tensor have the same dtype<br />
        flat_param.grad = flat_param_grad<br />
flat_param.grad = flat_param_grad<br />
    ret = self._writeback_orig_params()<br />
RuntimeError  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context<br />
RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'. Please ensure that the gradient and the tensor have the same dtype: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'. Please ensure that the gradient and the tensor have the same dtype<br />
============================================================<br />
pretrain_fsdp_torch2.1_minimal.py FAILED</p>
<hr />
<p>Failures:<br />
[1]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 2 (local_rank: 2)<br />
  exitcode  : 1 (pid: 348906)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
[2]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 3 (local_rank: 3)<br />
  exitcode  : 1 (pid: 348907)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
[3]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 4 (local_rank: 4)<br />
  exitcode  : 1 (pid: 348908)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
[4]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 5 (local_rank: 5)<br />
  exitcode  : 1 (pid: 348909)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
[5]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 6 (local_rank: 6)<br />
  exitcode  : 1 (pid: 348910)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
[6]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 7 (local_rank: 7)<br />
  exitcode  : 1 (pid: 348911)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html</p>
<hr />
<p>Root Cause (first observed failure):<br />
[0]:<br />
  time      : 2023-10-14_20:44:01<br />
  host      : ec3b2a9a542c<br />
  rank      : 1 (local_rank: 1)<br />
  exitcode  : 1 (pid: 348905)<br />
  error_file: <N/A><br />
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html<br />
============================================================<br />
```</p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p><strong>Environment 1</strong></p>
<p>```<br />
PyTorch version: 2.0.1+cu117<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.7<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.27.4<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.2.128<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A100-SXM4-80GB<br />
GPU 1: NVIDIA A100-SXM4-80GB<br />
GPU 2: NVIDIA A100-SXM4-80GB<br />
GPU 3: NVIDIA A100-SXM4-80GB<br />
GPU 4: NVIDIA A100-SXM4-80GB<br />
GPU 5: NVIDIA A100-SXM4-80GB<br />
GPU 6: NVIDIA A100-SXM4-80GB<br />
GPU 7: NVIDIA A100-SXM4-80GB</p>
<p>Nvidia driver version: 535.104.12<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      48 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             224<br />
On-line CPU(s) list:                0-223<br />
Vendor ID:                          AuthenticAMD<br />
Model name:                         AMD EPYC 7713 64-Core Processor<br />
CPU family:                         25<br />
Model:                              1<br />
Thread(s) per core:                 1<br />
Core(s) per socket:                 224<br />
Socket(s):                          1<br />
Stepping:                           1<br />
BogoMIPS:                           3999.99<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr wbnoinvd arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm arch_capabilities<br />
Virtualization:                     AMD-V<br />
Hypervisor vendor:                  KVM<br />
Virtualization type:                full<br />
L1d cache:                          14 MiB (224 instances)<br />
L1i cache:                          14 MiB (224 instances)<br />
L2 cache:                           112 MiB (224 instances)<br />
L3 cache:                           3.5 GiB (224 instances)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-223<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Mitigation; safe RET<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.23.1<br />
[pip3] onnx==1.14.0<br />
[pip3] pytorch-quantization==2.1.2<br />
[pip3] torch==2.0.1<br />
[pip3] torch-tensorrt==2.0.0.dev0<br />
[pip3] torchdata==0.7.0a0<br />
[pip3] torchtext==0.16.0a0<br />
[pip3] torchvision==0.16.0<br />
[pip3] triton==2.0.0<br />
[conda] Could not collect<br />
```</p>
<p><strong>Environment 2</strong></p>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.1.0+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.27.4<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.2.128<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA A100-SXM4-80GB<br />
GPU 1: NVIDIA A100-SXM4-80GB<br />
GPU 2: NVIDIA A100-SXM4-80GB<br />
GPU 3: NVIDIA A100-SXM4-80GB<br />
GPU 4: NVIDIA A100-SXM4-80GB<br />
GPU 5: NVIDIA A100-SXM4-80GB<br />
GPU 6: NVIDIA A100-SXM4-80GB<br />
GPU 7: NVIDIA A100-SXM4-80GB</p>
<p>Nvidia driver version: 535.104.12<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      48 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             224<br />
On-line CPU(s) list:                0-223<br />
Vendor ID:                          AuthenticAMD<br />
Model name:                         AMD EPYC 7713 64-Core Processor<br />
CPU family:                         25<br />
Model:                              1<br />
Thread(s) per core:                 1<br />
Core(s) per socket:                 224<br />
Socket(s):                          1<br />
Stepping:                           1<br />
BogoMIPS:                           3999.99<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr wbnoinvd arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm arch_capabilities<br />
Virtualization:                     AMD-V<br />
Hypervisor vendor:                  KVM<br />
Virtualization type:                full<br />
L1d cache:                          14 MiB (224 instances)<br />
L1i cache:                          14 MiB (224 instances)<br />
L2 cache:                           112 MiB (224 instances)<br />
L3 cache:                           3.5 GiB (224 instances)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-223<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Mitigation; safe RET<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.23.1<br />
[pip3] onnx==1.14.0<br />
[pip3] pytorch-quantization==2.1.2<br />
[pip3] torch==2.1.0<br />
[pip3] torch-tensorrt==2.0.0.dev0<br />
[pip3] torchdata==0.7.0a0<br />
[pip3] torchtext==0.16.0a0<br />
[pip3] torchvision==0.16.0<br />
[pip3] triton==2.1.0+e621604<br />
[conda] Could not collect<br />
```</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @wanchaol @fduwjj @wz337 @kiukchung @d4l3k @lucasllc @tianyu-l @gchanan @kadeng</p>]]></description>
      <pubDate>Sat, 14 Oct 2023 18:20:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/111317</guid>
    </item>
  </channel>
</rss>
