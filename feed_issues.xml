<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[inductor] Remove `divisible_by_8` from kernel argument descriptor</title>
      <link>https://github.com/pytorch/pytorch/pull/121566</link>
      <description><![CDATA[<p>Triton has deprecated the <code>divisible_by_8</code> hint.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 18:23:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121566</guid>
    </item>
    <item>
      <title>[inductor] Update triton pin</title>
      <link>https://github.com/pytorch/pytorch/pull/121268</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121268<br />
* #121568<br />
* #121556</p>]]></description>
      <pubDate>Tue, 05 Mar 2024 13:25:04 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121268</guid>
    </item>
    <item>
      <title>Inductor oneDNN Graph integration</title>
      <link>https://github.com/pytorch/pytorch/pull/120301</link>
      <description><![CDATA[<h2>Inductor-CPU integration with oneDNN Graph (base PR)</h2>
<p>Uses Inductor pattern-matcher to offload some compute to oneDNN Graph library, which has fusions for compute-intensive ops such as MHA &amp; MLP. This is the first PR that adds oneDNN Graph support, so that subsequent PRs can add more pattern replacements that use oneDNN Graph. This PR only adds a replacement for one pattern (GPT2 MHA/SDPA pattern, which is different for batch size 1 &amp; batch size &gt; 1). Currently, functionality is limited to inference &amp; only Linux is supported.</p>
<h4>Details</h4>
<p>In Inductor joint graph passes, if <code>config.onednn_graph</code> is enabled, or the env variable <code>TORCHINDUCTOR_ONEDNN_GRAPH=1</code> is used on a machine that supports some advanced ISAs, then if some specific patterns are matched, the corresponding ops are handled by oneDNN Graph. However, if oneDNN Graph is unable to match a pattern, then it's handled by the same ops that'd have handled it sans oneDNN Graph.</p>
<p>As opposed to other oneDNN kernels (created with oneDNN primitives API) &amp; oneMKL kernels, these kernels are compiled at Inductor compilation-time (in case of shape-mismatch at runtime, compilation for those static input shapes will occur again).</p>
<p>BF16 dtype requires a machine to support at least <code>AVX512_BF16</code> ISA. <code>AMX</code> ISA is also used, if available.<br />
While FP32 dtype support may also work on Xeon SP generation 1 (SkyLake SP), which does not support <code>AVX512_VNNI</code> ISA, the lowest machine configuration for which this functionality will be enabled in this PR is Xeon SP generation 2 (Cascade Lake), but may be changed in a subsequent PR to include Skylake SP platform support as well. The reason to not support Skylake SP at this point is that some consumer-grade CPUs also support the same AVX512 ISAs as SkyLake SP, but this implementation has not been verified on consumer-grade CPUs.</p>
<h4>Summary of other changes</h4>
<ul>
<li>Builds the Graph Compiler backend of oneDNN Graph, which is included in <code>libdnnl.a</code></li>
<li>Adds extensible support for more patterns to be handled with oneDNN Graph in subsequent PRs</li>
<li>Adds an API to check if oneDNN Graph was built.</li>
<li>Serialization of SDPA patterns can now be done for some corner-cases, such as only inference.</li>
<li>Serialized attention patterns  can also have <code>getitem</code> in them, which should be replaced by <code>operator.getitem</code>.</li>
<li>Currently, oneDNN Graph is not used if dynamic shapes are provided at compile time.</li>
<li>In oneDNN v3.3.x, the GPT2 MHA pattern needs an attention mask, so in order to match cases of attention mask not being present, an attention mask of all zeros is being created in this implementation. The performance is still better compared to the default implementation, which means it'd be even better with oneDNN v3.5, which would make attention mask optional for GPT2 MHA pattern.</li>
</ul>
<h3>Performance data</h3>
<p>This PR accelerates inference of <code>hf_GPT2</code> by offloading its MHA computation to oneDNN Graph. Other models did not suffer any regression/breakage.</p>
<p>These performance datapoints were collected for a single TorchBench run (TorchBench may have run the workload multiple times) of each configuration, and were not cherry-picked, so it's possible that performance may be even better with this feature enabled -</p>
<p>|MODEL |BATCH SIZE | DTYPE | BEFORE: Speedup over eager-mode with the default Inductor implementation | AFTER: Speedup over eager mode with the default Inductor implementation| Perf boost = (AFTER - BEFORE)/BEFORE * 100|<br />
|--------------------------|-------------|---------|-----------------------------|--------------------------|------------|<br />
|hf_GPT2| 1 | FP32 | 1.522x | 1.784x| 17.21%|<br />
|hf_GPT2| 1 | BF16 (AMP) | 1.795x | 2.602x| 44.91%|<br />
|hf_GPT2| 2 | FP32 |  1.313x |1.629x | 24.06%|<br />
|hf_GPT2|2| BF16 (AMP) | 1.556x | 2.107x | 35.41%|<br />
|hf_GPT2_large| 1 | FP32 | 1.380x |1.510x | 9.4%|<br />
|hf_GPT2_large| 1 | BF16 (AMP) | 1.208x | 1.653x | 36.83%|<br />
|hf_GPT2_large| 2 | FP32 | 1.188x | 1.508x | 26.93%|<br />
|hf_GPT2_large|2| BF16 (AMP) | 0.991x | 1.771x | 78.70%|</p>
<p>Config: 48 physical cores of one socket of Intel Xeon Platinum 8468H (Xeon SP 4th gen a.k.a. Sapphire Rapids) . Only one logical core of a physical core was used. Intel OpenMP &amp; tcmalloc were preloaded.</p>
<p>Example of running the workload:<br />
<code>TORCHINDUCTOR_ONEDNN_GRAPH=1 OMP_NUM_THREADS=48 MKL_NUM_THREADS=48 numactl --membind=0 -C 0-47 python benchmarks/dynamo/torchbench.py --performance --inference --inductor --float32 -dcpu --only hf_GPT2 --freezing --batch-size 2</code></p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @ashokei @jingxu10 @gujinghui @PenghuiCheng @jianyuh @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @voznesenskym @penguinwu @EikanWang @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 21 Feb 2024 01:26:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/120301</guid>
    </item>
    <item>
      <title>[inductor][perf] Replace inefficient use of set.</title>
      <link>https://github.com/pytorch/pytorch/pull/119746</link>
      <description><![CDATA[<p>It makes no sense to create a set in <code>O(N)</code> time and memory (including the case when the very first occurrence would match) to perform an amortized <code>O(1)</code> lookup with high constant factor instead of doing a simple scan in <code>O(N)</code> time and <code>O(1)</code> memory and lower constant factor.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Mon, 12 Feb 2024 19:22:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119746</guid>
    </item>
    <item>
      <title>Add CUTLASS kernel as choice for _int_mm() Inductor autotuning</title>
      <link>https://github.com/pytorch/pytorch/pull/119685</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #119685</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 12 Feb 2024 07:05:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119685</guid>
    </item>
    <item>
      <title>Check for releasing GIL at compiletime</title>
      <link>https://github.com/pytorch/pytorch/pull/116695</link>
      <description><![CDATA[<p>Introduce <code>conditional_gil_scoped_release</code> and use it in <code>wrap_pybind_function*</code> to avoid a runtime branch making the code cleaner and faster.</p>
<p>@albanD This is the GIL change extracted from #112607 as discussed.</p>
<p>Also fixes a potential use of a moved-from object introduced in #116560:<br />
- <code>f</code> is captured by value in a lambda that may be used called times<br />
- After <code>std::move(f)</code> the lambda is not safe to call anymore</p>
<p>CC @cyyever for that change</p>]]></description>
      <pubDate>Wed, 03 Jan 2024 04:21:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116695</guid>
    </item>
    <item>
      <title>dont treat size==1,stride==0 from inductor as diverging strides from eager w.r.t dynamic shapes</title>
      <link>https://github.com/pytorch/pytorch/pull/116435</link>
      <description><![CDATA[<p>Fixes https://github.com/pytorch/pytorch/issues/116433.</p>
<p>Putting this out as a tentative fix, but more discussion is in the github issue.</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #116435</p>]]></description>
      <pubDate>Tue, 26 Dec 2023 14:16:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116435</guid>
    </item>
  </channel>
</rss>
