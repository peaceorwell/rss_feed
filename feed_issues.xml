<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[2.3 dynamic shapes] backend='inductor' raised: LoweringException: AssertionError: indices must be int64, byte or bool. Got [torch.float32]</title>
      <link>https://github.com/pytorch/pytorch/issues/124006</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I installed the final RC version of pytorch 2.3, and ran the following code, errors occurs.</p>
<p>```</p>
<h1>minified.py</h1>
<p>import torch<br />
from torch import nn<br />
import os<br />
from torch import distributed as dist<br />
from torch.nn.parallel import DistributedDataParallel as DDP<br />
import torch._dynamo<br />
from torch.utils.checkpoint import checkpoint</p>
<h1>torch._dynamo.config.optimize_ddp = False</h1>
<p>device = torch.device(f'cuda:{0}')</p>
<p>rank = os.environ.get('LOCAL_RANK','-1')<br />
if int(rank) &gt;=0:<br />
    device = torch.device(f'cuda:{rank}')<br />
    print(device)<br />
    torch.cuda.set_device(device)<br />
    dist.init_process_group(backend='nccl', init_method='env://')</p>
<p>def apply_rotary_v2(x, pos_cos,pos_sin):<br />
    x1, x2 = x[..., 0::2].float(), x[..., 1::2].float() # b,n,h,d<br />
    return torch.cat([x1 * pos_cos - x2 * pos_sin, x2 * pos_cos + x1 * pos_sin], dim=-1).type_as(x)</p>
<p>class ResolutionDown(nn.Module):<br />
    r""" Patch Merging Layer.</p>
<pre><code>Args:
    output_resolution (tuple[int]): Resolution of output feature.
    dim (int): Number of output channels.
    norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
"""

def __init__(self, input_dim, output_dim, norm_layer=nn.LayerNorm, output_NCHW=False):
    super().__init__()
    self.output_dim = output_dim
    self.input_dim = input_dim
    self.norm = norm_layer(4 * input_dim) if norm_layer else nn.Identity()
    self.reduction = nn.Linear(4 * input_dim, output_dim, bias=False)
    self.output_NCHW = output_NCHW

def forward(self, x, last_group=None, cur_group=None):
    """
    x: B, H*W, C
    """

    x = torch.nn.functional.pixel_unshuffle(x, 2)
    x = x.permute(0, 2, 3, 1)

    x = self.norm(x)
    x = self.reduction(x)

    if self.output_NCHW:
        x = x.permute(0, 3, 1, 2)

    return x
</code></pre>
<p>class Mlp(nn.Module):<br />
    """ MLP as used in Vision Transformer, MLP-Mixer and related networks<br />
    """<br />
    def <strong>init</strong>(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU,<br />
                 fc2_bias=True,<br />
                 drop=0.):<br />
        super().<strong>init</strong>()<br />
        out_features = out_features or in_features<br />
        hidden_features = hidden_features or in_features<br />
        self.fc1 = nn.Linear(in_features, hidden_features)<br />
        self.act = act_layer()<br />
        self.drop1 = nn.Dropout(drop)<br />
        self.fc2 = nn.Linear(hidden_features, out_features, bias=fc2_bias)<br />
        self.drop2 = nn.Dropout(drop)</p>
<pre><code>def forward(self, x):
    x = self.fc1(x)
    x = self.act(x)
    x = self.drop1(x)
    x = self.fc2(x)
    x = self.drop2(x)
    return x
</code></pre>
<p>class RMSNorm(torch.nn.Module):<br />
    def <strong>init</strong>(self, dim: int, eps: float = 1e-5):<br />
        super().<strong>init</strong>()<br />
        self.eps = eps<br />
        self.weight = nn.Parameter(torch.ones(dim))</p>
<pre><code>def _norm(self, x):
    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

def forward(self, x):
    output = self._norm(x.float()).type_as(x)
    return output * self.weight
</code></pre>
<p>class WindowAttention(nn.Module):<br />
    r""" Window based multi-head self attention (W-MSA) module with relative position bias.<br />
    It supports both of shifted and non-shifted window.</p>
<pre><code>Args:
    dim (int): Number of input channels.
    window_size (int): The height and width of the window.
    num_heads (int): Number of attention heads.
    qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
    attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
    proj_drop (float, optional): Dropout ratio of output. Default: 0.0
"""

def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0.,
             shift_h=True,
             rotary_pos=False, fc2_bias=True, qk_norm_factor=1e-4,
             proj_drop=0.):

    super().__init__()
    self.dim = dim
    self.rotary_pos = rotary_pos
    self.window_size = window_size  # Wh, Ww
    self.shift_size = window_size // 4
    self.num_heads = num_heads
    head_dim = dim // num_heads
    self.scale = head_dim ** -0.5
    self.qk_norm_factor = qk_norm_factor

    self.shift_h = shift_h

    # define a parameter table of relative position bias
    assert rotary_pos, 'must be rotary pos embed'

    self.q = nn.Linear(dim, dim, bias=qkv_bias)
    self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
    self.attn_drop_ratio = attn_drop
    self.proj = nn.Linear(dim, dim, bias=fc2_bias)
    self.proj_drop = nn.Dropout(proj_drop)
    self.softmax = nn.Softmax(dim=-1)

def forward(self, x, H, W, pos_embed_q, pos_embed_kv, mask):
    """
    Args:
        x: input features with shape of (num_windows*B, N, C)
        mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
    """
    D = self.dim
    C = D//self.num_heads
    nH = self.num_heads
    if self.shift_h:
        window_size1 = self.window_size //2
        window_size2 = self.window_size
    else:
        window_size1 = self.window_size
        window_size2 = self.window_size // 2
    G=H*W//window_size1//window_size2
    N= window_size1 * window_size2

    # ========== window_partition ==============
    q = self.q(x).view(-1, H//window_size1, window_size1, W//window_size2, window_size2, nH, C) # 5,6
    q = q.permute(0,1,3,5,2,4,6).reshape(-1,G,nH,N,C) # B,G,H,N,C

    kv = self.kv(x) # B,H,W,2C
    kv1 = torch.roll(kv,(-self.shift_size,),(1 if self.shift_h else 2,))
    kv2 = torch.roll(kv,(self.shift_size,),(1 if self.shift_h else 2,))
    kv = torch.stack([kv1,kv2]).view(2,-1,H//window_size1,window_size1,W//window_size2,window_size2, #0,1,2,3,4,5
                                     2,nH,C) #6,7,8
    kv = kv.permute(6, 1, 2,4, 7, 0,3,5, 8).reshape(2, -1, G, nH,2*N, C)
    k,v = kv.unbind(0)

    if self.training and self.qk_norm_factor &gt; 0:
        qk_loss = (q ** 2).mean() * self.qk_norm_factor + (k ** 2).mean() * self.qk_norm_factor
    else:
        qk_loss = 0

    q = apply_rotary_v2(q, *pos_embed_q.unbind(0))
    k = apply_rotary_v2(k, *pos_embed_kv.unbind(0))

    if mask is not None:
        if self.training and self.attn_drop_ratio &gt; 0:
            with torch.no_grad():
                nmask = torch.rand(x.shape[0], G, 1, N, N * 2, device=q.device) &gt;= self.attn_drop_ratio
                nmask = torch.where(nmask, 0, -torch.inf)
                mask = mask + nmask
        mask = mask.type_as(q)
    x = torch.nn.functional.scaled_dot_product_attention(q, k, v.type_as(q), attn_mask=mask,
                                                         dropout_p=0)

    # ==============  window_reverse ==============
    x=x.view(-1,H//window_size1,W//window_size2,nH,window_size1,window_size2,C)
    x=x.permute(0,1,4,2,5,3,6).reshape(-1,H,W,D)

    x = self.proj(x)
    x = self.proj_drop(x)
    return x, qk_loss
</code></pre>
<p>class SwinTransformerBlock(nn.Module):<br />
    r""" Swin Transformer Block.</p>
<pre><code>Args:
    dim (int): Number of input channels.
    input_resolution (tuple[int]): Input resulotion.
    num_heads (int): Number of attention heads.
    window_size (int): Window size.
    shift_size (int): Shift size for SW-MSA.
    mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
    qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
    drop (float, optional): Dropout rate. Default: 0.0
    attn_drop (float, optional): Attention dropout rate. Default: 0.0
    drop_path (float, optional): Stochastic depth rate. Default: 0.0
    act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
    norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
"""

def __init__(self, dim, input_resolution, num_heads, window_size=8, shift_h=True,
             mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,
             qk_norm_factor=1e-4,
             act_layer=nn.GELU, norm_layer=nn.LayerNorm):
    super().__init__()
    self.dim = dim
    self.input_resolution = input_resolution
    self.num_heads = num_heads
    self.window_size = window_size
    self.mlp_ratio = mlp_ratio

    self.norm1 = norm_layer(dim)
    self.shift_h = shift_h

    self.attn = WindowAttention(
        dim, window_size=self.window_size, num_heads=num_heads, qkv_bias=qkv_bias,
        shift_h = shift_h,
        rotary_pos=True, fc2_bias=True, qk_norm_factor=qk_norm_factor,
        attn_drop=attn_drop, proj_drop=drop)

    self.drop_path = nn.Identity()
    self.norm2 = norm_layer(dim)
    mlp_hidden_dim = int(dim * mlp_ratio)
    self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop,
                   fc2_bias=True)

def forward(self, x, H, W, pos_embed_q,pos_embed_kv, mask):
    shortcut = x

    x = self.norm1(x) # B,H,W,C

    x, qk_loss = self.attn(x, H, W, pos_embed_q,pos_embed_kv, mask)

    x = shortcut + self.drop_path(x)

    mlp = self.mlp(self.norm2(x))
    x = x + self.drop_path(mlp)

    return x
</code></pre>
<p>class SwinLayer(nn.Module):<br />
    def <strong>init</strong>(self, input_dim, dim, input_resolution, depth, num_heads,<br />
                 window_size=16,<br />
                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0., qk_norm_factor=1e-4,<br />
                 norm_layer=nn.LayerNorm, use_checkpoint=False):</p>
<pre><code>    super().__init__()
    self.dim = dim
    self.num_heads = num_heads
    self.head_dim = dim // num_heads
    self.input_resolution = (input_resolution,input_resolution)
    self.depth = depth
    self.use_checkpoint = use_checkpoint
    self.norm = norm_layer(dim)

    self.pos_embed_qh = nn.Parameter(
        torch.rand(2, 1, num_heads, window_size*window_size//2, dim//num_heads//2), requires_grad=False)
    self.pos_embed_qw = nn.Parameter(
        torch.rand(2, 1, num_heads, window_size*window_size//2, dim//num_heads//2), requires_grad=False)

    self.pos_embed_kvh = nn.Parameter(
        torch.rand(2, 1, num_heads, window_size*window_size, dim//num_heads//2), requires_grad=False)

    self.pos_embed_kvw = nn.Parameter(
        torch.rand(2, 1, num_heads, window_size*window_size, dim//num_heads//2), requires_grad=False)

    self.window_size = window_size
    self.downsample = ResolutionDown(input_dim, dim, norm_layer=norm_layer)

    self.blocks = nn.ModuleList([
        SwinTransformerBlock(
            input_resolution=input_resolution,
            dim=dim, num_heads=num_heads, window_size=window_size,
            shift_h = i % 2 == 0,
            mlp_ratio=mlp_ratio, qk_norm_factor=qk_norm_factor,
            qkv_bias=qkv_bias, drop=drop,
            attn_drop=attn_drop[i] if isinstance(attn_drop,list) else attn_drop,
            drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path, norm_layer=norm_layer)
        for i in range(depth)])

    self.window_shift_maskh = nn.Parameter(torch.rand(1,1,1,self.window_size*self.window_size//2,
                                                      self.window_size*self.window_size).float(),
                                           requires_grad=False)
    self.window_shift_maskw = nn.Parameter(torch.rand(1,1,1,self.window_size*self.window_size//2,
                                                      self.window_size*self.window_size).float(),
                                           requires_grad=False)

def forward(self, x):  # shuffle_idx shape: FB,N
    x = self.downsample(x )
    H,W=x.shape[1],x.shape[2]
    # sample_idx shape: B,1,H,W should permute to B,H,W,1 for get_attn_mask/window_partition
    mask_h = self.window_shift_maskh
    mask_w = self.window_shift_maskh


    for idx, blk in enumerate(self.blocks):
        if blk.shift_h:
            mask = mask_h
            pos_embed_q = self.pos_embed_qh
            pos_embed_kv=self.pos_embed_kvh
        else:
            mask = mask_w
            pos_embed_q = self.pos_embed_qw
            pos_embed_kv=self.pos_embed_kvw

        x = torch.utils.checkpoint.checkpoint(blk,x,H,W, pos_embed_q,pos_embed_kv, mask, use_reentrant=True)

    x = self.norm(x)
    return x.permute(0, 3, 1, 2)

def get_num_block(self):
    return len(self.blocks)
</code></pre>
<p>model = nn.Sequential(SwinLayer(3,128,16,2,2),<br />
                      SwinLayer(128,256,16,2,4),)<br />
model = model.cuda()<br />
print(model)</p>
<p>if int(rank) &gt;=0:<br />
    model = DDP(model.cuda(),device_ids=[int(rank)], output_device=int(rank))<br />
optimizer = torch.optim.AdamW(model.parameters())</p>
<p>model = torch.compile(model)</p>
<p>x = torch.rand(2,3,256,256).cuda()<br />
x.requires_grad=True<br />
with torch.cuda.amp.autocast():<br />
    y=model(x)<br />
y[0].sum().backward()</p>
<p>optimizer.step()<br />
optimizer.zero_grad()<br />
torch.cuda.synchronize()</p>
<p>print('test 1 done')</p>
<p>x = torch.rand(2,3,192,192).cuda()<br />
x.requires_grad=True<br />
with torch.cuda.amp.autocast():<br />
    y=model(x)<br />
y[0].sum().backward()</p>
<p>optimizer.step()<br />
optimizer.zero_grad()<br />
torch.cuda.synchronize()<br />
print('test 2 done')</p>
<p>```</p>
<h3>Versions</h3>
<p>PyTorch version: 2.3.0+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.16.3<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.10.13 (main, Dec 19 2023, 08:15:18) [GCC 9.4.0] (64-bit runtime)<br />
Python platform: Linux-4.15.0-189-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: Quadro RTX 6000<br />
GPU 1: Quadro RTX 6000<br />
GPU 2: Quadro RTX 6000<br />
GPU 3: Quadro RTX 6000<br />
GPU 4: Quadro RTX 6000<br />
GPU 5: Quadro RTX 6000<br />
GPU 6: Quadro RTX 6000<br />
GPU 7: Quadro RTX 6000</p>
<p>Nvidia driver version: 535.54.03<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
CPU(s):                          64<br />
On-line CPU(s) list:             0-63<br />
Thread(s) per core:              2<br />
Core(s) per socket:              16<br />
Socket(s):                       2<br />
NUMA node(s):                    2<br />
Vendor ID:                       GenuineIntel<br />
CPU family:                      6<br />
Model:                           85<br />
Model name:                      Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz<br />
Stepping:                        7<br />
CPU MHz:                         999.997<br />
CPU max MHz:                     3900.0000<br />
CPU min MHz:                     1000.0000<br />
BogoMIPS:                        4600.00<br />
Virtualization:                  VT-x<br />
L1d cache:                       1 MiB<br />
L1i cache:                       1 MiB<br />
L2 cache:                        32 MiB<br />
L3 cache:                        44 MiB<br />
NUMA node0 CPU(s):               0-15,32-47<br />
NUMA node1 CPU(s):               16-31,48-63<br />
Vulnerability Itlb multihit:     KVM: Mitigation: Split huge pages<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Mitigation; TSX disabled<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==2.3.0<br />
[pip3] torch==2.3.0+cu118<br />
[pip3] torchlaunch==1.0<br />
[pip3] torchvision==0.18.0+cu118<br />
[conda] Could not collect</p>]]></description>
      <pubDate>Sat, 13 Apr 2024 04:29:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124006</guid>
    </item>
    <item>
      <title>[Inductor] Force the parallel depth as outer loop fusion depth</title>
      <link>https://github.com/pytorch/pytorch/pull/123899</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123899</p>
<p><strong>Summary</strong><br />
Fix issue: https://github.com/pytorch/pytorch/issues/123801 which brings performance regression of <code>pyhpc_turbulent_kinetic_energy</code> after outer loop fusion.</p>
<p><strong>Root Cause</strong></p>
<ul>
<li><a href="https://gist.github.com/leslie-fang-intel/54fe21ac8871fc63b9bf20fdb6edf209">Generated Kernel before Outer Loop Fusion</a></li>
<li>Taking below 2 kernels as example:<ul>
<li><a href="https://gist.github.com/leslie-fang-intel/54fe21ac8871fc63b9bf20fdb6edf209#file-pyhpc_turbulent_kinetic_energy-before-outer-loop-fusion-py-L255-L305">Kernel 0</a> has 2 loop levels with size [200, 200]. Parallelization is not feasible due to the inefficient number of elements determined by <a href="https://github.com/pytorch/pytorch/blob/aaec97a40364bb6ccfd968f28d309cfff8748d20/torch/_inductor/codegen/cpp.py#L2145-L2164"><code>decide_parallel_depth</code></a>. Therefore, the loop code will be generated with the <code>#pragma omp single</code> directive.</li>
<li><a href="https://gist.github.com/leslie-fang-intel/54fe21ac8871fc63b9bf20fdb6edf209#file-pyhpc_turbulent_kinetic_energy-before-outer-loop-fusion-py-L306-L316">Kernel 1</a> has 3 loop levels with size [200, 200, 26] which has enough number of elements to be parallelized.</li>
</ul>
</li>
<li><a href="https://gist.github.com/leslie-fang-intel/57a497b9d9c6aa82b1c6a686292fc887">Generated Kernel after Outer Loop Fusion</a></li>
<li>After outer loop fusion, <code>Kernel0</code> and <code>Kernel1</code> has been fused into one <a href="https://gist.github.com/leslie-fang-intel/57a497b9d9c6aa82b1c6a686292fc887#file-pyhpc_turbulent_kinetic_energy-after-outer-loop-fusion-py-L261-L497">OuterLoopFusedKernel</a>, the outer loop size is [200, 200] which does not contain enough number of elements to do parallelization.</li>
</ul>
<p>In this PR, we propose a fix for <code>loop_nest</code> with <code>OuterLoopFusedKernel</code>, we will enforce the parallelization with parallel depth as same as <code>outer_loop_fusion_depth</code>.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 11 Apr 2024 18:18:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123899</guid>
    </item>
    <item>
      <title>Utilize aot compile</title>
      <link>https://github.com/pytorch/pytorch/pull/123545</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121727<br />
* #120595<br />
* #116368<br />
* #121387<br />
* <strong>-&gt;</strong> #123545</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 07 Apr 2024 22:12:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123545</guid>
    </item>
    <item>
      <title>[Inductor] `test_mixed_mm_dynamic_shapes_cuda` is broken on multiple architectures</title>
      <link>https://github.com/pytorch/pytorch/issues/122747</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>observed<br />
<code>python test/inductor/test_torchinductor_codegen_dynamic_shapes.py -k test_mixed_mm_dynamic_shapes_cuda</code> failing on A2 (sm86/GA107) and V100 (sm70/GV100), among others.</p>
<h3>Versions</h3>
<p>Nightly build as of 3/26.</p>
<p>cc @ptrblck @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 15:33:38 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122747</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Add bmm support</title>
      <link>https://github.com/pytorch/pytorch/pull/121734</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #123932<br />
* <strong>-&gt;</strong> #121734<br />
* #121497<br />
* #123931<br />
* #123930</p>
<p>Add support for bmm ( batch matrix multiply ) op through the Cutlass backend.</p>
<p>Test Plan:<br />
 * CI<br />
 * Added test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 09:23:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121734</guid>
    </item>
    <item>
      <title>Add registration API for torch.compile-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/121387</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we intend to provide a registration API dedicated to eager-through-torch.compile. The major workflow of this API will be as follows.</p>
<ul>
<li>Load cache</li>
<li>Check cache according to the input tensors</li>
<li>Cache Hit: Run the cached kernel directly</li>
<li>Cache Miss: Run the registered python kernel</li>
</ul>
<p>Currently, this PR always fallback to python kernel now and cache mechanism will be implemented in another PR.</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121727<br />
* #120595<br />
* #116368<br />
* <strong>-&gt;</strong> #121387<br />
* #123545</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121387</guid>
    </item>
    <item>
      <title>Add a cache mechanism to accelerate torch.compile-for-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/116368</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121727<br />
* #120595<br />
* <strong>-&gt;</strong> #116368<br />
* #121387<br />
* #123545</p>
<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to enable a cache mechanism to accelerate <strong>eager-through-torch.compile</strong>. When <strong>eager-through-torch.compile</strong> is enabled, we will store a persistent config to cache the kernel information for the aten operation. </p>
<p>The persistent config consists of two parts - meta_info and kernel_path.</p>
<ul>
<li>meta_info: The input tensors' shape, stride, device type, data type, and symbolic flag.</li>
<li>kernel_path: The path of the kernel produced by Inductor.</li>
</ul>
<p>When an aten operation is registered, the <code>kernel_holder</code> will load the persistent config and parse it to build the cache map;  the meta_info is key, and the kernel library is the value.</p>
<p>Currently, this PR only supports static shape to guard the kernel. From the C++ class perspective, we defined to base class <code>TensorChecker</code> and <code>StaticTensorChecker</code> inherits the base class. In the future, we will implement a class to support dynamic shape by inheriting this base class as well.</p>
<p>Take a <code>mul</code> as an example.<br />
```python<br />
class MulKernel:<br />
  def <strong>init</strong>(self) -&gt; None:<br />
    pass</p>
<p>def <strong>call</strong>(self, <em>args: Any, </em><em>kwargs: Any) -&gt; Any:<br />
    with torch._C._SetExcludeDispatchKeyGuard(torch._C.DispatchKey.Python, False):<br />
      opt_fn = torch.compile(torch.ops.aten.mul, dynamic=False, options={<br />
          "aot_inductor.eager_mode": True,<br />
          "aot_inductor.eager_op_name": "mul_Tensor"<br />
        }<br />
      )<br />
      return opt_fn(</em>args, **kwargs)</p>
<p>torch_compile_op_lib_impl = torch.library.Library("aten", "IMPL")</p>
<p>_, overload_names = torch._C._jit_get_operation("aten::mul")<br />
schema = torch._C._get_schema("aten::mul", overload_name)<br />
reg_name = schema.name<br />
if schema.overload_name:<br />
  reg_name = f"{reg_name}.{schema.overload_name}"<br />
  torch_compile_op_lib_impl.impl(<br />
    reg_name,<br />
    MulKernel(),<br />
    "CUDA",<br />
    compile_mode=True)</p>
<p>a = torch.randn(1024, 1024, device=device)<br />
b = torch.randn(1024, 1024, device=device)<br />
warm_up_iter = 1000<br />
iter = 10000<br />
fn = torch.mul</p>
<h1>Warm up</h1>
<p>for _ in range(warm_up_iter):<br />
    fn(a, b)</p>
<h1>Collect performance</h1>
<p>beg = time.time()<br />
for _ in range(iter):<br />
    fn(a, b)<br />
end = time.time()<br />
print(f"E2E run: {end - beg}")<br />
```<br />
It will produce the config as follows.</p>
<p><code>json
[
    {
        "meta_info": [
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            },
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            }
        ],
        "kernel_path": "/tmp/torchinductor_eikan/e4/ce4jw46i5l2e7v3tvr2pyglpjmahnp7x3hxaqotrvxwoeh5t6qzc.so"
    }
]</code></p>
<p>Performance-wise, we collected mul.Tensor through torch.compile w/ 10000 runs(e2e). The data is as follows. And we will collect data when we support dynamic shape.</p>
<ul>
<li>Eager: ~266.11ms</li>
<li>W/O Cache: ~3455.54ms</li>
<li>W/ Cache and Cache Miss: ~3555.3ms</li>
<li>W/ Cache and Cache Hit: ~267.12ms</li>
</ul>
<p>Hardware:</p>
<ul>
<li>CPU: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz</li>
<li>GPU: CUDA A10</li>
</ul>
<p>Software:</p>
<ul>
<li>PyTorch Version: 39df084001c54cca5fe3174176f9b0206ddb7dcf</li>
<li>GPU Driver Version: 525.147.05</li>
<li>CUDA Version: 12.0</li>
</ul>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 03:21:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116368</guid>
    </item>
  </channel>
</rss>
