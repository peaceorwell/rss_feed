<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>Extend torch.utils._sympy.symbol for more Inductor symbols</title>
      <link>https://github.com/pytorch/pytorch/pull/125419</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #125419<br />
* #125395</p>
<p>I'm still missing a few, cdzq at least</p>
<p>Signed-off-by: Edward Z. Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;">&#101;&#122;&#121;&#97;&#110;&#103;&#64;&#109;&#101;&#116;&#97;&#46;&#99;&#111;&#109;</a></p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 02 May 2024 11:03:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125419</guid>
    </item>
    <item>
      <title>[wip][compiled autograd] runtime wrapper for verbose debugging</title>
      <link>https://github.com/pytorch/pytorch/pull/125417</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #125417</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Thu, 02 May 2024 10:26:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125417</guid>
    </item>
    <item>
      <title>[ignore] Test no parallel compile</title>
      <link>https://github.com/pytorch/pytorch/pull/125408</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #125408</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 02 May 2024 09:36:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125408</guid>
    </item>
    <item>
      <title>[Inductor max autotune] Minor fix for AlgorithmSelectorCache verification.</title>
      <link>https://github.com/pytorch/pytorch/pull/125407</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124928<br />
* <strong>-&gt;</strong> #125407<br />
* #125406<br />
* #124930</p>
<p>Existing benchmark verification code in select_algorithm.py assumes that ATen Kernels are always the first choice. This can lead to errors / exceptions.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Test Plan:<br />
 - CI<br />
- This actually broke some tests in the past for me, until I always ensured that the ATen Kernels are added first ( in mm.py / bmm.py )</p>]]></description>
      <pubDate>Thu, 02 May 2024 08:54:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125407</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Enabled nonzero workspace and Cutlass StreamK</title>
      <link>https://github.com/pytorch/pytorch/pull/125406</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124928<br />
* #125407<br />
* <strong>-&gt;</strong> #125406<br />
* #124930</p>
<p>Enable nonzero workspace and Cutlass StreamK for Inductor Cutlass GEMM ops.</p>
<p>This is a simpler rewrite of my original version of #119005 using @peterbell10 's workspace allocation mechanism from #117992</p>
<p>Test Plan:<br />
 - Additional unit test in test_cutlass_backend.py which specifically tests StreamK GEMM with workspace requirement<br />
 - CI</p>]]></description>
      <pubDate>Thu, 02 May 2024 08:54:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125406</guid>
    </item>
    <item>
      <title>Could not jit compile custom extension in dataparallel mode </title>
      <link>https://github.com/pytorch/pytorch/issues/125403</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Functions <code>torch.utils.cpp_extension.load_inline</code> and <code>torch.utils.cpp_extension.load</code> does not work correctly when called in DataPrallel mode</p>
<p>```<br />
import torch<br />
from torch.utils.cpp_extension import load_inline</p>
<h1>torch extensions cache should be cleared before the test</h1>
<p>if not torch.cuda.is_available() or torch.cuda.device_count() &lt; 2:<br />
    raise RuntimeError("Wrong env for the reproducer.")</p>
<p>class TestModel(torch.nn.Module):<br />
    def forward(self, x):<br />
        code = "int f() {return 2;}"<br />
        module = load_inline(<br />
                name='jit_extension',<br />
                cpp_sources=code,<br />
                functions='f',<br />
                verbose=True)<br />
        return x * module.f()</p>
<p>model = torch.nn.DataParallel(TestModel().cuda())<br />
output = model(torch.ones([10, 1, 1, 1], device="cuda"))<br />
assert torch.all(output == 2.)<br />
<code>The reason is while the first thread is budling the extension, the second one asks `JIT_EXTENSION_VERSIONER` for a current version, and the `JIT_EXTENTION_VERSIONER` returns the version of the first thread build. 
https://github.com/pytorch/pytorch/blob/main/torch/utils/cpp_extension.py#L1677-L1696
Second thread then skips lock file and tries to load non existed extension, failing with an error:</code><br />
Traceback (most recent call last):<br />
  File "/home/dlyakhov/miniconda3/lib/python3.12/runpy.py", line 198, in _run_module_as_main<br />
    return _run_code(code, main_globals, None,<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/miniconda3/lib/python3.12/runpy.py", line 88, in _run_code<br />
    exec(code, run_globals)<br />
  File "/home/dlyakhov/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/<strong>main</strong>.py", line 39, in <module><br />
    cli.main()<br />
  File "/home/dlyakhov/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 430, in main<br />
    run()<br />
  File "/home/dlyakhov/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 284, in run_file<br />
    runpy.run_path(target, run_name="<strong>main</strong>")<br />
  File "/home/dlyakhov/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 321, in run_path<br />
    return _run_module_code(code, init_globals, run_name,<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 135, in _run_module_code<br />
    _run_code(code, mod_globals, init_globals,<br />
  File "/home/dlyakhov/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 124, in _run_code<br />
    exec(code, run_globals)<br />
  File "/home/dlyakhov/Projects/pytorch/repro.py", line 20, in <module><br />
    output = model(torch.ones([10, 1, 1, 1], device="cuda"))<br />
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/Projects/pytorch/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/Projects/pytorch/torch/nn/modules/module.py", line 1541, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/Projects/pytorch/torch/nn/parallel/data_parallel.py", line 185, in forward<br />
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/Projects/pytorch/torch/nn/parallel/data_parallel.py", line 200, in parallel_apply<br />
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/Projects/pytorch/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply<br />
    output.reraise()<br />
  File "/home/dlyakhov/Projects/pytorch/torch/_utils.py", line 708, in reraise<br />
    raise exception<br />
ImportError: Caught ImportError in replica 1 on device 1.<br />
Original Traceback (most recent call last):<br />
  File "/home/dlyakhov/Projects/pytorch/torch/nn/parallel/parallel_apply.py", line 83, in _worker<br />
    output = module(*input, </strong>kwargs)<br />
             ^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/Projects/pytorch/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/Projects/pytorch/torch/nn/modules/module.py", line 1541, in _call_impl<br />
    return forward_call(</em>args, **kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/Projects/pytorch/repro.py", line 11, in forward<br />
    module = load_inline(<br />
             ^^^^^^^^^^^^<br />
  File "/home/dlyakhov/Projects/pytorch/torch/utils/cpp_extension.py", line 1644, in load_inline<br />
    return _jit_compile(<br />
           ^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/Projects/pytorch/torch/utils/cpp_extension.py", line 1745, in _jit_compile<br />
    return _import_module_from_library(name, build_directory, is_python_module)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/dlyakhov/Projects/pytorch/torch/utils/cpp_extension.py", line 2143, in _import_module_from_library<br />
    module = importlib.util.module_from_spec(spec)<br />
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "<frozen importlib._bootstrap>", line 813, in module_from_spec<br />
  File "<frozen importlib._bootstrap_external>", line 1289, in create_module<br />
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed<br />
ImportError: /home/dlyakhov/.cache/torch_extensions/py312_cu118/jit_extension/jit_extension.so: cannot open shared object file: No such file or directory<br />
```<br />
I would like to submit a PR with a fix: https://github.com/pytorch/pytorch/pull/125404</p>
<h3>Versions</h3>
<p>PyTorch version: 2.4.0a0+git8046de3<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.26.4<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:35:02) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA GeForce RTX 3090<br />
GPU 1: NVIDIA GeForce RTX 3090<br />
GPU 2: NVIDIA GeForce RTX 3090</p>
<p>Nvidia driver version: 525.147.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Byte Order:                         Little Endian<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
CPU(s):                             36<br />
On-line CPU(s) list:                0-35<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 18<br />
Socket(s):                          1<br />
NUMA node(s):                       1<br />
Vendor ID:                          GenuineIntel<br />
CPU family:                         6<br />
Model:                              85<br />
Model name:                         Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz<br />
Stepping:                           7<br />
CPU MHz:                            3000.000<br />
CPU max MHz:                        4800,0000<br />
CPU min MHz:                        1200,0000<br />
BogoMIPS:                           6000.00<br />
Virtualization:                     VT-x<br />
L1d cache:                          576 KiB<br />
L1i cache:                          576 KiB<br />
L2 cache:                           18 MiB<br />
L3 cache:                           24,8 MiB<br />
NUMA node0 CPU(s):                  0-35<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Mitigation; Enhanced IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Mitigation; TSX disabled<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512_vnni md_clear flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] pytorch-triton==3.0.0+45fff310c8<br />
[pip3] torch==2.4.0a0+git8046de3<br />
[conda] magma-cuda110             2.5.2                         1    pytorch<br />
[conda] mkl-include               2024.1.0              intel_691    intel<br />
[conda] mkl-static                2024.1.0              intel_691    intel<br />
[conda] pytorch-triton            3.0.0+45fff310c8          pypi_0    pypi<br />
[conda] torch                     2.4.0a0+git8046de3           dev_0    <develop></p>]]></description>
      <pubDate>Thu, 02 May 2024 08:38:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125403</guid>
    </item>
    <item>
      <title>`torch.compile` gives correct index values (if those are returned), but not the indexed values.</title>
      <link>https://github.com/pytorch/pytorch/issues/125387</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>(This is not 100% duplication of  #124357 (that one has a workaround by using <code>torch._dynamo.config.guard_nn_modules=True</code>, but that is not working for this issue.)</p>
<h1></h1>
<p>issue occurs for: <code>2.4.0.dev20240501+cu121</code> (possibly also earlier nightly versions)<br />
no issue : torch 2.2 / 2.3</p>
<h1>Description</h1>
<p><code>torch.compile</code> gives the correct index values (if the function return those index values), but if I use those indices to index into a tensor and return those selected values, the outputs (and the indices implied) stay the same after a few iterations of calling.</p>
<p>See the following (simple) code snippet. There is no effect in this example with or without adding <code>torch._dynamo.config.guard_nn_modules=True</code>, </p>
<h1>Code snippet</h1>
<p>```python<br />
import torch<br />
import torch._dynamo as dynamo<br />
import torch._inductor as inductor<br />
import torch.nn as nn</p>
<p>dynamo.reset()</p>
<h1>RETURN_INDEX = True: the outputs are 0, 1, 2, 3, 4, 5 (this is expected)</h1>
<h1>RETURN_INDEX = False: the outputs are 2, 3, 4, 5, 5, 5 (it should be 2, 3, 4, 5, 6, 7)</h1>
<p>RETURN_INDEX = True</p>
<p>class ToyModel(torch.nn.Module):<br />
    def <strong>init</strong>(self, return_index):<br />
        super(ToyModel, self).<strong>init</strong>()<br />
        self.value = -1<br />
        self.return_index = return_index<br />
        self.cache = torch.tensor([2, 3, 4, 5, 6, 7])</p>
<pre><code>def forward(self, value):
    self.value += 1
    if self.return_index:
        return self.value  # the outputs are: 0, 1, 2, 3, 4, 5
    else:
        return self.cache[self.value]  # the outputs are:  2, 3, 4, 5, 5, 5
</code></pre>
<p>model = ToyModel(return_index=RETURN_INDEX )<br />
model.forward = torch.compile(model.forward, mode="reduce-overhead", fullgraph=True)</p>
<p>values = [6, 8, 10, 12, 13, 14]<br />
for value in values:<br />
    output = model.forward(value)<br />
    print(f"output = {output}")<br />
```</p>
<p><code>RETURN_INDEX = True</code> give (<code>0, 1, 2, 3, 4, 5</code>):</p>
<p><code>bash
output = 0
output = 1
output = 2
output = 3
output = 4
output = 5</code></p>
<p><code>RETURN_INDEX = False</code> gives (<code>2, 3, 4, 5, 5, 5</code>):</p>
<p>```bash<br />
output = 2<br />
output = 3<br />
output = 4<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542] Ignored guard s0 + 1 == 3, this could result in accuracy problems<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542] Stack (most recent call last):<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "temp.py", line 29, in <module><br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     output = model.forward(value)<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 403, in _fn<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     return fn(<em>args, </em><em>kwargs)<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "temp.py", line 17, in forward<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     def forward(self, value):<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 403, in _fn<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     return fn(</em>args, <strong>kwargs)<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/external_utils.py", line 36, in inner<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     return fn(*args, </strong>kwargs)<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 991, in forward<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     return compiled_fn(full_args)<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 262, in runtime_wrapper<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     regenerated_out = gen_alias_from_base(<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/_aot_autograd/functional_utils.py", line 230, in gen_alias_from_base<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     out = torch._functionalize_apply_view_metas(<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "/usr/local/lib/python3.8/dist-packages/torch/fx/experimental/sym_node.py", line 355, in guard_int<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     r = self.shape_env.evaluate_expr(self.expr, self.hint, fx_node=self.fx_node)<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "/usr/local/lib/python3.8/dist-packages/torch/fx/experimental/recording.py", line 244, in wrapper<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     return fn(<em>args, </em>*kwargs)<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "/usr/local/lib/python3.8/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 4708, in evaluate_expr<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     self._check_frozen(expr, concrete_val)<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]   File "/usr/local/lib/python3.8/dist-packages/torch/fx/experimental/symbolic_shapes.py", line 4542, in _check_frozen<br />
W0502 10:15:04.605925 140253495338816 torch/fx/experimental/symbolic_shapes.py:4542]     log.warning("Ignored guard %s == %s, this could result in accuracy problems", expr, concrete_val, stack_info=True)<br />
output = 5<br />
output = 5<br />
output = 5</p>
<p>```</p>
<h3>Versions</h3>
<p>```bash<br />
Collecting environment information...<br />
PyTorch version: 2.4.0.dev20240501+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.20.3<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.8.10 (default, Nov 22 2023, 10:22:35)  [GCC 9.4.0] (64-bit runtime)<br />
Python platform: Linux-5.10.214-202.855.amzn2.x86_64-x86_64-with-glibc2.29<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA A10G<br />
Nvidia driver version: 535.161.08<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Byte Order:                         Little Endian<br />
Address sizes:                      48 bits physical, 48 bits virtual<br />
CPU(s):                             16<br />
On-line CPU(s) list:                0-15<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 8<br />
Socket(s):                          1<br />
NUMA node(s):                       1<br />
Vendor ID:                          AuthenticAMD<br />
CPU family:                         23<br />
Model:                              49<br />
Model name:                         AMD EPYC 7R32<br />
Stepping:                           0<br />
CPU MHz:                            3267.235<br />
BogoMIPS:                           5600.00<br />
Hypervisor vendor:                  KVM<br />
Virtualization type:                full<br />
L1d cache:                          256 KiB<br />
L1i cache:                          256 KiB<br />
L2 cache:                           4 MiB<br />
L3 cache:                           32 MiB<br />
NUMA node0 CPU(s):                  0-15<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection<br />
Vulnerability Spec rstack overflow: Mitigation; safe RET<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid</p>
<p>Versions of relevant libraries:<br />
[pip3] intel-extension-for-pytorch==2.2.0<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] natten==0.15.1+torch220cu118<br />
[pip3] numpy==1.24.3<br />
[pip3] onnx==1.16.0<br />
[pip3] onnxconverter-common==1.13.0<br />
[pip3] onnxruntime==1.17.3<br />
[pip3] onnxruntime-tools==1.7.0<br />
[pip3] pytorch-triton==3.0.0+45fff310c8<br />
[pip3] tf2onnx==1.16.1<br />
[pip3] torch==2.4.0.dev20240501+cu121<br />
[pip3] torchaudio==2.2.0.dev20240501+cu121<br />
[pip3] torchvision==0.19.0.dev20240501+cu121<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>
<p>(just like #124357)</p>]]></description>
      <pubDate>Thu, 02 May 2024 02:29:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125387</guid>
    </item>
    <item>
      <title>[compiled autograd][cudagraphs] Inputs runtime wrapper to move cpu scalars to cuda</title>
      <link>https://github.com/pytorch/pytorch/pull/125382</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #125382</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 01 May 2024 22:12:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125382</guid>
    </item>
    <item>
      <title>[Inductor] default block size for head_dim = 256 for flex attention</title>
      <link>https://github.com/pytorch/pytorch/pull/125380</link>
      <description><![CDATA[<h2>H100</h2>
<h3>torch.bfloat16</h3>
<p>No major change, as expected.<br />
<code>| Type    |   Speedup |   batch_size |   num_heads |   q_seq_len |   k_seq_len |   head_dim | score_mod   | dtype          |
|---------|-----------|--------------|-------------|-------------|-------------|------------|-------------|----------------|
| Average |     1.122 |              |             |             |             |            |             |                |
| Max     |     1.437 |            1 |          16 |         512 |         512 |        128 | head_bias   | torch.bfloat16 |
| Min     |     0.895 |            1 |          16 |        1024 |        1024 |         64 | head_bias   | torch.bfloat16 |</code></p>
<h3>torch.float32</h3>
<p>Before: OOM when <code>head_dim</code> = 256<br />
After:<br />
<code>| Type    |   Speedup |   batch_size |   num_heads |   q_seq_len |   k_seq_len |   head_dim | score_mod   | dtype         |
|---------|-----------|--------------|-------------|-------------|-------------|------------|-------------|---------------|
| Average |     2.231 |              |             |             |             |            |             |               |
| Max     |     3.760 |           16 |          16 |        4096 |        4096 |         64 | noop        | torch.float32 |
| Min     |     1.532 |            1 |          16 |         512 |         512 |        256 | causal_mask | torch.float32 |</code></p>
<h2>A100</h2>
<h3>torch.bfloat16</h3>
<p>Before:<br />
<code>| Type    |   Speedup |   batch_size |   num_heads |   q_seq_len |   k_seq_len |   head_dim | score_mod     | dtype          |
|---------|-----------|--------------|-------------|-------------|-------------|------------|---------------|----------------|
| Average |     0.587 |              |             |             |             |            |               |                |
| Max     |     0.960 |            1 |          16 |         512 |         512 |         64 | noop          | torch.bfloat16 |
| Min     |     0.017 |            8 |          16 |        4096 |        4096 |        256 | relative_bias | torch.bfloat16 |</code><br />
After:<br />
<code>| Type    |   Speedup |   batch_size |   num_heads |   q_seq_len |   k_seq_len |   head_dim | score_mod   | dtype          |
|---------|-----------|--------------|-------------|-------------|-------------|------------|-------------|----------------|
| Average |     0.756 |              |             |             |             |            |             |                |
| Max     |     0.931 |            1 |          16 |         512 |         512 |         64 | noop        | torch.bfloat16 |
| Min     |     0.467 |           16 |          16 |        1024 |        1024 |        256 | noop        | torch.bfloat16 |</code></p>
<h3>torch.float32</h3>
<p>Before: OOM when <code>head_dim</code> = 256<br />
After:<br />
<code>| Type    |   Speedup |   batch_size |   num_heads |   q_seq_len |   k_seq_len |   head_dim | score_mod   | dtype         |
|---------|-----------|--------------|-------------|-------------|-------------|------------|-------------|---------------|
| Average |     2.386 |              |             |             |             |            |             |               |
| Max     |     7.584 |           16 |          16 |         512 |         512 |         64 | noop        | torch.float32 |
| Min     |     0.948 |            1 |          16 |         512 |         512 |        256 | causal_mask | torch.float32 |</code><br />
cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 01 May 2024 21:15:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125380</guid>
    </item>
    <item>
      <title>[ignore] Test parallel compile with python threads</title>
      <link>https://github.com/pytorch/pytorch/pull/125372</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #125372</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 01 May 2024 18:49:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125372</guid>
    </item>
    <item>
      <title>[ATen-VK] Resolve compiler_flags to allow Mac build</title>
      <link>https://github.com/pytorch/pytorch/pull/125361</link>
      <description><![CDATA[<p>Summary:</p>
<h2><code>-Wmissing-prototypes</code></h2>
<p>In ATen-Vulkan, we often define functions in <code>.cpp</code> files without declaring them in <code>.h</code> files without hiding them in an anonymous namespace.</p>
<p>Example: <a href="https://github.com/pytorch/pytorch/blob/f1f142c44f81384afbdba5e451fc15744868bf26/aten/src/ATen/native/vulkan/impl/Packing.cpp#L299-L348"><code>Packing.cpp</code>'s channel_image_repacking()</a></p>
<p>On Mac, this results in a <code>-Wmissing-prototypes</code> warning, which is disabled in this change.</p>
<h2><code>-Wshadow</code></h2>
<p>In <code>Adapter.cpp</code>, we overwrite a variable called <code>properties</code>, which we fix in this change as opposed to disabling the warning.</p>
<p>Test Plan: CI</p>
<p>Differential Revision: D56850324</p>]]></description>
      <pubDate>Wed, 01 May 2024 16:17:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125361</guid>
    </item>
    <item>
      <title>support methods for torch.compiler.allow_in_graph</title>
      <link>https://github.com/pytorch/pytorch/issues/125244</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>this is a bug/feature report.</p>
<p>When using <code>torch.dynamo.allow_in_graph</code> on a method, it's not respected during compile.</p>
<p>```python<br />
import torch                                            </p>
<p>def custom_backend(gm, _):<br />
    gm.graph.print_tabular()                                                            <br />
    return gm.forward</p>
<p>def test_allow_in_graph_fn():<br />
    torch._dynamo.reset()</p>
<pre><code>@torch._dynamo.allow_in_graph
def add_fn(a, b):
    return a + b

class Model(torch.nn.Module):
    def forward(self, x):
        return add_fn(x, x)

model = Model()
model = torch.compile(model, backend=custom_backend)
x = torch.rand(10)
model(x)
</code></pre>
<p>```</p>
<p>This correctly creates a node for the <code>add_fn</code></p>
<p><code>placeholder    l_x_    L_x_                                                                 ()            {}
call_function  add_fn  &lt;function test_allow_in_graph_fn.&lt;locals&gt;.add_fn at 0x7f5c5646dea0&gt;  (l_x_, l_x_)  {}
output         output  output                                                               ((add_fn,),)  {}</code></p>
<p>```python<br />
def test_allow_in_graph_class_fn():<br />
    torch._dynamo.reset()</p>
<pre><code>class Foo:
    @torch._dynamo.allow_in_graph
    def add_fn(self, a, b):
        return a + b

class Model(torch.nn.Module):
    def forward(self, x):
        foo = Foo()
        print(id(foo.add_fn))
        return foo.add_fn(x, x)

print(id(Foo.add_fn))
model = Model()
model = torch.compile(model, backend=custom_backend)
</code></pre>
<p><code>This instead shows a node, for the built in add `+`</code><br />
placeholder    l_a_    L_a_                     ()            {}<br />
call_function  add     <built-in function add>  (l_a_, l_a_)  {}<br />
output         output  output                   ((add,),)     {}<br />
```</p>
<p>This is most likely due to the <code>id</code> being used by <code>allow_in_graph</code>, however, this changes in python after the instance is created.</p>
<p>The ideal solution would be if this was actually supported, alternatively I think some warning/error when <code>allow_on_graph</code> is used on a method could make sense.</p>
<h3>Alternatives</h3>
<p><code>torch.compiler.disable</code> works is being respected, but it creates graph breaks which have been causing downstream issues for our custom backend so the <code>allow_in_graph</code> behaviour is preferred.</p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng</p>]]></description>
      <pubDate>Tue, 30 Apr 2024 09:03:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125244</guid>
    </item>
    <item>
      <title>Remove Inductor IRs for legacy functional collectives</title>
      <link>https://github.com/pytorch/pytorch/pull/124992</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124992</p>
<p>This PR completely removes the Inductor IR for legacy functional collectives:<br />
- Removed the <code>CollectiveKernel</code> hiearchy and <code>Wait</code>, as well as the corresponding lowerings. These IRs are target (i.e. Python) specific and don't model node dependencies propoerly (e.g. they rely on <code>never_reuse_buffers</code> for correct behavior). They've been superceded by <code>ir._CollectiveKernel</code>.<br />
- Removed <code>InPlaceHint</code> and the scheduler logic for handling it. <code>InPlaceHint</code> is a codegen-time buffer reuse mechanism controlled by the IR's codegen. It's a bit hacky and overlaps with the default buffer reuse mechanism. Removing it since it is only used by legacy functional collectives.<br />
- Removed <code>OutputBuffer</code> and <code>MultiOutputNoSizeAssert</code> which are designed for and only used by legacy functional collectives.</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang @d4l3k @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 17:06:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124992</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Fix cutlass_utils.get_max_alignment() for strided layouts.</title>
      <link>https://github.com/pytorch/pytorch/pull/124930</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124928<br />
* #125407<br />
* #125406<br />
* <strong>-&gt;</strong> #124930</p>
<p>Fixes cutlass_utils.get_max_alignment() which was so far not checking the alignment properly. Basically<br />
the method so far assumed that the passed layout is contiguous and row-major, which does not have to be true.</p>
<p>Test Plan:<br />
CI - test_cutlass_backend.py to prevent regressions<br />
Added unit test</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 03:21:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124930</guid>
    </item>
    <item>
      <title>[Inductor max autotune] Make autotune_select_algorithm more robust</title>
      <link>https://github.com/pytorch/pytorch/pull/124928</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124928<br />
* #125407<br />
* #125406<br />
* #124930</p>
<p>This diff makes sure that a custom exception is thrown when no valid<br />
choices remain during autotuning. This allows to gracefully fall back<br />
to a default choice, even if that default choice has not been passed to<br />
autotune_select_algorithm.</p>
<p>Additionally, this diff handles RuntimeErrors during autotuning gracefully, e.g. the corresponding choice is ignored but it does not lead to the compilation failure of the entire model if a problematic choice is encountered during autotuning.<br />
( An error is being logged, though).</p>
<p>TODO:<br />
 * Add unit test<br />
 * Add an assertion that we use autune_in_subproc when CUTLASS backend is enabled</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 03:14:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124928</guid>
    </item>
    <item>
      <title>[inductor] Add thread+subproc-based worker_start_method</title>
      <link>https://github.com/pytorch/pytorch/pull/124682</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124682</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 15:47:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124682</guid>
    </item>
    <item>
      <title>DISABLED test_no_unnecessary_save (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124527</link>
      <description><![CDATA[<p>Platforms: linux, rocm, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_no_unnecessary_save&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24039472510">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 21 failures and 7 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_no_unnecessary_save</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 13:40:10 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124527</guid>
    </item>
    <item>
      <title>[inductor] Add ProcessPool-in-subproc worker_start_method</title>
      <link>https://github.com/pytorch/pytorch/pull/122941</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122941<br />
* #124682</p>
<p>In many environments using fork-based parallelism causes issues because user processes are not fork-safe.  This moves our parallel compile work pool into a subprocess that we control (that should be fork safe).</p>
<p>Perf run: https://github.com/pytorch/pytorch/actions/runs/8486887873</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 20:34:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122941</guid>
    </item>
    <item>
      <title>Short-term fix to preserve NJT metadata cache in torch.compile</title>
      <link>https://github.com/pytorch/pytorch/pull/122836</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121947<br />
* <strong>-&gt;</strong> #122836<br />
* #125418</p>
<p>Idea: close over min / max sequence length in the main NJT view func (<code>_nested_view_from_jagged</code>) so that view replay during fake-ification propagates these correctly in torch.compile.</p>
<p>For dynamic shapes support for min / max sequence length, this PR uses a hack that stores the values in <code>(val, 0)</code> shaped tensors.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D55448636">D55448636</a></p>]]></description>
      <pubDate>Wed, 27 Mar 2024 13:48:02 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122836</guid>
    </item>
    <item>
      <title>Increased compile time max GPUs to 512. Switched to int16_t DeviceIndex.</title>
      <link>https://github.com/pytorch/pytorch/pull/119639</link>
      <description><![CDATA[<p>Fixes #115331.</p>
<p>This PR increases the number of valid GPU devices to 512 (from 64) in order to future-proof PyTorch for providers that offer <a href="https://www.tensorwave.com/">single nodes with a large device count</a>. Until now, <code>DeviceIndex</code> was an <code>int8_t</code>, thus multiple changes were necessary:</p>
<ul>
<li><code>DeviceIndex</code> changed to <code>int16_t</code>. Updated consumers that assume it to be an <code>int8_t</code>.</li>
<li>Updated bounds checking for <code>torch.device()</code> in the Python frontend. Right now, we allow funny things like <code>torch.device('cpu', 200).index == -56</code>, which is undefined behavior. I inserted some checks to only allow values between 0 and <code>c10::Device::MAX_NUM_DEVICES - 1</code>.</li>
<li>Updated the <code>ArgumentInfo</code> struct as it hardcodes the device index as 8 bit field [^1]. Might be a breaking change, not sure if users rely on this.</li>
<li>Introduced <code>c10::Device::MAX_NUM_DEVICES</code> as a replacement for the old <code>C10_COMPILE_TIME_MAX_GPUS</code></li>
</ul>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225</p>
<p>[^1]: This field was unsigned, so I guess this has also been undef behavior the whole time? Our default device index is -1, so this always wrapped around to 255 when written to the <code>ArgumentInfo</code> struct. When I switched the <code>DeviceIndex</code> to <code>int16_t</code>, it actually stayed 255 after unpacking from <code>ArgumentInfo</code> again, as the <code>DeviceIndex</code> was now wide enough that it didn't wrap back to -1.</p>]]></description>
      <pubDate>Sat, 10 Feb 2024 05:14:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119639</guid>
    </item>
    <item>
      <title>DISABLED test_unwrap_storage_didnt_work_repro_cuda (__main__.TestInductorDynamicCUDA)</title>
      <link>https://github.com/pytorch/pytorch/issues/119231</link>
      <description><![CDATA[<p>Platforms: linux, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_unwrap_storage_didnt_work_repro_cuda&amp;suite=TestInductorDynamicCUDA&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/21240558316">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_unwrap_storage_didnt_work_repro_cuda</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_torchinductor_dynamic_shapes.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Mon, 05 Feb 2024 13:39:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/119231</guid>
    </item>
    <item>
      <title>[inductor] Added decomposition for _upsample_bilinear2d_aa</title>
      <link>https://github.com/pytorch/pytorch/pull/110170</link>
      <description><![CDATA[<p>Description:<br />
- Added decomposition for _upsample_bilinear2d_aa</p>
<p>Benchmark results:<br />
- CPU decomposition is very slow vs Eager<br />
  - about 3 times slower with a "single" kernel vs multiple kernels.<br />
- CUDA perfs are comparable on large inputs. On small inputs "Compiled" column results are bounded by a min exec time ~60us (used infrastructure) and "Just Triton" column shows the time taken by executing triton compiled code only without dynamo etc (See source below for benchcode details).<br />
```<br />
[-------------------------------------------- Interpolate bilinear, AA=true, cpu -------------------------------------------]<br />
                                                                                      |   Eager   |  Compiled  |  Just C++<br />
1 threads: ------------------------------------------------------------------------------------------------------------------<br />
      Input (1, 3, 500, 400) -&gt; (256, 256), torch.uint8, torch.contiguous_format      |    766.0  |   20620.2  |     20677.3<br />
      Input (1, 3, 500, 400) -&gt; (256, 256), torch.float32, torch.contiguous_format    |   1719.6  |   19530.7  |     19497.4<br />
      Input (1, 3, 500, 400) -&gt; (256, 256), torch.uint8, torch.channels_last          |    333.1  |   20856.8  |     21364.1<br />
      Input (1, 3, 500, 400) -&gt; (256, 256), torch.float32, torch.channels_last        |   2199.4  |   19880.4  |     19793.3<br />
      Input (4, 3, 500, 400) -&gt; (256, 256), torch.uint8, torch.contiguous_format      |   3217.5  |   83345.9  |     83517.6<br />
      Input (4, 3, 500, 400) -&gt; (256, 256), torch.float32, torch.contiguous_format    |   7111.5  |   75855.2  |     75608.2<br />
      Input (4, 3, 500, 400) -&gt; (256, 256), torch.uint8, torch.channels_last          |   1300.4  |   83555.3  |     83619.2<br />
      Input (4, 3, 500, 400) -&gt; (256, 256), torch.float32, torch.channels_last        |   8881.5  |   86082.9  |     85837.3</p>
<pre><code>  Input (1, 3, 1200, 1300) -&gt; (200, 300), torch.uint8, torch.contiguous_format    |   3577.0  |   15391.7  |     15341.8
  Input (1, 3, 1200, 1300) -&gt; (200, 300), torch.float32, torch.contiguous_format  |   9438.9  |   12377.3  |     12249.9
  Input (1, 3, 1200, 1300) -&gt; (200, 300), torch.uint8, torch.channels_last        |   1130.9  |   12413.4  |     12326.8
  Input (1, 3, 1200, 1300) -&gt; (200, 300), torch.float32, torch.channels_last      |  10247.6  |    8201.1  |      8060.1
  Input (4, 3, 1200, 1300) -&gt; (200, 300), torch.uint8, torch.contiguous_format    |  13913.0  |   62827.1  |     62963.2
  Input (4, 3, 1200, 1300) -&gt; (200, 300), torch.float32, torch.contiguous_format  |  37462.7  |   48888.1  |     48815.6
  Input (4, 3, 1200, 1300) -&gt; (200, 300), torch.uint8, torch.channels_last        |   4723.9  |   51666.5  |     51516.6
  Input (4, 3, 1200, 1300) -&gt; (200, 300), torch.float32, torch.channels_last      |  41864.6  |   34116.5  |     33966.8

  Input (1, 3, 300, 400) -&gt; (600, 700), torch.uint8, torch.contiguous_format      |   1864.5  |   35455.5  |     35566.7
  Input (1, 3, 300, 400) -&gt; (600, 700), torch.float32, torch.contiguous_format    |   2653.1  |   35303.6  |     35455.9
  Input (1, 3, 300, 400) -&gt; (600, 700), torch.uint8, torch.channels_last          |    510.8  |   35767.9  |     35901.2
  Input (1, 3, 300, 400) -&gt; (600, 700), torch.float32, torch.channels_last        |   5365.5  |   33153.8  |     33548.8
  Input (4, 3, 300, 400) -&gt; (600, 700), torch.uint8, torch.contiguous_format      |   8824.6  |  141773.7  |    139218.2
  Input (4, 3, 300, 400) -&gt; (600, 700), torch.float32, torch.contiguous_format    |  10564.3  |  140867.2  |    140051.1
  Input (4, 3, 300, 400) -&gt; (600, 700), torch.uint8, torch.channels_last          |   2093.3  |  153663.1  |    154263.3
  Input (4, 3, 300, 400) -&gt; (600, 700), torch.float32, torch.channels_last        |  21512.9  |  132679.4  |    132839.2
</code></pre>
<p>Times are in microseconds (us).</p>
<p>[------------------------------------------- Interpolate bilinear, AA=true, cuda --------------------------------------------]<br />
                                                                                        |  Eager   |  Compiled  |  Just Triton<br />
1 threads: -------------------------------------------------------------------------------------------------------------------<br />
      Input (1, 3, 500, 400) -&gt; (256, 256), torch.float32, torch.contiguous_format      |    11.3  |     65.3   |       22.3<br />
      Input (1, 3, 500, 400) -&gt; (256, 256), torch.float32, torch.channels_last          |    29.3  |     70.5   |       23.8<br />
      Input (4, 3, 500, 400) -&gt; (256, 256), torch.float32, torch.contiguous_format      |    42.1  |     70.1   |       33.8<br />
      Input (4, 3, 500, 400) -&gt; (256, 256), torch.float32, torch.channels_last          |    91.8  |     64.9   |       47.9</p>
<pre><code>  Input (1, 3, 1200, 1300) -&gt; (200, 300), torch.float32, torch.contiguous_format    |    85.9  |    117.2   |      117.9
  Input (1, 3, 1200, 1300) -&gt; (200, 300), torch.float32, torch.channels_last        |   216.9  |    163.9   |      166.0
  Input (4, 3, 1200, 1300) -&gt; (200, 300), torch.float32, torch.contiguous_format    |   331.8  |    471.0   |      472.0
  Input (4, 3, 1200, 1300) -&gt; (200, 300), torch.float32, torch.channels_last        |   841.7  |    647.4   |      650.8

  Input (1, 3, 300, 400) -&gt; (600, 700), torch.float32, torch.contiguous_format      |    30.1  |     64.7   |       28.4
  Input (1, 3, 300, 400) -&gt; (600, 700), torch.float32, torch.channels_last          |    54.2  |     65.7   |       39.3
  Input (4, 3, 300, 400) -&gt; (600, 700), torch.float32, torch.contiguous_format      |    95.6  |    109.8   |      109.5
  Input (4, 3, 300, 400) -&gt; (600, 700), torch.float32, torch.channels_last          |   187.8  |    232.8   |      232.8

  Input (1, 3, 2345, 2456) -&gt; (1234, 1345), torch.float32, torch.contiguous_format  |   246.0  |    225.0   |      229.2
  Input (1, 3, 2345, 2456) -&gt; (1234, 1345), torch.float32, torch.channels_last      |   778.7  |    575.0   |      574.9
  Input (4, 3, 2345, 2456) -&gt; (1234, 1345), torch.float32, torch.contiguous_format  |   872.5  |    898.3   |      898.7
  Input (4, 3, 2345, 2456) -&gt; (1234, 1345), torch.float32, torch.channels_last      |  2996.3  |   2289.4   |     2288.8

  Input (1, 3, 1234, 1345) -&gt; (2345, 2456), torch.float32, torch.contiguous_format  |   431.3  |    381.1   |      381.2
  Input (1, 3, 1234, 1345) -&gt; (2345, 2456), torch.float32, torch.channels_last      |   796.4  |   1042.8   |     1044.2
  Input (4, 3, 1234, 1345) -&gt; (2345, 2456), torch.float32, torch.contiguous_format  |  1323.6  |   1518.6   |     1515.9
  Input (4, 3, 1234, 1345) -&gt; (2345, 2456), torch.float32, torch.channels_last      |  2797.8  |   3728.6   |     3728.0

  Input (1, 3, 2345, 2456) -&gt; (120, 200), torch.float32, torch.contiguous_format    |   461.7  |    435.9   |      436.3
  Input (1, 3, 2345, 2456) -&gt; (120, 200), torch.float32, torch.channels_last        |   941.6  |    521.6   |      521.3
  Input (4, 3, 2345, 2456) -&gt; (120, 200), torch.float32, torch.contiguous_format    |  1832.3  |   1724.6   |     1725.6
  Input (4, 3, 2345, 2456) -&gt; (120, 200), torch.float32, torch.channels_last        |  3740.3  |   2040.9   |     2041.0
</code></pre>
<p>Times are in microseconds (us).<br />
```<br />
<a href="https://github.com/vfdev-5/pth-inductor-dev/blob/862599e5cbece5c41068cd16cc3afbf25f26be39/perf_interp_bilinear_aa_vs_gencode.py">Source</a></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @Xia-Weiwen @aakhundov</p>]]></description>
      <pubDate>Wed, 27 Sep 2023 08:41:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/110170</guid>
    </item>
  </channel>
</rss>
