<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[Inductor][1/n]Split cat customization</title>
      <link>https://github.com/pytorch/pytorch/pull/123045</link>
      <description><![CDATA[<p>Summary: Change the config and revise the group batch fusion in order not to reuse the exsiting pre_grad and post_grad fusion options</p>
<p>Test Plan:</p>
<h1>unit test</h1>
<p><code>buck2 test @mode/dev-nosan //caffe2/test/inductor:split_cat_fx_passes</code><br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/17732923560510096<br />
Network: Up: 15MiB  Down: 155MiB  (reSessionID-6a577a14-1772-42d9-9ae8-bfdc62f406a3)<br />
Jobs completed: 267487. Time elapsed: 2:39.7s.<br />
Cache hits: 99%. Commands: 104465 (cached: 104457, remote: 8, local: 0)<br />
Tests finished: Pass 11. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<p><code>buck2 test @mode/dev-nosan //caffe2/test/inductor/fb:split_cat_fx_passes_fb</code><br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/9007199283031382<br />
Network: Up: 28MiB  Down: 177MiB  (reSessionID-a3081518-7cba-4c83-b442-c16655ecb2cd)<br />
Jobs completed: 183164. Time elapsed: 1:41.4s.<br />
Cache hits: 99%. Commands: 75875 (cached: 75862, remote: 12, local: 1)<br />
Tests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<p><code>buck2 test @mode/dev-nosan //caffe2/test/inductor:group_batch_fusion</code><br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/10133099189612276<br />
Network: Up: 1.3MiB           Down: 3.1MiB           (reSessionID-0d312a2d-e19e-4ba6-9f96-7eb5863734e7)<br />
Discovered 9. Pass 0. Fail 0. Fatal 0. Skip 0. Timeout 0<br />
Network: Up: 1.4MiB  Down: 3.2MiB  (reSessionID-0d312a2d-e19e-4ba6-9f96-7eb5863734e7)<br />
Jobs completed: 68. Time elapsed: 2:19.9s.<br />
Cache hits: 0%. Commands: 13 (cached: 0, remote: 1, local: 12)<br />
Tests finished: Pass 9. Fail 0. Fatal 0. Skip 0. Build failure 0<br />
<code>buck2 test @mode/dev-nosan //caffe2/test/inductor:perf</code><br />
Test UI: https://www.internalfb.com/intern/testinfra/testrun/5066549804623287<br />
Network: Up: 1.5MiB  Down: 1.1MiB  (reSessionID-8d912a20-fceb-4698-89c3-d28e0708831f)<br />
Jobs completed: 164. Time elapsed: 1:42.2s.<br />
Cache hits: 0%. Commands: 13 (cached: 0, remote: 1, local: 12)<br />
Tests finished: Pass 57. Fail 0. Fatal 0. Skip 0. Build failure 0</p>
<h1>local reproduce</h1>
<p>case 1: with split cat<br />
<code>buck2 run @mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode batch-split --model_type "cmf" --flow_id 524546542</code><br />
optimus parameter sent to the scuba:<br />
<code>{'before_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GLL6RBZb-ssXJYcBAMzw0oaKtp80br0LAAAz', 'BatchLayernormFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GH1LAxcxv0Ae_BkFAHVav3K3oosDbr0LAAAz', 'BatchSigmoidPreGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GNb0jwR-Ukkqns4CAGRmOqucfedDbr0LAAAz', 'normalization_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GHsIQxm-hn3SPrgCAKq1E-HBsoZHbr0LAAAz', 'remove_split_with_size_one_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GOrJORmbMTV_xlQDAOwolqclPsIAbr0LAAAz', 'merge_getitem_cat_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GCqkmRblvVKybGUDACVxkwVIrWxLbr0LAAAz', 'merge_splits_pass_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GCB1QBfko_kVN0wFAKGjSZv4DJULbr0LAAAz', 'after_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GMwJPRmu4ry88swDAO1gdA5RCKIXbr0LAAAz', 'before_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GLXCORnNiKeQFmoDABR93CRKmP8Sbr0LAAAz', 'BatchMulPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GBMIPRnlwQyjSD4BANPuaMhV7MUjbr0LAAAz', 'after_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GJ9KPxkOv4LL8_0DAA65D4kh4JYDbr0LAAAz', 'inductor': Counter({'pattern_matcher_nodes': 2844, 'pattern_matcher_count': 2604, 'normalization_pass': 886, 'remove_split_with_size_one_pass': 748, 'merge_splits_pass': 82, 'merge_getitem_cat_pass': 11, 'scmerge_split_sections_removed': 4, 'batch_aten_mul': 4, 'batch_sigmoid': 2, 'batch_aten_sub': 2, 'batch_layernorm': 1, 'scmerge_split_added': 1, 'scmerge_cat_added': 1, 'scmerge_split_removed': 1, 'scmerge_cat_removed': 1, 'batch_aten_add': 1}), 'BatchAddPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GEcvPxmxBj-pd8gCABE1QgB-d6N6br0LAAAz', 'BatchSubPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GEvQxhYomJGj2FMBAEXXAI8Vgzhmbr0LAAAz'}</code><br />
P1202819405</p>
<p>case 2: without split cat<br />
<code>buck2 run @mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode batch --model_type "cmf" --flow_id 524546542</code><br />
optimus parameter sent to the scuba:<br />
<code>{'before_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GAY7PxmGthuyjSwEAHF_A767YbMkbr0LAAAz', 'BatchLayernormFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GLDPtBacXyybEOICAKaGCPatq5oabr0LAAAz', 'BatchSigmoidPreGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GBu7ORkiDJu42QAEAGmlVTgO_Mpbbr0LAAAz', 'after_recompile_pre_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GC893BZNl99ftY4BAHm5Z8sM4ptSbr0LAAAz', 'before_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GCAeuRYgzPO5RcsCAPO3Z7tdMNMKbr0LAAAz', 'BatchMulPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GHBIQxm1jlU-xhsFAONkzhh2mgknbr0LAAAz', 'after_recompile_post_grad': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GDoUPhmZ0noiaGMDAJHYuuiwHEAUbr0LAAAz', 'inductor': Counter({'pattern_matcher_nodes': 1189, 'pattern_matcher_count': 757, 'batch_aten_mul': 9, 'batch_aten_sub': 3, 'batch_sigmoid': 2, 'batch_aten_add': 2, 'batch_layernorm': 1, 'batch_linear_post_grad': 1}), 'BatchAddPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GAluthYxi8uxpI4BAIQDzn3OyywUbr0LAAAz', 'BatchSubPostGradFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GDjsJhTK5VAcot4CADIcAixghrYibr0LAAAz', 'PostGradBatchLinearFusion': 'https://www.internalfb.com/intern/everpaste/?color=0&amp;handle=GEPfJxfJwktC7wsEAA0QbkqYNuVAbr0LAAAz'}</code><br />
P1202823734</p>
<h1>e2e</h1>
<p>training_platform:fd4f02cd855f5cc0ccb49317a5a6c8bb</p>
<p>with split cat<br />
f546646358</p>
<p>without split cat<br />
f546647159</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sat, 30 Mar 2024 21:48:49 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123045</guid>
    </item>
    <item>
      <title>ImportError: cannot import name 'triton_key' from 'triton.compiler.compiler'</title>
      <link>https://github.com/pytorch/pytorch/issues/123042</link>
      <description><![CDATA[<p>Code:<br />
```python<br />
torch._dynamo.config.suppress_errors = True</p>
<p>self.pipe = OotdPipeline.from_pretrained(<br />
    MODEL_PATH,<br />
    unet_garm=unet_garm,<br />
    unet_vton=unet_vton,<br />
    vae=vae,<br />
    torch_dtype=torch.float16,<br />
    variant="fp16",<br />
    use_safetensors=True,<br />
    safety_checker=None,<br />
    requires_safety_checker=False,<br />
    local_files_only=True,<br />
).to(self.gpu_id)</p>
<p>self.pipe.unet_garm = torch.compile(self.pipe.unet_garm, mode="reduce-overhead", fullgraph=True)<br />
self.pipe.unet_vton = torch.compile(self.pipe.unet_vton, mode="reduce-overhead", fullgraph=True)<br />
<code>Error Message:</code>bash<br />
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_triton.py", line 57, in triton_hash_with_backend<br />
    from triton.compiler.compiler import triton_key<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
ImportError: cannot import name 'triton_key' from 'triton.compiler.compiler' (/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py)<br />
```</p>
<h3>Versions</h3>
<p>PyTorch version: 2.4.0.dev20240330+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.29.0<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-5.10.0-28-cloud-amd64-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 12.1.105<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA L4<br />
Nvidia driver version: 535.104.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             96<br />
On-line CPU(s) list:                0-95<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Xeon(R) CPU @ 2.20GHz<br />
CPU family:                         6<br />
Model:                              85<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 24<br />
Socket(s):                          2<br />
Stepping:                           7<br />
BogoMIPS:                           4400.42<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities<br />
Hypervisor vendor:                  KVM<br />
Virtualization type:                full<br />
L1d cache:                          1.5 MiB (48 instances)<br />
L1i cache:                          1.5 MiB (48 instances)<br />
L2 cache:                           48 MiB (48 instances)<br />
L3 cache:                           77 MiB (2 instances)<br />
NUMA node(s):                       2<br />
NUMA node0 CPU(s):                  0-23,48-71<br />
NUMA node1 CPU(s):                  24-47,72-95<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT Host state unknown<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown<br />
Vulnerability Retbleed:             Mitigation; Enhanced IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT Host state unknown</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.24.4<br />
[pip3] onnxruntime==1.16.2<br />
[pip3] pytorch-triton==0.0.1<br />
[pip3] torch==2.4.0.dev20240330+cu121<br />
[pip3] torchaudio==2.2.0.dev20240330+cu121<br />
[pip3] torchvision==0.19.0.dev20240330+cu121<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>]]></description>
      <pubDate>Sat, 30 Mar 2024 15:54:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123042</guid>
    </item>
    <item>
      <title>[aotinductor] Fix benchmarks with self.autocast for run_performance_test</title>
      <link>https://github.com/pytorch/pytorch/pull/122954</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122954<br />
* #122883</p>
<p>https://github.com/pytorch/pytorch/pull/110490 fixes the self.autocast in the <code>check_accuracy</code> function. Fix it in the <code>run_performance_test</code> function as well.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 23:29:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122954</guid>
    </item>
    <item>
      <title>Inductor: don't change the stride_order of FlexibleLayout if it's already the same as required</title>
      <link>https://github.com/pytorch/pytorch/pull/122945</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122945</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/122489</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 21:39:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122945</guid>
    </item>
    <item>
      <title>[AOTInductor] Support quantized linear on CPU with fbgemm</title>
      <link>https://github.com/pytorch/pytorch/pull/122939</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122939</p>
<p>Summary:<br />
Added support for quantized linear on CPU with fbgemm.<br />
Specifically, for torch.ops.quantized.linear_unpacked_dynamic_fp16, we<br />
decompose it into two steps, pack weight, and fbgemm's qlinear with<br />
packed weight.</p>
<p>Test Plan:<br />
Included in commit.<br />
test_aot_inductor::test_quantized_linear</p>
<p>Reviewers:</p>
<p>Subscribers:</p>
<p>Tasks:</p>
<p>Tags:<br />
@diff-train-skip-merge<br />
cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D55512029">D55512029</a></p>]]></description>
      <pubDate>Thu, 28 Mar 2024 20:03:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122939</guid>
    </item>
    <item>
      <title>fix amp for AOTInductor</title>
      <link>https://github.com/pytorch/pytorch/pull/122883</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122954<br />
* <strong>-&gt;</strong> #122883</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 28 Mar 2024 01:38:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122883</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Reuse inductor test for Intel GPU (PART 1)</title>
      <link>https://github.com/pytorch/pytorch/pull/122866</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122866<br />
* #121895<br />
* #122254<br />
* #121883</p>
<p>backend.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 27 Mar 2024 20:28:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122866</guid>
    </item>
    <item>
      <title>Inductor cpp wrapper: fix dtype of ShapeAsConstantBuffer</title>
      <link>https://github.com/pytorch/pytorch/pull/122297</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122297</p>
<p>For <code>at::scalar_tensor</code> the default dtype will be <code>float</code> (<a href="https://github.com/pytorch/pytorch/blob/0d8e960f74acd359358e0b729c4803d2b71849e5/aten/src/ATen/native/TensorFactories.cpp#L856">link to scalar_tensor</a>, <a href="https://github.com/pytorch/pytorch/blob/0d8e960f74acd359358e0b729c4803d2b71849e5/c10/core/TensorOptions.h#L551">link to default dtype</a>) if we don't set the <code>dtype</code> value. However, the input scalar value is not necessarily a <code>float</code> value. With <code>torch::tensor(x)</code>, the dtype of the tensor will be decided according to the dtype of the scalar.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 00:58:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122297</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Enable triton installation for Intel GPU</title>
      <link>https://github.com/pytorch/pytorch/pull/122254</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122866<br />
* #121895<br />
* <strong>-&gt;</strong> #122254<br />
* #121883</p>
<p>Intel GPU.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 17:14:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122254</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Add Inductor Intel GPU backend.</title>
      <link>https://github.com/pytorch/pytorch/pull/121895</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122866<br />
* <strong>-&gt;</strong> #121895<br />
* #122254<br />
* #121883</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 00:00:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121895</guid>
    </item>
    <item>
      <title>[WIP][Inductor Intel GPU backend Upstream] Register general runtime device for Intel GPU</title>
      <link>https://github.com/pytorch/pytorch/pull/121883</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122866<br />
* #121895<br />
* #122254<br />
* <strong>-&gt;</strong> #121883</p>
<p>interface for Intel GPU.</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 18:20:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121883</guid>
    </item>
  </channel>
</rss>
