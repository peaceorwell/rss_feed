<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>support methods for torch.compiler.allow_in_graph</title>
      <link>https://github.com/pytorch/pytorch/issues/125244</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>this is a bug/feature report.</p>
<p>When using <code>torch.dynamo.allow_in_graph</code> on a method, it's not respected during compile.</p>
<p>```python<br />
import torch                                            </p>
<p>def custom_backend(gm, _):<br />
    gm.graph.print_tabular()                                                            <br />
    return gm.forward</p>
<p>def test_allow_in_graph_fn():<br />
    torch._dynamo.reset()</p>
<pre><code>@torch._dynamo.allow_in_graph
def add_fn(a, b):
    return a + b

class Model(torch.nn.Module):
    def forward(self, x):
        return add_fn(x, x)

model = Model()
model = torch.compile(model, backend=custom_backend)
x = torch.rand(10)
model(x)
</code></pre>
<p>```</p>
<p>This correctly creates a node for the <code>add_fn</code></p>
<p><code>placeholder    l_x_    L_x_                                                                 ()            {}
call_function  add_fn  &lt;function test_allow_in_graph_fn.&lt;locals&gt;.add_fn at 0x7f5c5646dea0&gt;  (l_x_, l_x_)  {}
output         output  output                                                               ((add_fn,),)  {}</code></p>
<p>```python<br />
def test_allow_in_graph_class_fn():<br />
    torch._dynamo.reset()</p>
<pre><code>class Foo:
    @torch._dynamo.allow_in_graph
    def add_fn(self, a, b):
        return a + b

class Model(torch.nn.Module):
    def forward(self, x):
        foo = Foo()
        print(id(foo.add_fn))
        return foo.add_fn(x, x)

print(id(Foo.add_fn))
model = Model()
model = torch.compile(model, backend=custom_backend)
</code></pre>
<p><code>This instead shows a node, for the built in add `+`</code><br />
placeholder    l_a_    L_a_                     ()            {}<br />
call_function  add     <built-in function add>  (l_a_, l_a_)  {}<br />
output         output  output                   ((add,),)     {}<br />
```</p>
<p>This is most likely due to the <code>id</code> being used by <code>allow_in_graph</code>, however, this changes in python after the instance is created.</p>
<p>The ideal solution would be if this was actually supported, alternatively I think some warning/error when <code>allow_on_graph</code> is used on a method could make sense.</p>
<h3>Alternatives</h3>
<p><code>torch.compiler.disable</code> works is being respected, but it creates graph breaks which have been causing downstream issues for our custom backend so the <code>allow_in_graph</code> behaviour is preferred.</p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Tue, 30 Apr 2024 09:03:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125244</guid>
    </item>
    <item>
      <title>[Inductor] Properly package target info for triton.compile </title>
      <link>https://github.com/pytorch/pytorch/pull/125241</link>
      <description><![CDATA[<p>Triton updated the interface for <code>triton.compile</code> https://github.com/openai/triton/commit/5162346487b3e3ebc062d9697429bafad25f22f6 </p>
<p>The <code>target</code> argument to compile needs to be wrapped in a <code>GPUTarget</code> object. Without proper wrapping, we hit an assert in <code>compile</code>. If that assert is removed, Triton attempts to read device info from Torch while inside a torch thread, which hits an in bad fork assert. This change is required for compatibility with latest commits in Triton. The implementation is backwards compatible, so existing versions of Triton that work now continue to work.</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 30 Apr 2024 07:51:06 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125241</guid>
    </item>
    <item>
      <title>torch.no_grad() is not working for dynamo inductor backend </title>
      <link>https://github.com/pytorch/pytorch/issues/125236</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>torch dynamo context manager is not seeing the <code>torch.no_grad()</code> setting from torchbench framework.</p>
<p>This is the reproducer<br />
<code>python run_benchmark.py cpu --model hf_Bert --test eval --torchdynamo inductor</code></p>
<p>Here is the code snippet from torchbench that sets the no_grad()<br />
https://github.com/pytorch/benchmark/blob/main/torchbenchmark/util/framework/huggingface/model_factory.py#L120</p>
<p><code>with torch.no_grad():
            with self.amp_context():
                out = self.model(**self.example_inputs)</code><br />
but the torch dynamo inductor backend isn't seeing the grad disabled, and hence is not triggering the graph level optimizations.<br />
https://github.com/pytorch/pytorch/blob/v2.3.0/torch/_inductor/compile_fx.py#L1270</p>
<h3>Error logs</h3>
<p>no error log, but the torch.compile didn't trigger the fx graph  optimizations</p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>torch mainline as well as torch 2.3<br />
torchbench mainline<br />
Ubuntu 22.04<br />
aarch64 and x86_64</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Tue, 30 Apr 2024 07:23:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125236</guid>
    </item>
    <item>
      <title>[Inductor] [Distributed] DDP torch.compile model hangs on exit (python 3.8/3.9)</title>
      <link>https://github.com/pytorch/pytorch/issues/125235</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>On python3.8/3.9 environments hangs are being observed with DDP model+torch.compile on model exit. This is observed both on AMD/NV platforms on nightly and does not occur in equivalent python3.10+ environment.</p>
<p>The workaround for this so far has been to set <code>TORCHINDUCTOR_COMPILE_THREADS=1</code> in the same approach used in some distributed inductor unit tests e.g.  https://github.com/pytorch/pytorch/blob/761a7b84ba493e6113ad6dcc3899123801230cec/test/distributed/test_inductor_collectives.py#L98 which mentions # TODO: somehow inductor bg compile threads are causing hangs at exit with distributed work</p>
<p>Reproducer:<br />
https://gist.github.com/jataylo/50a88745acfc817e2e2f89a41232f308</p>
<h3>Versions</h3>
<p>PyTorch version: 2.4.0a0+git9dded14<br />
Is debug build: False<br />
CUDA used to build PyTorch: N/A<br />
ROCM used to build PyTorch: 6.0.32831-204d35d16</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.0.2 24012 af27734ed982b52a9f1be0f035ac91726fc697e4)<br />
CMake version: version 3.26.4<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.8.19 (default, Mar 20 2024, 19:58:24)  [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.19.0-42-generic-x86_64-with-glibc2.17<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: AMD Instinct MI250X/MI250 (gfx90a:sramecc+:xnack-)<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: 6.0.32831<br />
MIOpen runtime version: 3.0.0<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   48 bits physical, 48 bits virtual<br />
CPU(s):                          128<br />
On-line CPU(s) list:             0-127<br />
Thread(s) per core:              1<br />
Core(s) per socket:              64<br />
Socket(s):                       2<br />
NUMA node(s):                    2<br />
Vendor ID:                       AuthenticAMD<br />
CPU family:                      25<br />
Model:                           1<br />
Model name:                      AMD EPYC 7713 64-Core Processor<br />
Stepping:                        1<br />
Frequency boost:                 enabled<br />
CPU MHz:                         1500.000<br />
CPU max MHz:                     3720.7029<br />
CPU min MHz:                     1500.0000<br />
BogoMIPS:                        3992.54<br />
Virtualization:                  AMD-V<br />
L1d cache:                       4 MiB<br />
L1i cache:                       4 MiB<br />
L2 cache:                        64 MiB<br />
L3 cache:                        512 MiB<br />
NUMA node0 CPU(s):               0-63<br />
NUMA node1 CPU(s):               64-127<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Not affected<br />
Vulnerability Retbleed:          Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm</p>
<p>Versions of relevant libraries:<br />
[pip3] mypy==1.9.0<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.21.2<br />
[pip3] optree==0.11.0<br />
[pip3] torch==2.4.0a0+git9dded14<br />
[pip3] torchvision==0.19.0a0+96640af<br />
[pip3] triton==2.1.0<br />
[conda] No relevant packages</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang @d4l3k</p>]]></description>
      <pubDate>Tue, 30 Apr 2024 06:59:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125235</guid>
    </item>
    <item>
      <title>DISABLED test_variable_traverse (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/125229</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_variable_traverse&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24417478591">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 27 failures and 9 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_variable_traverse</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/var/lib/jenkins/workspace/test/test_autograd.py", line 4140, in test_variable_traverse
    out.backward(torch.randn(out.size()))
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_tensor.py", line 523, in backward
    torch.autograd.backward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/graph.py", line 767, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 735, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 315, in __call__
    raise e
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 302, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert
    return _compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py", line 70, in wrapper_function
    return function(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform
    tracer.run()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2230, in run
    super().run()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 880, in run
    while self.step():
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 795, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2387, in RETURN_VALUE
    self._return(inst)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2372, in _return
    self.output.compile_subgraph(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1093, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1285, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1376, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1357, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/__init__.py", line 1784, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
  File "/var/lib/jenkins/workspace/test/inductor/test_compiled_autograd.py", line 28, in inner_compiler
    return inductor.compile(gm_, example_inputs_)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/__init__.py", line 28, in compile
    return compile_fx(gm, example_inputs, config_patches=options)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1276, in compile_fx
    return flatten_graph_inputs(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 2669, in flatten_graph_inputs
    compiled_fn = compile_gm(GmWrapper(gm, spec), inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1454, in compile_fx
    return aot_autograd(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 156, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1358, in fw_compiler_base
    return inner_compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 483, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 779, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1692, in compile_to_fn
    return self.compile_to_module().call
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1639, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2456, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_jenkins/li/clijpb5demivkey23tqvxnrvmvexuemq45jyxw4qi735t6oydulb.py", line 52, in <module>
    async_compile.wait(globals())
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3058, in wait
    scope[key] = result.result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2867, in result
    return self.result_fn()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2336, in future
    result = get_result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2164, in load_fn
    future.result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2189, in _worker_compile_cpp
    compile_file(input_path, output_path, shlex.split(cmd))
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2060, in compile_file
    raise exc.CppCompileError(cmd, output) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inner_compiler' raised:
CppCompileError: C++ compile error

Command:
g++ /tmp/torchinductor_jenkins/y4/cy4cpawkjlxkmdi5q377bbzfprxlg4jaehhxvv2wwpvnsvgolc64.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=1 -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/py_3.10/include/python3.10 -L/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/lib -L/opt/conda/envs/py_3.10/lib -L/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -mavx2 -mfma -DCPU_CAPABILITY_AVX2 -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_jenkins/y4/cy4cpawkjlxkmdi5q377bbzfprxlg4jaehhxvv2wwpvnsvgolc64.so

Output:
/tmp/torchinductor_jenkins/y4/cy4cpawkjlxkmdi5q377bbzfprxlg4jaehhxvv2wwpvnsvgolc64.cpp:2:10: fatal error: /tmp/tmpwy_5hkqj/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h: No such file or directory
    2 | #include "/tmp/tmpwy_5hkqj/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h"
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


To execute this test, run the following from the base repo dir:
     python test/test_autograd.py -k test_variable_traverse

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 30 Apr 2024 04:45:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125229</guid>
    </item>
    <item>
      <title>DISABLED test_issue106555 (__main__.TestCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/125228</link>
      <description><![CDATA[<p>Platforms: linux, rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_issue106555&amp;suite=TestCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24413362476">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_issue106555</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "inductor/test_compiled_autograd.py", line 541, in test_issue106555
    output_tensor = model(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "inductor/test_compiled_autograd.py", line 512, in forward
    y = torch.utils.checkpoint.checkpoint(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 481, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/function.py", line 571, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 254, in forward
    outputs = run_function(*args)
  File "inductor/test_compiled_autograd.py", line 520, in _forward
    x = x + self.module_with_jit_1(x)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "inductor/test_compiled_autograd.py", line 501, in forward
    output = bias_sigmoid_mul_jit(x1, x2, self.linear_2_bias)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "inductor/test_compiled_autograd.py", line 484, in bias_sigmoid_mul
    def bias_sigmoid_mul(x1, x2, bias):
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 991, in forward
    return compiled_fn(full_args)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 130, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/utils.py", line 118, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 188, in rng_functionalization_wrapper
    return compiled_fw(args)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 968, in __call__
    return self.current_callable(inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 946, in run
    return compiled_fn(new_inputs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/cudagraph_trees.py", line 368, in deferred_cudagraphify
    fn, out = cudagraphify(model, inputs, new_static_input_idxs, *args, **kwargs)
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/cudagraph_trees.py", line 390, in cudagraphify
    manager = get_container(device_index).get_tree_manager()
  File "/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_inductor/cudagraph_trees.py", line 325, in get_container
    lock = get_obj(local, "tree_manager_locks")[device_index]
TypeError: 'builtin_function_or_method' object is not subscriptable

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test_compiled_autograd.py -k test_issue106555

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 30 Apr 2024 04:45:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125228</guid>
    </item>
    <item>
      <title>DISABLED test_var_mean_differentiable (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/125220</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_var_mean_differentiable&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24409880535">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 44 workflow(s) with 132 failures and 44 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_var_mean_differentiable</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<details><summary>Sample error message</summary>

```
Traceback (most recent call last):
  File "/var/lib/jenkins/workspace/test/test_autograd.py", line 4055, in test_var_mean_differentiable
    torch.autograd.backward(r1, grad)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/graph.py", line 767, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 735, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 315, in __call__
    raise e
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py", line 302, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state, skip=1)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert
    return _compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py", line 70, in wrapper_function
    return function(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform
    tracer.run()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2230, in run
    super().run()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 880, in run
    while self.step():
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 795, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2387, in RETURN_VALUE
    self._return(inst)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2372, in _return
    self.output.compile_subgraph(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1093, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1285, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1376, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1357, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 127, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/__init__.py", line 1784, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
  File "/var/lib/jenkins/workspace/test/inductor/test_compiled_autograd.py", line 28, in inner_compiler
    return inductor.compile(gm_, example_inputs_)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/__init__.py", line 28, in compile
    return compile_fx(gm, example_inputs, config_patches=options)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1276, in compile_fx
    return flatten_graph_inputs(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 2669, in flatten_graph_inputs
    compiled_fn = compile_gm(GmWrapper(gm, spec), inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1454, in compile_fx
    return aot_autograd(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 156, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1358, in fw_compiler_base
    return inner_compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 483, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 779, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1692, in compile_to_fn
    return self.compile_to_module().call
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1639, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2456, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_jenkins/nd/cndvljvii7kpsfrzrtjfuaca36smxuqgalsxrzu33vgtnt5nwcgz.py", line 118, in <module>
    async_compile.wait(globals())
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3058, in wait
    scope[key] = result.result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2867, in result
    return self.result_fn()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2336, in future
    result = get_result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2164, in load_fn
    future.result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/conda/envs/py_3.10/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2189, in _worker_compile_cpp
    compile_file(input_path, output_path, shlex.split(cmd))
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper
    r = func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 2060, in compile_file
    raise exc.CppCompileError(cmd, output) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inner_compiler' raised:
CppCompileError: C++ compile error

Command:
g++ /tmp/torchinductor_jenkins/4v/c4vllsrkkrkxck7raqicocnahxztleava4tngpsw224wwmafohc5.cpp -shared -fPIC -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -D_GLIBCXX_USE_CXX11_ABI=1 -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/py_3.10/include/python3.10 -L/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/lib -L/opt/conda/envs/py_3.10/lib -L/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/lib -ltorch -ltorch_cpu -lgomp -ltorch_python -lc10 -mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma -DCPU_CAPABILITY_AVX512 -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -march=native -fopenmp -D C10_USING_CUSTOM_GENERATED_MACROS -o /tmp/torchinductor_jenkins/4v/c4vllsrkkrkxck7raqicocnahxztleava4tngpsw224wwmafohc5.so

Output:
/tmp/torchinductor_jenkins/4v/c4vllsrkkrkxck7raqicocnahxztleava4tngpsw224wwmafohc5.cpp:2:10: fatal error: /tmp/tmp_b99aukq/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h: No such file or directory
    2 | #include "/tmp/tmp_b99aukq/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h"
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True


To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ASAN=1 PYTORCH_TEST_WITH_UBSAN=1 python test/test_autograd.py -k test_var_mean_differentiable

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```

</details>

<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 30 Apr 2024 01:39:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/125220</guid>
    </item>
    <item>
      <title>[inductor] autotune benchmark support for cpu</title>
      <link>https://github.com/pytorch/pytorch/pull/125159</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124021<br />
* <strong>-&gt;</strong> #125159<br />
* #125152</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 29 Apr 2024 06:57:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125159</guid>
    </item>
    <item>
      <title>[inductor][cpp] move some common cpp utils to cpp_utils.py</title>
      <link>https://github.com/pytorch/pytorch/pull/125152</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124021<br />
* #125159<br />
* <strong>-&gt;</strong> #125152</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Mon, 29 Apr 2024 05:28:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/125152</guid>
    </item>
    <item>
      <title>Remove Inductor IRs for legacy functional collectives</title>
      <link>https://github.com/pytorch/pytorch/pull/124992</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124992</p>
<p>This PR completely removes the Inductor IR for legacy functional collectives:<br />
- Removed the <code>CollectiveKernel</code> hiearchy and <code>Wait</code>, as well as the corresponding lowerings. These IRs are target (i.e. Python) specific and don't model node dependencies propoerly (e.g. they rely on <code>never_reuse_buffers</code> for correct behavior). They've been superceded by <code>ir._CollectiveKernel</code>.<br />
- Removed <code>InPlaceHint</code> and the scheduler logic for handling it. <code>InPlaceHint</code> is a codegen-time buffer reuse mechanism controlled by the IR's codegen. It's a bit hacky and overlaps with the default buffer reuse mechanism. Removing it since it is only used by legacy functional collectives.<br />
- Removed <code>OutputBuffer</code> and <code>MultiOutputNoSizeAssert</code> which are designed for and only used by legacy functional collectives.</p>
<p>cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang @d4l3k @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 17:06:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124992</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Fix cutlass_utils.get_max_alignment() for strided layouts.</title>
      <link>https://github.com/pytorch/pytorch/pull/124930</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124928<br />
* <strong>-&gt;</strong> #124930<br />
* #124929</p>
<p>Fixes cutlass_utils.get_max_alignment() which was so far not checking the alignment properly. Basically<br />
the method so far assumed that the passed layout is contiguous and row-major, which does not have to be true.</p>
<p>Test Plan:<br />
CI - test_cutlass_backend.py to prevent regressions<br />
Added unit test</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 03:21:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124930</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Remove epilogue nodes from Kernel call</title>
      <link>https://github.com/pytorch/pytorch/pull/124929</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124928<br />
* #124930<br />
* <strong>-&gt;</strong> #124929</p>
<p>Minor refactoring:<br />
Remove unused "fused epilogue node" arguments from some method  Kernel call signatures.</p>
<p>Test Plan:<br />
Covered by current tests in test_cutlass_backend.py - no functional change.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 03:14:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124929</guid>
    </item>
    <item>
      <title>[Inductor max autotune] Make autotune_select_algorithm more robust</title>
      <link>https://github.com/pytorch/pytorch/pull/124928</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* <strong>-&gt;</strong> #124928<br />
* #124930<br />
* #124929</p>
<p>This diff makes sure that a custom exception is thrown when no valid<br />
choices remain during autotuning. This allows to gracefully fall back<br />
to a default choice, even if that default choice has not been passed to<br />
autotune_select_algorithm.</p>
<p>Additionally, this diff handles RuntimeErrors during autotuning gracefully, e.g. the corresponding choice is ignored but it does not lead to the compilation failure of the entire model if a problematic choice is encountered during autotuning.<br />
( An error is being logged, though).</p>
<p>TODO:<br />
 * Add unit test<br />
 * Add an assertion that we use autune_in_subproc when CUTLASS backend is enabled</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 25 Apr 2024 03:14:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124928</guid>
    </item>
    <item>
      <title>torch.compile error: Unsupported reduction type from torch.float32 to torch.int64 </title>
      <link>https://github.com/pytorch/pytorch/issues/124821</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>when trying the following code, it will throw an error. I think it's related to int64, when I change the input type int float32, there is no such a problem.<br />
```<br />
import numpy as np<br />
import torch</p>
<h1>Model definition</h1>
<p>class M(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.v2_0 = torch.nn.Parameter(torch.empty([1, 22, 51], dtype=torch.int64), requires_grad=False)</p>
<pre><code>def forward(self, _args):
    v2_0 = self.v2_0
    getitem = _args
    max_1 = getitem.max(0)
    getattr_1 = max_1.values
    max_2 = torch.max(getitem, v2_0)
    return (getattr_1, max_2)
</code></pre>
<p>m = M()</p>
<p>inp =  torch.from_numpy(np.zeros((22, 51),dtype=np.int64))<br />
m(inp) # this line is OK<br />
opt = torch.compile(m, fullgraph=True, backend='inductor', mode=None)<br />
opt(inp) # this line will crash<br />
```</p>
<h3>Error logs</h3>
<p>C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0] Error in codegen for ComputedBuffer(name='buf0', layout=FixedLayout('cpu', torch.int64, size=[51], stride=[1]), data=Reduction(<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   'cpu',<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   torch.int64,<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   def inner_fn(index, rindex):<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]       i0 = index<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]       r0 = rindex<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]       tmp0 = ops.load(arg1_1, i0 + 51 * r0)<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]       return tmp0<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   ,<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   ranges=[51],<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   reduction_ranges=[22],<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   reduction_type=max,<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   origin_node=getitem,<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0]   origins={max_1}<br />
C0424 14:47:08.387000 139979197380416 torch/_inductor/scheduler.py:789] [0/0] ))<br />
W0424 14:47:08.389000 139979197380416 torch/_dynamo/repro/after_dynamo.py:108] [0/0] Compiled Fx GraphModule failed. Creating script to minify the error.<br />
W0424 14:47:08.390000 139979197380416 torch/_dynamo/debug_utils.py:276] [0/0] Writing minified repro to:<br />
W0424 14:47:08.390000 139979197380416 torch/_dynamo/debug_utils.py:276] [0/0] /home/zhangzihan/nnsmith/torch_compile_debug/run_2024_04_24_14_47_08_390122-pid_1824613/minifier/minifier_launcher.py<br />
W0424 14:47:08.390000 139979197380416 torch/_dynamo/repro/after_dynamo.py:108] [0/0] Compiled Fx GraphModule failed. Creating script to minify the error.<br />
W0424 14:47:08.391000 139979197380416 torch/_dynamo/debug_utils.py:276] [0/0] Writing minified repro to:<br />
W0424 14:47:08.391000 139979197380416 torch/_dynamo/debug_utils.py:276] [0/0] /home/zhangzihan/nnsmith/torch_compile_debug/run_2024_04_24_14_47_08_390122-pid_1824613/minifier/minifier_launcher.py<br />
Traceback (most recent call last):<br />
  File "/home/zhangzihan/nnsmith/./bug_1.py", line 26, in <module><br />
    opt(inp) # this line will crash<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 403, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 977, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state, skip=1)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 411, in _convert_frame_assert<br />
    return _compile(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_utils_internal.py", line 70, in wrapper_function<br />
    return function(*args, </strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(<em>args, </em><em>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 700, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 568, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1116, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 173, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 515, in transform<br />
    tracer.run()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2237, in run<br />
    super().run()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 875, in run<br />
    while self.step():<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 790, in step<br />
    self.dispatch_table<a href="self, inst">inst.opcode</a><br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2394, in RETURN_VALUE<br />
    self._return(inst)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2379, in _return<br />
    self.output.compile_subgraph(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1082, in compile_subgraph<br />
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(<em>args, </em><em>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1274, in compile_and_call_fx_graph<br />
    compiled_fn = self.call_user_compiler(gm)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/<em>dynamo/output_graph.py", line 1365, in call_user_compiler<br />
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1346, in call_user_compiler<br />
    compiled_fn = compiler_fn(gm, self.example_inputs())<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 105, in debug_wrapper<br />
    compiled_gm = compiler_fn(copy.deepcopy(gm), example_inputs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 105, in debug_wrapper<br />
    compiled_gm = compiler_fn(copy.deepcopy(gm), example_inputs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/<strong>init</strong>.py", line 1742, in <strong>call</strong><br />
    return compile_fx(model</em>, inputs_, config_patches=self.config)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1416, in compile_fx<br />
    return aot_autograd(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 65, in compiler_fn<br />
    cg = aot_module_simplified(gm, example_inputs, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 958, in aot_module_simplified<br />
    compiled_fn = create_aot_dispatcher_function(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 685, in create_aot_dispatcher_function<br />
    compiled_fn = compiler_fn(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 470, in aot_wrapper_dedupe<br />
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in aot_wrapper_synthetic_base<br />
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 149, in aot_dispatch_base<br />
    compiled_fw = compiler(fw_module, updated_flat_args)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(<em>args, </em><em>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1320, in fw_compiler_base<br />
    return inner_compile(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper<br />
    inner_compiled_fn = compiler_fn(gm, example_inputs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/debug.py", line 304, in inner<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(<em>args, </em><em>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 477, in compile_fx_inner<br />
    compiled_graph = fx_codegen_and_compile(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/contextlib.py", line 79, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 752, in fx_codegen_and_compile<br />
    compiled_fn = graph.compile_to_fn()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1541, in compile_to_fn<br />
    return self.compile_to_module().call<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(<em>args, </em><em>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1484, in compile_to_module<br />
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1440, in codegen<br />
    self.scheduler.codegen()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 268, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 2485, in codegen<br />
    self.get_backend(device).codegen_node(node)  # type: ignore[possibly-undefined]<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/cpp.py", line 3865, in codegen_node<br />
    cpp_kernel_proxy.codegen_nodes(nodes)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/cpp.py", line 3517, in codegen_nodes<br />
    vec_kernel = codegen_kernel(<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/cpp.py", line 3411, in codegen_kernel<br />
    run(kernel)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/cpp.py", line 3423, in run<br />
    node.run(vars, reduction_vars)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 765, in run<br />
    self.codegen(index_vars)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/scheduler.py", line 787, in codegen<br />
    self._body(<em>index_vars)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/ir.py", line 7760, in <strong>call</strong><br />
    result = self.root_block()<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/ir.py", line 7917, in <strong>call</strong><br />
    return InterpreterShim(graph, submodules).run(V.get_ops_handler())<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/ir.py", line 7662, in run<br />
    return super().run(</em>args, </strong>kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/fx/interpreter.py", line 145, in run<br />
    self.env[node] = self.run_node(node)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/ir.py", line 7658, in run_node<br />
    return super().run_node(n)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/fx/interpreter.py", line 202, in run_node<br />
    return getattr(self, n.op)(n.target, args, kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/fx/interpreter.py", line 296, in call_method<br />
    return getattr(self_obj, target)(<em>args_tail, </em>*kwargs)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/sizevars.py", line 641, in store_reduction<br />
    return self._inner.store_reduction(name, self._simplify(index), value)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/common.py", line 1570, in store_reduction<br />
    return self.store_reduction(name, index, value)<br />
  File "/home/zhangzihan/micromamba/envs/nnsmith-nightly/lib/python3.10/site-packages/torch/_inductor/codegen/cpp.py", line 2585, in store_reduction<br />
    raise AssertionError(<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
AssertionError: Unsupported reduction type from torch.float32 to torch.int64</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>Minifier script written to /home/zhangzihan/nnsmith/torch_compile_debug/run_2024_04_24_14_47_08_390122-pid_1824613/minifier/minifier_launcher.py. Run this script to find the smallest traced graph which reproduces this error.</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True</p>
<h3>Minified repro</h3>
<p>from math import inf<br />
import torch<br />
from torch import tensor, device<br />
import torch.fx as fx<br />
import torch._dynamo<br />
from torch._dynamo.testing import rand_strided<br />
from torch._dynamo.debug_utils import run_fwd_maybe_bwd</p>
<p>import torch._dynamo.config<br />
import torch._inductor.config<br />
import torch._functorch.config<br />
import torch.fx.experimental._config</p>
<p>from torch.nn import *<br />
class Repro(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.L__self___v2_0 = torch.nn.Parameter(torch.randn([1, 22, 51], dtype=torch.int64))</p>
<pre><code>def forward(self, L_args_ : torch.Tensor):
    l_args_ = L_args_
    v2_0 = self.L__self___v2_0
    max_1 = l_args_.max(0)
    getattr_1 = max_1[0];  max_1 = None
    max_2 = torch.max(l_args_, v2_0);  l_args_ = v2_0 = None
    return (getattr_1, max_2)
</code></pre>
<p>mod = Repro()</p>
<p>def load_args(reader):<br />
    buf0 = reader.storage('fdc2a09f82bac0b6462614f84f565597f805bdca', 8976, dtype_hint=torch.int64)<br />
    reader.tensor(buf0, (22, 51), dtype=torch.int64, is_leaf=True)  # L_args_<br />
load_args._version = 0</p>
<p>if <strong>name</strong> == '<strong>main</strong>':<br />
    from torch._dynamo.repro.after_dynamo import run_repro<br />
    run_repro(mod, load_args, accuracy=False, command='minify',<br />
        save_dir='/home/zhangzihan/nnsmith/torch_compile_debug/run_2024_04_24_14_47_08_390122-pid_1824613/minifier/checkpoints', autocast=False, backend=None)</p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.4.0.dev20240422+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.3 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.29.2<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-15-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: NVIDIA GeForce RTX 4090<br />
GPU 1: NVIDIA GeForce RTX 4090</p>
<p>Nvidia driver version: 525.147.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             32<br />
On-line CPU(s) list:                0-31<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         13th Gen Intel(R) Core(TM) i9-13900K<br />
CPU family:                         6<br />
Model:                              183<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 24<br />
Socket(s):                          1<br />
Stepping:                           1<br />
CPU max MHz:                        5800.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           5990.40<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          896 KiB (24 instances)<br />
L1i cache:                          1.3 MiB (24 instances)<br />
L2 cache:                           32 MiB (12 instances)<br />
L3 cache:                           36 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-31<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+45fff310c8<br />
[pip3] torch==2.4.0.dev20240422+cu118<br />
[pip3] torchaudio==2.2.0.dev20240422+cu118<br />
[pip3] torchvision==0.19.0.dev20240422+cu118<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @desertfire @chenyang78</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 22:51:13 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124821</guid>
    </item>
    <item>
      <title>Ensure only builtins functions are wrapped in new frame for torch.compile</title>
      <link>https://github.com/pytorch/pytorch/pull/124720</link>
      <description><![CDATA[<p>Fixes #124269</p>
<p>I'm not sure which cases the existing check via <code>filename</code> is supposed to handle, but if the comment is right and <code>external_utils.wrap_inline</code> is supposed to be applied only on builtin functions, a check for <code>types.BuiltinFunctionType</code> should be sufficient.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 02:52:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124720</guid>
    </item>
    <item>
      <title>Compile doesn't guard on user NN module attribute</title>
      <link>https://github.com/pytorch/pytorch/issues/124717</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>TorchTune relies on mutating user NN module's attribute to decide whether to e.g. apply LoRA technique. The usage pattern looks like this:</p>
<p>```python<br />
import contextlib<br />
import torch</p>
<p>class TestModuleWithAdapter(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.disabled = False</p>
<pre><code>def forward(self, x):
    out = x * x
    if self.disabled:
        return out
    out = out + 1
    return out
</code></pre>
<p>@contextlib.contextmanager<br />
def disable_adapter(mod):<br />
    mod.disabled = True<br />
    try:<br />
        yield<br />
    finally:<br />
        mod.disabled = False</p>
<p>mod = TestModuleWithAdapter()<br />
mod = torch.compile(mod)<br />
x = torch.randn(4, 4)<br />
out_enable = mod(x)<br />
with disable_adapter(mod):<br />
    out_disable = mod(x)<br />
assert torch.allclose(out_enable, x * x + 1)<br />
assert torch.allclose(out_disable, x * x)   # &lt;-- fails<br />
```</p>
<p>User expects that by wrapping second <code>mod(x)</code> call with <code>with disable_adapter(mod):</code>, the <code>if self.disabled:</code> branch in TestModuleWithAdapter.forward takes effect. But currently we don't detect that an attribute of the compiled module is mutated and thus recompile is not triggered. This causes mismatch in behavior between eager and compile mode.</p>
<p>(Corresponding issue on TorchTune repo: https://github.com/pytorch/torchtune/issues/721)</p>
<h3>Versions</h3>
<p>PyTorch nightly</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78</p>]]></description>
      <pubDate>Tue, 23 Apr 2024 01:11:45 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124717</guid>
    </item>
    <item>
      <title>torch.compile does not work since 2.2.1 on MacOS for some models</title>
      <link>https://github.com/pytorch/pytorch/issues/124497</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>The execution hangs when first calling the model to warm-up. <em>After</em> will never be printed. This is on an Apple M3 Max. Smaller models work sometimes.</p>
<p>```python<br />
import torch<br />
from transformers import AutoModelForImageClassification</p>
<p>neural_network = AutoModelForImageClassification.from_pretrained("microsoft/resnet-50")<br />
input_data = torch.randn(1, 3, 228, 228)<br />
print(neural_network(input_data))<br />
neural_network_c = torch.compile(neural_network, backend="inductor")<br />
print("Before")<br />
neural_network_c(input_data)<br />
print("After")<br />
```</p>
<p>The last version that worked was torch 2.2.0. </p>
<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.2.1<br />
Is debug build: False<br />
CUDA used to build PyTorch: None<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: macOS 14.4.1 (arm64)<br />
GCC version: Could not collect<br />
Clang version: 15.0.0 (clang-1500.3.9.4)<br />
CMake version: version 3.28.4<br />
Libc version: N/A</p>
<p>Python version: 3.11.8 (main, Feb  6 2024, 21:21:21) [Clang 15.0.0 (clang-1500.1.0.2.5)] (64-bit runtime)<br />
Python platform: macOS-14.4.1-arm64-arm-64bit<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Apple M3 Max</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] onnx==1.16.0<br />
[pip3] torch==2.2.1<br />
[conda] Could not collect<br />
```</p>
<p>Thanks for your help!</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @malfet @albanD @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 08:14:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124497</guid>
    </item>
    <item>
      <title>DISABLED test_multi_backward (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124491</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, slow, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_multi_backward&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/24025614016">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_multi_backward</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 19 Apr 2024 07:40:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124491</guid>
    </item>
    <item>
      <title>[inductor] switch assume_aligned_inputs to False</title>
      <link>https://github.com/pytorch/pytorch/pull/124336</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124336</p>
<p>In #123319, we guard some behavior behind the <code>assume_aligned_inputs</code> config option. If we set this to <code>False</code>, then the behavior added in #123319 becomes the default behavior. See the referenced PR for more details about the behavior affected.</p>
<p>Side effects:<br />
* It's possible that this will hurt performance in some scenarios. For example, if an unaligned input is used in a matmul, it might be better to perform the clone to align it first.<br />
* This will occasionally cause recompiles. Specifically: the check we perform (<code>(storage_offset * get_dtype_size(dtype)) % ALIGNMENT == 0</code>) can be guarded on if the storage_offset becomes dynamic. storage_offset becomes dynamic during automatic_dynamic_shapes after a shape or stride changes. Previously, this was increasing graph breaks in cpu inductor torchbench tests (but is fixed by more carefully guarding checks on alignment, so that we don't run them and generate guards unless actually needed).</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 17 Apr 2024 15:14:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124336</guid>
    </item>
    <item>
      <title>[inductor] Enable fx graph caching by default</title>
      <link>https://github.com/pytorch/pytorch/pull/124091</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124091</p>
<p>Summary: But leave it off in internally for starters</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 11:35:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124091</guid>
    </item>
    <item>
      <title>[WIP][inductor] refine loop split logic</title>
      <link>https://github.com/pytorch/pytorch/pull/124060</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124060</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 05:52:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124060</guid>
    </item>
    <item>
      <title>[inductor][cpp] GEMM template</title>
      <link>https://github.com/pytorch/pytorch/pull/124021</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124021<br />
* #125159<br />
* #125152</p>
<p>cc @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 14 Apr 2024 06:21:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124021</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] DO NOT REVIEW - to be split up</title>
      <link>https://github.com/pytorch/pytorch/pull/121492</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121492<br />
* #124928<br />
* #124930<br />
* #124929</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121492</guid>
    </item>
    <item>
      <title>Fix typo under torch/_inductor directory</title>
      <link>https://github.com/pytorch/pytorch/pull/119658</link>
      <description><![CDATA[<p>This PR fixes typo in comments and msgs under <code>torch/_inductor</code> directory, and also changes the corresponding test.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov</p>]]></description>
      <pubDate>Sun, 11 Feb 2024 10:05:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119658</guid>
    </item>
    <item>
      <title>`torch.func.functional_call` doesn't work with compiled models</title>
      <link>https://github.com/pytorch/pytorch/issues/97909</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>When creating a custom model with <code>nn.Module</code> and compiling it with <code>torch.compile()</code>, the output of <code>torch.func.functional_call()</code> remains the same even if providing different <code>parameter_and_buffer_dicts</code>. Below is an example:<br />
```python<br />
import torch<br />
import torch.nn as nn</p>
<p>class LinearNet(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.linear = nn.Linear(1, 3)</p>
<pre><code>def forward(self, x):
    return self.linear(x)
</code></pre>
<p>inputs = torch.randn(1, 1)</p>
<p>print('instantiate w/ nn.Module + compile:')<br />
model = torch.compile(LinearNet())<br />
params = dict(model.named_parameters())<br />
print(torch.func.functional_call(model, params, inputs))</p>
<h1>set parameters to 0 so that outputs should also be 0</h1>
<p>for name, param in params.items():<br />
    params[name] = torch.zeros_like(param)<br />
print(torch.func.functional_call(model, params, inputs))</p>
<p>print('instantiate w/ nn.Linear + compile:')<br />
model = torch.compile(nn.Linear(1, 3))<br />
params = dict(model.named_parameters())<br />
print(torch.func.functional_call(model, params, inputs))<br />
for name, param in params.items():<br />
    params[name] = torch.zeros_like(param)<br />
print(torch.func.functional_call(model, params, inputs))</p>
<p>print('instantiate w/ nn.Module + no compile:')<br />
model = LinearNet()<br />
params = dict(model.named_parameters())<br />
print(torch.func.functional_call(model, params, inputs))<br />
for name, param in params.items():<br />
    params[name] = torch.zeros_like(param)<br />
print(torch.func.functional_call(model, params, inputs))</p>
<p><code>Outputs:</code><br />
instantiate w/ nn.Module + compile:<br />
tensor([[-0.2246,  0.4658,  0.3510]], grad_fn=<CompiledFunctionBackward>)<br />
tensor([[-0.2246,  0.4658,  0.3510]], grad_fn=<CompiledFunctionBackward>)<br />
instantiate w/ nn.Linear + compile:<br />
tensor([[ 0.3361,  0.3872, -0.6998]], grad_fn=<AddmmBackward0>)<br />
tensor([[0., 0., 0.]])<br />
instantiate w/ nn.Module + no compile:<br />
tensor([[-0.2540, -0.5778,  1.1222]], grad_fn=<AddmmBackward0>)<br />
tensor([[0., 0., 0.]])<br />
```<br />
Updates: In PyTorch 2.2, the 2nd result for "nn.Linear + compile" becomes non-zero.</p>
<details>
<summary>Versions</summary>

PyTorch version: 2.0.0
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.2 LTS (x86_64)
GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.35

Python version: 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.19.0-38-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA RTX A5000
Nvidia driver version: 530.30.02
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Address sizes:                   46 bits physical, 48 bits virtual
Byte Order:                      Little Endian
CPU(s):                          20
On-line CPU(s) list:             0-19
Vendor ID:                       GenuineIntel
Model name:                      12th Gen Intel(R) Core(TM) i7-12700
CPU family:                      6
Model:                           151
Thread(s) per core:              2
Core(s) per socket:              12
Socket(s):                       1
Stepping:                        2
CPU max MHz:                     4900.0000
CPU min MHz:                     800.0000
BogoMIPS:                        4224.00
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities
Virtualization:                  VT-x
L1d cache:                       512 KiB (12 instances)
L1i cache:                       512 KiB (12 instances)
L2 cache:                        12 MiB (9 instances)
L3 cache:                        25 MiB (1 instance)
NUMA node(s):                    1
NUMA node0 CPU(s):               0-19
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Not affected
Vulnerability Retbleed:          Not affected
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected

Versions of relevant libraries:
[pip3] numpy==1.23.5
[pip3] torch==2.0.0
[pip3] torchaudio==2.0.0
[pip3] torchvision==0.15.0
[pip3] triton==2.0.0
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0           py310h7f8727e_0  
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0  
[conda] mkl_random                1.2.2           py310h00e6091_0  
[conda] numpy                     1.23.5          py310hd5efca6_0  
[conda] numpy-base                1.23.5          py310h8e6c178_0  
[conda] pytorch                   2.0.0           py3.10_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_3    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.0.0               py310_cu117    pytorch
[conda] torchtriton               2.0.0                     py310    pytorch
[conda] torchvision               0.15.0              py310_cu117    pytorch
</details>

<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @soumith @wconstab @ngimel @Xia-Weiwen @desertfire</p>]]></description>
      <pubDate>Wed, 29 Mar 2023 12:54:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/97909</guid>
    </item>
  </channel>
</rss>
