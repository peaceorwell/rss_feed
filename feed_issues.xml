<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>[NestedTensor] torch.compile silent specialization on _max_seqlen</title>
      <link>https://github.com/pytorch/pytorch/issues/124205</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>```<br />
from torch.nested._internal.nested_tensor import NestedTensor<br />
import torch</p>
<p>@torch.compile<br />
@torch._dynamo.config.patch(error_on_recompile=True)<br />
def sample_fun(njt: NestedTensor) -&gt; NestedTensor:<br />
    njt = njt.clamp(0.1, 0.5)<br />
    # if this is NOT specialized - this should lead to second printed output be larger than first<br />
    njt *= njt._max_seqlen<br />
    return njt</p>
<p>def create_njt(el_per_row):<br />
    torch.manual_seed(0)<br />
    njt = NestedTensor(<br />
        values=torch.randn(10 * el_per_row, device="cuda"),<br />
        offsets=torch.arange(11, device="cuda") * el_per_row,<br />
    )<br />
    # This sets _max_seqlen in cache<br />
    print(njt._max_seqlen)<br />
    return njt</p>
<p>with torch.inference_mode():<br />
    # This works<br />
    print(sample_fun(create_njt(el_per_row=1)).values())<br />
    # But this printed output should be 2x as big - and it is NOT as max_seqlen is specialized in generated code.<br />
    print(sample_fun(create_njt(el_per_row=2)).values())<br />
```</p>
<h3>Versions</h3>
<p>trunk</p>
<h3>Behavior</h3>
<p>As illustrated in code snippet above, NestedTensor._max_seqlen property is being silently specialized when compiled - problematic behavior (silent hardcoding of value used for tracing) can be reproduced in both <code>inductor</code> and <code>eager</code> compilation modes.</p>
<p>Expected behavior:<br />
* compiler generated dynamic code that doesn't specialize into specific value of NestedTensor<br />
OR at least fails prominently if specialization occurs - and guards kick in.</p>
<p>Observed behavior:<br />
* success that leads to wrong outputs, as torch.compile produces specialized code without adding guards.</p>
<p>cc @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 11:31:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124205</guid>
    </item>
    <item>
      <title>DISABLED test_comprehensive_ones_cuda_int32 (__main__.TestInductorOpInfoCUDA)</title>
      <link>https://github.com/pytorch/pytorch/issues/124202</link>
      <description><![CDATA[<p>Platforms: inductor</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_comprehensive_ones_cuda_int32&amp;suite=TestInductorOpInfoCUDA&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23870713949">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_comprehensive_ones_cuda_int32</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_torchinductor_opinfo.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 10:39:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124202</guid>
    </item>
    <item>
      <title>[torch.compile][FlopCounter] AssertionError: Global is not OptimizedModule._orig_mod</title>
      <link>https://github.com/pytorch/pytorch/issues/124196</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>The error is coming from the <code>FlopCounterMode</code>. This seems like a tensor subclass. So, opening an issue.</p>
<p>The resulting flops probably needs to be cleaned up as well.</p>
<p>cc @Chillee </p>
<p>~~~<br />
import torch<br />
from torch.utils.flop_counter import FlopCounterMode</p>
<p>class SimpleMod(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.linear = torch.nn.Linear(3, 3)</p>
<pre><code>def forward(self, x):
    return torch.sin(self.linear(x))
</code></pre>
<p>inp = torch.randn(3, 3, device='cuda', requires_grad=True)</p>
<h1>mod = torch.nn.Linear(3, 3).to(device="cuda")</h1>
<p>mod = SimpleMod().to(device="cuda")</p>
<p>mod = torch.compile(mod, backend="eager")</p>
<p>flop_counter = FlopCounterMode(mod)</p>
<p>with flop_counter:<br />
    for i in range(20):<br />
        mod(inp).sum().backward()<br />
~~~</p>
<h3>Error logs</h3>
<p>~~~<br />
Module                         FLOP    % Total</p>
<hr />
<p>Global                          216    100.00%<br />
 - aten.addmm                   108     50.00%<br />
 - aten.mm                      108     50.00%<br />
 OptimizedModule                162     75.00%<br />
  - aten.addmm                   54     25.00%<br />
  - aten.mm                     108     50.00%<br />
  OptimizedModule._orig_mod     162     75.00%<br />
   - aten.addmm                  54     25.00%<br />
   - aten.mm                    108     50.00%<br />
Traceback (most recent call last):<br />
  File "/data/users/anijain/pytorch2/examples/valerie.py", line 23, in <module><br />
    mod(inp).sum().backward()<br />
    ^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/nn/modules/module.py", line 1582, in _call_impl<br />
    result = forward_call(</em>args, <strong>kwargs)<br />
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/_dynamo/eval_frame.py", line 387, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/nn/modules/module.py", line 1595, in _call_impl<br />
    hook_result = hook(self, args, result)<br />
                  ^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/utils/flop_counter.py", line 412, in f<br />
    outputs = _pytreeify_preserve_structure(self._create_post_module(name))(outputs)<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/utils/flop_counter.py", line 330, in nf<br />
    out = f(</em>flat_args)<br />
          ^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/autograd/function.py", line 571, in apply<br />
    return super().apply(<em>args, </em>*kwargs)  # type: ignore[misc]<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/data/users/anijain/pytorch2/torch/utils/flop_counter.py", line 420, in forward<br />
    assert self.parents[-1] == name, f"{self.parents[-1]} is not {name}"<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^<br />
AssertionError: Global is not OptimizedModule._orig_mod<br />
~~~</p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>N/A</p>
<p>cc @ezyang @msaroufim @bdhirsh @chauhang</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 10:08:31 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124196</guid>
    </item>
    <item>
      <title>Re-enable AsyncCompile.warm_pool in 3.12 Inductor</title>
      <link>https://github.com/pytorch/pytorch/issues/124192</link>
      <description><![CDATA[<p>This PR (https://github.com/pytorch/pytorch/pull/122146/files#diff-c9b517f8db609ffa866804dfa2689188a4fee20abacaa0b0dca91625c1b5cb8d) disabled <code>AsyncCompile.warm_pool()</code> for 3.12 due to the following error:</p>
<p><code>/home/williamwen/local/installs/python3.12/debug/install/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1695471) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/home/williamwen/local/installs/python3.12/debug/install/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/home/williamwen/local/installs/python3.12/debug/install/lib/python3.12/concurrent/futures/process.py", line 389, in run
    self.join_executor_internals()
  File "/home/williamwen/local/installs/python3.12/debug/install/lib/python3.12/concurrent/futures/process.py", line 576, in join_executor_internals
    self._join_executor_internals()
  File "/home/williamwen/local/installs/python3.12/debug/install/lib/python3.12/concurrent/futures/process.py", line 581, in _join_executor_internals
    self.shutdown_workers()
  File "/home/williamwen/local/installs/python3.12/debug/install/lib/python3.12/concurrent/futures/process.py", line 569, in shutdown_workers
    self.call_queue.put_nowait(None)
  File "/home/williamwen/local/installs/python3.12/debug/install/lib/python3.12/multiprocessing/queues.py", line 138, in put_nowait
    return self.put(obj, False)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/williamwen/local/installs/python3.12/debug/install/lib/python3.12/multiprocessing/queues.py", line 94, in put
    self._start_thread()
  File "/home/williamwen/local/installs/python3.12/debug/install/lib/python3.12/multiprocessing/queues.py", line 192, in _start_thread
    self._thread.start()
  File "/home/williamwen/local/installs/python3.12/debug/install/lib/python3.12/threading.py", line 992, in start
    _start_new_thread(self._bootstrap, ())
RuntimeError: can't create new thread at interpreter shutdown
(hangs)</code></p>
<p>We should re-enable this call.</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @masnesral </p>]]></description>
      <pubDate>Tue, 16 Apr 2024 09:26:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124192</guid>
    </item>
    <item>
      <title>[inductor] disable comprehensive padding in fbcode</title>
      <link>https://github.com/pytorch/pytorch/pull/124191</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124191</p>
<p>Comprehension padding cause small NE change and fail an internal test. Disable it for internal use case to mitigate.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D56197430">D56197430</a></p>]]></description>
      <pubDate>Tue, 16 Apr 2024 09:24:03 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124191</guid>
    </item>
    <item>
      <title>Back out "[inductor] comprehensive padding (#120758)"</title>
      <link>https://github.com/pytorch/pytorch/pull/124188</link>
      <description><![CDATA[<p>Summary:<br />
Original commit changeset: ee3da41bea4e</p>
<p>Original Phabricator Diff: D56169861</p>
<p>This diff cause deterministic NE regression. see more details in:<br />
https://www.internalfb.com/sandcastle/job/4503600878465749?no_redirect=true</p>
<p>Test Plan:<br />
buck2 test 'fbcode//mode/opt' fbcode//aps_models/ads/icvr/tests/ne/e2e_deterministic_tests:fm_tests -- --exact 'aps_models/ads/icvr/tests/ne/e2e_deterministic_tests:fm_tests - aps_models.ads.icvr.tests.ne.e2e_deterministic_tests.icvr_fm_test.ICVR_FM_DeterministicTest: test_icvr_fm_pt2_fsdp_multi_gpus'</p>
<p>https://www.internalfb.com/intern/testinfra/testrun/17451448585965825</p>
<p>Differential Revision: D56195121</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 08:54:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124188</guid>
    </item>
    <item>
      <title>[dynamo][nn_module] Enable torch.compile/disable as decorators on the class</title>
      <link>https://github.com/pytorch/pytorch/pull/124187</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124121<br />
* #124109<br />
* #124108<br />
* <strong>-&gt;</strong> #124187<br />
* #124185</p>
<p>Support something like. This is UI change, so please review carefully.</p>
<p>~~~<br />
        @torch._dynamo.disable<br />
        class SimpleLinear(torch.nn.Module):<br />
            def <strong>init</strong>(self):<br />
                super().<strong>init</strong>()<br />
                self.layer0 = torch.nn.Linear(4, 4)</p>
<pre><code>        def forward(self, inp):
            return self.layer0(torch.sigmoid(inp))

    @torch.compile(backend=cnts)
    class SimpleModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.layer0 = SimpleLinear()
            self.layer1 = torch.nn.Linear(4, 4)

        def forward(self, inp):
            z = self.layer0(torch.sin(inp))
            return self.layer1(z)
</code></pre>
<p>~~~</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 08:52:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124187</guid>
    </item>
    <item>
      <title>DISABLED test_diagonal_expanded_v (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124183</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_diagonal_expanded_v&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23870662293">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_diagonal_expanded_v</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 07:39:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124183</guid>
    </item>
    <item>
      <title>Enable aot compile to support aten operations with scalar input like alpha</title>
      <link>https://github.com/pytorch/pytorch/pull/124177</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120595<br />
* #124070<br />
* <strong>-&gt;</strong> #124177<br />
* #116368<br />
* #121387<br />
* #123545</p>
<p>Some operations have a scalar input parameter, like <code>torch.add(a, b, alpha=2.0)</code>.  Currently, the aot compile does not support such a case because it requires the signature of the captured graph to align with the operation's signature. This means that some inputs in the captured graph may be scalar(float, int, bool, etc.). It breaks the assumption of <code>compile_fx_aot</code> as it assumes all the example inputs are tensor - https://github.com/pytorch/pytorch/blob/0f6ce45bcbd7026c00da43db0317ede10830378b/torch/_inductor/compile_fx.py#L1048</p>
<p>This PR intends to support such cases by allowing not-aligned signature and filtering out the non-Tensor parameters. </p>
<p>Captured graph for <code>torch.add(a, b, alpha=2.0)</code></p>
<p>```<br />
opcode         name      target           args              kwargs</p>
<hr />
<p>placeholder    arg0_1    arg0_1           ()                {}<br />
placeholder    arg1_1    arg1_1           ()                {}<br />
call_function  add       aten.add.Tensor  (arg0_1, arg1_1)  {'alpha': 2.0}<br />
output         output_1  output           ((add,),)         {}<br />
```</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 07:05:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124177</guid>
    </item>
    <item>
      <title>torch.compile :   RuntimeError: "foreach_tensor_copy" not implemented for 'Int'</title>
      <link>https://github.com/pytorch/pytorch/issues/124170</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Repro: clone pytorch/torchchat<br />
```<br />
jf download GICWmAAijYKQf_QKAGizRfEEmI8EbpZBAAAB --file "cp.tgz"<br />
tzr xvzf cp.tgz # for sample input model</p>
<p>export MODEL_PATH=checkpoints/stories15M/stories15M.pt<br />
export MODEL_NAME=stories15M<br />
export MODEL_DIR=/tmp</p>
<p>python generate.py --dtype bfloat16 --device cuda --compile --checkpoint-path checkpoints/stories15M/stories15M.pt --temperature 0<br />
```</p>
<p>Repro in torch/chat ci: https://github.com/pytorch/torchchat/actions/runs/8706471125/job/23879108867?pr=213<br />
<code>python generate.py --dtype bfloat16 --device cuda --compile --checkpoint-path checkpoints/stories15M/stories15M.pt --temperature 0
  /opt/conda/lib/python3.11/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
    warnings.warn(
  /opt/conda/lib/python3.11/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
    warnings.warn(
  Traceback (most recent call last):
    File "/pytorch/torchchat/generate.py", line 512, in &lt;module&gt;
      cli()
    File "/pytorch/torchchat/generate.py", line 508, in cli
      main(args)
    File "/pytorch/torchchat/generate.py", line 489, in main
      _main(
    File "/pytorch/torchchat/generate.py", line 434, in _main
      y, metrics = generate(
                   ^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
      return func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
    File "/pytorch/torchchat/generate.py", line 285, in generate
      generated_tokens, _ = decode_n_tokens(
                            ^^^^^^^^^^^^^^^^
    File "/pytorch/torchchat/generate.py", line 137, in decode_n_tokens
      next_token, next_prob = decode_one_token(
                              ^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 410, in _fn
      return fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
    File "/pytorch/torchchat/generate.py", line 115, in decode_one_token
      def decode_one_token(
    File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 410, in _fn
      return fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
      return fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py", line 991, in forward
      return compiled_fn(full_args)
             ^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 130, in runtime_wrapper
      all_outs = call_func_at_runtime_with_args(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 116, in call_func_at_runtime_with_args
      out = normalize_as_list(f(args))
                              ^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 181, in rng_functionalization_wrapper
      return compiled_fw(args)
             ^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_inductor/codecache.py", line 960, in __call__
      return self.current_callable(inputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 910, in run
      return compiled_fn(new_inputs)
             ^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 369, in deferred_cudagraphify
      return fn(inputs)
             ^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 853, in run
      return model(new_inputs)
             ^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1781, in run
      out = self._run(new_inputs, function_id)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1859, in _run
      return self.record_function(new_inputs, function_id)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1890, in record_function
      node = CUDAGraphNode(
             ^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 849, in __init__
      recording_inputs = self._allocate_and_copy_recording_inputs(inputs)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/conda/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1526, in _allocate_and_copy_recording_inputs
      self._copy_inputs_and_remove_from_src(recording_inputs, inputs)
    File "/opt/conda/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 941, in _copy_inputs_and_remove_from_src
      torch._foreach_copy_(dst_tensors, src_tensors)
    File "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py", line 78, in __torch_function__
      return func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  RuntimeError: "foreach_tensor_copy" not implemented for 'Int'
  Error: Process completed with exit code 1.</code></p>
<h3>Versions</h3>
<p>host: Linux x86 with CUDA in Pytorch CI with nightlies</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 05:52:54 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124170</guid>
    </item>
    <item>
      <title>DISABLED test_detach (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124165</link>
      <description><![CDATA[<p>Platforms: rocm, linux, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_detach&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23863568735">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_detach</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 04:45:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124165</guid>
    </item>
    <item>
      <title>DISABLED test_dependent_backward (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124162</link>
      <description><![CDATA[<p>Platforms: rocm, asan, linux, slow</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_dependent_backward&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23859695791">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_dependent_backward</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>ConnectionTimeoutError: Connect timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/inductor/test_compiled_autograd.py -2 (connected: false, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)<br />
headers: {}</p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000</p>]]></description>
      <pubDate>Tue, 16 Apr 2024 01:39:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124162</guid>
    </item>
    <item>
      <title>DISABLED test_dep_nograd (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/124153</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_dep_nograd&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23856825772">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 68 workflow(s) with 204 failures and 68 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_dep_nograd</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 22:39:48 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124153</guid>
    </item>
    <item>
      <title>WIP: [inductor] consider pointwise nodes when deciding reduction hint</title>
      <link>https://github.com/pytorch/pytorch/pull/124131</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124131</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 17:21:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124131</guid>
    </item>
    <item>
      <title>[4/x][AMD][Lowering Enablement] Enabling meta internal AOTInductor compilation on ROCM</title>
      <link>https://github.com/pytorch/pytorch/pull/124123</link>
      <description><![CDATA[<p>Summary: as title</p>
<p>Test Plan: CI &amp; unit test</p>
<p>Differential Revision: D56163334</p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 16:47:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124123</guid>
    </item>
    <item>
      <title>[inductor] Improve stability of scaled softmax</title>
      <link>https://github.com/pytorch/pytorch/pull/124119</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124119</p>
<p>This adds a pattern which replaces:<br />
<code>python
   scale(x) - scale(x).amax(dim, keepdim=True)</code><br />
with<br />
<code>python
   scale(x - x.amax(dim, keepdim=True))</code><br />
where <code>scale</code> can be either multiplication or division by a scalar,<br />
or a tensor that is broadcast in the <code>dim</code> dimension.</p>
<p>We can find this pattern inside of the decomposed graph of:<br />
<code>python
F.softmax(scale(x), dim=dim)</code></p>
<p>This has the effect of both reducing the chance of hitting the <code>fma</code><br />
issue and also means we avoid recomputing <code>scale(x)</code> inside and outside<br />
the reduction which may be significant if we can remove an extra division.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 15:43:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124119</guid>
    </item>
    <item>
      <title>[dynamo] Disable __call__ of user compiler returned nn modules</title>
      <link>https://github.com/pytorch/pytorch/pull/124108</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #124121<br />
* #124109<br />
* <strong>-&gt;</strong> #124108<br />
* #124187<br />
* #124185</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 13:37:39 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124108</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Disable epilogue fusions</title>
      <link>https://github.com/pytorch/pytorch/pull/124107</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* <strong>-&gt;</strong> #124107<br />
* #121734<br />
* #123932<br />
* #123930<br />
* #121497<br />
* #124106</p>
<p>This diff disables Cutlass backend EVT epilogue fusions. It does not yet contain the removal of most of the underlying implementation. </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 13:23:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124107</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Fix flaky test ( CUDA IMA )</title>
      <link>https://github.com/pytorch/pytorch/pull/124106</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124107<br />
* #121734<br />
* #123932<br />
* #123930<br />
* #121497<br />
* <strong>-&gt;</strong> #124106</p>
<p>A unit test within test_cutlass_backend.py can fail with CUDA illegal memory accesses due to the fact that some CUTLASS Kernels contain bugs.</p>
<p>By using autotuning in subprocesses, this CUDA illegal memory access simply<br />
leads to the buggy Cutlass Kernels being filtered out, instead of causing it<br />
to bring down the entire process.</p>
<p>Test Plan:<br />
This is a change to a unit test. It's recommended to use autotune_in_subproc when using the Cutlass backend anyway.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 13:19:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124106</guid>
    </item>
    <item>
      <title>[WIP] Add scalar support for eager-through-torch.compile</title>
      <link>https://github.com/pytorch/pytorch/pull/124070</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120595<br />
* <strong>-&gt;</strong> #124070<br />
* #124177<br />
* #116368<br />
* #121387<br />
* #123545</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 07:04:35 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124070</guid>
    </item>
    <item>
      <title>[WIP][inductor] refine loop split logic</title>
      <link>https://github.com/pytorch/pytorch/pull/124060</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124060</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 15 Apr 2024 05:52:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124060</guid>
    </item>
    <item>
      <title>[inductor][cpp] GEMM template</title>
      <link>https://github.com/pytorch/pytorch/pull/124021</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #124021</p>
<p>cc @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 14 Apr 2024 06:21:12 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/124021</guid>
    </item>
    <item>
      <title>[2.3 dynamic shapes] backend='inductor' raised: LoweringException: AssertionError: indices must be int64, byte or bool. Got [torch.float32]</title>
      <link>https://github.com/pytorch/pytorch/issues/124006</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I installed the final RC version of pytorch 2.3, and ran the following code, errors occurs.</p>
<p>```</p>
<h1>minified.py</h1>
<p>import torch<br />
from torch import nn<br />
import os<br />
from torch import distributed as dist<br />
from torch.nn.parallel import DistributedDataParallel as DDP<br />
import torch._dynamo<br />
from torch.utils.checkpoint import checkpoint</p>
<h1>torch._dynamo.config.optimize_ddp = False</h1>
<p>device = torch.device(f'cuda:{0}')</p>
<p>rank = os.environ.get('LOCAL_RANK','-1')<br />
if int(rank) &gt;=0:<br />
    device = torch.device(f'cuda:{rank}')<br />
    print(device)<br />
    torch.cuda.set_device(device)<br />
    dist.init_process_group(backend='nccl', init_method='env://')</p>
<p>def apply_rotary_v2(x, pos_cos,pos_sin):<br />
    x1, x2 = x[..., 0::2].float(), x[..., 1::2].float() # b,n,h,d<br />
    return torch.cat([x1 * pos_cos - x2 * pos_sin, x2 * pos_cos + x1 * pos_sin], dim=-1).type_as(x)</p>
<p>class ResolutionDown(nn.Module):<br />
    r""" Patch Merging Layer.</p>
<pre><code>Args:
    output_resolution (tuple[int]): Resolution of output feature.
    dim (int): Number of output channels.
    norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
"""

def __init__(self, input_dim, output_dim, norm_layer=nn.LayerNorm, output_NCHW=False):
    super().__init__()
    self.output_dim = output_dim
    self.input_dim = input_dim
    self.norm = norm_layer(4 * input_dim) if norm_layer else nn.Identity()
    self.reduction = nn.Linear(4 * input_dim, output_dim, bias=False)
    self.output_NCHW = output_NCHW

def forward(self, x, last_group=None, cur_group=None):
    """
    x: B, H*W, C
    """

    x = torch.nn.functional.pixel_unshuffle(x, 2)
    x = x.permute(0, 2, 3, 1)

    x = self.norm(x)
    x = self.reduction(x)

    if self.output_NCHW:
        x = x.permute(0, 3, 1, 2)

    return x
</code></pre>
<p>class Mlp(nn.Module):<br />
    """ MLP as used in Vision Transformer, MLP-Mixer and related networks<br />
    """<br />
    def <strong>init</strong>(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU,<br />
                 fc2_bias=True,<br />
                 drop=0.):<br />
        super().<strong>init</strong>()<br />
        out_features = out_features or in_features<br />
        hidden_features = hidden_features or in_features<br />
        self.fc1 = nn.Linear(in_features, hidden_features)<br />
        self.act = act_layer()<br />
        self.drop1 = nn.Dropout(drop)<br />
        self.fc2 = nn.Linear(hidden_features, out_features, bias=fc2_bias)<br />
        self.drop2 = nn.Dropout(drop)</p>
<pre><code>def forward(self, x):
    x = self.fc1(x)
    x = self.act(x)
    x = self.drop1(x)
    x = self.fc2(x)
    x = self.drop2(x)
    return x
</code></pre>
<p>class RMSNorm(torch.nn.Module):<br />
    def <strong>init</strong>(self, dim: int, eps: float = 1e-5):<br />
        super().<strong>init</strong>()<br />
        self.eps = eps<br />
        self.weight = nn.Parameter(torch.ones(dim))</p>
<pre><code>def _norm(self, x):
    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

def forward(self, x):
    output = self._norm(x.float()).type_as(x)
    return output * self.weight
</code></pre>
<p>class WindowAttention(nn.Module):<br />
    r""" Window based multi-head self attention (W-MSA) module with relative position bias.<br />
    It supports both of shifted and non-shifted window.</p>
<pre><code>Args:
    dim (int): Number of input channels.
    window_size (int): The height and width of the window.
    num_heads (int): Number of attention heads.
    qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
    attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
    proj_drop (float, optional): Dropout ratio of output. Default: 0.0
"""

def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0.,
             shift_h=True,
             rotary_pos=False, fc2_bias=True, qk_norm_factor=1e-4,
             proj_drop=0.):

    super().__init__()
    self.dim = dim
    self.rotary_pos = rotary_pos
    self.window_size = window_size  # Wh, Ww
    self.shift_size = window_size // 4
    self.num_heads = num_heads
    head_dim = dim // num_heads
    self.scale = head_dim ** -0.5
    self.qk_norm_factor = qk_norm_factor

    self.shift_h = shift_h

    # define a parameter table of relative position bias
    assert rotary_pos, 'must be rotary pos embed'

    self.q = nn.Linear(dim, dim, bias=qkv_bias)
    self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
    self.attn_drop_ratio = attn_drop
    self.proj = nn.Linear(dim, dim, bias=fc2_bias)
    self.proj_drop = nn.Dropout(proj_drop)
    self.softmax = nn.Softmax(dim=-1)

def forward(self, x, H, W, pos_embed_q, pos_embed_kv, mask):
    """
    Args:
        x: input features with shape of (num_windows*B, N, C)
        mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
    """
    D = self.dim
    C = D//self.num_heads
    nH = self.num_heads
    if self.shift_h:
        window_size1 = self.window_size //2
        window_size2 = self.window_size
    else:
        window_size1 = self.window_size
        window_size2 = self.window_size // 2
    G=H*W//window_size1//window_size2
    N= window_size1 * window_size2

    # ========== window_partition ==============
    q = self.q(x).view(-1, H//window_size1, window_size1, W//window_size2, window_size2, nH, C) # 5,6
    q = q.permute(0,1,3,5,2,4,6).reshape(-1,G,nH,N,C) # B,G,H,N,C

    kv = self.kv(x) # B,H,W,2C
    kv1 = torch.roll(kv,(-self.shift_size,),(1 if self.shift_h else 2,))
    kv2 = torch.roll(kv,(self.shift_size,),(1 if self.shift_h else 2,))
    kv = torch.stack([kv1,kv2]).view(2,-1,H//window_size1,window_size1,W//window_size2,window_size2, #0,1,2,3,4,5
                                     2,nH,C) #6,7,8
    kv = kv.permute(6, 1, 2,4, 7, 0,3,5, 8).reshape(2, -1, G, nH,2*N, C)
    k,v = kv.unbind(0)

    if self.training and self.qk_norm_factor &gt; 0:
        qk_loss = (q ** 2).mean() * self.qk_norm_factor + (k ** 2).mean() * self.qk_norm_factor
    else:
        qk_loss = 0

    q = apply_rotary_v2(q, *pos_embed_q.unbind(0))
    k = apply_rotary_v2(k, *pos_embed_kv.unbind(0))

    if mask is not None:
        if self.training and self.attn_drop_ratio &gt; 0:
            with torch.no_grad():
                nmask = torch.rand(x.shape[0], G, 1, N, N * 2, device=q.device) &gt;= self.attn_drop_ratio
                nmask = torch.where(nmask, 0, -torch.inf)
                mask = mask + nmask
        mask = mask.type_as(q)
    x = torch.nn.functional.scaled_dot_product_attention(q, k, v.type_as(q), attn_mask=mask,
                                                         dropout_p=0)

    # ==============  window_reverse ==============
    x=x.view(-1,H//window_size1,W//window_size2,nH,window_size1,window_size2,C)
    x=x.permute(0,1,4,2,5,3,6).reshape(-1,H,W,D)

    x = self.proj(x)
    x = self.proj_drop(x)
    return x, qk_loss
</code></pre>
<p>class SwinTransformerBlock(nn.Module):<br />
    r""" Swin Transformer Block.</p>
<pre><code>Args:
    dim (int): Number of input channels.
    input_resolution (tuple[int]): Input resulotion.
    num_heads (int): Number of attention heads.
    window_size (int): Window size.
    shift_size (int): Shift size for SW-MSA.
    mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
    qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
    drop (float, optional): Dropout rate. Default: 0.0
    attn_drop (float, optional): Attention dropout rate. Default: 0.0
    drop_path (float, optional): Stochastic depth rate. Default: 0.0
    act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
    norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
"""

def __init__(self, dim, input_resolution, num_heads, window_size=8, shift_h=True,
             mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,
             qk_norm_factor=1e-4,
             act_layer=nn.GELU, norm_layer=nn.LayerNorm):
    super().__init__()
    self.dim = dim
    self.input_resolution = input_resolution
    self.num_heads = num_heads
    self.window_size = window_size
    self.mlp_ratio = mlp_ratio

    self.norm1 = norm_layer(dim)
    self.shift_h = shift_h

    self.attn = WindowAttention(
        dim, window_size=self.window_size, num_heads=num_heads, qkv_bias=qkv_bias,
        shift_h = shift_h,
        rotary_pos=True, fc2_bias=True, qk_norm_factor=qk_norm_factor,
        attn_drop=attn_drop, proj_drop=drop)

    self.drop_path = nn.Identity()
    self.norm2 = norm_layer(dim)
    mlp_hidden_dim = int(dim * mlp_ratio)
    self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop,
                   fc2_bias=True)

def forward(self, x, H, W, pos_embed_q,pos_embed_kv, mask):
    shortcut = x

    x = self.norm1(x) # B,H,W,C

    x, qk_loss = self.attn(x, H, W, pos_embed_q,pos_embed_kv, mask)

    x = shortcut + self.drop_path(x)

    mlp = self.mlp(self.norm2(x))
    x = x + self.drop_path(mlp)

    return x
</code></pre>
<p>class SwinLayer(nn.Module):<br />
    def <strong>init</strong>(self, input_dim, dim, input_resolution, depth, num_heads,<br />
                 window_size=16,<br />
                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0., qk_norm_factor=1e-4,<br />
                 norm_layer=nn.LayerNorm, use_checkpoint=False):</p>
<pre><code>    super().__init__()
    self.dim = dim
    self.num_heads = num_heads
    self.head_dim = dim // num_heads
    self.input_resolution = (input_resolution,input_resolution)
    self.depth = depth
    self.use_checkpoint = use_checkpoint
    self.norm = norm_layer(dim)

    self.pos_embed_qh = nn.Parameter(
        torch.rand(2, 1, num_heads, window_size*window_size//2, dim//num_heads//2), requires_grad=False)
    self.pos_embed_qw = nn.Parameter(
        torch.rand(2, 1, num_heads, window_size*window_size//2, dim//num_heads//2), requires_grad=False)

    self.pos_embed_kvh = nn.Parameter(
        torch.rand(2, 1, num_heads, window_size*window_size, dim//num_heads//2), requires_grad=False)

    self.pos_embed_kvw = nn.Parameter(
        torch.rand(2, 1, num_heads, window_size*window_size, dim//num_heads//2), requires_grad=False)

    self.window_size = window_size
    self.downsample = ResolutionDown(input_dim, dim, norm_layer=norm_layer)

    self.blocks = nn.ModuleList([
        SwinTransformerBlock(
            input_resolution=input_resolution,
            dim=dim, num_heads=num_heads, window_size=window_size,
            shift_h = i % 2 == 0,
            mlp_ratio=mlp_ratio, qk_norm_factor=qk_norm_factor,
            qkv_bias=qkv_bias, drop=drop,
            attn_drop=attn_drop[i] if isinstance(attn_drop,list) else attn_drop,
            drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path, norm_layer=norm_layer)
        for i in range(depth)])

    self.window_shift_maskh = nn.Parameter(torch.rand(1,1,1,self.window_size*self.window_size//2,
                                                      self.window_size*self.window_size).float(),
                                           requires_grad=False)
    self.window_shift_maskw = nn.Parameter(torch.rand(1,1,1,self.window_size*self.window_size//2,
                                                      self.window_size*self.window_size).float(),
                                           requires_grad=False)

def forward(self, x):  # shuffle_idx shape: FB,N
    x = self.downsample(x )
    H,W=x.shape[1],x.shape[2]
    # sample_idx shape: B,1,H,W should permute to B,H,W,1 for get_attn_mask/window_partition
    mask_h = self.window_shift_maskh
    mask_w = self.window_shift_maskh


    for idx, blk in enumerate(self.blocks):
        if blk.shift_h:
            mask = mask_h
            pos_embed_q = self.pos_embed_qh
            pos_embed_kv=self.pos_embed_kvh
        else:
            mask = mask_w
            pos_embed_q = self.pos_embed_qw
            pos_embed_kv=self.pos_embed_kvw

        x = torch.utils.checkpoint.checkpoint(blk,x,H,W, pos_embed_q,pos_embed_kv, mask, use_reentrant=True)

    x = self.norm(x)
    return x.permute(0, 3, 1, 2)

def get_num_block(self):
    return len(self.blocks)
</code></pre>
<p>model = nn.Sequential(SwinLayer(3,128,16,2,2),<br />
                      SwinLayer(128,256,16,2,4),)<br />
model = model.cuda()<br />
print(model)</p>
<p>if int(rank) &gt;=0:<br />
    model = DDP(model.cuda(),device_ids=[int(rank)], output_device=int(rank))<br />
optimizer = torch.optim.AdamW(model.parameters())</p>
<p>model = torch.compile(model)</p>
<p>x = torch.rand(2,3,256,256).cuda()<br />
x.requires_grad=True<br />
with torch.cuda.amp.autocast():<br />
    y=model(x)<br />
y[0].sum().backward()</p>
<p>optimizer.step()<br />
optimizer.zero_grad()<br />
torch.cuda.synchronize()</p>
<p>print('test 1 done')</p>
<p>x = torch.rand(2,3,192,192).cuda()<br />
x.requires_grad=True<br />
with torch.cuda.amp.autocast():<br />
    y=model(x)<br />
y[0].sum().backward()</p>
<p>optimizer.step()<br />
optimizer.zero_grad()<br />
torch.cuda.synchronize()<br />
print('test 2 done')</p>
<p>```</p>
<h3>Versions</h3>
<p>PyTorch version: 2.3.0+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.6 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.16.3<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.10.13 (main, Dec 19 2023, 08:15:18) [GCC 9.4.0] (64-bit runtime)<br />
Python platform: Linux-4.15.0-189-generic-x86_64-with-glibc2.31<br />
Is CUDA available: True<br />
CUDA runtime version: 11.8.89<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: <br />
GPU 0: Quadro RTX 6000<br />
GPU 1: Quadro RTX 6000<br />
GPU 2: Quadro RTX 6000<br />
GPU 3: Quadro RTX 6000<br />
GPU 4: Quadro RTX 6000<br />
GPU 5: Quadro RTX 6000<br />
GPU 6: Quadro RTX 6000<br />
GPU 7: Quadro RTX 6000</p>
<p>Nvidia driver version: 535.54.03<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   46 bits physical, 48 bits virtual<br />
CPU(s):                          64<br />
On-line CPU(s) list:             0-63<br />
Thread(s) per core:              2<br />
Core(s) per socket:              16<br />
Socket(s):                       2<br />
NUMA node(s):                    2<br />
Vendor ID:                       GenuineIntel<br />
CPU family:                      6<br />
Model:                           85<br />
Model name:                      Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz<br />
Stepping:                        7<br />
CPU MHz:                         999.997<br />
CPU max MHz:                     3900.0000<br />
CPU min MHz:                     1000.0000<br />
BogoMIPS:                        4600.00<br />
Virtualization:                  VT-x<br />
L1d cache:                       1 MiB<br />
L1i cache:                       1 MiB<br />
L2 cache:                        32 MiB<br />
L3 cache:                        44 MiB<br />
NUMA node0 CPU(s):               0-15,32-47<br />
NUMA node1 CPU(s):               16-31,48-63<br />
Vulnerability Itlb multihit:     KVM: Mitigation: Split huge pages<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Mitigation; TSX disabled<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==2.3.0<br />
[pip3] torch==2.3.0+cu118<br />
[pip3] torchlaunch==1.0<br />
[pip3] torchvision==0.18.0+cu118<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Sat, 13 Apr 2024 04:29:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/124006</guid>
    </item>
    <item>
      <title>torch.compiler.disable(nn.Module) does not disable hooks</title>
      <link>https://github.com/pytorch/pytorch/issues/123979</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>hooks are still traced with <code>torch.compile.disable(model)</code>. This is related to skip compiling TP/SP hooks for DTensors<br />
```<br />
import torch</p>
<p>class SimpleModel(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.layer0 = torch.nn.Linear(4, 4)<br />
        self.layer1 = torch.nn.Linear(4, 4)<br />
    def forward(self, inp):<br />
        z = self.layer0(inp)<br />
        return self.layer1(z)</p>
<p>def hook(module, args):<br />
    inp = args[0].sin().cos()<br />
    return (inp,)</p>
<p>model = SimpleModel()<br />
model.layer0.register_forward_pre_hook(hook)<br />
model.layer0 = torch.compiler.disable(model.layer0)<br />
opt_model = torch.compile(model)<br />
opt_model(torch.randn((4,)))<br />
```</p>
<p>search "sin().cos()" in logs<br />
<code>V0412 15:17:08.074000 140326368437376 torch/_dynamo/symbolic_convert.py:739] [0/0_1] [__trace_source] TRACE starts_line /data/users/weif/pytorch-official/pytorch/torch/nn/modules/module.py:1571 in _call_impl (Module._call_impl) (inline depth: 1)
V0412 15:17:08.074000 140326368437376 torch/_dynamo/symbolic_convert.py:739] [0/0_1] [__trace_source]                             args_result = hook(self, args)
V0412 15:17:08.074000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE LOAD_FAST hook [ListIteratorVariable(length=1, index=1)]
V0412 15:17:08.074000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE LOAD_FAST self [ListIteratorVariable(length=1, index=1), UserFunctionVariable()]
V0412 15:17:08.074000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE LOAD_FAST args [ListIteratorVariable(length=1, index=1), UserFunctionVariable(), UnspecializedNNModuleVariable()]
V0412 15:17:08.074000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE CALL_FUNCTION 2 [ListIteratorVariable(length=1, index=1), UserFunctionVariable(), UnspecializedNNModuleVariable(), TupleVariable()]
V0412 15:17:08.075000 140326368437376 torch/_dynamo/symbolic_convert.py:2449] [0/0_1] INLINING &lt;code object hook at 0x7fa047071fd0, file "/data/users/weif/pytorch-official/pytorch/test_nn_module.py", line 13&gt;, inlined according trace_rules.lookup inlined by default
V0412 15:17:08.075000 140326368437376 torch/_dynamo/symbolic_convert.py:739] [0/0_1] [__trace_source] TRACE starts_line /data/users/weif/pytorch-official/pytorch/test_nn_module.py:14 in hook (hook) (inline depth: 2)
**V0412 15:17:08.075000 140326368437376 torch/_dynamo/symbolic_convert.py:739] [0/0_1] [__trace_source]         inp = args[0].sin().cos()**
V0412 15:17:08.075000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE LOAD_FAST args []
V0412 15:17:08.075000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE LOAD_CONST 0 [TupleVariable()]
V0412 15:17:08.075000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE BINARY_SUBSCR None [TupleVariable(), ConstantVariable()]
V0412 15:17:08.075000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE LOAD_ATTR sin [LazyVariableTracker()]
V0412 15:17:08.076000 140326368437376 torch/_dynamo/output_graph.py:2046] [0/0_1] create_graph_input L_inp_ L['inp']
V0412 15:17:08.076000 140326368437376 torch/_dynamo/variables/builder.py:1964] [0/0_1] wrap_to_fake L['inp'] (4,) StatefulSymbolicContext(dynamic_sizes=[&lt;DimDynamic.STATIC: 2&gt;], constraint_sizes=[None], view_base_context=None, tensor_source=LocalSource(local_name='inp', cell_or_freevar=False), shape_env_to_source_to_symbol_cache={}) &lt;class 'torch.Tensor'&gt;
V0412 15:17:08.078000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE CALL_FUNCTION 0 [GetAttrVariable()]
V0412 15:17:08.080000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE LOAD_ATTR cos [TensorVariable()]
V0412 15:17:08.080000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE CALL_FUNCTION 0 [GetAttrVariable()]
V0412 15:17:08.081000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE STORE_FAST inp [TensorVariable()]
V0412 15:17:08.082000 140326368437376 torch/_dynamo/symbolic_convert.py:739] [0/0_1] [__trace_source] TRACE starts_line /data/users/weif/pytorch-official/pytorch/test_nn_module.py:15 in hook (hook) (inline depth: 2)
V0412 15:17:08.082000 140326368437376 torch/_dynamo/symbolic_convert.py:739] [0/0_1] [__trace_source]         return (inp,)
V0412 15:17:08.082000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE LOAD_FAST inp []
V0412 15:17:08.082000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE BUILD_TUPLE 1 [TensorVariable()]
V0412 15:17:08.082000 140326368437376 torch/_dynamo/symbolic_convert.py:762] [0/0_1] TRACE RETURN_VALUE None [TupleVariable()]</code></p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78</p>]]></description>
      <pubDate>Fri, 12 Apr 2024 14:20:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123979</guid>
    </item>
    <item>
      <title>[Inductor max autotune] Make autotuning robust against very slow Kernels</title>
      <link>https://github.com/pytorch/pytorch/pull/123932</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124107<br />
* #121734<br />
* <strong>-&gt;</strong> #123932<br />
* #123930<br />
* #121497<br />
* #124106</p>
<p>If a Kernel does not return in a reasonable amount of time during autotuning, it can delay inductor compilation a lot. This change introduces soft / hard kill timeouts and a mechanism to kill Kernels being profiled in subprocesses if they take too long.</p>
<p>Correspondingly, a few new config options are introduced within _inductor/config.py - all of them with inline docs.</p>
<p>Test Plan:<br />
Existing tests within test_max_autotune.py and test_cutlass_backend.py ) cover the new codepaths.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 12 Apr 2024 03:51:15 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123932</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Fix tests: skipIfROCm always skips when using as class annotation</title>
      <link>https://github.com/pytorch/pytorch/pull/123930</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124107<br />
* #121734<br />
* #123932<br />
* <strong>-&gt;</strong> #123930<br />
* #121497<br />
* #124106</p>
<p>I previously added @skipIfRocm as a class annotation within test/inductor/test_cutlass_backend.py - turns out this annotation always skips if applied at class level, so I need to skip Cutlass tests on ROCm differently..</p>]]></description>
      <pubDate>Fri, 12 Apr 2024 03:30:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123930</guid>
    </item>
    <item>
      <title>DISABLED test_custom_function_saved_tensors (__main__.TestAutogradWithCompiledAutograd)</title>
      <link>https://github.com/pytorch/pytorch/issues/123927</link>
      <description><![CDATA[<p>Platforms: asan, linux, rocm, mac, macos</p>
<p>This test was disabled because it is failing in CI. See <a href="https://hud.pytorch.org/flakytest?name=test_custom_function_saved_tensors&amp;suite=TestAutogradWithCompiledAutograd&amp;limit=100">recent examples</a> and the most recent trunk <a href="https://github.com/pytorch/pytorch/runs/23739064495">workflow logs</a>.</p>
<p>Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes.</p>
<p><strong>Debugging instructions (after clicking on the recent samples link):</strong><br />
DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.<br />
To find relevant log snippets:<br />
1. Click on the workflow logs linked above<br />
2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.<br />
3. Grep for <code>test_custom_function_saved_tensors</code><br />
4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.</p>
<p>Test file path: <code>inductor/test_compiled_autograd.py</code></p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 12 Apr 2024 01:39:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123927</guid>
    </item>
    <item>
      <title>[Inductor] Force the parallel depth as outer loop fusion depth</title>
      <link>https://github.com/pytorch/pytorch/pull/123899</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123899</p>
<p><strong>Summary</strong><br />
Fix issue: https://github.com/pytorch/pytorch/issues/123801 which brings performance regression of <code>pyhpc_turbulent_kinetic_energy</code> after outer loop fusion.</p>
<p><strong>Root Cause</strong></p>
<ul>
<li><a href="https://gist.github.com/leslie-fang-intel/54fe21ac8871fc63b9bf20fdb6edf209">Generated Kernel before Outer Loop Fusion</a></li>
<li>Taking below 2 kernels as example:<ul>
<li><a href="https://gist.github.com/leslie-fang-intel/54fe21ac8871fc63b9bf20fdb6edf209#file-pyhpc_turbulent_kinetic_energy-before-outer-loop-fusion-py-L255-L305">Kernel 0</a> has 2 loop levels with size [200, 200]. Parallelization is not feasible due to the inefficient number of elements determined by <a href="https://github.com/pytorch/pytorch/blob/aaec97a40364bb6ccfd968f28d309cfff8748d20/torch/_inductor/codegen/cpp.py#L2145-L2164"><code>decide_parallel_depth</code></a>. Therefore, the loop code will be generated with the <code>#pragma omp single</code> directive.</li>
<li><a href="https://gist.github.com/leslie-fang-intel/54fe21ac8871fc63b9bf20fdb6edf209#file-pyhpc_turbulent_kinetic_energy-before-outer-loop-fusion-py-L306-L316">Kernel 1</a> has 3 loop levels with size [200, 200, 26] which has enough number of elements to be parallelized.</li>
</ul>
</li>
<li><a href="https://gist.github.com/leslie-fang-intel/57a497b9d9c6aa82b1c6a686292fc887">Generated Kernel after Outer Loop Fusion</a></li>
<li>After outer loop fusion, <code>Kernel0</code> and <code>Kernel1</code> has been fused into one <a href="https://gist.github.com/leslie-fang-intel/57a497b9d9c6aa82b1c6a686292fc887#file-pyhpc_turbulent_kinetic_energy-after-outer-loop-fusion-py-L261-L497">OuterLoopFusedKernel</a>, the outer loop size is [200, 200] which does not contain enough number of elements to do parallelization.</li>
</ul>
<p>In this PR, we propose a fix for <code>loop_nest</code> with <code>OuterLoopFusedKernel</code>, we will enforce the parallelization with parallel depth as same as <code>outer_loop_fusion_depth</code>.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 11 Apr 2024 18:18:55 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123899</guid>
    </item>
    <item>
      <title>inductor cpp wrapper: add GIL release back</title>
      <link>https://github.com/pytorch/pytorch/pull/123897</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123897</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/123517.<br />
This PR adds the GIL release (originally added in https://github.com/pytorch/pytorch/pull/111888) back following the suggestion here: https://github.com/pytorch/pytorch/pull/123897#discussion_r1562509705.<br />
We added a default constructor and an assignment operator for the <code>RAIIPyObject</code> class<br />
 (https://github.com/pytorch/pytorch/pull/123897#discussion_r1566262575) in order to declare the <code>custom_op_wrapper</code> outside of the GIL acquisition scope.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 11 Apr 2024 17:48:19 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123897</guid>
    </item>
    <item>
      <title>[inductor] Optionally save inductor cache in test-reports</title>
      <link>https://github.com/pytorch/pytorch/pull/123779</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #123779</p>
<p>Summary:</p>
<p>Test Plan:</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 10 Apr 2024 14:43:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123779</guid>
    </item>
    <item>
      <title>Utilize aot compile</title>
      <link>https://github.com/pytorch/pytorch/pull/123545</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120595<br />
* #124070<br />
* #124177<br />
* #116368<br />
* #121387<br />
* <strong>-&gt;</strong> #123545</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 07 Apr 2024 22:12:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/123545</guid>
    </item>
    <item>
      <title>torch.compile dynamo fails indexing into array from internal mutable state</title>
      <link>https://github.com/pytorch/pytorch/issues/123535</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>the code</p>
<p>```py<br />
import torch<br />
import torch.nn as nn<br />
import logging</p>
<p>class CompiledClass(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.nums = torch.tensor([1,2,3,4,5,6,7,8,9,10])<br />
        self.t = 5</p>
<pre><code>def forward(self):
    self.num = self.nums[self.t//12]
    self.t += 1
    return self.num
</code></pre>
<p>m = CompiledClass()<br />
m = torch.compile(m, backend="eager")</p>
<p>torch._logging.set_logs(dynamo = logging.DEBUG)<br />
torch._dynamo.config.verbose = True</p>
<h1>the first call works</h1>
<p>m()</p>
<h1>the second call causes a failure</h1>
<p>m()<br />
```</p>
<h3>Error logs</h3>
<p>```<br />
...<br />
[2024-04-08 00:21:47,967] [0/0_1] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile<br />
[2024-04-08 00:21:47,967] [0/0_1] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /workspaces/torch-indexing-reproduction/main.py, line 14 in forward>], graph_break=False)<br />
[2024-04-08 00:21:47,968] [0/0_1] torch._dynamo.output_graph.<strong>graph_code: [DEBUG] TRACED GRAPH<br />
[2024-04-08 00:21:47,968] [0/0_1] torch._dynamo.output_graph.__graph_code: [DEBUG]  ===== __compiled_fn_0 =====<br />
[2024-04-08 00:21:47,968] [0/0_1] torch._dynamo.output_graph.__graph_code: [DEBUG]  <eval_with_key>.0 class GraphModule(torch.nn.Module):<br />
[2024-04-08 00:21:47,968] [0/0_1] torch._dynamo.output_graph.__graph_code: [DEBUG]     def forward(self, L_self_nums : torch.Tensor):<br />
[2024-04-08 00:21:47,968] [0/0_1] torch._dynamo.output_graph.__graph_code: [DEBUG]         l_self_nums = L_self_nums<br />
[2024-04-08 00:21:47,968] [0/0_1] torch._dynamo.output_graph.__graph_code: [DEBUG]       <br />
[2024-04-08 00:21:47,968] [0/0_1] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /workspaces/torch-indexing-reproduction/main.py:12, code: self.num = self.nums[self.t//12]<br />
[2024-04-08 00:21:47,968] [0/0_1] torch._dynamo.output_graph.__graph_code: [DEBUG]         getitem = l_self_nums[0];  l_self_nums = None<br />
[2024-04-08 00:21:47,968] [0/0_1] torch._dynamo.output_graph.__graph_code: [DEBUG]         return (getitem,)<br />
[2024-04-08 00:21:47,968] [0/0_1] torch._dynamo.output_graph.__graph_code: [DEBUG]       <br />
[2024-04-08 00:21:47,968] [0/0_1] torch._dynamo.output_graph.__graph_code: [DEBUG] <br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG] Tabulate module missing, please install tabulate to log the graph in tabular format, logging code instead:<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG]  ===== __compiled_fn_0 =====<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG]  <eval_with_key>.0 class GraphModule(torch.nn.Module):<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG]     def forward(self, L_self_nums : torch.Tensor):<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG]         l_self_nums = L_self_nums<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG]       <br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG]         # File: /workspaces/torch-indexing-reproduction/main.py:12, code: self.num = self.nums[self.t//12]<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG]         getitem = l_self_nums[0];  l_self_nums = None<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG]         return (getitem,)<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG]       <br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph: [DEBUG] <br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph_sizes: [DEBUG] TRACED GRAPH TENSOR SIZES<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph_sizes: [DEBUG] ===== __compiled_fn_0 =====<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l_self_nums: (10,)<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph_sizes: [DEBUG] getitem: ()<br />
[2024-04-08 00:21:47,969] [0/0_1] torch._dynamo.output_graph.__graph_sizes: [DEBUG] <br />
[2024-04-08 00:21:47,970] [0/0_1] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function eager<br />
[2024-04-08 00:21:47,970] [0/0_1] torch._dynamo.output_graph: [INFO] Step 2: done compiler function eager<br />
[2024-04-08 00:21:48,034] [0/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards<br />
[2024-04-08 00:21:48,034] [0/0_1] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['self'].nums.size()[0] 10 None<br />
[2024-04-08 00:21:48,034] [0/0_1] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['self'].nums.stride()[0] 1 None<br />
[2024-04-08 00:21:48,034] [0/0_1] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['self'].nums.storage_offset() 0 None<br />
[2024-04-08 00:21:48,034] [0/0_1] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['self'].nums.size()[0] == 10<br />
[2024-04-08 00:21:48,034] [0/0_1] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['self'].nums.stride()[0] == 1<br />
[2024-04-08 00:21:48,035] [0/0_1] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['self'].nums.storage_offset() == 0<br />
[2024-04-08 00:21:48,035] [0/0_1] torch._dynamo.guards.__guards: [DEBUG] GUARDS:<br />
[2024-04-08 00:21:48,035] [0/0_1] torch._dynamo.guards.__guards: [DEBUG] ___check_type_id(L['self'], 94819259526288)                   # self.num = self.nums[self.t//12]  # orkspaces/torch-indexing-reproduction/main.py:12 in forward<br />
[2024-04-08 00:21:48,035] [0/0_1] torch._dynamo.guards.__guards: [DEBUG] ___check_type_id(L['self'].t, 140484762481376)                # self.num = self.nums[self.t//12]  # orkspaces/torch-indexing-reproduction/main.py:12 in forward<br />
[2024-04-08 00:21:48,036] [0/0_1] torch._dynamo.guards.__guards: [DEBUG] L['self'].t == 5                                              # self.num = self.nums[self.t//12]  # orkspaces/torch-indexing-reproduction/main.py:12 in forward<br />
[2024-04-08 00:21:48,036] [0/0_1] torch._dynamo.guards.__guards: [DEBUG] hasattr(L['self'].nums, '_dynamo_dynamic_indices') == False   # self.num = self.nums[self.t//12]  # orkspaces/torch-indexing-reproduction/main.py:12 in forward<br />
[2024-04-08 00:21:48,036] [0/0_1] torch._dynamo.guards.__guards: [DEBUG] utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:379 in init_ambient_guards<br />
[2024-04-08 00:21:48,037] [0/0_1] torch._dynamo.guards.__guards: [DEBUG] (<strong><em>skip</em>backend_check() or </strong>_current_backend() == ___lookup_backend(140481868239056))  # _dynamo/output_graph.py:385 in init_ambient_guards<br />
[2024-04-08 00:21:48,037] [0/0_1] torch._dynamo.guards.__guards: [DEBUG] ___compile_config_hash() == '88a14d47e62622e2d97d70c8d06ad8bd'  # _dynamo/output_graph.py:387 in init_ambient_guards<br />
[2024-04-08 00:21:48,037] [0/0_1] torch._dynamo.guards.__guards: [DEBUG] check_tensor(L['self'].nums, Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.int64, device=None, requires_grad=False, size=[10], stride=[1])  # self.num = self.nums[self.t//12]  # orkspaces/torch-indexing-reproduction/main.py:12 in forward<br />
[2024-04-08 00:21:48,043] torch._dynamo.eval_frame: [DEBUG] skipping: _fn (reason: in skipfiles, file: /home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py)<br />
[2024-04-08 00:21:48,043] torch._dynamo.eval_frame: [DEBUG] skipping: is_tracing (reason: in skipfiles, file: /home/vscode/.local/lib/python3.11/site-packages/torch/jit/_trace.py)<br />
[2024-04-08 00:21:48,044] torch._dynamo.eval_frame: [DEBUG] skipping: is_scripting (reason: in skipfiles, file: /home/vscode/.local/lib/python3.11/site-packages/torch/_jit_internal.py)<br />
[2024-04-08 00:21:48,044] torch._dynamo.eval_frame: [DEBUG] skipping: nothing (reason: in skipfiles, file: /home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py)<br />
[2024-04-08 00:21:48,044] torch._dynamo.eval_frame: [DEBUG] skipping: __exit</strong> (reason: in skipfiles, file: /usr/local/lib/python3.11/contextlib.py)<br />
[2024-04-08 00:21:48,044] torch._dynamo.eval_frame: [DEBUG] skipping: <strong>exit</strong> (reason: in skipfiles, file: /usr/local/lib/python3.11/contextlib.py)<br />
[2024-04-08 00:21:48,046] torch._dynamo.eval_frame: [DEBUG] skipping: <strong>setattr</strong> (reason: in skipfiles, file: /home/vscode/.local/lib/python3.11/site-packages/torch/nn/modules/module.py)<br />
[2024-04-08 00:21:48,046] torch._dynamo.eval_frame: [DEBUG] skipping: <strong>instancecheck</strong> (reason: in skipfiles, file: /home/vscode/.local/lib/python3.11/site-packages/torch/nn/parameter.py)<br />
[2024-04-08 00:21:48,046] torch._dynamo.eval_frame: [DEBUG] Unsetting top-level compile config hash: 88a14d47e62622e2d97d70c8d06ad8bd<br />
[2024-04-08 00:21:48,046] torch._dynamo.eval_frame: [DEBUG] Setting top-level compile config hash: 88a14d47e62622e2d97d70c8d06ad8bd<br />
[2024-04-08 00:21:48,047] [0/1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward /workspaces/torch-indexing-reproduction/main.py:11<br />
[2024-04-08 00:21:48,048] [0/1] torch.fx.experimental.symbolic_shapes: [INFO] create_env<br />
[2024-04-08 00:21:48,048] [0/1] torch._dynamo.symbolic_convert.<strong>trace_source: [DEBUG] TRACE starts_line /workspaces/torch-indexing-reproduction/main.py:11 in forward (CompiledClass.forward)<br />
[2024-04-08 00:21:48,048] [0/1] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]         def forward(self):<br />
[2024-04-08 00:21:48,048] [0/1] torch._dynamo.symbolic_convert: [DEBUG] TRACE RESUME 0 []<br />
[2024-04-08 00:21:48,049] [0/1] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /workspaces/torch-indexing-reproduction/main.py:12 in forward (CompiledClass.forward)<br />
[2024-04-08 00:21:48,049] [0/1] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             self.num = self.nums[self.t//12]<br />
[2024-04-08 00:21:48,049] [0/1] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self []<br />
[2024-04-08 00:21:48,049] [0/1] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR nums [LazyVariableTracker()]<br />
[2024-04-08 00:21:48,050] [0/1] torch._dynamo.output_graph: [DEBUG] create_graph_input L_self_nums L['self'].nums<br />
[2024-04-08 00:21:48,051] [0/1] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['self'].nums (10,) [<DimDynamic.STATIC: 2>] [None]<br />
[2024-04-08 00:21:48,052] [0/1] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self [TensorVariable()]<br />
[2024-04-08 00:21:48,052] [0/1] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR t [TensorVariable(), UnspecializedNNModuleVariable()]<br />
[2024-04-08 00:21:48,052] [0/1] torch._dynamo.variables.builder: [DEBUG] automatic dynamic int L['self'].t val 6 != 5<br />
[2024-04-08 00:21:48,052] [0/1] torch.fx.experimental.symbolic_shapes: [INFO] create_symbol s0 = 6 for L['self'].t [-9223372036854775808, 9223372036854775807]<br />
[2024-04-08 00:21:48,053] [0/1] torch._dynamo.output_graph: [DEBUG] create_graph_input L_self_t L['self'].t<br />
[2024-04-08 00:21:48,053] [0/1] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 12 [TensorVariable(), SymNodeVariable()]<br />
[2024-04-08 00:21:48,053] [0/1] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_OP 2 [TensorVariable(), SymNodeVariable(), ConstantVariable(int)]<br />
[2024-04-08 00:21:48,053] [0/1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call floordiv from /workspaces/torch-indexing-reproduction/main.py:12 in forward (CompiledClass.forward)<br />
[2024-04-08 00:21:48,053] [0/1] torch._dynamo.output_graph.__trace_call: [DEBUG]         self.num = self.nums[self.t//12]<br />
[2024-04-08 00:21:48,053] [0/1] torch._dynamo.output_graph.__trace_call: [DEBUG]                              ~~~~~~^^~~<br />
[2024-04-08 00:21:48,055] [0/1] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_SUBSCR None [TensorVariable(), SymNodeVariable()]<br />
[2024-04-08 00:21:48,055] [0/1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call select from /workspaces/torch-indexing-reproduction/main.py:12 in forward (CompiledClass.forward)<br />
[2024-04-08 00:21:48,055] [0/1] torch._dynamo.output_graph.__trace_call: [DEBUG]         self.num = self.nums[self.t//12]<br />
[2024-04-08 00:21:48,055] [0/1] torch._dynamo.output_graph.__trace_call: [DEBUG]                    ~~~~~~~~~^^^^^^^^^^^^<br />
[2024-04-08 00:21:48,099] [0/1] torch.fx.experimental.symbolic_shapes: [INFO] eval -(s0//12) &lt;= 10 [guard added] at orkspaces/torch-indexing-reproduction/main.py:12 in forward (_meta_registrations.py:4831 in meta_select)<br />
[2024-04-08 00:21:48,101] [0/1] torch.fx.experimental.symbolic_shapes: [DEBUG] eval (s0//12) &gt;= 10 == False [statically known]<br />
[2024-04-08 00:21:48,102] [0/1] torch.fx.experimental.symbolic_shapes: [DEBUG] eval (s0//12) &gt;= 0 == False [statically known]<br />
[2024-04-08 00:21:48,103] torch._dynamo.eval_frame: [DEBUG] Unsetting top-level compile config hash: 88a14d47e62622e2d97d70c8d06ad8bd<br />
Traceback (most recent call last):<br />
  File "/workspaces/torch-indexing-reproduction/main.py", line 25, in <module><br />
    m()<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn<br />
    return fn(*args, </strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl<br />
    return self._call_impl(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl<br />
    return forward_call(</em>args, <strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 655, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 727, in _convert_frame<br />
    result = inner_convert(frame, cache_entry, hooks, frame_state)<br />
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 383, in _convert_frame_assert<br />
    compiled_product = _compile(<br />
                       ^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 646, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
        ^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 562, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 151, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 527, in transform<br />
    tracer.run()<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2128, in run<br />
    super().run()<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 818, in run<br />
    and self.step()<br />
        ^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 781, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 470, in wrapper<br />
    return inner_fn(self, inst)<br />
           ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 249, in impl<br />
    self.push(fn_var.call_function(self, self.popn(nargs), {}))<br />
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 594, in call_function<br />
    return wrap_fx_proxy(tx, proxy)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1314, in wrap_fx_proxy<br />
    return wrap_fx_proxy_cls(target_cls=TensorVariable, </em><em>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1399, in wrap_fx_proxy_cls<br />
    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)<br />
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1525, in get_fake_value<br />
    raise TorchRuntimeError(str(e)).with_traceback(e.<strong>traceback</strong>) from None<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1486, in get_fake_value<br />
    ret_val = wrap_fake_exception(<br />
              ^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1027, in wrap_fake_exception<br />
    return fn()<br />
           ^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1487, in <lambda><br />
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)<br />
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1592, in run_node<br />
    raise RuntimeError(fn_str + str(e)).with_traceback(e.<strong>traceback</strong>) from e<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1571, in run_node<br />
    return node.target(</em>args, <strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/utils/_stats.py", line 20, in wrapper<br />
    return fn(*args, </strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1392, in __torch_dispatch</strong><br />
    return self.dispatch(func, types, args, kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1712, in dispatch<br />
    r = func(<em>args, </em><em>kwargs)<br />
        ^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/_ops.py", line 513, in <strong>call</strong><br />
    return self._op(</em>args, <strong>(kwargs or {}))<br />
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/<em>meta_registrations.py", line 4836, in meta_select<br />
    index = index if index &gt;= 0 else index + size<br />
                     ^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/<strong>init</strong>.py", line 365, in <strong>bool</strong><br />
    return self.node.bool</em>()<br />
           ^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py", line 392, in bool_<br />
    return self.guard_bool("", 0)<br />
           ^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py", line 358, in guard_bool<br />
    r = self.shape_env.evaluate_expr(self.expr, self.hint, fx_node=self.fx_node)<br />
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/fx/experimental/recording.py", line 226, in wrapper<br />
    return fn(*args, </strong>kwargs)<br />
           ^^^^^^^^^^^^^^^^^^^<br />
  File "/home/vscode/.local/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py", line 3575, in evaluate_expr<br />
    assert static_expr == hint, f"{static_expr} != {hint}"<br />
           ^^^^^^^^^^^^^^^^^^^<br />
torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method select of type object at 0x7fc52646b8a0>(<em>(FakeTensor(..., size=(10,), dtype=torch.int64), 0, (s0//12)), </em>*{}):<br />
False != True</p>
<p>from user code:<br />
   File "/workspaces/torch-indexing-reproduction/main.py", line 12, in forward<br />
    self.num = self.nums[self.t//12]</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True</p>
<p>[2024-04-08 00:21:48,118] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:<br />
[2024-04-08 00:21:48,118] torch._dynamo.utils: [INFO] Function, Runtimes (s)<br />
[2024-04-08 00:21:48,118] torch._dynamo.utils: [INFO] _compile.<locals>.compile_inner, 0.0934<br />
[2024-04-08 00:21:48,118] torch._dynamo.utils: [INFO] OutputGraph.call_user_compiler, 0.0002<br />
```</p>
<h3>Minified repro</h3>
<p>torch._dynamo.debug_utils did not produce a minified repro</p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.2.2+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Debian GNU/Linux 12 (bookworm) (x86_64)<br />
GCC version: (Debian 12.2.0-14) 12.2.0<br />
Clang version: Could not collect<br />
CMake version: Could not collect<br />
Libc version: glibc-2.36</p>
<p>Python version: 3.11.8 (main, Mar 12 2024, 11:41:52) [GCC 12.2.0] (64-bit runtime)<br />
Python platform: Linux-6.2.0-1019-azure-x86_64-with-glibc2.36<br />
Is CUDA available: False<br />
CUDA runtime version: No CUDA<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: No CUDA<br />
Nvidia driver version: No CUDA<br />
cuDNN version: No CUDA<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      48 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             2<br />
On-line CPU(s) list:                0,1<br />
Vendor ID:                          AuthenticAMD<br />
Model name:                         AMD EPYC 7763 64-Core Processor<br />
CPU family:                         25<br />
Model:                              1<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 1<br />
Socket(s):                          1<br />
Stepping:                           1<br />
BogoMIPS:                           4890.84<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat npt nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload umip vaes vpclmulqdq rdpid fsrm<br />
Virtualization:                     AMD-V<br />
Hypervisor vendor:                  Microsoft<br />
Virtualization type:                full<br />
L1d cache:                          32 KiB (1 instance)<br />
L1i cache:                          32 KiB (1 instance)<br />
L2 cache:                           512 KiB (1 instance)<br />
L3 cache:                           32 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0,1<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Mitigation; safe RET, no microcode<br />
Vulnerability Spec store bypass:    Vulnerable<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] torch==2.2.2<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78</p>]]></description>
      <pubDate>Sun, 07 Apr 2024 16:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/123535</guid>
    </item>
    <item>
      <title>[inductor] modify the output_stride of ConcatKernel</title>
      <link>https://github.com/pytorch/pytorch/pull/122761</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122761</p>
<p>Fix https://github.com/pytorch/pytorch/issues/121613.<br />
Modify the <code>output_stride</code> of <code>ConcatKernel</code>: If any input to <code>Concat</code> is <code>Pointwise</code>, check the layout of all inputs to <code>Pointwise</code>, if any of the inputs is in channels_last format, set channels_last strides for the <code>output_stride</code>.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 19:57:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122761</guid>
    </item>
    <item>
      <title>[inductor] Modify the rules for freezing the layout of x.unwrap_view() in convert_to_reinterpret_view</title>
      <link>https://github.com/pytorch/pytorch/pull/122760</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122760</p>
<p>Fix https://github.com/pytorch/pytorch/issues/121607</p>
<p>Modify the rules for freezing the layout of <code>x.unwrap_view()</code> in <code>convert_to_reinterpret_view</code>: If any read of <code>x.unwrap_view()</code> is in channels_last format, freeze the layout of <code>x.unwrap_view()</code> to channels_last format. </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 26 Mar 2024 19:30:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122760</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Add bmm support</title>
      <link>https://github.com/pytorch/pytorch/pull/121734</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124107<br />
* <strong>-&gt;</strong> #121734<br />
* #123932<br />
* #123930<br />
* #121497<br />
* #124106</p>
<p>Add support for bmm ( batch matrix multiply ) op through the Cutlass backend.</p>
<p>Test Plan:<br />
 * CI<br />
 * Added test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 09:23:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121734</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Tolerate dynamic shapes</title>
      <link>https://github.com/pytorch/pytorch/pull/121497</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #124107<br />
* #121734<br />
* #123932<br />
* #123930<br />
* <strong>-&gt;</strong> #121497<br />
* #124106</p>
<p>Previously, when the Cutlass backend was enabled, using dynamic shapes could lead to exceptions during JIT.</p>
<p>With this change, there are guards in place to just disable the Cutlass backend if dynamic dimensions are involved.</p>
<p>In addition, if no choices for a GEMM are available using the selected backends, then an ATen Kernel is used as fallback, even if the ATen backend is not enabled.</p>
<p>Test:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 07:55:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121497</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] DO NOT REVIEW - to be split up</title>
      <link>https://github.com/pytorch/pytorch/pull/121492</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121492<br />
* #124107<br />
* #121734<br />
* #123932<br />
* #123930<br />
* #121497<br />
* #124106</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121492</guid>
    </item>
    <item>
      <title>Add registration API for torch.compile-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/121387</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we intend to provide a registration API dedicated to eager-through-torch.compile. The major workflow of this API will be as follows.</p>
<ul>
<li>Load cache</li>
<li>Check cache according to the input tensors</li>
<li>Cache Hit: Run the cached kernel directly</li>
<li>Cache Miss: Run the AOTI to produce kernel and run the produced kernel. If AOTI fails to produce the kernel, invoke the python fallback function.</li>
</ul>
<p>Currently, this PR always fallback to python kernel now and cache mechanism will be implemented in another PR - https://github.com/pytorch/pytorch/pull/116368</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120595<br />
* #124070<br />
* #124177<br />
* #116368<br />
* <strong>-&gt;</strong> #121387<br />
* #123545</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121387</guid>
    </item>
    <item>
      <title>[PT2D] Make the speedup benchmark works with DDP + CompiledAutograd</title>
      <link>https://github.com/pytorch/pytorch/pull/121315</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121315</p>
<p>https://github.com/pytorch/pytorch/pull/120454 is the original PR. It does not cover the optimizer part, which may be the root cause of breaking the dashboard. This PR save a new copy of optimizer as well.</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54591562/">D54591562</a></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 09:35:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121315</guid>
    </item>
    <item>
      <title>Torch compile does not work on python 3.12</title>
      <link>https://github.com/pytorch/pytorch/issues/120233</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Currently torch, as of 2.2.0 does not support torch compile with python 3.12</p>
<p>See following PR for example: https://github.com/pytorch/pytorch/pull/117853</p>
<ol>
<li>We need to be able to use python 3.12 with torch.compile feature. </li>
<li>Include triton with Linux 3.12 wheel </li>
<li>Enable Python 3.12 and torch.compile CI testsing</li>
</ol>
<p>cc: @albanD @malfet </p>
<h3>Versions</h3>
<p>2.2.0</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Tue, 20 Feb 2024 06:14:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120233</guid>
    </item>
    <item>
      <title>Add a cache mechanism to accelerate torch.compile-for-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/116368</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120595<br />
* #124070<br />
* #124177<br />
* <strong>-&gt;</strong> #116368<br />
* #121387<br />
* #123545</p>
<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to enable a cache mechanism to accelerate <strong>eager-through-torch.compile</strong>. When <strong>eager-through-torch.compile</strong> is enabled, we will store a persistent config to cache the kernel information for the aten operation. </p>
<p>The persistent config consists of two parts - meta_info and kernel_path.</p>
<ul>
<li>meta_info: The input tensors' shape, stride, device type, data type, and symbolic flag.</li>
<li>kernel_path: The path of the kernel produced by Inductor.</li>
</ul>
<p>When an aten operation is registered, the <code>kernel_holder</code> will load the persistent config and parse it to build the cache map;  the meta_info is key, and the kernel library is the value.</p>
<p>Currently, this PR only supports static shape to guard the kernel.</p>
<p>Take a <code>mul</code> as an example.<br />
```python<br />
class MulKernel:<br />
  def <strong>init</strong>(self) -&gt; None:<br />
    pass</p>
<p>def <strong>call</strong>(self, <em>args: Any, </em><em>kwargs: Any) -&gt; Any:<br />
    with torch._C._SetExcludeDispatchKeyGuard(torch._C.DispatchKey.Python, False):<br />
      opt_fn = torch.compile(torch.ops.aten.mul, dynamic=False, options={<br />
          "aot_inductor.eager_mode": True,<br />
          "aot_inductor.eager_op_name": "mul_Tensor"<br />
        }<br />
      )<br />
      return opt_fn(</em>args, **kwargs)</p>
<p>torch_compile_op_lib_impl = torch.library.Library("aten", "IMPL")</p>
<p>_, overload_names = torch._C._jit_get_operation("aten::mul")<br />
schema = torch._C._get_schema("aten::mul", overload_name)<br />
reg_name = schema.name<br />
if schema.overload_name:<br />
  reg_name = f"{reg_name}.{schema.overload_name}"<br />
  torch_compile_op_lib_impl.impl(<br />
    reg_name,<br />
    MulKernel(),<br />
    "CUDA",<br />
    compile_mode=True)</p>
<p>a = torch.randn(1024, 1024, device=device)<br />
b = torch.randn(1024, 1024, device=device)<br />
warm_up_iter = 1000<br />
iter = 10000<br />
fn = torch.mul</p>
<h1>Warm up</h1>
<p>for _ in range(warm_up_iter):<br />
    fn(a, b)</p>
<h1>Collect performance</h1>
<p>beg = time.time()<br />
for _ in range(iter):<br />
    fn(a, b)<br />
end = time.time()<br />
print(f"E2E run: {end - beg}")<br />
```<br />
It will produce the config as follows.</p>
<p><code>json
[
    {
        "meta_info": [
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            },
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            }
        ],
        "kernel_path": "/tmp/torchinductor_eikan/e4/ce4jw46i5l2e7v3tvr2pyglpjmahnp7x3hxaqotrvxwoeh5t6qzc.so"
    }
]</code></p>
<p>Performance-wise, we collected mul.Tensor through torch.compile w/ 10000 runs(e2e). The data is as follows. And we will collect data when we support dynamic shape.</p>
<ul>
<li>Eager: ~266.11ms</li>
<li>W/O Cache: ~3455.54ms</li>
<li>W/ Cache and Cache Miss: ~3555.3ms</li>
<li>W/ Cache and Cache Hit: ~267.12ms</li>
</ul>
<p>Hardware:</p>
<ul>
<li>CPU: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz</li>
<li>GPU: CUDA A10</li>
</ul>
<p>Software:</p>
<ul>
<li>PyTorch Version: 39df084001c54cca5fe3174176f9b0206ddb7dcf</li>
<li>GPU Driver Version: 525.147.05</li>
<li>CUDA Version: 12.0</li>
</ul>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 03:21:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116368</guid>
    </item>
    <item>
      <title>torch.compile mode="max-autotune" precision appears to be lower</title>
      <link>https://github.com/pytorch/pytorch/issues/96693</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I'm sorry, this is going to be a terrible bug report, but I wasn't able to simplify the problem. </p>
<p>I'm running a training run with a model that is compiled with <code>torch.compile</code>. The blue line below is loss with the <code>default</code> mode, the red line below is loss with <code>max-autotune</code>, the training runs are otherwise identical:</p>
<p><img alt="spiky_comparison" src="https://user-images.githubusercontent.com/22680696/224837823-788ccd48-6fb0-4099-9c87-46926dca134f.png" /></p>
<p>It appears as if precision/stability is noticeably lower on the max-autotune training run. This is running on an RTXA600 GPU. The forward pass is wrapped in <br />
<code>torch.autocast</code>, there are no graph breaks or warning messages. Max-autotune code is also noticeably a bit faster, so it is definitely doing something that the default mode is not.</p>
<p>Feel free to close this issue if this is not enough info. I could also test more specific things or try to make a mini version to get to the bottom of this, but I have no idea what to look for, or which part would be relevant.</p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>```<br />
PyTorch version: 2.1.0.dev20230307<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Pop!_OS 22.04 LTS (x86_64)<br />
GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0<br />
Clang version: Could not collect<br />
CMake version: Could not collect<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-6.2.0-76060200-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU<br />
Nvidia driver version: 525.85.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>Versions of relevant libraries:<br />
[pip3] lion-pytorch==0.0.7<br />
[pip3] numpy==1.23.5<br />
[pip3] torch==2.1.0.dev20230307<br />
[pip3] torchaudio==2.0.0.dev20230307<br />
[pip3] torchvision==0.15.0.dev20230307<br />
[conda] blas                      1.0                         mkl<br />
[conda] lion-pytorch              0.0.7                    pypi_0    pypi<br />
[conda] mkl                       2021.4.0           h06a4308_640<br />
[conda] mkl-service               2.4.0           py310h7f8727e_0<br />
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0<br />
[conda] mkl_random                1.2.2           py310h00e6091_0<br />
[conda] numpy                     1.23.5          py310hd5efca6_0<br />
[conda] numpy-base                1.23.5          py310h8e6c178_0<br />
[conda] pytorch                   2.1.0.dev20230307 py3.10_cuda11.8_cudnn8.7.0_0    pytorch-nightly<br />
[conda] pytorch-cuda              11.8                 h7e8668a_3    pytorch-nightly<br />
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly<br />
[conda] torchaudio                2.0.0.dev20230307     py310_cu118    pytorch-nightly<br />
[conda] torchtriton               2.0.0+b8b470bc59           py310    pytorch-nightly<br />
[conda] torchvision               0.15.0.dev20230307     py310_cu118    pytorch-nightly</p>
<p>```</p>
<p>cc @ezyang @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @wconstab @soumith @ngimel</p>]]></description>
      <pubDate>Mon, 13 Mar 2023 13:46:59 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/96693</guid>
    </item>
    <item>
      <title>torch.compile breaks reproducibility</title>
      <link>https://github.com/pytorch/pytorch/issues/94855</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Adding torch.compile does not ensure deterministic results after setting a seed (and ensuring all the steps here: https://pytorch.org/docs/stable/notes/randomness.html#:~:text=Reproducibility%20Completely%20reproducible%20results%20are%20not%20guaranteed%20across,and%20GPU%20executions%2C%20even%20when%20using%20identical%20seeds.). </p>
<p>I've been stuck trying to debug why my results are non-deterministic across runs. Finally, removing torch.compile ensures that results across multiple runs are the same. This can be easily reproduced by having multiple runs of a model with torch.compile enabled. </p>
<h3>Error logs</h3>
<p><em>No response</em></p>
<h3>Minified repro</h3>
<p><em>No response</em></p>
<h3>Versions</h3>
<p>PyTorch version: 2.0.0.dev20230213+cu117<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.7<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 20.04.5 LTS (x86_64)<br />
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.25.0<br />
Libc version: glibc-2.31</p>
<p>Python version: 3.8.15 (default, Nov 24 2022, 15:19:38)  [GCC 11.2.0] (64-bit runtime)<br />
Python platform: Linux-5.4.0-1085-azure-x86_64-with-glibc2.17<br />
Is CUDA available: True<br />
CUDA runtime version: 11.7.99<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration:<br />
GPU 0: NVIDIA A100 80GB PCIe<br />
GPU 1: NVIDIA A100 80GB PCIe<br />
GPU 2: NVIDIA A100 80GB PCIe<br />
GPU 3: NVIDIA A100 80GB PCIe</p>
<p>Nvidia driver version: 510.73.08<br />
cuDNN version: Probably one of the following:<br />
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.5.0<br />
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.5.0<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                    x86_64<br />
CPU op-mode(s):                  32-bit, 64-bit<br />
Byte Order:                      Little Endian<br />
Address sizes:                   48 bits physical, 48 bits virtual<br />
CPU(s):                          96<br />
On-line CPU(s) list:             0-95<br />
Thread(s) per core:              1<br />
Core(s) per socket:              48<br />
Socket(s):                       2<br />
NUMA node(s):                    4<br />
Vendor ID:                       AuthenticAMD<br />
CPU family:                      25<br />
Model:                           1<br />
Model name:                      AMD EPYC 7V13 64-Core Processor<br />
Stepping:                        1<br />
CPU MHz:                         2478.466<br />
BogoMIPS:                        4890.88<br />
Hypervisor vendor:               Microsoft<br />
Virtualization type:             full<br />
L1d cache:                       3 MiB<br />
L1i cache:                       3 MiB<br />
L2 cache:                        48 MiB<br />
L3 cache:                        384 MiB<br />
NUMA node0 CPU(s):               0-23<br />
NUMA node1 CPU(s):               24-47<br />
NUMA node2 CPU(s):               48-71<br />
NUMA node3 CPU(s):               72-95<br />
Vulnerability Itlb multihit:     Not affected<br />
Vulnerability L1tf:              Not affected<br />
Vulnerability Mds:               Not affected<br />
Vulnerability Meltdown:          Not affected<br />
Vulnerability Mmio stale data:   Not affected<br />
Vulnerability Spec store bypass: Vulnerable<br />
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling<br />
Vulnerability Srbds:             Not affected<br />
Vulnerability Tsx async abort:   Not affected<br />
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr arat umip vaes vpclmulqdq rdpid</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.23.5<br />
[pip3] pytorch-lightning==1.6.3<br />
[pip3] pytorch-triton==2.0.0+0d7e753227<br />
[pip3] torch==2.0.0.dev20230213+cu117<br />
[pip3] torch-nebula==0.15.9<br />
[pip3] torch-ort==1.13.1<br />
[pip3] torchaudio==2.0.0.dev20230214+cu117<br />
[pip3] torchmetrics==0.7.1<br />
[pip3] torchvision==0.15.0.dev20230214+cu117<br />
[conda] magma-cuda117             2.6.1                         1    pytorch<br />
[conda] mkl                       2021.4.0                 pypi_0    pypi<br />
[conda] mkl-include               2021.4.0                 pypi_0    pypi<br />
[conda] numpy                     1.23.5                   pypi_0    pypi<br />
[conda] pytorch-lightning         1.6.3                    pypi_0    pypi<br />
[conda] pytorch-triton            2.0.0+0d7e753227          pypi_0    pypi<br />
[conda] torch                     2.0.0.dev20230213+cu117          pypi_0    pypi<br />
[conda] torch-nebula              0.15.9                   pypi_0    pypi<br />
[conda] torch-ort                 1.13.1                   pypi_0    pypi<br />
[conda] torchaudio                2.0.0.dev20230214+cu117          pypi_0    pypi<br />
[conda] torchmetrics              0.7.1                    pypi_0    pypi<br />
[conda] torchvision               0.15.0.dev20230214+cu117          pypi_0    pypi</p>
<p>cc @pbelevich @mruberry @kurtamohler @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @soumith @wconstab @ngimel @yanboliang @Xia-Weiwen @desertfire</p>]]></description>
      <pubDate>Tue, 14 Feb 2023 13:47:26 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/94855</guid>
    </item>
  </channel>
</rss>
