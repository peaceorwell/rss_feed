<?xml version='1.0' encoding='UTF-8'?>
<rss version="2.0">
  <channel>
    <title>GitHub issues Feed</title>
    <link>https://github.com/username/repo/issues</link>
    <description>Recent issues from GitHub repo</description>
    <item>
      <title>GPT2ForSequenceClassification accuracy test fails for CUDA AOT Inductor export when its SDPA pattern is mapped to the SDPA op</title>
      <link>https://github.com/pytorch/pytorch/issues/122429</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>In #121866, GPT2 SDPA pattern is being mapped to <code>torch.nn.functional.scaled_dot_product_attention</code>.<br />
With CUDA, <code>GPT2ForSequenceClassification</code> <a href="https://github.com/pytorch/pytorch/actions/runs/8319111885/job/22762567770">fails on CI for Inductor AOT export path</a>, but not in other CI checks.</p>
<p>Marking it as an expected failure in #121866, and creating this ticket for tracking the failure.</p>
<p>Thanks!</p>
<h3>Versions</h3>
<p>PyTorch main branch</p>
<p>cc @drisspg @eellison </p>]]></description>
      <pubDate>Thu, 21 Mar 2024 11:27:56 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122429</guid>
    </item>
    <item>
      <title>[inductor] Fix issue with randint + symbolic shapes</title>
      <link>https://github.com/pytorch/pytorch/pull/122428</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122428</p>
<p>Fixes #122405</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 11:20:29 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122428</guid>
    </item>
    <item>
      <title>`torch.compile` should result in an optimized module where `module.training` is the same as in the unoptimized module</title>
      <link>https://github.com/pytorch/pytorch/issues/122414</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>Hi, basically what the title says.<br />
The current behavior of <code>torch.compile</code> is imo quite unexpected and can lead users to the false belief that a model is in eval mode.</p>
<h3>Alternatives</h3>
<p>Alternatively, it would be a good idea to add to the documentation of <code>torch.compile</code> that the resulting optimized module always is in train mode.</p>
<h3>Additional context</h3>
<p><em>No response</em></p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 07:45:52 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122414</guid>
    </item>
    <item>
      <title>randint, compile and dynamic shape</title>
      <link>https://github.com/pytorch/pytorch/issues/122405</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>The following call to randint fails when the shape of the function input changes:</p>
<p>```python<br />
import torch</p>
<p>@torch.compile()<br />
def get_traj_idx(lengths: torch.Tensor, num_slices: int) -&gt; torch.Tensor:<br />
    return torch.randint(lengths.shape[0], (num_slices,), device=lengths.device)</p>
<p>lengths = torch.zeros(10, dtype=torch.long)<br />
get_traj_idx(lengths, num_slices=4)<br />
lengths = torch.zeros(11, dtype=torch.long)<br />
get_traj_idx(lengths, num_slices=4)<br />
```</p>
<p>Error log:<br />
```<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
RuntimeError: prims::inductor_randint() Expected a value of type 'int' for argument 'high' but instead found type 'Node'.<br />
Position: 1<br />
Value: arg0_1<br />
Declaration: prims::inductor_randint(SymInt low, SymInt high, SymInt[] size, Tensor seed) -&gt; Tensor<br />
Cast error details: Unable to cast Python instance of type <class 'torch.fx.node.Node'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True</p>
<p>```</p>
<h3>Versions</h3>
<p>Nightly</p>
<p>cc @pbelevich @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 05:35:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122405</guid>
    </item>
    <item>
      <title>Handle JIT test failure when the GPU is newer than the CUDA compiler </title>
      <link>https://github.com/pytorch/pytorch/pull/122402</link>
      <description><![CDATA[<p>The test uses the CUDA compute capabilities of the current device to<br />
compile an extension. If nvcc is older than the device, it will fail<br />
with a message like "Unsupported gpu architecture 'compute_80'"<br />
resulting in a <code>RuntimeError: Error building extension 'cudaext_archflags'</code><br />
ultimately failing the test.</p>
<p>This checks for this case and allows execution to continue</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/51950</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 04:02:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122402</guid>
    </item>
    <item>
      <title>Handle JIT test failure when the GPU is newer than the CUDA compiler or vice versa</title>
      <link>https://github.com/pytorch/pytorch/pull/122400</link>
      <description><![CDATA[<p>The test may fail because it either uses target flags newer than the GPU resulting in failures loading the compiled binary or targetting a GPU for which CUDA has no support yet/anymore</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 04:00:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122400</guid>
    </item>
    <item>
      <title>Link compiled protobuf files to protobuf::libprotobuf </title>
      <link>https://github.com/pytorch/pytorch/pull/122398</link>
      <description><![CDATA[<p>This correctly propagates the required compile flags (such as <code>-DPROTOBUF_USE_DLLS</code> for shared builds of protobuf).</p>
<p>Fixes https://github.com/pytorch/pytorch/issues/106297</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 03:57:41 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122398</guid>
    </item>
    <item>
      <title> [Inductor] Add a test for creating a cpu inductor-&gt; triton backend</title>
      <link>https://github.com/pytorch/pytorch/pull/122396</link>
      <description><![CDATA[<p>Summary: Currently there is a test for adding a backend in test/inductor/test_extension_backend.py for a cpp backend with a new device. However there is no such test for the Triton backend; it should be possible for a user to create and register your own ExtensionWrapperCodegen and ExtensionSchedulingfor another non-CUDA device and be able to generate Triton code. For simplicity I have chosen to use a CPU device, as I think it's plausible someone might want to create a CPU Triton backend.</p>
<p>Unfortunately the generation and running of the code is quite tightly coupled so I've had to use a mocked function to extract the code before running. Suggestions are welcome for better ways to do this.</p>
<p>This is a stepping off point for some additional PRs to make the Triton code path less CUDA specific, as currently there would be no way to test this avenue.</p>
<p>Test plan:<br />
```<br />
frames [('total', 1), ('ok', 1)]<br />
stats [('calls_captured', 3), ('unique_graphs', 1)]<br />
inductor [('intermediate_hooks', 1)]<br />
aot_autograd [('total', 1), ('ok', 1)]<br />
.</p>
<hr />
<p>Ran 1 test in 0.394s<br />
```</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 03:38:01 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122396</guid>
    </item>
    <item>
      <title>cache torch.compile graphs on disk to avoid recompilation</title>
      <link>https://github.com/pytorch/pytorch/issues/122395</link>
      <description><![CDATA[<h3>üöÄ The feature, motivation and pitch</h3>
<p>Running inference with minimal changes, but have to run torch.compile every time... especially painful in max. tuning modes.</p>
<h3>Alternatives</h3>
<p><em>No response</em></p>
<h3>Additional context</h3>
<p><em>No response</em></p>]]></description>
      <pubDate>Thu, 21 Mar 2024 03:02:32 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122395</guid>
    </item>
    <item>
      <title>[inductor][cpu]adv_inception_v3, gluon_inception_v3 and inception_v3 AMP performance regression</title>
      <link>https://github.com/pytorch/pytorch/issues/122393</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>AMP static shape performance regression in 2024-03-18</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>timm_models</td>
      <td>adv_inception_v3</td>
      <td>multiple</td>
      <td>128</td>
      <td>3.695396</td>
      <td>0.087057353</td>
      <td>0.321711394046788</td>
      <td>30.958788</td>
      <td>128</td>
      <td>4.924577</td>
      <td>0.065420102</td>
      <td>0.322166329646854</td>
      <td>33.211353</td>
      <td>0.75</td>
      <td>1.0</td>
      <td>0.75</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>multiple</td>
      <td>128</td>
      <td>3.700601</td>
      <td>0.08712846399999999</td>
      <td>0.3224276810068639</td>
      <td>30.544114</td>
      <td>128</td>
      <td>4.877504</td>
      <td>0.06528794</td>
      <td>0.31844218850176004</td>
      <td>32.577501</td>
      <td>0.76</td>
      <td>0.99</td>
      <td>0.75</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>inception_v3</td>
      <td>multiple</td>
      <td>128</td>
      <td>3.7809</td>
      <td>0.086533052</td>
      <td>0.3271728163068</td>
      <td>30.467435</td>
      <td>128</td>
      <td>4.956273</td>
      <td>0.065406192</td>
      <td>0.32417094344241604</td>
      <td>32.360837</td>
      <td>0.76</td>
      <td>0.99</td>
      <td>0.76</td>
      <td>1.06</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>adv_inception_v3</td>
      <td>single</td>
      <td>1</td>
      <td>4.391231</td>
      <td>0.020491818</td>
      <td>0.089984306447958</td>
      <td>27.533994</td>
      <td>1</td>
      <td>6.074862</td>
      <td>0.014556469</td>
      <td>0.08842854038227801</td>
      <td>29.358509</td>
      <td>0.72</td>
      <td>0.98</td>
      <td>0.71</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>single</td>
      <td>1</td>
      <td>4.406967</td>
      <td>0.020548244</td>
      <td>0.090555433215948</td>
      <td>27.204902</td>
      <td>1</td>
      <td>6.18311</td>
      <td>0.014545653</td>
      <td>0.08993737252083</td>
      <td>29.311159</td>
      <td>0.71</td>
      <td>0.99</td>
      <td>0.71</td>
      <td>1.08</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>inception_v3</td>
      <td>single</td>
      <td>1</td>
      <td>4.47947</td>
      <td>0.020806962</td>
      <td>0.09320416207014</td>
      <td>27.136373</td>
      <td>1</td>
      <td>6.288863</td>
      <td>0.014541316</td>
      <td>0.091448344163708</td>
      <td>29.336782</td>
      <td>0.71</td>
      <td>0.98</td>
      <td>0.7</td>
      <td>1.08</td>
    </tr>
  </tbody>

</table>

<p>AMP dynamic shape performance regression in 2024-03-19</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
   <tr>
      <td>timm_models</td>
      <td>adv_inception_v3</td>
      <td>single</td>
      <td>1.0</td>
      <td>4.358126</td>
      <td>0.02061505</td>
      <td>0.0898429853963</td>
      <td>27.534621</td>
      <td>1</td>
      <td>6.102014</td>
      <td>0.014657954</td>
      <td>0.089443040519356</td>
      <td>29.887889</td>
      <td>0.71</td>
      <td>1.0</td>
      <td>0.71</td>
      <td>1.09</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>gluon_inception_v3</td>
      <td>single</td>
      <td>1.0</td>
      <td>4.411057</td>
      <td>0.020516146</td>
      <td>0.09049788942632199</td>
      <td>27.462184</td>
      <td>1</td>
      <td>6.181562</td>
      <td>0.014640228</td>
      <td>0.090499477076136</td>
      <td>29.459571</td>
      <td>0.71</td>
      <td>1.0</td>
      <td>0.71</td>
      <td>1.07</td>
    </tr>
    <tr>
      <td>timm_models</td>
      <td>inception_v3</td>
      <td>single</td>
      <td>1.0</td>
      <td>4.490309</td>
      <td>0.020474836</td>
      <td>0.091938340364324</td>
      <td>27.457952</td>
      <td>1</td>
      <td>6.265146</td>
      <td>0.014763517</td>
      <td>0.092495589478482</td>
      <td>29.428536</td>
      <td>0.72</td>
      <td>1.01</td>
      <td>0.72</td>
      <td>1.07</td>
    </tr>
  </tbody>


</table>
<p>SW info</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>d6015d42</td>
      <td>main</td>
      <td>1ef0a39e</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>5bc7f7f97760d2d485621f9f30d0316e1f2440c6</td>
      <td>main</td>
      <td>41286f1505ffb214d386d72e4b72ebd680a4a475</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.18.0a0+0325175</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>
</table>

<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh <strong>thread</strong> inference performance timm_models <strong>model</strong> amp first static/dynamic</p>
<p>Suspected guilty commit: https://github.com/pytorch/pytorch/commit/ffabb25c489df1dc631a577c12a0c843c8b202f3<br />
<a href="https://github.com/pytorch/pytorch/files/14693268/timm_models-adv_inception_v3-inference-amp-dynamic-default-single-performance-drop_guilty_commit.log">timm_models-adv_inception_v3-inference-amp-dynamic-default-single-performance-drop_guilty_commit.log</a></p>
<p>cc @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Thu, 21 Mar 2024 02:03:09 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122393</guid>
    </item>
    <item>
      <title>[Quant][PT2E] Enable linear-binary(-unary) post-op recipe for X86Inductor quantizer</title>
      <link>https://github.com/pytorch/pytorch/pull/122387</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122387<br />
* #122288</p>
<p>As the title<br />
<strong>Test plan</strong><br />
python test_quantization.py -k test_linear_binary</p>
<p>cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 22:29:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122387</guid>
    </item>
    <item>
      <title>[torch.compile] `conv_add` optimization will omit the `alpha` argument</title>
      <link>https://github.com/pytorch/pytorch/issues/122382</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>The <code>conv_add</code> optimization overlooks the <code>alpha</code> parameter. In the model provided, the <code>torch.compile</code> method assumes <code>alpha=1</code> by default, whereas the actual value of <code>alpha</code> is 0.85. This discrepancy leads to a variation in the output compared to a straightforward execution.</p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(420)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()
    self.conv = torch.nn.Conv2d(3, 2, 1, stride=1, padding=1)
    self.other_tensor = torch.randn(2, 1, 1)

def forward(self, x1):
    v1 = self.conv(x1)
    v2 = torch.add(v1, self.other_tensor, alpha=0.875)
    return v2
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(1, 3, 2, 2)</p>
<p>with torch.no_grad():<br />
    print(func(x.clone()))</p>
<pre><code>func1 = torch.compile(func)
print(func1(x.clone()))

print(torch.allclose(func.other_tensor, func1.other_tensor)) # True
</code></pre>
<p>"""<br />
tensor([[[[ 1.7728,  1.7728,  1.7728,  1.7728],<br />
          [ 1.7728,  2.1860,  1.7604,  1.7728],<br />
          [ 1.7728,  0.9550,  2.3261,  1.7728],<br />
          [ 1.7728,  1.7728,  1.7728,  1.7728]],</p>
<pre><code>     [[ 0.2579,  0.2579,  0.2579,  0.2579],
      [ 0.2579, -0.5048,  0.6618,  0.2579],
      [ 0.2579,  0.9659,  0.0888,  0.2579],
      [ 0.2579,  0.2579,  0.2579,  0.2579]]]])
</code></pre>
<p>tensor([[[[ 1.9624,  1.9624,  1.9624,  1.9624],<br />
          [ 1.9624,  2.3757,  1.9500,  1.9624],<br />
          [ 1.9624,  1.1447,  2.5158,  1.9624],<br />
          [ 1.9624,  1.9624,  1.9624,  1.9624]],</p>
<pre><code>     [[ 0.2639,  0.2639,  0.2639,  0.2639],
      [ 0.2639, -0.4989,  0.6677,  0.2639],
      [ 0.2639,  0.9718,  0.0947,  0.2639],
      [ 0.2639,  0.2639,  0.2639,  0.2639]]]])
</code></pre>
<p>"""<br />
```</p>
<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.3.0.dev20240301+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0<br />
Clang version: 11.0.0 (https://github.com/aflgo/aflgo.git fa125da5d70621daf7141c6279877c97708c8c1f)<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56)  [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-26-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060<br />
Nvidia driver version: 525.147.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             24<br />
On-line CPU(s) list:                0-23<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family:                         6<br />
Model:                              151<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 16<br />
Socket(s):                          1<br />
Stepping:                           2<br />
CPU max MHz:                        5200.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           6374.40<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          640 KiB (16 instances)<br />
L1i cache:                          768 KiB (16 instances)<br />
L2 cache:                           14 MiB (10 instances)<br />
L3 cache:                           30 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu121<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[pip3] triton==2.2.0<br />
[conda] numpy                     1.26.4                   pypi_0    pypi<br />
[conda] pytorch-triton            3.0.0+901819d2b6          pypi_0    pypi<br />
[conda] torch                     2.3.0.dev20240301+cu121          pypi_0    pypi<br />
[conda] torchaudio                2.2.0.dev20240301+cu118          pypi_0    pypi<br />
[conda] torchvision               0.18.0.dev20240301+cu118          pypi_0    pypi<br />
[conda] triton                    2.2.0                    pypi_0    pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 20:22:33 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122382</guid>
    </item>
    <item>
      <title>[torch.compile] A potential OOB access in CUDA for attention model</title>
      <link>https://github.com/pytorch/pytorch/issues/122381</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>For the following model, the result optimized by <code>torch.compile</code> on CUDA is totally wrong, which may out-of-bound access some data.</p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(420)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self, d_model, dropout_p=0.1, inv_scale_factor=1e-08):
    super().__init__()
    self.d_model = d_model
    self.dropout_p = dropout_p
    self.inv_scale_factor = inv_scale_factor
    self.dropout = torch.nn.Dropout(dropout_p)

def forward(self, query, key, value):
    qk = torch.matmul(query, key.transpose(-2, -1))
    scaled_qk = qk.div(self.inv_scale_factor)
    softmax_qk = scaled_qk.softmax(dim=-1)
    dropout_qk = self.dropout(softmax_qk)
    return dropout_qk
</code></pre>
<p>device = "cuda"<br />
d_model = 1<br />
func = Model(10).to(device)</p>
<p>query = torch.randn(1, 5, 10).to(device)</p>
<p>key = torch.randn(1, 5, 10).to(device)</p>
<p>value = torch.randn(1, 5, 10).to(device)</p>
<p>with torch.no_grad():<br />
    naive_result = func(query.clone(), key.clone(), value.clone())</p>
<pre><code>func1 = torch.compile(func)
jit_result = func1(query.clone(), key.clone(), value.clone())

print(naive_result)
print(jit_result)
print(torch._dynamo.utils.counters["inductor"])
</code></pre>
<p>"""<br />
tensor([[[0.0000, 0.0000, 0.0000, 1.1111, 0.0000],<br />
         [0.0000, 0.0000, 1.1111, 0.0000, 0.0000],<br />
         [0.0000, 0.0000, 0.0000, 0.0000, 1.1111],<br />
         [0.0000, 0.0000, 0.0000, 1.1111, 0.0000],<br />
         [0.0000, 0.0000, 1.1111, 0.0000, 0.0000]]], device='cuda:0')<br />
tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],<br />
         [0.0000e+00, 0.0000e+00, 1.6732e+03, 0.0000e+00, 0.0000e+00],<br />
         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0731e-10],<br />
         [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4720e-03, 0.0000e+00],<br />
         [0.0000e+00, 0.0000e+00, 4.0205e+06, 0.0000e+00, 0.0000e+00]]],<br />
       device='cuda:0')<br />
Counter({'pattern_matcher_count': 4, 'pattern_matcher_nodes': 4})<br />
"""<br />
```</p>
<p>This model has been minimized; therefore, if any operator is removed, it won't trigger the bug. Additionally, I've printed the patterns optimized by the Inductor to assist with debugging.</p>
<p><code>view (sum_1, [4]) Match(..., [], {'arg': sum_1, 'size': [4]}) CallFunction(aten.view.default, KeywordArg('arg'), KeywordArg('size'))
view_2 (None, [1, 5, 5]) Match(..., [], {'arg': bmm, 'size': [1, 5, 5]}) CallFunction(aten.view.default, KeywordArg('arg'), KeywordArg('size'))
view_1 (None, [1, 10, 5]) Match(..., [], {'arg': expand_1, 'size': [1, 10, 5]}) CallFunction(aten.view.default, KeywordArg('arg'), KeywordArg('size'))
view (None, [1, 5, 10]) Match(..., [], {'arg': expand, 'size': [1, 5, 10]}) CallFunction(aten.view.default, KeywordArg('arg'), KeywordArg('size'))
rand ([1, 5, 5],) Match(..., [[1, 5, 5]], {'dtype': torch.float32, 'device': device(type='cuda', index=0), 'pin_memory': False}) CallFunctionVarArgs(aten.rand.default)</code></p>
<p>It seems that this is <strong>not</strong> related to the <code>fuse_attention</code> optimization</p>
<h3>Versions</h3>
<p>```<br />
Collecting environment information...<br />
PyTorch version: 2.3.0.dev20240301+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0<br />
Clang version: 11.0.0 (https://github.com/aflgo/aflgo.git fa125da5d70621daf7141c6279877c97708c8c1f)<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56)  [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-26-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060<br />
Nvidia driver version: 525.147.05<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      46 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             24<br />
On-line CPU(s) list:                0-23<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family:                         6<br />
Model:                              151<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 16<br />
Socket(s):                          1<br />
Stepping:                           2<br />
CPU max MHz:                        5200.0000<br />
CPU min MHz:                        800.0000<br />
BogoMIPS:                           6374.40<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          640 KiB (16 instances)<br />
L1i cache:                          768 KiB (16 instances)<br />
L2 cache:                           14 MiB (10 instances)<br />
L3 cache:                           30 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit:        Not affected<br />
Vulnerability L1tf:                 Not affected<br />
Vulnerability Mds:                  Not affected<br />
Vulnerability Meltdown:             Not affected<br />
Vulnerability Mmio stale data:      Not affected<br />
Vulnerability Retbleed:             Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds:                Not affected<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu121<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[pip3] triton==2.2.0<br />
[conda] numpy                     1.26.4                   pypi_0    pypi<br />
[conda] pytorch-triton            3.0.0+901819d2b6          pypi_0    pypi<br />
[conda] torch                     2.3.0.dev20240301+cu121          pypi_0    pypi<br />
[conda] torchaudio                2.2.0.dev20240301+cu118          pypi_0    pypi<br />
[conda] torchvision               0.18.0.dev20240301+cu118          pypi_0    pypi<br />
[conda] triton                    2.2.0                    pypi_0    pypi</p>
<p>```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 20:09:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122381</guid>
    </item>
    <item>
      <title>[torch.compile] `UnbindCatRemover` returns `KeyError: 'dim'` in `get_transform_params`</title>
      <link>https://github.com/pytorch/pytorch/issues/122380</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>The following model triggers  <code>KeyError: 'dim'</code> in <code>get_transform_params</code>, which is under <code>UnbindCatRemover</code>. Here is the link: https://github.com/pytorch/pytorch/blob/3e6fdea3900ce33d973893d00619b9bbe6bf8168/torch/_inductor/fx_passes/split_cat.py#L932</p>
<p>```<br />
import torch</p>
<p>torch.manual_seed(0)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super(Model, self).__init__()

def forward(self, x):
    t1 = torch.unbind(x)
    t2 = torch.stack(t1, dim=1)
    t3 = torch.tanh(t2)
    return t3
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(2, 3, 4)</p>
<p>with torch.no_grad():<br />
    func1 = torch.compile(func)<br />
    print(func1(x.clone()).shape)<br />
    """<br />
    File "torch/_inductor/fx_passes/split_cat.py", line 1085, in merge_unbind_stack<br />
    UnbindCatRemover().remove_unbind(match.graph, unbind_node)<br />
    File "torch/_inductor/fx_passes/split_cat.py", line 890, in remove_unbind<br />
        super().simplify(graph, unbind_node, split_sections)<br />
    File "torch/_inductor/fx_passes/split_cat.py", line 500, in simplify<br />
        transform_params_list = self.get_transform_params(<br />
    File "torch/_inductor/fx_passes/split_cat.py", line 932, in get_transform_params<br />
        split_dim = unbind_node.kwargs["dim"]<br />
    torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
    KeyError: 'dim'<br />
    """<br />
```</p>
<p>Run the model with <code>TORCHINDUCTOR_FREEZING=1 TORCHINDUCTOR_PERMUTE_FUSION=1 python test.py</code></p>
<h3>Versions</h3>
<p>```<br />
Versions<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 19:29:42 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122380</guid>
    </item>
    <item>
      <title>[torch.compile] `split_cat_norm` and `cat_mutated` cause the optimized model to return a tensor with wrong shape</title>
      <link>https://github.com/pytorch/pytorch/issues/122379</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>The following model should return a tensor with shape <code>[10, 3, 64, 64]</code>, however, it seems that <code>split_cat_norm</code> and <code>cat_mutated</code> cause the optimized model to return a tensor with wrong shape, which is <code>[2, 3, 64, 64]</code></p>
<p>```py<br />
import torch</p>
<p>torch.manual_seed(0)</p>
<p>class Model(torch.nn.Module):</p>
<pre><code>def __init__(self):
    super().__init__()
    self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)
    self.indices = [i for i in range(0, 10)]

def forward(self, x):
    split_tensors = torch.split(x, 1, 0) # len(split_tensors) == 10
    chosen_tensors = [split_tensors[i] for i in self.indices if i in range(0, 10)]
    result = torch.cat(chosen_tensors, 0)
    return result
</code></pre>
<p>func = Model().to('cpu')</p>
<p>x = torch.randn(10, 3, 64, 64)</p>
<p>with torch.no_grad():<br />
    func1 = torch.compile(func)<br />
    print(func1(x.clone()).shape)<br />
    # torch.Size([2, 3, 64, 64])<br />
    print(torch._dynamo.utils.counters['inductor'])<br />
    # Counter({'pattern_matcher_nodes': 11, 'pattern_matcher_count': 5, 'split_cat_norm': 2, 'cat_mutated': 1})</p>
<pre><code>print(func(x.clone()).shape)
# torch.Size([10, 3, 64, 64])
</code></pre>
<p>```</p>
<h3>Versions</h3>
<p>```<br />
Versions<br />
PyTorch version: 2.3.0.dev20240301+cu118<br />
Is debug build: False<br />
CUDA used to build PyTorch: 11.8<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.1 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: 14.0.0-1ubuntu1.1<br />
CMake version: version 3.22.1<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)<br />
Python platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35<br />
Is CUDA available: False<br />
CUDA runtime version: 11.5.119<br />
CUDA_MODULE_LOADING set to: N/A<br />
GPU models and configuration: Could not collect<br />
Nvidia driver version: Could not collect<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture: x86_64<br />
CPU op-mode(s): 32-bit, 64-bit<br />
Address sizes: 46 bits physical, 48 bits virtual<br />
Byte Order: Little Endian<br />
CPU(s): 24<br />
On-line CPU(s) list: 0-23<br />
Vendor ID: GenuineIntel<br />
Model name: 12th Gen Intel(R) Core(TM) i9-12900K<br />
CPU family: 6<br />
Model: 151<br />
Thread(s) per core: 2<br />
Core(s) per socket: 16<br />
Socket(s): 1<br />
Stepping: 2<br />
CPU max MHz: 5200.0000<br />
CPU min MHz: 800.0000<br />
BogoMIPS: 6374.40<br />
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities<br />
Virtualization: VT-x<br />
L1d cache: 640 KiB (16 instances)<br />
L1i cache: 768 KiB (16 instances)<br />
L2 cache: 14 MiB (10 instances)<br />
L3 cache: 30 MiB (1 instance)<br />
NUMA node(s): 1<br />
NUMA node0 CPU(s): 0-23<br />
Vulnerability Gather data sampling: Not affected<br />
Vulnerability Itlb multihit: Not affected<br />
Vulnerability L1tf: Not affected<br />
Vulnerability Mds: Not affected<br />
Vulnerability Meltdown: Not affected<br />
Vulnerability Mmio stale data: Not affected<br />
Vulnerability Retbleed: Not affected<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence<br />
Vulnerability Srbds: Not affected<br />
Vulnerability Tsx async abort: Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==3.0.0+901819d2b6<br />
[pip3] torch==2.3.0.dev20240301+cu118<br />
[pip3] torchaudio==2.2.0.dev20240301+cu118<br />
[pip3] torchvision==0.18.0.dev20240301+cu118<br />
[conda] numpy 1.26.4 pypi_0 pypi<br />
[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi<br />
[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi<br />
[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi<br />
```</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 19:19:37 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122379</guid>
    </item>
    <item>
      <title>[inductor][cpu]AMP models regression in 2024-03-18 nightly release</title>
      <link>https://github.com/pytorch/pytorch/issues/122376</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>AMP models regression in 2024-03-18 nightly release</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>suite</th>
      <th>name</th>
      <th>thread</th>
      <th>batch_size_new</th>
      <th>speed_up_new</th>
      <th>inductor_new</th>
      <th>eager_new</th>
      <th>compilation_latency_new</th>
      <th>batch_size_old</th>
      <th>speed_up_old</th>
      <th>inductor_old</th>
      <th>eager_old</th>
      <th>compilation_latency_old</th>
      <th>Ratio Speedup(New/old)</th>
      <th>Eager Ratio(old/new)</th>
      <th>Inductor Ratio(old/new)</th>
      <th>Compilation_latency_Ratio(old/new)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fasterrcnn_r_101_c4</td>
      <td>multiple</td>
      <td>1</td>
      <td>1.012302</td>
      <td>0.7957253409999999</td>
      <td>0.805514354144982</td>
      <td>95.832818</td>
      <td>1</td>
      <td>1.203682</td>
      <td>0.669263386</td>
      <td>0.8055802909872519</td>
      <td>43.129035</td>
      <td>0.84</td>
      <td>1.0</td>
      <td>0.84</td>
      <td>0.45</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fasterrcnn_r_101_dc5</td>
      <td>multiple</td>
      <td>1</td>
      <td>0.956352</td>
      <td>0.803115706</td>
      <td>0.768061311664512</td>
      <td>85.517156</td>
      <td>1</td>
      <td>1.089762</td>
      <td>0.703060918</td>
      <td>0.766169072121516</td>
      <td>40.733206</td>
      <td>0.88</td>
      <td>1.0</td>
      <td>0.88</td>
      <td>0.48</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fasterrcnn_r_101_fpn</td>
      <td>multiple</td>
      <td>1</td>
      <td>0.935737</td>
      <td>0.331914183</td>
      <td>0.31058438185787096</td>
      <td>97.961635</td>
      <td>1</td>
      <td>1.374018</td>
      <td>0.229089251</td>
      <td>0.314772754480518</td>
      <td>62.612029</td>
      <td>0.68</td>
      <td>1.01</td>
      <td>0.69</td>
      <td>0.64</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fasterrcnn_r_50_c4</td>
      <td>multiple</td>
      <td>1</td>
      <td>1.026714</td>
      <td>0.876279987</td>
      <td>0.8996889305727179</td>
      <td>79.27709</td>
      <td>1</td>
      <td>1.151954</td>
      <td>0.7809155369999999</td>
      <td>0.8995787765092979</td>
      <td>37.740279</td>
      <td>0.89</td>
      <td>1.0</td>
      <td>0.89</td>
      <td>0.48</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fasterrcnn_r_50_fpn</td>
      <td>multiple</td>
      <td>1</td>
      <td>0.975481</td>
      <td>0.28479742199999997</td>
      <td>0.277814474009982</td>
      <td>80.631157</td>
      <td>1</td>
      <td>1.283427</td>
      <td>0.215888012</td>
      <td>0.277076503577124</td>
      <td>57.022144</td>
      <td>0.76</td>
      <td>1.0</td>
      <td>0.76</td>
      <td>0.71</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fcos_r_50_fpn</td>
      <td>multiple</td>
      <td>1</td>
      <td>0.940634</td>
      <td>0.11080157800000001</td>
      <td>0.104223731520452</td>
      <td>66.293681</td>
      <td>1</td>
      <td>1.243711</td>
      <td>0.084062273</td>
      <td>0.104549173615103</td>
      <td>56.252617</td>
      <td>0.76</td>
      <td>1.0</td>
      <td>0.76</td>
      <td>0.85</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fasterrcnn_r_101_c4</td>
      <td>single</td>
      <td>1</td>
      <td>1.367577</td>
      <td>4.225108481</td>
      <td>5.778161181120538</td>
      <td>85.802776</td>
      <td>1</td>
      <td>1.993762</td>
      <td>2.910399221</td>
      <td>5.802643371659403</td>
      <td>28.592706</td>
      <td>0.69</td>
      <td>1.0</td>
      <td>0.69</td>
      <td>0.33</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fasterrcnn_r_101_dc5</td>
      <td>single</td>
      <td>1</td>
      <td>1.499827</td>
      <td>2.422276194</td>
      <td>3.6329952372184384</td>
      <td>76.851578</td>
      <td>1</td>
      <td>2.090189</td>
      <td>1.7399104170000002</td>
      <td>3.6367416145988134</td>
      <td>30.4916</td>
      <td>0.72</td>
      <td>1.0</td>
      <td>0.72</td>
      <td>0.4</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fasterrcnn_r_101_fpn</td>
      <td>single</td>
      <td>1</td>
      <td>2.160757</td>
      <td>1.272126372</td>
      <td>2.7487559631836036</td>
      <td>87.654239</td>
      <td>1</td>
      <td>4.556652</td>
      <td>0.606570378</td>
      <td>2.763930126054456</td>
      <td>50.540969</td>
      <td>0.47</td>
      <td>1.01</td>
      <td>0.48</td>
      <td>0.58</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fasterrcnn_r_50_c4</td>
      <td>single</td>
      <td>1</td>
      <td>1.329753</td>
      <td>4.029631535</td>
      <td>5.358414622560855</td>
      <td>70.912749</td>
      <td>1</td>
      <td>1.825476</td>
      <td>2.931040107</td>
      <td>5.350543370365932</td>
      <td>25.357939</td>
      <td>0.73</td>
      <td>1.0</td>
      <td>0.73</td>
      <td>0.36</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fasterrcnn_r_50_dc5</td>
      <td>single</td>
      <td>1</td>
      <td>1.429977</td>
      <td>2.20184759</td>
      <td>3.14859141120543</td>
      <td>62.155583</td>
      <td>1</td>
      <td>1.812988</td>
      <td>1.7458810679999999</td>
      <td>3.1652614257111837</td>
      <td>27.296013</td>
      <td>0.79</td>
      <td>1.01</td>
      <td>0.79</td>
      <td>0.44</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fasterrcnn_r_50_fpn</td>
      <td>single</td>
      <td>1</td>
      <td>2.252651</td>
      <td>0.968045299</td>
      <td>2.1806682108376494</td>
      <td>72.647359</td>
      <td>1</td>
      <td>4.253783</td>
      <td>0.511789954</td>
      <td>2.1770434058959824</td>
      <td>47.561992</td>
      <td>0.53</td>
      <td>1.0</td>
      <td>0.53</td>
      <td>0.65</td>
    </tr>
    <tr>
      <td>torchbench</td>
      <td>detectron2_fcos_r_50_fpn</td>
      <td>single</td>
      <td>1</td>
      <td>2.077448</td>
      <td>0.887069576</td>
      <td>1.842840916522048</td>
      <td>59.677085</td>
      <td>1</td>
      <td>2.650123</td>
      <td>0.703863566</td>
      <td>1.8653250251186178</td>
      <td>49.328029</td>
      <td>0.78</td>
      <td>1.01</td>
      <td>0.79</td>
      <td>0.83</td>
    </tr>
  </tbody>

### Versions

</table>
<p>SW info</p>
<table border="1" class="dataframe table">
  <thead>
    <tr style="text-align: right;">
      <th>name</th>
      <th>target_branch</th>
      <th>target_commit</th>
      <th>refer_branch</th>
      <th>refer_commit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torchbench</td>
      <td>main</td>
      <td>d6015d42</td>
      <td>main</td>
      <td>1ef0a39e</td>
    </tr>
    <tr>
      <td>torch</td>
      <td>main</td>
      <td>5bc7f7f97760d2d485621f9f30d0316e1f2440c6</td>
      <td>main</td>
      <td>41286f1505ffb214d386d72e4b72ebd680a4a475</td>
    </tr>
    <tr>
      <td>torchvision</td>
      <td>main</td>
      <td>0.18.0a0+0325175</td>
      <td>main</td>
      <td>0.18.0a0+2c127da</td>
    </tr>
    <tr>
      <td>torchtext</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
      <td>main</td>
      <td>0.16.0a0+b0ebddc</td>
    </tr>
    <tr>
      <td>torchaudio</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
      <td>main</td>
      <td>2.2.0a0+87aeb55</td>
    </tr>
    <tr>
      <td>torchdata</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
      <td>main</td>
      <td>0.7.1a0+0790338</td>
    </tr>
    <tr>
      <td>dynamo_benchmarks</td>
      <td>main</td>
      <td>nightly</td>
      <td>main</td>
      <td>nightly</td>
    </tr>
  </tbody>
</table>

</table>
<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/main/scripts/modelbench/inductor_single_run.sh">inductor_single_run.sh</a><br />
bash inductor_single_run.sh <strong>thread</strong> inference performance torchbench <strong>model</strong> amp first static/dynamic<br />
<a href="https://github.com/pytorch/pytorch/files/14681282/torchbench-detectron2_fcos_r_50_fpn-inference-amp-static-default-single-performance-drop_guilty_commit.log">torchbench-detectron2_fcos_r_50_fpn-inference-amp-static-default-single-performance-drop_guilty_commit.log</a></p>
<p>Suspected guilty commit: https://github.com/pytorch/pytorch/commit/07caea5c12a0ae1d9c40028ec7686e3f8aef328a<br />
cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @WeizhuoZhang-intel @chuanqi129</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 19:10:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122376</guid>
    </item>
    <item>
      <title>DISABLED test_mlp_contiguous_relu_compile_cutlass (__main__.SparseSemiStructuredTensorCompileTest)</title>
      <link>https://github.com/pytorch/pytorch/issues/122352</link>
      <description><![CDATA[<p>Platforms: rocm</p>
<p>This test was disabled because it is failing on main branch (<a href="https://torch-ci.com/failure?failureCaptures=%5B%22test_sparse_semi_structured.py%3A%3ASparseSemiStructuredTensorCompileTest%3A%3Atest_mlp_contiguous_relu_compile_cutlass%22%5D">recent examples</a>).</p>
<p>cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 14:44:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122352</guid>
    </item>
    <item>
      <title>[Inductor] Fix unbacked symbol in stride when using item()</title>
      <link>https://github.com/pytorch/pytorch/pull/122298</link>
      <description><![CDATA[<p>Fixes #122296</p>
<p>Test: python test/inductor/test_torchinductor_dynamic_shapes.py -k test_item_unbacked_stride_nobreak_cuda </p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 01:35:25 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122298</guid>
    </item>
    <item>
      <title>Inductor cpp wrapper: fix dtype of ShapeAsConstantBuffer</title>
      <link>https://github.com/pytorch/pytorch/pull/122297</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122297</p>
<p>For <code>at::scalar_tensor</code> the default dtype will be <code>float</code> (<a href="https://github.com/pytorch/pytorch/blob/0d8e960f74acd359358e0b729c4803d2b71849e5/aten/src/ATen/native/TensorFactories.cpp#L856">link to scalar_tensor</a>, <a href="https://github.com/pytorch/pytorch/blob/0d8e960f74acd359358e0b729c4803d2b71849e5/c10/core/TensorOptions.h#L551">link to default dtype</a>) if we don't set the <code>dtype</code> value. However, the input scalar value is not necessarily a <code>float</code> value. With <code>torch::tensor(x)</code>, the dtype of the tensor will be decided according to the dtype of the scalar.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 00:58:21 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122297</guid>
    </item>
    <item>
      <title>AOIInductor: Dynamic Shapes Specificiaton fails for SAM </title>
      <link>https://github.com/pytorch/pytorch/issues/122294</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>I am trying to aot_compile a SAM model specifying dynamic shapes fail with the following error:<br />
```BASH<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR] Error while creating guard:<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR] Name: ''<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Source: shape_env<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Create Function: SHAPE_ENV<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Guard Types: None<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Code List: None<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Object Weakref: None<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     Guarded Class Weakref: None<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR] Created at:<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]   File "/home/fabian/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 509, in transform<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     tracer = InstructionTranslator(<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]   File "/home/fabian/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in <strong>init</strong><br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     output=OutputGraph(<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]   File "/home/fabian/.local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 297, in <strong>init</strong><br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     self.init_ambient_guards()<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]   File "/home/fabian/.local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 371, in init_ambient_guards<br />
[2024-03-20 09:39:49,187] [0/0] torch._guards: [ERROR]     self.guards.add(ShapeEnvSource().make_guard(GuardBuilder.SHAPE_ENV))</p>
<hr />
<p>AssertionError                            Traceback (most recent call last)<br />
File /tmp/model.py:21<br />
     19 n_labels = torch.export.Dim("n_labels", min=1, max=12)<br />
     20 example_inputs = (img, points, labels)<br />
---&gt; 21 so_path = torch._export.aot_compile(<br />
     22     model,<br />
     23     example_inputs,<br />
     24     dynamic_shapes={<br />
     25                     "img":{},<br />
     26                     "points": {1: n_labels},<br />
     27                     "labels": {1: n_labels}},<br />
     28     # Specify the generated shared library path<br />
     29     options={"aot_inductor.output_path": os.path.join(os.getcwd(), "model.so")},<br />
     30 )</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_export/<strong>init</strong>.py:1143, in aot_compile(f, args, kwargs, constraints, dynamic_shapes, options, remove_runtime_assertions, disable_constraint_solver)<br />
   1139     constraints = _process_dynamic_shapes(f, args, kwargs, dynamic_shapes)<br />
   1141 # We want to export to Torch IR here to utilize the pre_grad passes in<br />
   1142 # inductor, which run on Torch IR.<br />
-&gt; 1143 gm = _export_to_torch_ir(<br />
   1144     f,<br />
   1145     args,<br />
   1146     kwargs,<br />
   1147     constraints,<br />
   1148     disable_constraint_solver=disable_constraint_solver<br />
   1149 )<br />
   1150 flat_example_inputs = pytree.arg_tree_leaves(<em>args, </em>*(kwargs or {}))<br />
   1152 with torch.no_grad():</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_export/<strong>init</strong>.py:516, in _export_to_torch_ir(f, args, kwargs, constraints, preserve_module_call_signature, disable_constraint_solver)<br />
    514     module_call_specs: Dict[str, Dict[str, pytree.TreeSpec]] = {}<br />
    515     with _wrap_submodules(f, preserve_module_call_signature, module_call_specs):<br />
--&gt; 516         gm_torch_level, _ = torch._dynamo.export(<br />
    517             f,<br />
    518             constraints=constraints,<br />
    519             assume_static_by_default=True,<br />
    520             tracing_mode="symbolic",<br />
    521             disable_constraint_solver=disable_constraint_solver,<br />
    522         )(<br />
    523             <em>args,<br />
    524             </em>*kwargs,<br />
    525         )<br />
    526 except (ConstraintViolationError, ValueRangeError) as e:<br />
    527     raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: TRY200</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1342, in export.<locals>.inner(<em>args, </em><em>kwargs)<br />
   1340 # TODO(voz): We may have instances of <code>f</code> that mutate inputs, we should track sideeffects and reject.<br />
   1341 try:<br />
-&gt; 1342     result_traced = opt_f(</em>args, **kwargs)<br />
   1343 except ConstraintViolationError as e:<br />
   1344     constraint_violation_error = e</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1509     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1510 else:<br />
-&gt; 1511     return self._call_impl(*args, </strong>kwargs)</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1515 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1516 # this function, and just call forward.<br />
   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1518         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1519         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1520     return forward_call(</em>args, **kwargs)<br />
   1522 try:<br />
   1523     result = None</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:489, in _TorchDynamoContext.<strong>call</strong>.<locals>._fn(<em>args, </em><em>kwargs)<br />
    487     dynamo_config_ctx.<strong>enter</strong>()<br />
    488 try:<br />
--&gt; 489     return fn(</em>args, **kwargs)<br />
    490 finally:<br />
    491     set_eval_frame(prior)</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, <em>args, </em><em>kwargs)<br />
   1509     return self._compiled_call_impl(</em>args, <strong>kwargs)  # type: ignore[misc]<br />
   1510 else:<br />
-&gt; 1511     return self._call_impl(*args, </strong>kwargs)</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, <em>args, </em><em>kwargs)<br />
   1515 # If we don't have any hooks, we want to skip the rest of the logic in<br />
   1516 # this function, and just call forward.<br />
   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks<br />
   1518         or _global_backward_pre_hooks or _global_backward_hooks<br />
   1519         or _global_forward_hooks or _global_forward_pre_hooks):<br />
-&gt; 1520     return forward_call(</em>args, **kwargs)<br />
   1522 try:<br />
   1523     result = None</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:655, in catch_errors_wrapper.<locals>.catch_errors(frame, cache_entry, frame_state)<br />
    652             return hijacked_callback(frame, cache_entry, hooks, frame_state)<br />
    654 with compile_lock, _disable_current_modes():<br />
--&gt; 655     return callback(frame, cache_entry, hooks, frame_state)</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:383, in convert_frame_assert.<locals>._convert_frame_assert(frame, cache_entry, hooks, frame_state)<br />
    370 signpost_event(<br />
    371     "dynamo",<br />
    372     "_convert_frame_assert._compile",<br />
   (...)<br />
    379     },<br />
    380 )<br />
    382 with config.patch(_patch_config_if_changed()):<br />
--&gt; 383     compiled_product = _compile(<br />
    384         frame.f_code,<br />
    385         frame.f_globals,<br />
    386         frame.f_locals,<br />
    387         frame.f_builtins,<br />
    388         compiler_fn,<br />
    389         one_graph,<br />
    390         export,<br />
    391         export_constraints,<br />
    392         hooks,<br />
    393         cache_size,<br />
    394         frame,<br />
    395         frame_state=frame_state,<br />
    396         compile_id=compile_id,<br />
    397     )<br />
    398 return compiled_product</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:646, in _compile(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id)<br />
    644 with compile_context(CompileContext(compile_id)):<br />
    645     try:<br />
--&gt; 646         guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
    647         return guarded_code<br />
    648     except (<br />
    649         Unsupported,<br />
    650         TorchRuntimeError,<br />
   (...)<br />
    657         BisectValidationException,<br />
    658     ) as e:</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:244, in dynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper(<em>args, </em><em>kwargs)<br />
    242 with torch.profiler.record_function(f"{key} (dynamo_timed)"):<br />
    243     t0 = time.time()<br />
--&gt; 244     r = func(</em>args, **kwargs)<br />
    245     time_spent = time.time() - t0<br />
    246 compilation_time_metrics[key].append(time_spent)</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:626, in _compile.<locals>.compile_inner(code, one_graph, hooks, transform)<br />
    624 assert output.guards is not None<br />
    625 CleanupManager.instance[out_code] = output.cleanups<br />
--&gt; 626 check_fn = CheckFunctionManager(<br />
    627     output,<br />
    628     hooks.guard_fail_fn if hooks else None,<br />
    629 )<br />
    631 guarded_code = GuardedCode(out_code, check_fn.check_fn)<br />
    633 if not output.is_empty_graph() and hooks.guard_export_fn is not None:<br />
    634     # We should not run the guard_export_fn when Dynamo does not<br />
    635     # generate any graph. This can happen in export when TorchDynamo<br />
    636     # generated bytecode has some reconstruction logic for mutated<br />
    637     # variables which can trigger TorchDynamo on the children frames but<br />
    638     # they are benign and do not generate any new graphs.</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/guards.py:1011, in CheckFunctionManager.<strong>init</strong>(self, output_graph, guard_fail_fn)<br />
   1000     if (<br />
   1001         not config.guard_nn_modules<br />
   1002         and guard.is_nn_module()<br />
   (...)<br />
   1007         and (config.skip_nnmodule_hook_guards or "hooks" not in guard.name)<br />
   1008     ):<br />
   1009         continue<br />
-&gt; 1011     guard.create(builder)<br />
   1012 self.check_fn = self.compile_check_fn(builder, guards, guard_fail_fn)<br />
   1013 self._weakrefs.clear()</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_guards.py:246, in Guard.create(self, builder)<br />
    244 def create(self, builder: GuardBuilderBase):<br />
    245     try:<br />
--&gt; 246         return self.create_fn(builder, self)<br />
    247     except Exception:<br />
    248         log.error("Error while creating guard:\n%s", str(self).rstrip())</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/_dynamo/guards.py:670, in GuardBuilder.SHAPE_ENV(self, guard)<br />
    668 else:<br />
    669     equalities_inputs = None<br />
--&gt; 670 guards = output_graph.shape_env.produce_guards(<br />
    671     [a.fake for a in fs],<br />
    672     [a.source for a in fs],<br />
    673     constraint_inputs=constraint_inputs,<br />
    674     equalities_inputs=equalities_inputs,<br />
    675     source_ref=self.source_ref,<br />
    676     # Export keeps static.<br />
    677     ignore_static=(not self.check_fn_manager.output_graph.export),<br />
    678 )<br />
    679 output_graph.shape_env.freeze()<br />
    680 for shape_guard in guards:</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:2630, in ShapeEnv.produce_guards(self, placeholders, sources, source_ref, constraint_inputs, equalities_inputs, _simplified, ignore_static)<br />
   2627     return symint.node.expr<br />
   2629 for src1, src2 in equalities_inputs.source_pairs:<br />
-&gt; 2630     s1, s2 = get_symbol(src1), get_symbol(src2)<br />
   2631     concrete_val = self.evaluate_expr(sympy.Eq(s1, s2))<br />
   2632     if not concrete_val:</p>
<p>File ~/.local/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:2626, in ShapeEnv.produce_guards.<locals>.get_symbol(tensor_dim_src)<br />
   2624 fake = placeholders[source_index[tensor_dim_src.base.name()]]<br />
   2625 symint = fake.shape[tensor_dim_src.idx]<br />
-&gt; 2626 assert isinstance(symint, torch.SymInt)<br />
   2627 return symint.node.expr</p>
<p>AssertionError: </p>
<p>```</p>
<h2>Reproduce</h2>
<p>Consider the following model:<br />
```Python<br />
import os<br />
import torch</p>
<p>class SAMInterface(torch.nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()<br />
        self.conv1 = torch.nn.Conv2d(3, 32, 3)</p>
<pre><code>def forward(self, img, points, labels):
    x = self.conv1(img)
    return x
</code></pre>
<p>with torch.no_grad():<br />
    device = "cuda" if torch.cuda.is_available() else "cpu"<br />
    model = SAMInterface().to(device=device)<br />
    img=torch.randn(1, 3, 1024, 1024, device=device)<br />
    points = torch.Tensor([[[500.0, 630.5]]])<br />
    labels = torch.Tensor([[[1]]])<br />
    n_labels = torch.export.Dim("n_labels", min=1, max=12)<br />
    example_inputs = (img, points, labels)<br />
    so_path = torch._export.aot_compile(<br />
        model,<br />
        example_inputs,<br />
        dynamic_shapes={<br />
                        "img": {},<br />
                        "points": {1: n_labels},<br />
                        "labels": {1: n_labels}},<br />
        # Specify the generated shared library path<br />
        options={"aot_inductor.output_path": os.path.join(os.getcwd(), "model.so")},<br />
    )<br />
<code>``
The forward function of</code>SamInterface<code>takes three arguments</code>img<code>,</code>points<code>, and</code>labels<code>. For inference, the batch size is always one, but users can modify the number of labels and points from one to twelve. AOT Compilation fails with the error posted above. I tried several variations of the</code>dynamic_shapes` input, but none succeeded. How can I aot_compile such a model? </p>
<h3>Versions</h3>
<p>Collecting environment information...<br />
PyTorch version: 2.2.1+cu121<br />
Is debug build: False<br />
CUDA used to build PyTorch: 12.1<br />
ROCM used to build PyTorch: N/A</p>
<p>OS: Ubuntu 22.04.4 LTS (x86_64)<br />
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br />
Clang version: Could not collect<br />
CMake version: version 3.28.3<br />
Libc version: glibc-2.35</p>
<p>Python version: 3.10.13 (main, Sep  5 2023, 06:03:44) [GCC 11.4.0] (64-bit runtime)<br />
Python platform: Linux-6.2.0-35-generic-x86_64-with-glibc2.35<br />
Is CUDA available: True<br />
CUDA runtime version: Could not collect<br />
CUDA_MODULE_LOADING set to: LAZY<br />
GPU models and configuration: GPU 0: Quadro T1000<br />
Nvidia driver version: 535.161.07<br />
cuDNN version: Could not collect<br />
HIP runtime version: N/A<br />
MIOpen runtime version: N/A<br />
Is XNNPACK available: True</p>
<p>CPU:<br />
Architecture:                       x86_64<br />
CPU op-mode(s):                     32-bit, 64-bit<br />
Address sizes:                      39 bits physical, 48 bits virtual<br />
Byte Order:                         Little Endian<br />
CPU(s):                             12<br />
On-line CPU(s) list:                0-11<br />
Vendor ID:                          GenuineIntel<br />
Model name:                         Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz<br />
CPU family:                         6<br />
Model:                              158<br />
Thread(s) per core:                 2<br />
Core(s) per socket:                 6<br />
Socket(s):                          1<br />
Stepping:                           10<br />
CPU max MHz:                        4500,0000<br />
CPU min MHz:                        800,0000<br />
BogoMIPS:                           5199.98<br />
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust sgx bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp sgx_lc md_clear flush_l1d arch_capabilities<br />
Virtualization:                     VT-x<br />
L1d cache:                          192 KiB (6 instances)<br />
L1i cache:                          192 KiB (6 instances)<br />
L2 cache:                           1,5 MiB (6 instances)<br />
L3 cache:                           12 MiB (1 instance)<br />
NUMA node(s):                       1<br />
NUMA node0 CPU(s):                  0-11<br />
Vulnerability Gather data sampling: Mitigation; Microcode<br />
Vulnerability Itlb multihit:        KVM: Mitigation: VMX disabled<br />
Vulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable<br />
Vulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Meltdown:             Mitigation; PTI<br />
Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable<br />
Vulnerability Retbleed:             Mitigation; IBRS<br />
Vulnerability Spec rstack overflow: Not affected<br />
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl<br />
Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization<br />
Vulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected<br />
Vulnerability Srbds:                Mitigation; Microcode<br />
Vulnerability Tsx async abort:      Not affected</p>
<p>Versions of relevant libraries:<br />
[pip3] fast-pytorch-kmeans==0.2.0.1<br />
[pip3] flake8==6.0.0<br />
[pip3] mypy==1.4.1<br />
[pip3] mypy-extensions==1.0.0<br />
[pip3] numpy==1.26.4<br />
[pip3] pytorch-triton==2.1.0+3c400e7818<br />
[pip3] torch==2.2.1<br />
[pip3] torch-tb-profiler==0.4.1<br />
[pip3] torchaudio==2.1.0.dev20230714+cu121<br />
[pip3] torchdata==0.7.0<br />
[pip3] torchprofile==0.0.4<br />
[pip3] torchtext==0.16.0<br />
[pip3] torchvision==0.17.1<br />
[pip3] triton==2.2.0<br />
[conda] Could not collect</p>
<p>cc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4</p>]]></description>
      <pubDate>Wed, 20 Mar 2024 00:45:44 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122294</guid>
    </item>
    <item>
      <title>[inductor][cpu] pyhpc_equation_of_state fp32 static shape default/cpp wrapper multiple thread performance  test crashed</title>
      <link>https://github.com/pytorch/pytorch/issues/122283</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>new_failures in 2024-03-11<br />
| suite | name | accuracy | perf | reason(reference only)<br />
-- | -- | -- | -- | --<br />
torchbench | pyhpc_equation_of_state | ‚àö | NaN | pyhpc_equation_of_state, torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised: AssertionError: ((41616, 26), (1082016,))</p>
<h3>Versions</h3>
<p>SW info<br />
| name | target_branch | target_commit | refer_branch | refer_commit<br />
-- | -- | -- | -- | --<br />
torchbench | main | d6015d42 | main | 1ef0a39e<br />
torch | main | 9f235971f02e0d53038f5bcef9b7018be2ac8c6d | main | f11f2b0d55b1aa322f73f4bb521beaf9d4563603<br />
torchvision | main | 0.18.0a0+2c127da | main | 0.18.0a0+2c127da<br />
torchtext | main | 0.16.0a0+b0ebddc | main | 0.16.0a0+b0ebddc<br />
torchaudio | main | 2.2.0a0+87aeb55 | main | 2.2.0a0+87aeb55<br />
torchdata | main | 0.7.1a0+0790338 | main | 0.7.1a0+0790338<br />
dynamo_benchmarks | main | nightly | main | nightly</p>
<p>Error:<br />
```<br />
loading model: 0it [00:00, ?it/s]cpu  eval  pyhpc_equation_of_state            </p>
<p>ERROR:common:Backend dynamo failed in warmup()<br />
Traceback (most recent call last):<br />
  File "/workspace/pytorch/benchmarks/dynamo/common.py", line 2634, in warmup<br />
    fn(model, example_inputs)<br />
  File "/workspace/pytorch/torch/_dynamo/eval_frame.py", line 437, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/workspace/pytorch/torch/_dynamo/convert_frame.py", line 921, in catch_errors<br />
    return callback(frame, cache_entry, hooks, frame_state, skip=1)<br />
  File "/workspace/pytorch/torch/_dynamo/convert_frame.py", line 786, in _convert_frame<br />
    result = inner_convert(<br />
  File "/workspace/pytorch/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert<br />
    return _compile(<br />
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner<br />
    return func(</em>args, <strong>kwds)<br />
  File "/workspace/pytorch/torch/_dynamo/convert_frame.py", line 676, in _compile<br />
    guarded_code = compile_inner(code, one_graph, hooks, transform)<br />
  File "/workspace/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/workspace/pytorch/torch/_dynamo/convert_frame.py", line 535, in compile_inner<br />
    out_code = transform_code_object(code, transform)<br />
  File "/workspace/pytorch/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object<br />
    transformations(instructions, code_options)<br />
  File "/workspace/pytorch/torch/_dynamo/convert_frame.py", line 165, in _fn<br />
    return fn(<em>args, </em><em>kwargs)<br />
  File "/workspace/pytorch/torch/_dynamo/convert_frame.py", line 500, in transform<br />
    tracer.run()<br />
  File "/workspace/pytorch/torch/_dynamo/symbolic_convert.py", line 2149, in run<br />
    super().run()<br />
  File "/workspace/pytorch/torch/_dynamo/symbolic_convert.py", line 810, in run<br />
    and self.step()<br />
  File "/workspace/pytorch/torch/_dynamo/symbolic_convert.py", line 773, in step<br />
    getattr(self, inst.opname)(inst)<br />
  File "/workspace/pytorch/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE<br />
    self.output.compile_subgraph(<br />
  File "/workspace/pytorch/torch/_dynamo/output_graph.py", line 964, in compile_subgraph<br />
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)<br />
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner<br />
    return func(</em>args, <strong>kwds)<br />
  File "/workspace/pytorch/torch/_dynamo/output_graph.py", line 1161, in compile_and_call_fx_graph<br />
    compiled_fn = self.call_user_compiler(gm)<br />
  File "/workspace/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/workspace/pytorch/torch/_dynamo/output_graph.py", line 1234, in call_user_compiler<br />
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(<br />
  File "/workspace/pytorch/torch/_dynamo/output_graph.py", line 1215, in call_user_compiler<br />
    compiled_fn = compiler_fn(gm, self.example_inputs())<br />
  File "/workspace/pytorch/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper<br />
    compiled_gm = compiler_fn(gm, example_inputs)<br />
  File "/workspace/pytorch/torch/_dynamo/backends/inductor.py", line 16, in inductor<br />
    return compile_fx(<em>args, </em><em>kwargs)<br />
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner<br />
    return func(</em>args, <strong>kwds)<br />
  File "/workspace/pytorch/torch/_inductor/compile_fx.py", line 1373, in compile_fx<br />
    return aot_autograd(<br />
  File "/workspace/pytorch/torch/_dynamo/backends/common.py", line 58, in compiler_fn<br />
    cg = aot_module_simplified(gm, example_inputs, </strong>kwargs)<br />
  File "/workspace/pytorch/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified<br />
    compiled_fn = create_aot_dispatcher_function(<br />
  File "/workspace/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(<em>args, </em><em>kwargs)<br />
  File "/workspace/pytorch/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function<br />
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/workspace/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe<br />
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/workspace/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base<br />
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)<br />
  File "/workspace/pytorch/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base<br />
    compiled_fw = compiler(fw_module, updated_flat_args)<br />
  File "/workspace/pytorch/torch/_inductor/compile_fx.py", line 1108, in fw_compiler_freezing<br />
    optimized_function = inner_compile(<br />
  File "/workspace/pytorch/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper<br />
    inner_compiled_fn = compiler_fn(gm, example_inputs)<br />
  File "/workspace/pytorch/torch/_inductor/debug.py", line 304, in inner<br />
    return fn(</em>args, <strong>kwargs)<br />
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner<br />
    return func(*args, </strong>kwds)<br />
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner<br />
    return func(<em>args, </em><em>kwds)<br />
  File "/workspace/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(</em>args, <strong>kwargs)<br />
  File "/workspace/pytorch/torch/_inductor/compile_fx.py", line 463, in compile_fx_inner<br />
    compiled_graph = fx_codegen_and_compile(<br />
  File "/workspace/pytorch/torch/_inductor/compile_fx.py", line 738, in fx_codegen_and_compile<br />
    compiled_fn = graph.compile_to_fn()<br />
  File "/workspace/pytorch/torch/_inductor/graph.py", line 1296, in compile_to_fn<br />
    return self.compile_to_module().call<br />
  File "/workspace/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(*args, </strong>kwargs)<br />
  File "/workspace/pytorch/torch/_inductor/graph.py", line 1239, in compile_to_module<br />
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()<br />
  File "/workspace/pytorch/torch/_inductor/graph.py", line 1196, in codegen<br />
    self.scheduler = Scheduler(self.buffers)<br />
  File "/workspace/pytorch/torch/_dynamo/utils.py", line 262, in time_wrapper<br />
    r = func(<em>args, </em>*kwargs)<br />
  File "/workspace/pytorch/torch/_inductor/scheduler.py", line 1309, in <strong>init</strong><br />
    self.fuse_nodes()<br />
  File "/workspace/pytorch/torch/_inductor/scheduler.py", line 1683, in fuse_nodes<br />
    self.fuse_nodes_once()<br />
  File "/workspace/pytorch/torch/_inductor/scheduler.py", line 1822, in fuse_nodes_once<br />
    node3 = self.get_backend(device).fuse(node1, node2)<br />
  File "/workspace/pytorch/torch/_inductor/codegen/cpp.py", line 3531, in fuse<br />
    assert vars1 == vars2, (vars1, vars2)<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
AssertionError: ((41616, 26), (1082016,))</p>
<p>Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information</p>
<p>You can suppress this exception and fall back to eager by setting:<br />
    import torch._dynamo<br />
    torch._dynamo.config.suppress_errors = True</p>
<p>0<br />
```</p>
<p>Repro:<br />
<a href="https://github.com/chuanqi129/inductor-tools/blob/yudong/aws_auto/scripts/modelbench/inductor_single_run.sh">inductor_single_test.sh</a><br />
<code>shell
bash inductor_single_test.sh single inference performance torchbench pyhpc_equation_of_state float32 first static default 0</code></p>
<p>Suspected guilty commit: https://github.com/pytorch/pytorch/commit/49d1fd31cf5424899fdbb73654d95bf3478c0bae</p>
<p>log: <br />
<a href="https://github.com/pytorch/pytorch/files/14663904/torchbench-pyhpc_equation_of_state-inference-float32-static-default-multiple-performance-crash_guilty_commit.log.txt">torchbench-pyhpc_equation_of_state-inference-float32-static-default-multiple-performance-crash_guilty_commit.log.txt</a></p>
<p>cc @zxd1997066  @WeizhuoZhang-intel @chuanqi129 @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 22:11:51 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/122283</guid>
    </item>
    <item>
      <title>Use graph.find_nodes in inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/122256</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122331<br />
* #122258<br />
* #122257<br />
* <strong>-&gt;</strong> #122256<br />
* #122255<br />
* #121565<br />
* #122330</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 17:27:14 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122256</guid>
    </item>
    <item>
      <title>Use graph.find_nodes in inductor/fx_passes</title>
      <link>https://github.com/pytorch/pytorch/pull/122255</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122331<br />
* #122258<br />
* #122257<br />
* #122256<br />
* <strong>-&gt;</strong> #122255<br />
* #121565<br />
* #122330</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 19 Mar 2024 17:27:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122255</guid>
    </item>
    <item>
      <title>Inductor: Don't clamp views when the views come from split_with_sizes</title>
      <link>https://github.com/pytorch/pytorch/pull/122149</link>
      <description><![CDATA[<p>Summary:<br />
Fixes #122126</p>
<p><code>split_with_sizes</code> don't need clamping.</p>
<p>Test Plan: Added test + CI</p>
<p>Differential Revision: D55043320</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Mon, 18 Mar 2024 16:04:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122149</guid>
    </item>
    <item>
      <title>[inductor] Add torch.while_loop support to JIT Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/122069</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122069</p>
<p>Summary: <code>torch.while_loop</code> HOP support is added to JIT Inductor. The test coverage is limited due to the functionality constraints of the upstream <code>torch.while_loop</code> op in Dynamo / Export. When those are lifted, we'll add more tests (see TODO-s in the test file).</p>
<p>AOT Inductor support will be added in a follow-up PR.</p>
<p>Test Plan:</p>
<p>```<br />
$ python test/inductor/test_control_flow.py<br />
...</p>
<hr />
<p>Ran 38 tests in 159.387s</p>
<p>OK<br />
```</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 17 Mar 2024 18:29:50 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122069</guid>
    </item>
    <item>
      <title>[aot_inductor] added runtime checks for input/output tensors in debug compile mode</title>
      <link>https://github.com/pytorch/pytorch/pull/122047</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #122047</p>
<p>This PR added runtime checks to guard the dtypes and shapes of input/output tensors.<br />
Currently, we enable these only for debug compilation<br />
(i.e. aot_inductor.debug_compile is True) in abi_compatible mode.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>
<p>Differential Revision: <a href="https://our.internmc.facebook.com/intern/diff/D54993148">D54993148</a></p>]]></description>
      <pubDate>Sat, 16 Mar 2024 23:22:07 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122047</guid>
    </item>
    <item>
      <title>[inductor] Use python constants in IndexPropagation</title>
      <link>https://github.com/pytorch/pytorch/pull/122031</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #115435<br />
* #121924<br />
* <strong>-&gt;</strong> #122031</p>
<p>In the next PR I have the IR <code>ops.neg(ops.constant(0.0, torch.float32))</code><br />
which should be folded to <code>ops.constant(-0.0, torch.float32)</code> but it seems that<br />
<code>sympy.Float(-0.0)</code> doesn't respect the sign of the zero and so we instead<br />
get a positive zero constant.</p>
<p>Here, I work around this by doing the constant folding with python arithmetic<br />
which does respect signed zeros.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sat, 16 Mar 2024 13:00:20 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/122031</guid>
    </item>
    <item>
      <title>[inductor] Lower divide by constant as multiplication by reciprocal</title>
      <link>https://github.com/pytorch/pytorch/pull/121924</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #115435<br />
* <strong>-&gt;</strong> #121924<br />
* #122031</p>
<p>Fixes #101039</p>
<p>This lowers division by a constant value to be multipication by reciprocal.<br />
The same optimization is applied in eager mode on CUDA:</p>
<p>https://github.com/pytorch/pytorch/blob/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu#L36-L38</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Thu, 14 Mar 2024 11:41:00 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121924</guid>
    </item>
    <item>
      <title>GPT2 SDPA inference pattern-matching for Inductor</title>
      <link>https://github.com/pytorch/pytorch/pull/121866</link>
      <description><![CDATA[<h3>Summary</h3>
<p>With this PR, SDPA pattern of GPT2 is being mapped to <code>torch.nn.functional.scaled_dot_product_attention</code>.<br />
While GPT2 supports both a causal mask &amp; an attention mask, this PR considers the case of attention mask being absent.<br />
TorchBench inference workload for GPT2 also doesn't use an attention-mask.</p>
<p><a href="https://github.com/pytorch/pytorch/actions/runs/8319111885/job/22762567770">CUDA AOT Inductor</a> CI job's <code>GPT2ForSequenceClassification</code> accuracy test failed, so its failure would be considered expected.<br />
Created #122429 to track that particular issue.</p>
<h3>CPU performance data with TorchBench</h3>
<p>|MODEL |BATCH SIZE | DTYPE | BEFORE: Speedup over eager-mode with the default Inductor implementation | AFTER: Speedup over eager mode with SDPA op mapped| Perf boost = (AFTER - BEFORE)/BEFORE * 100|<br />
|--------------------------|-------------|---------|-----------------------------|--------------------------|------------|<br />
|hf_GPT2| 1 | FP32 | 1.522x | 1.791x| 17.67%|<br />
|hf_GPT2| 1 | BF16 (AMP) | 1.795x | 2.387x| 32.98%|<br />
|hf_GPT2| 2 | FP32 |  1.313x |1.629x | 19.3%|<br />
|hf_GPT2|2| BF16 (AMP) | 1.556x | 1.924x | 23.65%|<br />
|hf_GPT2_large| 1 | FP32 | 1.380x |1.585x | 12.93%|<br />
|hf_GPT2_large| 1 | BF16 (AMP) | 1.208x | 1.567x | 22.91%|<br />
|hf_GPT2_large| 2 | FP32 | 1.188x | 1.490x | 25.42%|<br />
|hf_GPT2_large|2| BF16 (AMP) | 0.991x | 1.575x | 58.93%|</p>
<p>Machine - Intel(R) Xeon(R) Platinum 8468H (Xeon 4th gen Sapphire Rapids)<br />
48 physical cores were used. Intel OpenMP &amp; libtcmalloc were preloaded.</p>
<p>Example command -<br />
<code>OMP_NUM_THREADS=48 MKL_NUM_THREADS=48 numactl --membind=0 --cpunodebind=0 -C 0-47 python benchmarks/dynamo/torchbench.py --performance --inference --inductor --float32 -dcpu --only hf_GPT2_large --freezing --batch-size 1</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 15:27:30 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121866</guid>
    </item>
    <item>
      <title>[Inductor] Fix for WrapperCodeGen.statically_known_int_or_none</title>
      <link>https://github.com/pytorch/pytorch/pull/121808</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121808</p>
<p>There's obviously a small typo in WrapperCodeGen.statically_known_int_or_none,<br />
where the return value of a call to V.graph._shape_env._maybe_evaluate_static<br />
is being discarded.</p>
<p>This fix changes that to work how it was likely intended to.</p>
<p>Test Plan:<br />
CI</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 13 Mar 2024 05:33:11 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121808</guid>
    </item>
    <item>
      <title>[Inductor cutlass backend] Add bmm support</title>
      <link>https://github.com/pytorch/pytorch/pull/121734</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* <strong>-&gt;</strong> #121734<br />
* #121497<br />
* #121491<br />
* #121490</p>
<p>Add support for bmm ( batch matrix multiply ) op through the Cutlass backend.</p>
<p>Test Plan:<br />
 * CI<br />
 * Added test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Tue, 12 Mar 2024 09:23:08 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121734</guid>
    </item>
    <item>
      <title>[Inductor] [ReImplement] Outer Loop Fusion for CPP Backend</title>
      <link>https://github.com/pytorch/pytorch/pull/121625</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121625</p>
<p><strong>Summary</strong><br />
Re-implement of https://github.com/pytorch/pytorch/pull/121064</p>
<p><strong>Test Plan</strong><br />
<code>python -u -m pytest -s -v test_cpu_repro.py -k test_outer_loop_fusion</code></p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Sun, 10 Mar 2024 23:44:28 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121625</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] Tolerate dynamic shapes</title>
      <link>https://github.com/pytorch/pytorch/pull/121497</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121734<br />
* <strong>-&gt;</strong> #121497<br />
* #121491<br />
* #121490</p>
<p>Previously, when the Cutlass backend was enabled, using dynamic shapes could lead to exceptions during JIT.</p>
<p>With this change, there are guards in place to just disable the Cutlass backend if dynamic dimensions are involved.</p>
<p>In addition, if no choices for a GEMM are available using the selected backends, then an ATen Kernel is used as fallback, even if the ATen backend is not enabled.</p>
<p>Test:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 07:55:23 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121497</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] DO NOT REVIEW - to be split up</title>
      <link>https://github.com/pytorch/pytorch/pull/121492</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #121492<br />
* #121734<br />
* #121497<br />
* #121491<br />
* #121490</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:43 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121492</guid>
    </item>
    <item>
      <title>[Inductor Cutlass backend] GEMM size threshold for Cutlass backend usage</title>
      <link>https://github.com/pytorch/pytorch/pull/121491</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121734<br />
* #121497<br />
* <strong>-&gt;</strong> #121491<br />
* #121490</p>
<ul>
<li>
<p>Adds a configurable GEMM size threshold for the usage of Cutlass GEMM Kernels <strong>_inductor.config.cutlass_backend_min_gemm_size</strong></p>
</li>
<li>
<p>During GEMM algorithm choice generation: <strong>if no viable choices can be generated using the configured backends, the ATen backend will be used as a fallback backend</strong>, even if it is not enabled in <strong>_inductor.config.max_autotune_gemm_backends</strong></p>
</li>
</ul>
<p>Test plan:<br />
CI<br />
Additional unit test in test_cutlass_backend.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:36 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121491</guid>
    </item>
    <item>
      <title>[Inductor] Make codecache CUDA compilation more robust &amp; flexible</title>
      <link>https://github.com/pytorch/pytorch/pull/121490</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #121492<br />
* #121734<br />
* #121497<br />
* #121491<br />
* <strong>-&gt;</strong> #121490</p>
<p>Minor changes which make the CUDA compilation within _inductor/codecache.py<br />
more robust and flexible.</p>
<p>Test plan:<br />
CI<br />
Additional test in test_codecache.py</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Fri, 08 Mar 2024 06:38:27 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121490</guid>
    </item>
    <item>
      <title>Add registration API for torch.compile-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/121387</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we intend to provide a registration API dedicated to eager-through-torch.compile. The major workflow of this API will be as follows.</p>
<ul>
<li>Load cache</li>
<li>Check cache according to the input tensors</li>
<li>Cache Hit: Run the cached kernel directly</li>
<li>Cache Miss: Run the registered python kernel</li>
</ul>
<p>Currently, this PR always fallback to python kernel now and cache mechanism will be implemented in another PR.</p>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120595<br />
* #116449<br />
* #116368<br />
* #121727<br />
* <strong>-&gt;</strong> #121387<br />
* #121296</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @chauhang</p>]]></description>
      <pubDate>Wed, 06 Mar 2024 22:35:58 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/121387</guid>
    </item>
    <item>
      <title>Tensors with multiple unbacked SymInt dims don't work in Inductor</title>
      <link>https://github.com/pytorch/pytorch/issues/120548</link>
      <description><![CDATA[<h3>üêõ Describe the bug</h3>
<p>Internal xref: https://fb.workplace.com/groups/6829516587176185/posts/6892427944218382/</p>
<p>The use case here is modeling 2D/3D variable size data (e.g., images/videos of varying input dimensionality)</p>
<p>This doesn't work:</p>
<p>```<br />
import torch<br />
import torch._dynamo.config</p>
<p>torch._dynamo.config.capture_scalar_outputs = True</p>
<p>@torch.compile(fullgraph=True)<br />
def f(x):<br />
    s = x.tolist()<br />
    return torch.empty(*s)</p>
<p>f(torch.tensor([3, 4, 5]))<br />
```</p>
<p>Fails with</p>
<p>```<br />
  File "/data/users/ezyang/a/pytorch/torch/<em>inductor/graph.py", line 976, in run_node<br />
    dense = torch._prims_common.is_non_overlapping_and_dense(n.meta["val"])<br />
  File "/data/users/ezyang/a/pytorch/torch/_prims_common/<strong>init</strong>.py", line 356, in is_non_overlapping_and_dense<br />
    if is_contiguous(a) or is_channels_last_contiguous(a):<br />
  File "/data/users/ezyang/a/pytorch/torch/_prims_common/<strong>init</strong>.py", line 244, in is_contiguous<br />
    if y != expected_stride:<br />
  File "/data/users/ezyang/a/pytorch/torch/<strong>init</strong>.py", line 374, in <strong>bool</strong><br />
    return self.node.bool</em>()<br />
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/sym_node.py", line 432, in bool_<br />
    return self.guard_bool("", 0)<br />
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/sym_node.py", line 374, in guard_bool<br />
    r = self.shape_env.evaluate_expr(self.expr, self.hint, fx_node=self.fx_node)<br />
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/recording.py", line 255, in wrapper<br />
    return event.run(self)<br />
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/recording.py", line 156, in run<br />
    return self.f(<em>args, </em>*kwargs)<br />
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/symbolic_shapes.py", line 3914, in evaluate_expr<br />
    raise self._make_data_dependent_error(expr.xreplace(self.var_to_val), expr)<br />
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:<br />
GuardOnDataDependentSymNode: Could not guard on data-dependent expression Ne(Max(1, u2), u2) (unhinted: Ne(Max(1, u2), u2)).  (Size-like symbols: u2)</p>
<p>Potential framework code culprit (scroll up for full backtrace):<br />
  File "/data/users/ezyang/a/pytorch/torch/_prims_common/<strong>init</strong>.py", line 244, in is_contiguous<br />
    if y != expected_stride:</p>
<p>For more information, run with TORCH_LOGS="dynamic"<br />
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u2"<br />
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1<br />
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing<br />
```</p>
<p>Notably, graph capture is all fine; we are only choking in Inductor. cc @gchanan @zou3519 @kadeng @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire</p>
<p>It is tempting to add guard_size_oblivious to torch/_prims_common/<strong>init</strong>.py but this is a bit questionable: the tensors we create here should be obviously contiguous (since we built them with torch.empty(*s)), and adding that here will cause us to report that it is NOT contiguous, which is not what we want. Here's a better fix:</p>
<p><code>diff --git a/torch/_inductor/graph.py b/torch/_inductor/graph.py
index 51af2a51c49..fb8810a94e3 100644
--- a/torch/_inductor/graph.py
+++ b/torch/_inductor/graph.py
@@ -973,7 +973,7 @@ class GraphLowering(torch.fx.Interpreter):
                 n.meta["val"], torch.Tensor
             ):
                 strides = n.meta["val"].stride()
-                dense = torch._prims_common.is_non_overlapping_and_dense(n.meta["val"])
+                dense = torch.ops.aten.is_non_overlapping_and_dense.default(n.meta["val"])
                 # requiring a stride order for a non-dense output wouldn't
                 # recreate the same strides, and would fail with view, defer for now.
                 if dense and len(strides):</code></p>
<p>We then choke at</p>
<p><code>File "/data/users/ezyang/a/pytorch/torch/_inductor/graph.py", line 980, in run_node                                                        
    stride_order = ir.get_stride_order(strides)                                                                                              
  File "/data/users/ezyang/a/pytorch/torch/_inductor/ir.py", line 198, in get_stride_order                                                   
    sorted_idx: List[int] = argsort(seq)                                                                                                     
  File "/data/users/ezyang/a/pytorch/torch/_inductor/utils.py", line 762, in argsort                                                         
    return list(reversed(sorted(a_r, key=getter, reverse=True)))  # noqa: C413                                                               
  File "/data/users/ezyang/a/pytorch/torch/__init__.py", line 374, in __bool__           
    return self.node.bool_()  
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/sym_node.py", line 432, in bool_  
    return self.guard_bool("", 0)
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/sym_node.py", line 374, in guard_bool
    r = self.shape_env.evaluate_expr(self.expr, self.hint, fx_node=self.fx_node)                                                             
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/recording.py", line 255, in wrapper            
    return event.run(self)                                            
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/recording.py", line 156, in run
    return self.f(*args, **kwargs)
  File "/data/users/ezyang/a/pytorch/torch/fx/experimental/symbolic_shapes.py", line 3914, in evaluate_expr        
    raise self._make_data_dependent_error(expr.xreplace(self.var_to_val), expr)            
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:                                                                          
GuardOnDataDependentSymNode: Could not guard on data-dependent expression Max(1, u1)*Max(1, u2) &lt; Max(1, u2) (unhinted: Max(1, u1)*Max(1, u2)
 &lt; Max(1, u2)).  (Size-like symbols: u1, u2)</code></p>
<p>So it seems we need to thread the needle and avoid attempting to directly compute ordering on strides if we actually know it's contiguous out of band.</p>
<p>Also cc @shunting314 for NHWC striding optimization.</p>
<p>cc @msaroufim @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @amjames </p>
<h3>Versions</h3>
<p>main</p>]]></description>
      <pubDate>Fri, 23 Feb 2024 21:22:05 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/issues/120548</guid>
    </item>
    <item>
      <title>[inductor] Remove identity from ops.scan</title>
      <link>https://github.com/pytorch/pytorch/pull/119727</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #122137<br />
* #119430<br />
* <strong>-&gt;</strong> #119727<br />
* #122136</p>
<p>Currently scan has an <code>init</code> argument which must be the identity of the<br />
combine function. This isn't strictly necessary if we are more careful about<br />
keeping track of the first element and avoid combining it with anything.</p>
<p>This does additionally require that there are no active load masks, since we can't<br />
do the <code>where_cond</code> any more. However, this shouldn't be possible anyway since<br />
scans are always realized and only fused via the scheduler.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Mon, 12 Feb 2024 15:27:53 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/119727</guid>
    </item>
    <item>
      <title>Add a cache mechanism to accelerate torch.compile-for-eager</title>
      <link>https://github.com/pytorch/pytorch/pull/116368</link>
      <description><![CDATA[<p>This PR is a follow-up of RFC https://github.com/pytorch/pytorch/issues/115545.</p>
<p>In this PR, we are trying to enable a cache mechanism to accelerate <strong>eager-through-torch.compile</strong>. When <strong>eager-through-torch.compile</strong> is enabled, we will store a persistent config to cache the kernel information for the aten operation. </p>
<p>The persistent config consists of two parts - meta_info and kernel_path.</p>
<ul>
<li>meta_info: The input tensors' shape, stride, device type, data type, and symbolic flag.</li>
<li>kernel_path: The path of the kernel produced by Inductor.</li>
</ul>
<p>When an aten operation is registered, the <code>kernel_holder</code> will load the persistent config and parse it to build the cache map;  the meta_info is key, and the kernel library is the value.</p>
<p>Currently, this PR only supports static shape to guard the kernel. From the C++ class perspective, we defined to base class <code>TensorChecker</code> and <code>StaticTensorChecker</code> inherits the base class. In the future, we will implement a class to support dynamic shape by inheriting this base class as well.</p>
<p>Take a <code>mul</code> as an example.<br />
```python<br />
class MulKernel:<br />
  def <strong>init</strong>(self) -&gt; None:<br />
    pass</p>
<p>def <strong>call</strong>(self, <em>args: Any, </em><em>kwargs: Any) -&gt; Any:<br />
    with torch._C._SetExcludeDispatchKeyGuard(torch._C.DispatchKey.Python, False):<br />
      opt_fn = torch.compile(torch.ops.aten.mul, dynamic=False, options={<br />
          "aot_inductor.eager_mode": True,<br />
          "aot_inductor.eager_op_name": "mul_Tensor"<br />
        }<br />
      )<br />
      return opt_fn(</em>args, **kwargs)</p>
<p>torch_compile_op_lib_impl = torch.library.Library("aten", "IMPL")</p>
<p>_, overload_names = torch._C._jit_get_operation("aten::mul")<br />
schema = torch._C._get_schema("aten::mul", overload_name)<br />
reg_name = schema.name<br />
if schema.overload_name:<br />
  reg_name = f"{reg_name}.{schema.overload_name}"<br />
  torch_compile_op_lib_impl.impl(<br />
    reg_name,<br />
    MulKernel(),<br />
    "CUDA",<br />
    compile_mode=True)</p>
<p>a = torch.randn(1024, 1024, device=device)<br />
b = torch.randn(1024, 1024, device=device)<br />
warm_up_iter = 1000<br />
iter = 10000<br />
fn = torch.mul</p>
<h1>Warm up</h1>
<p>for _ in range(warm_up_iter):<br />
    fn(a, b)</p>
<h1>Collect performance</h1>
<p>beg = time.time()<br />
for _ in range(iter):<br />
    fn(a, b)<br />
end = time.time()<br />
print(f"E2E run: {end - beg}")<br />
```<br />
It will produce the config as follows.</p>
<p><code>json
[
    {
        "meta_info": [
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            },
            {
                "is_symbolic": "false",
                "device_type": "cuda",
                "dtype": "torch.float32",
                "sizes": "[1024, 1024]",
                "strides": "[1024, 1]"
            }
        ],
        "kernel_path": "/tmp/torchinductor_eikan/e4/ce4jw46i5l2e7v3tvr2pyglpjmahnp7x3hxaqotrvxwoeh5t6qzc.so"
    }
]</code></p>
<p>Performance-wise, we collected mul.Tensor through torch.compile w/ 10000 runs(e2e). The data is as follows. And we will collect data when we support dynamic shape.</p>
<ul>
<li>Eager: ~266.11ms</li>
<li>W/O Cache: ~3455.54ms</li>
<li>W/ Cache and Cache Miss: ~3555.3ms</li>
<li>W/ Cache and Cache Hit: ~267.12ms</li>
</ul>
<p>Hardware:</p>
<ul>
<li>CPU: Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz</li>
<li>GPU: CUDA A10</li>
</ul>
<p>Software:</p>
<ul>
<li>PyTorch Version: 39df084001c54cca5fe3174176f9b0206ddb7dcf</li>
<li>GPU Driver Version: 525.147.05</li>
<li>CUDA Version: 12.0</li>
</ul>
<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* #120595<br />
* #116449<br />
* <strong>-&gt;</strong> #116368<br />
* #121727<br />
* #121387<br />
* #121296</p>
<p>cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Sun, 24 Dec 2023 03:21:18 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/116368</guid>
    </item>
    <item>
      <title>[inductor] Disable fp contraction and add option to use precise division</title>
      <link>https://github.com/pytorch/pytorch/pull/115435</link>
      <description><![CDATA[<p>Stack from <a href="https://github.com/ezyang/ghstack">ghstack</a> (oldest at bottom):<br />
* <strong>-&gt;</strong> #115435<br />
* #121924<br />
* #122031</p>
<p>Fixes #101039</p>
<p>In the issue we have something like:<br />
<code>python
(x / 1e-6) + (y / 1e-6)</code><br />
which triton turns into<br />
<code>python
fma(x, 1e6, y * 1e6)</code></p>
<p>Where we have one division calculated to float32 precision,<br />
and the other calculated to infinite precision as part of the <code>fma</code>.<br />
In the issue this leads to a bad normalization in softmax.</p>
<p>Here we disallow the "fp fusion" optimization which generates these fma instructions.</p>
<p>cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler</p>]]></description>
      <pubDate>Fri, 08 Dec 2023 10:25:22 GMT</pubDate>
      <guid isPermaLink="true">https://github.com/pytorch/pytorch/pull/115435</guid>
    </item>
  </channel>
</rss>
